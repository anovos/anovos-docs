# Setting up Anovos on AWS EMR

For large workloads, you can set up _Anovos_ on [AWS EMR](https://aws.amazon.com/emr/).
AWS EMR provides managed services to easily run Apache Spark and other big data workloads.

## Downloading & Installing Anovos

Clone the Anovos repository into you local environment using the following command:

```shell
git clone --depth 1 https://github.com/anovos/anovos.git
```

_**Note**: Using the `--branch` flag allows you to select a specific release of Anovos._
_For example, adding `--branch v1.0.1` will give you the state of the 1.0.1 release._
_If you omit the flag, you will get the latest development version of Anovos, which might not_
_be fully functional or exhibit unexpected behavior._

After cloning, go to the `anovos` directory and execute the following command to clean and
build the latest modules in the `dist` folder:

```shell
make clean build
```

## Create a workflow script

To launch the workflow on AWS EMR, we need a single Python script as the entry point.
Hence, we'll create a `main.py` script that invokes the _Anovos'_ workflow runner:

```python
import sys
from anovos import workflow

workflow.run(config_path=sys.argv[1], run_type="emr")
```

It takes an [_Anovos_ configuration file](../config_file.md) and submits it to the _Anovos_
workflow runner, together with the `run_type` specific to AWS EMR.

## Copy all required files into an S3 bucket

Copy the `main.py` script created in the previous step and the following files to [AWS S3](https://aws.amazon.com/s3/):

- `/anovos/dist/anovos.zip`
  This file contains all Anovos modules. You will need the compressed (.zip) file for importing the modules as –py-files.
- `/anovos/dist/data/income_dataset` (optional)
  This folder contains our demo dataset, the `income dataset`.
- `/anovos/dist/configs.yaml`
  This is the sample _Anovos_ configuration file that describes how the `income dataset`
  should be analyzed and processed.
  Ensure that all input and output paths in `configs.yaml`, such as `final_report_path`, `file_path`,
  `appended_metric_path` or `output_path`, are set to the desired S3 locations.
- `/anovos/bin/aws_bootstrap_files/setup_on_aws_emr_5_30_and_above.sh`
  This shell script installs all the packages that are required to run _Anovos_ on EMR.
  Copy the [requirements.txt](https://github.com/anovos/anovos/blob/main/requirements.txt)
  in the repo to another available S3 bucket.
  Edit the first line in `setup_on_aws_emr_5_30_and_above.sh` to fit your project.
  Now you can use it as a bootstrap file for AWS EMR.
- `/anovos/data/metric_dictionary.csv`
  This is a static dictionary file for the different metrics generated by _Anovos_ in the
  [Data Report](../data-reports/overview.md).
  It helps generate the _Wiki_ tab in the [final _Anovos_ report](../data-reports/final_report.md),
  where the metrics of the different modules and submodules are summarized.
- `/anovos/jars/histogrammar*.jar`
  These jars contain external dependencies of _Anovos_.
  Specify the correct version 2.12 or 2.11, based on your environment’s Scala version.

To learn how to install the AWS CLI, see the
[AWS documentation](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html).

You can use the following command to copy files from your local machine to an S3 bucket:

```shell
aws s3 cp --recursive  <local file path> <s3 path> --profile  <profile name>
```

## Create a cluster

Use the following software configuration:
- Emr-5.33.0
- Hadoop-2.10.1
- Spark-2.4.7
- Hive-2.3.7
- TensorFlow 2.4.1

- Spark Submit details:
    - Deploy mode : `client`
    - Spark-submit options :

```shell
        --num-executors 1000
        --executor-cores 4
        --executor-memory 20g
        --driver-memory 20G
        --driver-cores 4
        --conf spark.driver.maxResultSize=15g
        --conf spark.yarn.am.memoryOverhead=1000m
        --conf spark.executor.memoryOverhead=2000m
        --conf spark.kryo.referenceTracking=false
        --conf spark.network.timeout=18000s
        --conf spark.executor.heartbeatInterval=12000s
        --conf spark.dynamicAllocation.executorIdleTimeout=12000s
        --conf spark.rpc.message.maxSize=1024
        --conf spark.yarn.maxAppAttempts=1
        --conf spark.speculation=false
        --conf spark.kryoserializer.buffer.max=1024
        --conf spark.executor.extraJavaOptions=-XX:+UseG1GC
        --conf spark.driver.extraJavaOptions=-XX:+UseG1GC
        --packages org.apache.spark:spark-avro_2.11:2.4.0
        --jars /anovos/jars/histogrammar-sparksql_2.11-1.0.20.jar,/anovos/jars/histogrammar_2.11-1.0.20.jar
        --py-files {s3_bucket}/anovos.zip
```

When launching a cluster, the driver/worker node memory size should be more than the total configured memory.
For example, for the above config, there should be at least 20 + 2 = 22 GB per worker or driver.
So the user must select machine types with around 26 GB of RAM or more.

Example machines: - m3.2xlarge - 8 core, 30 GB.

The above histogram jars version and avro package version should follow the Scala version.

- Application location: S3 path of main.py file
- Arguments for main.py file - `s3://<s3-bucket>/configs.yaml emr`

- Final Spark Submit command example :

```shell
spark-submit --deploy-mode client
             --num-executors 1000
             --executor-cores 4
             --executor-memory 20g
             --driver-memory 20G
             --driver-cores 4
             --conf spark.driver.maxResultSize=15g
             --conf spark.yarn.am.memoryOverhead=1000m
             --conf spark.executor.memoryOverhead=2000m
             --conf spark.kryo.referenceTracking=false
             --conf spark.network.timeout=18000s
             --conf spark.executor.heartbeatInterval=12000s
             --conf spark.dynamicAllocation.executorIdleTimeout=12000s
             --conf spark.rpc.message.maxSize=1024
             --conf spark.yarn.maxAppAttempts=1
             --conf spark.speculation=false
             --conf spark.kryoserializer.buffer.max=1024
             --conf spark.executor.extraJavaOptions=-XX:+UseG1GC
             --conf spark.driver.extraJavaOptions=-XX:+UseG1GC
             --packages org.apache.spark:spark-avro_2.11:2.4.0
             --jars s3://<s3-bucket>/jars/histogrammar-sparksql_2.11-1.0.20.jar,s3://<s3-bucket>/jars/histogrammar_2.11-1.0.20.jar
             --py-files s3://<s3-bucket>/anovos.zip s3://<s3-bucket>/main.py s3://<s3-bucket>/configs.yaml emr
```

- Bootstrap Actions:
  - Script location: specify the bootstrap_shell_script_s3_path/setup_on_aws_emr_5_30_and_above.sh
