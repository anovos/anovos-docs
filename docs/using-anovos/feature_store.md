# Feature Store Integration

Feature stores are an essential building block of a modern MLOps setup.
For an introduction into the concept and an overview of available options and vendors, see the
[Feature Store Comparison & Evaluation](https://mlops.community/learn/feature-store/)
on the [MLOps Community website](https://mlops.community/).

_Anovos_ provides integration with [Feast](https://www.feast.dev), a widely used open source feature store,
out of the box.
Using the same abstractions, it is straightforward to integrate _Anovos_ with other feature stores.

If there is a particular feature store integration you'd like to see supported by _Anovos_,
[let us know!](../community/communication.md)

## Using _Anovos_ with Feast

The following guide describes how to use _Anovos_ to push data to Feast.
We assume that you are familiar with the fundamentals of both _Anovos_ workflows and Feast.

For an introduction to Feast, see [ðŸ“– the Fest Quickstart guide](https://docs.feast.dev/getting-started/quickstart).

### Prerequisites

In order to use _Anovos_ with Feast, you need to install it:

```bash
pip install feast
```

Next, we'll instantiate a new Feast repository:

```bash
feast init anovos_repo
```

ðŸ¤“  _Note: You can also use an existing repository._

### Adding the Feast export to your _Anovos_ workflow

To export data to Feat at the end of a workflow run, you need to add the `write_feast_features` block
to the configuration file. (To learn more about the configuration file in general and available options,
see [ðŸ“– the configuration file documentation](config_file.md).)

You can use the following template as a starting point:

```yaml
write_feast_features:
  file_path: "../anovos_repo/"                     # the location of your Feast repository
  entity:
    name: "income"                                 # the Feast entity
    description: "this entity is a ...."           # the entity description used by Feast
    id_col: 'ifa'                                  # ...
  file_source:
    description: 'data source description'         # the data source description used by Feast
    owner: "me@business.com"                       # the data source owner registered in Feast
    timestamp_col: 'event_time'                    # ...
    create_timestamp_col: 'create_time_col'        # ...
  feature_view:
    name: 'income_view'                            # ...
    owner: 'view@owner.com'                        # the view owner registered in Feast
    ttl_in_seconds: 3600                           # ...
    create_timestamps: True                        # ...
  feature_service:                                 # ...
    name: 'income_feature_service'                 # ...
  file_configs:
    mode: overwrite                                # ...
```

Let's break this down!

```yaml
entity:
    name: "income"
    description: "this entity is a ...."
    id_col: 'ifa'
```

```yaml
file_source:
    description: 'data source description'
    owner: "me@business.com"
    timestamp_col: 'event_time'
    create_timestamp_col: 'create_time_col'
```

```yaml
feature_view:
    name: 'income_view'
    owner: 'view@owner.com'
    ttl_in_seconds: 3600
    create_timestamps: True
```

```yaml
feature_service:
  name: 'income_feature_service'
```

```yaml
file_configs:
  mode: overwrite
```

### Exporting data to Feast

Run your anovos workflow with the configuration above

### Apply new features and materialize the feature data in the offline store

Once the workflow has finished, switch into the folder of `anovos_feature` repository and
apply the changed feature definitions and materialize the features:

```bash
cd anovos_repo
feast apply
feast materialize `date "+%Y-%m-%dT"` `date "+%Y-%m-%dT%H:%M:%S"`
```

### Verify feature definitions in Feast UI

To verify that the features have been loaded correctly, you can check them using Feast's UI.
Run

```bash
feast ui
```

and access the Feast UI at [`http://127.0.0.1:8888`](http://127.0.0.1:8888).

There, ...

### Retrieve feature data from Feast

```python
from feast import FeatureStore
import pandas as pd

store = FeatureStore(repo_path="./anovos_repo")

# ACCESS HISTORICAL FEATURES

# Either read directly from parquet file generated by the Anovos workflow or generate manually
entity_df = pd.DataFrame.from_dict(
  {
      "ifa": ['', '', '' ''], # add some ids
      "event_timestamp": [
          datetime(2022, 5, 31, 14, 59, 42), # add datetiem entries corresponding to timestamps of the generated features
          datetime(2022, 5, 31, 8, 12, 10),
          datetime(2022, 5, 31, 16, 40, 26),
          datetime(2022, 5, 31, 15, 1, 12),
      ],
  }
)

# Fetch the training data from Feast
training_df = store.get_historical_features(
  entity_df=entity_df,
  features=[
      "income_view:age",
      "income_view:workclass",
      "income_view:education",
      "income_view:occupation",
      # add more features here
  ],
).to_df()

# Train your model

...
```

## Integrating _Anovos_ with other feature stores
