{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Anovos Docs Everything you need to know about Anovos and its major component microservices. \u200b What is Anovos? Anovos is a an open source project, built by data scientist for the data science community, that brings automation to the feature engineering process. By rethinking ingestion and transformation, and including deeper analytics, drift identification, and stability analysis, Anovos improves productivity and helps data scientists build more resilient, higher performing models. Interested in contributing? Check out our Contributors' Page . What's Powering Anovos? The Anovos stack consists of several microservices, and a grpc API: Red hair crookshanks bludger Marauder\u2019s Map Prongs sunshine daisies butter mellow Ludo Bagman. Beaters gobbledegook N.E.W.T., Honeydukes eriseD inferi Wormtail. Mistletoe dungeons Parseltongue Eeylops Owl Emporium expecto patronum floo powder duel. Gillyweed portkey, keeper Godric\u2019s Hollow telescope, splinched fire-whisky silver Leprechaun O.W.L. stroke the spine. Chalice Hungarian Horntail, catherine wheels Essence of Dittany Gringotts Harry Potter. Prophecies Yaxley green eyes Remembrall horcrux hand of the servant. Devil\u2019s snare love potion Ravenclaw, Professor Sinistra time-turner steak and kidney pie. Cabbage Daily Prophet letters from no one Dervish and Banges leg. Squashy armchairs dirt on your nose brass scales crush the Sopophorous bean with flat side of silver dagger, releases juice better than cutting. Full moon Whomping Willow three turns should do it lemon drops. Locomotor trunks owl treats that will be 50 points, Mr. Potter. Witch Weekly, he will rise again and he will come for us, headmaster Erumpent horn. Fenrir Grayback horseless carriages \u2018zis is a chance many would die for! Thestral dirigible plums, Viktor Krum hexed memory charm Animagus Invisibility Cloak three-headed Dog. Half-Blood Prince Invisibility Cloak cauldron cakes, hiya Harry! Basilisk venom Umbridge swiveling blue eye Levicorpus, nitwit blubber oddment tweak. Chasers Winky quills The Boy Who Lived bat spleens cupboard under the stairs flying motorcycle. Sirius Black Holyhead Harpies, you\u2019ve got dirt on your nose. Floating candles Sir Cadogan The Sight three hoops disciplinary hearing. Grindlewald pig\u2019s tail Sorcerer's Stone biting teacup. Side-along dragon-scale suits Filch 20 points, Mr. Potter. Toad-like smile Flourish and Blotts he knew I\u2019d come back Quidditch World Cup. Fat Lady baubles banana fritters fairy lights Petrificus Totalus. So thirsty, deluminator firs\u2019 years follow me 12 inches of parchment. Head Boy start-of-term banquet Cleansweep Seven roaring lion hat. Unicorn blood crossbow mars is bright tonight, feast Norwegian Ridgeback. Come seek us where our voices sound, we cannot sing above the ground, Ginny Weasley bright red. Fanged frisbees, phoenix tears good clean match.. Getting Started with Anovos First time using Anovos? No worries, here's an easy way to get started: Steps to Install ANOVOS Installation via public registry To install ANOVOS package via pypi, please execute the following commands: pip3 install anovos Installation via Git To install ANOVOS package via git, please execute the following commands: pip3 install \"git+https://github.com/anovos/anovos.git\" Get Started Once installed, packages can be imported and the required functionality can be called in the user flow in any application or notebook. Refer to the following links to get started : quick-start guide example notebooks documentation Steps to Run ANOVOS using spark-submit After checking out via Git clone, please follow the below instructions to run the E2E ML Anovos Package on the sample income dataset: First execute the following command to clean folder, build the latest modules: make clean build There are 2 ways to run using spark-submit after this: Follow A, if you have a working environment already and would just like to use the same configs. (Note: version dependencies are to be ensured by user) Follow B, if you want to run via dockers A. Running via User's local environment Check the pre-requisites - ANOVOS requires Spark (2.4.x), Python (3.7.*), Java(8). Check version using the following commands: spark-submit --version python --version java -version Set environment variables - $JAVA_HOME , $SPARK_HOME , $PYSPARK_PYTHON , and $PATH Ensure spark-submit and pyspark is working without any issues. Execute the following commands to run the end to end pipeline: cd dist/ nohup ./spark-submit.sh > run.txt & Check result of end to end run tail -f run.txt B. Running via Dockers Note: Kindly ensure the machine has ~15 GB free space atleast when running using Dockers Install docker on your machine (https://docs.docker.com/get-docker/) Set docker settings to use atleast 8GB memory and 4+ cores. Below image shows setting docker settings on Docker Desktop: Ensure dockers is successfully installed by executing the following commands to check docker image and docker container respectively: docker image ls docker ps Create docker image with the following command: (Note: Step #1 should have copied a \"Dockerfile\" to the directory the following command is being executed in) docker build -t mw_ds_feature_machine:0.1 . Check if docker image was successfully created: docker image ls Run the docker container using the following command: docker run -t -i -v $(PWD):/temp mw_ds_feature_machine:0.1 Check if docker container is successfully running: docker ps -a To explore the generated output folder, execute the following commands: docker exec -it <container_id> bash Once run has completed please exit the docker run by sending SIGINT - ^C (CTRL + C) Running on custom dataset The above E2E ANOVOS package run executed for the sample income dataset can be customized and run for any user dataset. This can be done configuring the config/config.yaml file and by defining the flow of run in src/main.py file. ANOVOS Package Documentation Please find the detailed documentation of ANOVOS package here. Reference Links Setting up pyspark and installing on Windows from Towards Data Science Get Help Need a little help getting started? We're here! Check out the FAQs - When there are questions, we document the answers. Join the MLOps Commuhity . Look for the #Anovos channel. Submit an issue on Github .","title":"Home"},{"location":"#the-anovos-docs","text":"Everything you need to know about Anovos and its major component microservices. \u200b","title":"The Anovos Docs"},{"location":"#what-is-anovos","text":"Anovos is a an open source project, built by data scientist for the data science community, that brings automation to the feature engineering process. By rethinking ingestion and transformation, and including deeper analytics, drift identification, and stability analysis, Anovos improves productivity and helps data scientists build more resilient, higher performing models. Interested in contributing? Check out our Contributors' Page .","title":"What is Anovos?"},{"location":"#whats-powering-anovos","text":"The Anovos stack consists of several microservices, and a grpc API: Red hair crookshanks bludger Marauder\u2019s Map Prongs sunshine daisies butter mellow Ludo Bagman. Beaters gobbledegook N.E.W.T., Honeydukes eriseD inferi Wormtail. Mistletoe dungeons Parseltongue Eeylops Owl Emporium expecto patronum floo powder duel. Gillyweed portkey, keeper Godric\u2019s Hollow telescope, splinched fire-whisky silver Leprechaun O.W.L. stroke the spine. Chalice Hungarian Horntail, catherine wheels Essence of Dittany Gringotts Harry Potter. Prophecies Yaxley green eyes Remembrall horcrux hand of the servant. Devil\u2019s snare love potion Ravenclaw, Professor Sinistra time-turner steak and kidney pie. Cabbage Daily Prophet letters from no one Dervish and Banges leg. Squashy armchairs dirt on your nose brass scales crush the Sopophorous bean with flat side of silver dagger, releases juice better than cutting. Full moon Whomping Willow three turns should do it lemon drops. Locomotor trunks owl treats that will be 50 points, Mr. Potter. Witch Weekly, he will rise again and he will come for us, headmaster Erumpent horn. Fenrir Grayback horseless carriages \u2018zis is a chance many would die for! Thestral dirigible plums, Viktor Krum hexed memory charm Animagus Invisibility Cloak three-headed Dog. Half-Blood Prince Invisibility Cloak cauldron cakes, hiya Harry! Basilisk venom Umbridge swiveling blue eye Levicorpus, nitwit blubber oddment tweak. Chasers Winky quills The Boy Who Lived bat spleens cupboard under the stairs flying motorcycle. Sirius Black Holyhead Harpies, you\u2019ve got dirt on your nose. Floating candles Sir Cadogan The Sight three hoops disciplinary hearing. Grindlewald pig\u2019s tail Sorcerer's Stone biting teacup. Side-along dragon-scale suits Filch 20 points, Mr. Potter. Toad-like smile Flourish and Blotts he knew I\u2019d come back Quidditch World Cup. Fat Lady baubles banana fritters fairy lights Petrificus Totalus. So thirsty, deluminator firs\u2019 years follow me 12 inches of parchment. Head Boy start-of-term banquet Cleansweep Seven roaring lion hat. Unicorn blood crossbow mars is bright tonight, feast Norwegian Ridgeback. Come seek us where our voices sound, we cannot sing above the ground, Ginny Weasley bright red. Fanged frisbees, phoenix tears good clean match..","title":"What's Powering Anovos?"},{"location":"#getting-started-with-anovos","text":"First time using Anovos? No worries, here's an easy way to get started:","title":"Getting Started with Anovos"},{"location":"#steps-to-install-anovos","text":"","title":"Steps to Install ANOVOS"},{"location":"#installation-via-public-registry","text":"To install ANOVOS package via pypi, please execute the following commands: pip3 install anovos","title":"Installation via public registry"},{"location":"#installation-via-git","text":"To install ANOVOS package via git, please execute the following commands: pip3 install \"git+https://github.com/anovos/anovos.git\"","title":"Installation via Git"},{"location":"#get-started","text":"Once installed, packages can be imported and the required functionality can be called in the user flow in any application or notebook. Refer to the following links to get started : quick-start guide example notebooks documentation","title":"Get Started"},{"location":"#steps-to-run-anovos-using-spark-submit","text":"After checking out via Git clone, please follow the below instructions to run the E2E ML Anovos Package on the sample income dataset: First execute the following command to clean folder, build the latest modules: make clean build There are 2 ways to run using spark-submit after this: Follow A, if you have a working environment already and would just like to use the same configs. (Note: version dependencies are to be ensured by user) Follow B, if you want to run via dockers","title":"Steps to Run ANOVOS using spark-submit"},{"location":"#a-running-via-users-local-environment","text":"Check the pre-requisites - ANOVOS requires Spark (2.4.x), Python (3.7.*), Java(8). Check version using the following commands: spark-submit --version python --version java -version Set environment variables - $JAVA_HOME , $SPARK_HOME , $PYSPARK_PYTHON , and $PATH Ensure spark-submit and pyspark is working without any issues. Execute the following commands to run the end to end pipeline: cd dist/ nohup ./spark-submit.sh > run.txt & Check result of end to end run tail -f run.txt","title":"A. Running via User's local environment"},{"location":"#b-running-via-dockers","text":"Note: Kindly ensure the machine has ~15 GB free space atleast when running using Dockers Install docker on your machine (https://docs.docker.com/get-docker/) Set docker settings to use atleast 8GB memory and 4+ cores. Below image shows setting docker settings on Docker Desktop: Ensure dockers is successfully installed by executing the following commands to check docker image and docker container respectively: docker image ls docker ps Create docker image with the following command: (Note: Step #1 should have copied a \"Dockerfile\" to the directory the following command is being executed in) docker build -t mw_ds_feature_machine:0.1 . Check if docker image was successfully created: docker image ls Run the docker container using the following command: docker run -t -i -v $(PWD):/temp mw_ds_feature_machine:0.1 Check if docker container is successfully running: docker ps -a To explore the generated output folder, execute the following commands: docker exec -it <container_id> bash Once run has completed please exit the docker run by sending SIGINT - ^C (CTRL + C)","title":"B. Running via Dockers"},{"location":"#running-on-custom-dataset","text":"The above E2E ANOVOS package run executed for the sample income dataset can be customized and run for any user dataset. This can be done configuring the config/config.yaml file and by defining the flow of run in src/main.py file.","title":"Running on custom dataset"},{"location":"#anovos-package-documentation","text":"Please find the detailed documentation of ANOVOS package here.","title":"ANOVOS Package Documentation"},{"location":"#reference-links","text":"Setting up pyspark and installing on Windows from Towards Data Science","title":"Reference Links"},{"location":"#get-help","text":"Need a little help getting started? We're here! Check out the FAQs - When there are questions, we document the answers. Join the MLOps Commuhity . Look for the #Anovos channel. Submit an issue on Github .","title":"Get Help"},{"location":"about/","text":"Introducing Anovos Data Science teams spend up to 80% of their time on feature engineering and still end up building models with poor resilience properties. Anovos seeks to address both these issues (lack of modeler productivity and insufficient model resilience) by enabling data scientists to understand the stability properties of the data they work with and then creating a transformation methodology that allows the building of features anchored by stable data, which in turn produce resilient models that break less often when deployed. Unlike current feature engineering workflows, which are ad hoc, error-prone, and modeler-driven, Anovos seeks to inject process-driven efficiency into feature creation based on innovations in understanding the stability of data and how data items impact the features they anchor. With Anovos, ML models will be more consistent, more accurate, and able to deliver results faster. The process of building models is more automated and procedural, saving time and decreasing cost. The Team Behind Anovos Anovos was built by the data scientists at Mobilewalla, a team of highly talented and experienced data scientists who have years of experience in machine learning (ML) and AI and are applying ML techniques to some of the largest consumer data sets available. This experience has brought to light an internal need to create tools to simplify and speed up the feature engineering process, increasing our own efficiency. We've been able to replicate and now open source these tools, bringing our learnings to the community. The Mobilewalla Feature Mart currently offers teams supplemental data to make their ML models more accurate and predictive; Anovos, a set of open source libraries, enables them to take that data, enhancing their internal data sets and making their modeling more systematic and procedural, creating cost and time efficiencies, and building more predictive models.","title":"About"},{"location":"about/#introducing-anovos","text":"Data Science teams spend up to 80% of their time on feature engineering and still end up building models with poor resilience properties. Anovos seeks to address both these issues (lack of modeler productivity and insufficient model resilience) by enabling data scientists to understand the stability properties of the data they work with and then creating a transformation methodology that allows the building of features anchored by stable data, which in turn produce resilient models that break less often when deployed. Unlike current feature engineering workflows, which are ad hoc, error-prone, and modeler-driven, Anovos seeks to inject process-driven efficiency into feature creation based on innovations in understanding the stability of data and how data items impact the features they anchor. With Anovos, ML models will be more consistent, more accurate, and able to deliver results faster. The process of building models is more automated and procedural, saving time and decreasing cost.","title":"Introducing Anovos"},{"location":"about/#the-team-behind-anovos","text":"Anovos was built by the data scientists at Mobilewalla, a team of highly talented and experienced data scientists who have years of experience in machine learning (ML) and AI and are applying ML techniques to some of the largest consumer data sets available. This experience has brought to light an internal need to create tools to simplify and speed up the feature engineering process, increasing our own efficiency. We've been able to replicate and now open source these tools, bringing our learnings to the community. The Mobilewalla Feature Mart currently offers teams supplemental data to make their ML models more accurate and predictive; Anovos, a set of open source libraries, enables them to take that data, enhancing their internal data sets and making their modeling more systematic and procedural, creating cost and time efficiencies, and building more predictive models.","title":"The Team Behind Anovos"},{"location":"community/code-of-conduct/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at coc@anovos.ai. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of Conduct"},{"location":"community/code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"community/code-of-conduct/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"community/code-of-conduct/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"community/code-of-conduct/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"community/code-of-conduct/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"community/code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at coc@anovos.ai. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"community/code-of-conduct/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"Enforcement Guidelines"},{"location":"community/code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"community/communication/","text":"There are different ways and channels that you can use as a community member or maintainer to raise questions, collaborate or to have some fun! Slack The community has a channel on the ML-Ops Community Slack called #anovos. Feel free to join us. This is the perfect place to have a first point of contact with the community. The maintainers and some contributors hang out on it. GitHub Issues The Anovos repository has issues enables. If you need some help, you have questions or ideas you can open an issue in this repository. If you have any questions about how to raise an issue, feel free to drop a message on Slack. Contributors Mailing List If you want to follow discussions about the Anovos development or you want to become a contributor there is a mailing list about that. This is not right place for general questions related to how Anovos works. The focus is 100% on ongoing or future development. Google Group: anovos-contributors anovos-contributors@googlegroups.com We have a monthly meeting to sync about ongoing work or discuss about future development effort around the various components. You will be added to it automatically when joining the mailing list. There is an agenda available via Google Docs feel free to add your item in there before the meeting.","title":"Communication"},{"location":"community/communication/#slack","text":"The community has a channel on the ML-Ops Community Slack called #anovos. Feel free to join us. This is the perfect place to have a first point of contact with the community. The maintainers and some contributors hang out on it.","title":"Slack"},{"location":"community/communication/#github-issues","text":"The Anovos repository has issues enables. If you need some help, you have questions or ideas you can open an issue in this repository. If you have any questions about how to raise an issue, feel free to drop a message on Slack.","title":"GitHub Issues"},{"location":"community/communication/#contributors-mailing-list","text":"If you want to follow discussions about the Anovos development or you want to become a contributor there is a mailing list about that. This is not right place for general questions related to how Anovos works. The focus is 100% on ongoing or future development. Google Group: anovos-contributors anovos-contributors@googlegroups.com We have a monthly meeting to sync about ongoing work or discuss about future development effort around the various components. You will be added to it automatically when joining the mailing list. There is an agenda available via Google Docs feel free to add your item in there before the meeting.","title":"Contributors Mailing List"},{"location":"community/contributing/","text":"Welcome to the Anovos Community! We're excited to bring the immense experience of the Mobilewalla team to the data science community, and we'd love to have you join us! Here you'll find all the details you need to get started as an Anovos contributor. Getting Started with Anovos Anovos is an open source project that brings automation to the feature engineering process. To get Anovos up and running on your local machine, follow the Getting Started Guide All repos, sample data, and notebooks you need are located in the Anovos Github Organization How To Get Involved We love your input! We want to make contributing to this project as easy and transparent as possible, whether it's: Reporting a bug Discussing the current state of the code Submitting a fix Proposing new features Testing out new features Contributing to the docs Giving talks about Anovos at Meetups, conferences and webinars The latest information about how to interact with the project maintainers and broader community is kept in COMMUNICATION.md . Contribute to Anovos Pull requests are the best way to propose changes to the codebase. We use GitHub flow and everything happens through pull requests. We welcome your pull requests; to make it simple, here are the steps to contribute: Fork the repo you're updating and create your branch from main. If you've added code that should be tested, add tests. If you've changed APIs, update the documentation. Ensure the test suite passes. Issue that pull request! Any contributions you make will be under the Apache Software License In short, when you submit code changes, your submissions are understood to be under the same Apache License that covers the project. Feel free to contact the maintainers if that's a concern. Write Thorough Commit Messages Help reviewers know what you're contributing by writing good commit messages. To standardize how things are done, your first line of your commit is your subject , followed by a blank line, then a message describing what the commit does. We use the following guidelines suggested by Chris Beams : Commit messages The first line of the commit message is the subject , this should be followed by a blank line and then a message describing the intent and purpose of the commit. These guidelines are based upon a post by Chris Beams . When you commit, you are accepting our DCO: Developer Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 1 Letterman Drive Suite D4700 San Francisco, CA, 94129 Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. When you run git commit make sure you sign-off the commit by typing git commit --signoff or git commit -s The commit subject-line should start with an uppercase letter The commit subject-line should not exceed 72 characters in length The commit subject-line should not end with punctuation (., etc) Note: please do not use the GitHub suggestions feature, since it will not allow your commits to be signed-off. When giving a commit body, be sure to: Leave a blank line after the subject-line Make sure all lines are wrapped to 72 characters Here's an example that would be accepted: Add luke to the contributors' _index.md file We need to add luke to the contributors' _index.md file as a contributor. Signed-off-by: Hans <hans@anovos.ai> Some invalid examples: (feat) Add page about X to documentation This example does not follow the convention by adding a custom scheme of (feat) Update the documentation for page X so including fixing A, B, C and D and F. This example will be truncated in the GitHub UI and via git log --oneline If you would like to ammend your commit follow this guide: Git: Rewriting History Report bugs using GitHub's issues All bugs are tracked using Github issues. If you find something that needs to be addressed, open a new issue; it's easy! License By contributing, you agree that your contributions will be licensed under its Apache License, adhere to the Developer Certificate of Origin, and adhere to our code of conduct.","title":"Contributing to Anovos"},{"location":"community/contributing/#getting-started-with-anovos","text":"Anovos is an open source project that brings automation to the feature engineering process. To get Anovos up and running on your local machine, follow the Getting Started Guide All repos, sample data, and notebooks you need are located in the Anovos Github Organization","title":"Getting Started with Anovos"},{"location":"community/contributing/#how-to-get-involved","text":"We love your input! We want to make contributing to this project as easy and transparent as possible, whether it's: Reporting a bug Discussing the current state of the code Submitting a fix Proposing new features Testing out new features Contributing to the docs Giving talks about Anovos at Meetups, conferences and webinars The latest information about how to interact with the project maintainers and broader community is kept in COMMUNICATION.md .","title":"How To Get Involved"},{"location":"community/contributing/#contribute-to-anovos","text":"Pull requests are the best way to propose changes to the codebase. We use GitHub flow and everything happens through pull requests. We welcome your pull requests; to make it simple, here are the steps to contribute: Fork the repo you're updating and create your branch from main. If you've added code that should be tested, add tests. If you've changed APIs, update the documentation. Ensure the test suite passes. Issue that pull request! Any contributions you make will be under the Apache Software License In short, when you submit code changes, your submissions are understood to be under the same Apache License that covers the project. Feel free to contact the maintainers if that's a concern.","title":"Contribute to Anovos"},{"location":"community/contributing/#write-thorough-commit-messages","text":"Help reviewers know what you're contributing by writing good commit messages. To standardize how things are done, your first line of your commit is your subject , followed by a blank line, then a message describing what the commit does. We use the following guidelines suggested by Chris Beams :","title":"Write Thorough Commit Messages"},{"location":"community/contributing/#commit-messages","text":"The first line of the commit message is the subject , this should be followed by a blank line and then a message describing the intent and purpose of the commit. These guidelines are based upon a post by Chris Beams . When you commit, you are accepting our DCO: Developer Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 1 Letterman Drive Suite D4700 San Francisco, CA, 94129 Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. When you run git commit make sure you sign-off the commit by typing git commit --signoff or git commit -s The commit subject-line should start with an uppercase letter The commit subject-line should not exceed 72 characters in length The commit subject-line should not end with punctuation (., etc) Note: please do not use the GitHub suggestions feature, since it will not allow your commits to be signed-off. When giving a commit body, be sure to: Leave a blank line after the subject-line Make sure all lines are wrapped to 72 characters Here's an example that would be accepted: Add luke to the contributors' _index.md file We need to add luke to the contributors' _index.md file as a contributor. Signed-off-by: Hans <hans@anovos.ai> Some invalid examples: (feat) Add page about X to documentation This example does not follow the convention by adding a custom scheme of (feat) Update the documentation for page X so including fixing A, B, C and D and F. This example will be truncated in the GitHub UI and via git log --oneline If you would like to ammend your commit follow this guide: Git: Rewriting History","title":"Commit messages"},{"location":"community/contributing/#report-bugs-using-githubs-issues","text":"All bugs are tracked using Github issues. If you find something that needs to be addressed, open a new issue; it's easy!","title":"Report bugs using GitHub's issues"},{"location":"community/contributing/#license","text":"By contributing, you agree that your contributions will be licensed under its Apache License, adhere to the Developer Certificate of Origin, and adhere to our code of conduct.","title":"License"},{"location":"community/license/","text":"License.md Copyright 2021 Anovos Developers Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS Third-Party License overview of included 3rd party libraries The Anovos project is licensed under the terms of the Apache 2.0 License . However, Anovos includes several third-party Open-Source libraries, which are licensed under their own respective Open-Source licenses. Pandas License: BSD 3-Clause License https://github.com/pandas-dev/pandas/blob/master/LICENSE scripy License: BSD 3-Clause License https://github.com/scipy/scipy/blob/master/LICENSE.txt scikit-learn License: BSD 3-Clause License https://github.com/scikit-learn/scikit-learn/blob/main/COPYING joblib License: BSD 3-Clause License https://github.com/joblib/joblib/blob/master/LICENSE.txt boto3 License: Apache 2.0 License https://github.com/boto/boto3/blob/develop/LICENSE pyarrow License: Apache 2.0 License https://github.com/apache/arrow/blob/master/LICENSE.txt sparkpickle License: Apache 2.0 License https://github.com/src-d/sparkpickle/blob/master/LICENSE s3path License: Apache-2.0 License https://github.com/liormizr/s3path/blob/master/LICENSE statsmodels License: BSD 3-Clause License https://github.com/statsmodels/statsmodels/blob/main/LICENSE.txt pybind11 License: BSD 3-Clause License https://github.com/pybind/pybind11/blob/master/LICENSE popmon License: MIT License https://github.com/ing-bank/popmon/blob/master/LICENSE seaborn License: BSD 3-Clause License https://github.com/mwaskom/seaborn/blob/master/LICENSE varclushi License: GPL 3.0 License https://github.com/jingtt/varclushi/blob/master/LICENSE pytest Licsnse: MIT License https://docs.pytest.org/en/latest/license.html datapane License: Apache 2.0 License https://github.com/datapane/datapane/blob/master/LICENSE","title":"License"},{"location":"community/license/#licensemd","text":"Copyright 2021 Anovos Developers Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS","title":"License.md"},{"location":"community/license/#third-party-license-overview-of-included-3rd-party-libraries","text":"The Anovos project is licensed under the terms of the Apache 2.0 License . However, Anovos includes several third-party Open-Source libraries, which are licensed under their own respective Open-Source licenses.","title":"Third-Party License overview of included 3rd party libraries"},{"location":"community/license/#pandas","text":"License: BSD 3-Clause License https://github.com/pandas-dev/pandas/blob/master/LICENSE","title":"Pandas"},{"location":"community/license/#scripy","text":"License: BSD 3-Clause License https://github.com/scipy/scipy/blob/master/LICENSE.txt","title":"scripy"},{"location":"community/license/#scikit-learn","text":"License: BSD 3-Clause License https://github.com/scikit-learn/scikit-learn/blob/main/COPYING","title":"scikit-learn"},{"location":"community/license/#joblib","text":"License: BSD 3-Clause License https://github.com/joblib/joblib/blob/master/LICENSE.txt","title":"joblib"},{"location":"community/license/#boto3","text":"License: Apache 2.0 License https://github.com/boto/boto3/blob/develop/LICENSE","title":"boto3"},{"location":"community/license/#pyarrow","text":"License: Apache 2.0 License https://github.com/apache/arrow/blob/master/LICENSE.txt","title":"pyarrow"},{"location":"community/license/#sparkpickle","text":"License: Apache 2.0 License https://github.com/src-d/sparkpickle/blob/master/LICENSE","title":"sparkpickle"},{"location":"community/license/#s3path","text":"License: Apache-2.0 License https://github.com/liormizr/s3path/blob/master/LICENSE","title":"s3path"},{"location":"community/license/#statsmodels","text":"License: BSD 3-Clause License https://github.com/statsmodels/statsmodels/blob/main/LICENSE.txt","title":"statsmodels"},{"location":"community/license/#pybind11","text":"License: BSD 3-Clause License https://github.com/pybind/pybind11/blob/master/LICENSE","title":"pybind11"},{"location":"community/license/#popmon","text":"License: MIT License https://github.com/ing-bank/popmon/blob/master/LICENSE","title":"popmon"},{"location":"community/license/#seaborn","text":"License: BSD 3-Clause License https://github.com/mwaskom/seaborn/blob/master/LICENSE","title":"seaborn"},{"location":"community/license/#varclushi","text":"License: GPL 3.0 License https://github.com/jingtt/varclushi/blob/master/LICENSE","title":"varclushi"},{"location":"community/license/#pytest","text":"Licsnse: MIT License https://docs.pytest.org/en/latest/license.html","title":"pytest"},{"location":"community/license/#datapane","text":"License: Apache 2.0 License https://github.com/datapane/datapane/blob/master/LICENSE","title":"datapane"},{"location":"docs/anovos-intro/","text":"Introducing Anovos Data Science teams spend up to 80% of their time on feature engineering and still end up building models with poor resilience properties. Anovos seeks to address both these issues (lack of modeler productivity and insufficient model resilience) by enabling data scientists to understand the stability properties of the data they work with and then creating a transformation methodology that allows the building of features anchored by stable data, which in turn produce resilient models that break less often when deployed. Unlike current feature engineering workflows, which are ad hoc, error-prone, and modeler-driven, Anovos seeks to inject process-driven efficiency into feature creation based on innovations in understanding the stability of data and how data items impact the features they anchor. With Anovos, ML models will be more consistent, more accurate, and able to deliver results faster. The process of building models is more automated and procedural, saving time and decreasing cost. The Team Behind Anovos Anovos was built by the data scientists at Mobilewalla, a team of highly talented and experienced data scientists who have years of experience in machine learning (ML) and AI and are applying ML techniques to some of the largest consumer data sets available. This experience has brought to light an internal need to create tools to simplify and speed up the feature engineering process, increasing our own efficiency. We've been able to replicate and now open source these tools, bringing our learnings to the community. The Mobilewalla Feature Mart currently offers teams supplemental data to make their ML models more accurate and predictive; Anovos, a set of open source libraries, enables them to take that data, enhancing their internal data sets and making their modeling more systematic and procedural, creating cost and time efficiencies, and building more predictive models.","title":"Anovos Intro"},{"location":"docs/anovos-intro/#introducing-anovos","text":"Data Science teams spend up to 80% of their time on feature engineering and still end up building models with poor resilience properties. Anovos seeks to address both these issues (lack of modeler productivity and insufficient model resilience) by enabling data scientists to understand the stability properties of the data they work with and then creating a transformation methodology that allows the building of features anchored by stable data, which in turn produce resilient models that break less often when deployed. Unlike current feature engineering workflows, which are ad hoc, error-prone, and modeler-driven, Anovos seeks to inject process-driven efficiency into feature creation based on innovations in understanding the stability of data and how data items impact the features they anchor. With Anovos, ML models will be more consistent, more accurate, and able to deliver results faster. The process of building models is more automated and procedural, saving time and decreasing cost.","title":"Introducing Anovos"},{"location":"docs/anovos-intro/#the-team-behind-anovos","text":"Anovos was built by the data scientists at Mobilewalla, a team of highly talented and experienced data scientists who have years of experience in machine learning (ML) and AI and are applying ML techniques to some of the largest consumer data sets available. This experience has brought to light an internal need to create tools to simplify and speed up the feature engineering process, increasing our own efficiency. We've been able to replicate and now open source these tools, bringing our learnings to the community. The Mobilewalla Feature Mart currently offers teams supplemental data to make their ML models more accurate and predictive; Anovos, a set of open source libraries, enables them to take that data, enhancing their internal data sets and making their modeling more systematic and procedural, creating cost and time efficiencies, and building more predictive models.","title":"The Team Behind Anovos"},{"location":"docs/anovos-modules-overview/association-evaluator/","text":"Module ANOVOS.association_evaluator This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target variable. Association between attributes is measured by: correlation_matrix variable_clustering Association between an attribute and binary target is measured by: IV_calculation IG_calculation Columns which are subjected to these analysis can be controlled by right combination of arguments - list_of_cols and drop_cols. All functions have following common arguments: idf : Input dataframe ist_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols : This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. print_impact : This argument is to print out the statistics. correlation_matrix This function calculates correlation coefficient statistical, which measures the strength of the relationship between the relative movements of two attributes. Pearson\u2019s correlation coefficient is a standard approach of measuring correlation between two variables. However, it has some drawbacks: a) It works only with continuous variables, b) It only accounts for a linear relationship between variables, and c) It is sensitive to outliers. To avoid these issues, we are computing Phik (\ud835\udf19k), which is a new and practical correlation coefficient that works consistently between categorical, ordinal and interval variables, captures non-linear dependency and reverts to the Pearson correlation coefficient in case of a bivariate normal input distribution. The correlation coefficient is calculated for every pair of attributes and its value lies between 0 and 1, where 0 means there is no correlation between the two attributes and 1 means strong correlation. However, this methodology have drawbacks of its own as it is found to be more computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). Further, there is no indication of the direction of the relationship. More detail can be referred from the source paper . This function returns a correlation matrix dataframe of schema \u2013 attribute, . Correlation between attribute X and Y can be found at intersection of a) row with value X in \u2018attribute\u2019 column and b) column \u2018Y\u2019 (or row with value Y in \u2018attribute\u2019 column and column \u2018X\u2019). idf list_of_cols drop_cols stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator. print_impact variable_clustering Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. The function is leveraging VarClusHi library to do variable clustering; however, this library is not implemented in a scalable manner due to which the analysis is done on a sample dataset. Further, it is found to be a bit computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). It returns a Spark Dataframe with schema \u2013 Cluster, Attribute, RS_Ratio. The attribute with the lowest (1 \u2014 RS_Ratio) can be chosen as a representative of the cluster while discarding the other attributes from that cluster. This can also help in achieving the dimension reduction, if required. idf list_of_cols drop_cols sample_size : Sample size used for performing variable cluster. Default is 100,000. stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator. This is used to remove single value columns from the analysis purpose. stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator. This is used for MMM imputation as Variable Clustering doesn\u2019t work with missing values. print_impact IV_calculation Information Value (IV) is simple and powerful technique to conduct attribute relevance analysis. It measures how well an attribute is able to distinguish between a binary target variable i.e. label 0 from label 1, and hence helps in ranking attributes on the basis of their importance. In the heart of IV methodology are groups (bins) of observations. For categorical attributes, usually each category is a bin while numerical attributes need to be split into categories. IV = \u2211 (% of non-events - % of events) * WOE where: WOE = In(% of non-events \u2797 % of events) % of event = % label 1 in a bin % of non-event = % label 0 in a bin General rule of thumb while creating the bins are that a) each bin should have at least 5% of the observations, b) the WOE should be monotonic, i.e. either growing or decreasing with the bins, and c) missing values should be binned separately. An article from listendata.com can be referred for good understanding of IV & WOE concepts. idf list_of_cols drop_cols label_col : Name of label or target column in the input dataset event_label : Value of event (label 1) in the label column encoding_configs : This argument takes input in dictionary format with keys related to binning operation - 'bin_method' (default 'equal_frequency'), 'bin_size' (default 10) and 'monotonicity_check' (default 0). monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature and can be expensive operation. print_impact IG_calculation Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event log\u2061(%event)-(1-%event) log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i ) log\u2061(1-%\u3016event\u3017_i) idf list_of_cols drop_cols label_col : Name of label or target column in the input dataset event_label : Value of event (label 1) in the label column encoding_configs : This argument takes input in dictionary format with keys related to binning operation - 'bin_method' (default 'equal_frequency'), 'bin_size' (default 10) and 'monotonicity_check' (default 0). monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature and can be expensive operation. print_impact","title":"Association Evaluator"},{"location":"docs/anovos-modules-overview/association-evaluator/#module-anovosassociation_evaluator","text":"This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target variable. Association between attributes is measured by: correlation_matrix variable_clustering Association between an attribute and binary target is measured by: IV_calculation IG_calculation Columns which are subjected to these analysis can be controlled by right combination of arguments - list_of_cols and drop_cols. All functions have following common arguments: idf : Input dataframe ist_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols : This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. print_impact : This argument is to print out the statistics.","title":"Module ANOVOS.association_evaluator"},{"location":"docs/anovos-modules-overview/association-evaluator/#correlation_matrix","text":"This function calculates correlation coefficient statistical, which measures the strength of the relationship between the relative movements of two attributes. Pearson\u2019s correlation coefficient is a standard approach of measuring correlation between two variables. However, it has some drawbacks: a) It works only with continuous variables, b) It only accounts for a linear relationship between variables, and c) It is sensitive to outliers. To avoid these issues, we are computing Phik (\ud835\udf19k), which is a new and practical correlation coefficient that works consistently between categorical, ordinal and interval variables, captures non-linear dependency and reverts to the Pearson correlation coefficient in case of a bivariate normal input distribution. The correlation coefficient is calculated for every pair of attributes and its value lies between 0 and 1, where 0 means there is no correlation between the two attributes and 1 means strong correlation. However, this methodology have drawbacks of its own as it is found to be more computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). Further, there is no indication of the direction of the relationship. More detail can be referred from the source paper . This function returns a correlation matrix dataframe of schema \u2013 attribute, . Correlation between attribute X and Y can be found at intersection of a) row with value X in \u2018attribute\u2019 column and b) column \u2018Y\u2019 (or row with value Y in \u2018attribute\u2019 column and column \u2018X\u2019). idf list_of_cols drop_cols stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator. print_impact","title":"correlation_matrix"},{"location":"docs/anovos-modules-overview/association-evaluator/#variable_clustering","text":"Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. The function is leveraging VarClusHi library to do variable clustering; however, this library is not implemented in a scalable manner due to which the analysis is done on a sample dataset. Further, it is found to be a bit computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). It returns a Spark Dataframe with schema \u2013 Cluster, Attribute, RS_Ratio. The attribute with the lowest (1 \u2014 RS_Ratio) can be chosen as a representative of the cluster while discarding the other attributes from that cluster. This can also help in achieving the dimension reduction, if required. idf list_of_cols drop_cols sample_size : Sample size used for performing variable cluster. Default is 100,000. stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator. This is used to remove single value columns from the analysis purpose. stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator. This is used for MMM imputation as Variable Clustering doesn\u2019t work with missing values. print_impact","title":"variable_clustering"},{"location":"docs/anovos-modules-overview/association-evaluator/#iv_calculation","text":"Information Value (IV) is simple and powerful technique to conduct attribute relevance analysis. It measures how well an attribute is able to distinguish between a binary target variable i.e. label 0 from label 1, and hence helps in ranking attributes on the basis of their importance. In the heart of IV methodology are groups (bins) of observations. For categorical attributes, usually each category is a bin while numerical attributes need to be split into categories. IV = \u2211 (% of non-events - % of events) * WOE where: WOE = In(% of non-events \u2797 % of events) % of event = % label 1 in a bin % of non-event = % label 0 in a bin General rule of thumb while creating the bins are that a) each bin should have at least 5% of the observations, b) the WOE should be monotonic, i.e. either growing or decreasing with the bins, and c) missing values should be binned separately. An article from listendata.com can be referred for good understanding of IV & WOE concepts. idf list_of_cols drop_cols label_col : Name of label or target column in the input dataset event_label : Value of event (label 1) in the label column encoding_configs : This argument takes input in dictionary format with keys related to binning operation - 'bin_method' (default 'equal_frequency'), 'bin_size' (default 10) and 'monotonicity_check' (default 0). monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature and can be expensive operation. print_impact","title":"IV_calculation"},{"location":"docs/anovos-modules-overview/association-evaluator/#ig_calculation","text":"Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event log\u2061(%event)-(1-%event) log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i ) log\u2061(1-%\u3016event\u3017_i) idf list_of_cols drop_cols label_col : Name of label or target column in the input dataset event_label : Value of event (label 1) in the label column encoding_configs : This argument takes input in dictionary format with keys related to binning operation - 'bin_method' (default 'equal_frequency'), 'bin_size' (default 10) and 'monotonicity_check' (default 0). monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature and can be expensive operation. print_impact","title":"IG_calculation"},{"location":"docs/anovos-modules-overview/data-analyzer/","text":"Module ANOVOS.stats_generator This module generates all descriptive statistics related to the ingested data. Descriptive statistics are broken down into different metric types and each function corresponds to one metric type. - global_summary - measures_of_counts - measures_of_centralTendency - measures_of_cardinality - measures_of_dispersion - measures_of_percentiles - measures_of_shape Columns which are subjected to these analysis can be controlled by right combination of arguments - list_of_cols and drop_cols. All above functions require following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols : This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. print_impact : This argument is to print out the statistics. global_summary The global summary function computes the following universal statistics/metrics and returns a Spark Dataframe with schema \u2013 metric, value. - No. of rows - No. of columns - No. of categorical columns along with column names - No. of numerical columns along with the column names - No. of non-numerical non-categorical columns such as date type, array type etc. along with column names measures_of_counts The Measures of Counts function computes different count metric for each column (interchangeably called as attribute in the document). It returns a Spark Dataframe with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. - Fill Count/Rate: It is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary7 functionality of Spark SQL. - Missing Count/Rate: It is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. - Non Zero Count/Rate: It is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a subfunction nonzeroCount_computation, which is later called under measures_of_counts. Under the hood, it leverage Multivariate Statistical Summary8 of Spark MLlib. measures_of_centralTendency The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark Dataframe with schema \u2013 attribute, mean, median, mode, mode_pct. Mean: It is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Median: It is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Mode: It is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns) Mode Pct: It is defined as % of rows seen with Mode value. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns) measures_of_counts The Measures of Counts function computes different count metric for each column (interchangeably called as attribute in the document). It returns a Spark Dataframe with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. Fill Count/Rate: It is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary7 functionality of Spark SQL. Missing Count/Rate: It is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. Non Zero Count/Rate: It is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a subfunction nonzeroCount_computation, which is later called under measures_of_counts. Under the hood, it leverage Multivariate Statistical Summary8 of Spark MLlib. measures_of_centralTendency The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark Dataframe with schema \u2013 attribute, mean, median, mode, mode_pct. Mean: It is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Median: It is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Mode: It is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns) Mode Pct: It is defined as % of rows seen with Mode value. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns) measures_of_cardinality The Measures of Cardinality function provides statistics that are related to unique values seen in an attribute. These statistics are calculated only for discrete columns (categorical + Integer/Long columns). It returns a Spark Dataframe with schema \u2013 attribute, unique_values, IDness. Unique Values: It is defined as distinct value count of a column. It relies on a subfunction uniqueCount_computation for its computation and leverage countDistinct9 functionality of Spark SQL. IDness: It is calculated as Unique Values divided by non-null values seen in a column. Non-null values count is used instead of total count because too many null values can give misleading results even if the column have all unique values (except null). It uses subfunctions - uniqueCount_computation and missingCount_computation. measures_of_dispersion The Measures of Dispersion function provides statistics that describe the spread of a numerical attribute. Alternatively, these statistics are also known as measures of spread. It returns a Spark Dataframe with schema \u2013 attribute, stddev, variance, cov, IQR, range. Standard Deviation (stddev): It measures how concentrated an attribute is around the mean or average and mathematically computed as below. It leverage \u2018stddev\u2019 statistic from summary functionality of Spark SQL. s= X- X2n -1 where: ` `X is an attribute value X is attribute mean n is no. of rows Variance: It is squared value of Standard Deviation. Coefficient of Variance (cov): It is computed as ratio of Standard Deviation & Mean. It leverage \u2018stddev\u2019 and \u2018mean\u2019 statistic from summary functionality of Spark SQL. Interquartile Range (IQR): It describes the difference between the third quartile (75th percentile) and the first quartile (25th percentile), telling us about the range where middle half values are seen. It leverage \u201825%\u2019 and \u201875%\u2019 statistics from summary functionality of Spark SQL. Range: It is simply the difference between the maximum value and the minimum value. It leverage \u2018min\u2019 and \u2018max\u2019 statistics from summary functionality of Spark SQL. measures_of_percentiles The Measures of Percentiles function provides statistics at different percentiles, where Nth percentile can be interpreted as N% of rows have value lesser than or equal to Nth percentile value. It is prominently used to quick detection of skewness or outlier. Alternatively, these statistics are also known as measures of position. These statistics are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max. It leverage \u2018N%\u2019 statistics from summary functionality of Spark SQL where N is 0 for min and 100 for max. measures_of_shape The Measures of Shapes function provides statistics related to the shape of attribute\u2019s distribution. Alternatively, these statistics are also known as measures of moment and are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, skewness, kurtosis. Skewness: It describes how much skewed values are, relative to a perfect bell curve that is observed in normal distribution, and also the direction of skew. If the majority of the values are at the left and the right tail is longer, we say that the distribution is skewed right or positively skewed; if the peak is toward the right and the left tail is longer, we say that the distribution is skewed left or negatively skewed. It leverage skewness10 functionality of Spark SQL. (Excess) Kurtosis: It describes how tall and sharp the central peak is, relative to a perfect bell curve that is observed in normal distribution. The reference standard is a normal distribution, which has a kurtosis of 3. In token of this, often the excess kurtosis is presented: excess kurtosis is simply kurtosis\u22123. Higher (positive) values indicate a higher, sharper peak; lower (negative) values indicate a lower, less distinct peak. It leverage kurtosis11 functionality of Spark SQL.","title":"Data Analyzer"},{"location":"docs/anovos-modules-overview/data-analyzer/#module-anovosstats_generator","text":"This module generates all descriptive statistics related to the ingested data. Descriptive statistics are broken down into different metric types and each function corresponds to one metric type. - global_summary - measures_of_counts - measures_of_centralTendency - measures_of_cardinality - measures_of_dispersion - measures_of_percentiles - measures_of_shape Columns which are subjected to these analysis can be controlled by right combination of arguments - list_of_cols and drop_cols. All above functions require following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols : This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. print_impact : This argument is to print out the statistics.","title":"Module ANOVOS.stats_generator"},{"location":"docs/anovos-modules-overview/data-analyzer/#global_summary","text":"The global summary function computes the following universal statistics/metrics and returns a Spark Dataframe with schema \u2013 metric, value. - No. of rows - No. of columns - No. of categorical columns along with column names - No. of numerical columns along with the column names - No. of non-numerical non-categorical columns such as date type, array type etc. along with column names","title":"global_summary"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_counts","text":"The Measures of Counts function computes different count metric for each column (interchangeably called as attribute in the document). It returns a Spark Dataframe with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. - Fill Count/Rate: It is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary7 functionality of Spark SQL. - Missing Count/Rate: It is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. - Non Zero Count/Rate: It is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a subfunction nonzeroCount_computation, which is later called under measures_of_counts. Under the hood, it leverage Multivariate Statistical Summary8 of Spark MLlib.","title":"measures_of_counts"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_centraltendency","text":"The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark Dataframe with schema \u2013 attribute, mean, median, mode, mode_pct. Mean: It is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Median: It is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Mode: It is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns) Mode Pct: It is defined as % of rows seen with Mode value. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns)","title":"measures_of_centralTendency"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_counts_1","text":"The Measures of Counts function computes different count metric for each column (interchangeably called as attribute in the document). It returns a Spark Dataframe with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. Fill Count/Rate: It is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary7 functionality of Spark SQL. Missing Count/Rate: It is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. Non Zero Count/Rate: It is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a subfunction nonzeroCount_computation, which is later called under measures_of_counts. Under the hood, it leverage Multivariate Statistical Summary8 of Spark MLlib.","title":"measures_of_counts"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_centraltendency_1","text":"The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark Dataframe with schema \u2013 attribute, mean, median, mode, mode_pct. Mean: It is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Median: It is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Mode: It is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns) Mode Pct: It is defined as % of rows seen with Mode value. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns)","title":"measures_of_centralTendency"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_cardinality","text":"The Measures of Cardinality function provides statistics that are related to unique values seen in an attribute. These statistics are calculated only for discrete columns (categorical + Integer/Long columns). It returns a Spark Dataframe with schema \u2013 attribute, unique_values, IDness. Unique Values: It is defined as distinct value count of a column. It relies on a subfunction uniqueCount_computation for its computation and leverage countDistinct9 functionality of Spark SQL. IDness: It is calculated as Unique Values divided by non-null values seen in a column. Non-null values count is used instead of total count because too many null values can give misleading results even if the column have all unique values (except null). It uses subfunctions - uniqueCount_computation and missingCount_computation.","title":"measures_of_cardinality"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_dispersion","text":"The Measures of Dispersion function provides statistics that describe the spread of a numerical attribute. Alternatively, these statistics are also known as measures of spread. It returns a Spark Dataframe with schema \u2013 attribute, stddev, variance, cov, IQR, range. Standard Deviation (stddev): It measures how concentrated an attribute is around the mean or average and mathematically computed as below. It leverage \u2018stddev\u2019 statistic from summary functionality of Spark SQL. s= X- X2n -1 where: ` `X is an attribute value X is attribute mean n is no. of rows Variance: It is squared value of Standard Deviation. Coefficient of Variance (cov): It is computed as ratio of Standard Deviation & Mean. It leverage \u2018stddev\u2019 and \u2018mean\u2019 statistic from summary functionality of Spark SQL. Interquartile Range (IQR): It describes the difference between the third quartile (75th percentile) and the first quartile (25th percentile), telling us about the range where middle half values are seen. It leverage \u201825%\u2019 and \u201875%\u2019 statistics from summary functionality of Spark SQL. Range: It is simply the difference between the maximum value and the minimum value. It leverage \u2018min\u2019 and \u2018max\u2019 statistics from summary functionality of Spark SQL.","title":"measures_of_dispersion"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_percentiles","text":"The Measures of Percentiles function provides statistics at different percentiles, where Nth percentile can be interpreted as N% of rows have value lesser than or equal to Nth percentile value. It is prominently used to quick detection of skewness or outlier. Alternatively, these statistics are also known as measures of position. These statistics are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max. It leverage \u2018N%\u2019 statistics from summary functionality of Spark SQL where N is 0 for min and 100 for max.","title":"measures_of_percentiles"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_shape","text":"The Measures of Shapes function provides statistics related to the shape of attribute\u2019s distribution. Alternatively, these statistics are also known as measures of moment and are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, skewness, kurtosis. Skewness: It describes how much skewed values are, relative to a perfect bell curve that is observed in normal distribution, and also the direction of skew. If the majority of the values are at the left and the right tail is longer, we say that the distribution is skewed right or positively skewed; if the peak is toward the right and the left tail is longer, we say that the distribution is skewed left or negatively skewed. It leverage skewness10 functionality of Spark SQL. (Excess) Kurtosis: It describes how tall and sharp the central peak is, relative to a perfect bell curve that is observed in normal distribution. The reference standard is a normal distribution, which has a kurtosis of 3. In token of this, often the excess kurtosis is presented: excess kurtosis is simply kurtosis\u22123. Higher (positive) values indicate a higher, sharper peak; lower (negative) values indicate a lower, less distinct peak. It leverage kurtosis11 functionality of Spark SQL.","title":"measures_of_shape"},{"location":"docs/anovos-modules-overview/data-ingest/","text":"Module ANOVOS.data_ingest This module consists of functions to read the dataset as Spark Dataframe, concatenate/join with other functions (if required), and perform some basic ETL actions such as selecting, deleting, renaming and/or recasting columns. List of functions included in this module are: - read_dataset - write_dataset - concatenate_dataset - join_dataset - delete_column - select_column - rename_column - recast_column read_dataset This function reads the input data path and return a Spark Dataframe. Under the hood, this function is based on generic Load functionality of Spark SQL1. It requires following arguments: file_path : file (or directory) path where the input data is saved. File path can be a local path or s3 path (when running with AWS cloud services) file_type : file format of the input data. Currently, we support csv, parquet or avro. Avro data source requires an external package to run, which can be configured with spark-submit options (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs (optional): Rest of the valid configurations can be passed through this argument in a dictionary format. All the key/value pairs written in this argument are passed as options to DataFrameReader, which is created using SparkSession.read. write_dataset This function saves Spark Dataframe in the user provided output path. Like read_dataset, this function is based on generic Save functionality of Spark SQL. It requires following arguments: idf : Spark Dataframe to be saved file_path : file (or directory) path where the output data is to be saved. File path can be a local path or s3 path (when running with AWS cloud services) file_type : file format of the input data. Currently, we support csv, parquet or avro. Avro data source requires an external package to run, which can be configured with spark-submit options (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs (optional): Rest of the valid configuration can be passed through this argument in a dictionary format e.g., repartition, mode, compression, header, delimiter etc. All the key/value pairs written in this argument are passed as options to DataFrameWriter is available using Dataset.write operator. If number of repartitions mentioned through this argument is less than the existing Dataframe partitions, then coalesce operation2 is used instead of repartition operation to make the execution efficient. This is because the coalesce operation doesn\u2019t require any shuffling like repartition which is known to be an expensive step. concatenate_dataset This function combines multiple dataframes into a single dataframe. To make the operation efficient, pairwise concatenation is done on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function is leveraging union3 functionality of Spark SQL. It requires following arguments: *idfs : Varying number of dataframes to be concatenated method_type : index or name. This argument needs to be entered as a keyword argument. \u201cindex\u201d method involves concatenating the dataframes by column index. In case, sequence of column is not fixed among the dataframe, this method should be avoided. \u201cname\u201d method involves concatenating by columns names. 1st dataframe passed under idfs will define the final columns in the concatenated dataframe and it will throw error if any column in 1st dataframe is not available in any of other dataframes. join_dataset This function joins multiple dataframes into a single dataframe by a joining key column. To make the operation efficient, pairwise joining is done on the dataframes, instead of joining one dataframe at a time to the bigger dataframe. This function is leveraging join4 functionality of Spark SQL. It requires following arguments: *idfs : Varying number of all dataframes to be joined join_cols : Key column(s) to join all dataframes together. In case of multiple columns to join, they can be passed in a list format or a single text format where different column names are separated by pipe delimiter \u201c|\u201d join_type : \u201cinner\u201d, \u201cfull\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cleft_semi\u201d, \u201cleft_anti\u201d delete_column This function is used to delete specific columns from the input data. It is executed using drop functionality5 of Spark SQL. It is advisable to use this function if number of columns to delete are lesser than number of columns to select, otherwise it is recommended to use select_column. It requires following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are required to be deleted from the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d print_impact : This argument is to compare number of columns before and after the operation. select_column This function is used to select specific columns from the input data. It is executed using select operation6 of spark dataframe. It is advisable to use this function if number of columns to select are lesser than number of columns to drop, otherwise it is recommended to use delete_column. It requires following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are required to be deleted from the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d print_impact : This argument is to compare number of columns before and after the operation. rename_column This function is used to rename columns of the input data. Multiple columns can be renamed, however, sequence in which they passed as argument is critical and needs to be consistent between list_of_cols and list_of_newcols. It requires following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are required to be renamed in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d list_of_newcols : This argument, in a list format, is used to specify the new column name i.e. first element in list_of_cols will be original column name and corresponding first column in list_of_newcols will be new column name. print_impact : This argument is to compare column names before and after the operation. recast_column This function is used to modify the datatype of columns. Multiple columns can be casted, however, sequence in which they passed as argument is critical and needs to be consistent between list_of_cols and list_of_dtypes. It requires following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are required to be recast in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d list_of_dtypes : This argument, in a list format, is used to specify the datatype i.e. first element in list_of_cols will column name and corresponding element in list_of_dtypes will be new datatype such as float, integer, string, double, decimal etc. (case insensitive). print_impact : This argument is to compare schema before and after the operation.","title":"Data Ingest"},{"location":"docs/anovos-modules-overview/data-ingest/#module-anovosdata_ingest","text":"This module consists of functions to read the dataset as Spark Dataframe, concatenate/join with other functions (if required), and perform some basic ETL actions such as selecting, deleting, renaming and/or recasting columns. List of functions included in this module are: - read_dataset - write_dataset - concatenate_dataset - join_dataset - delete_column - select_column - rename_column - recast_column","title":"Module ANOVOS.data_ingest"},{"location":"docs/anovos-modules-overview/data-ingest/#read_dataset","text":"This function reads the input data path and return a Spark Dataframe. Under the hood, this function is based on generic Load functionality of Spark SQL1. It requires following arguments: file_path : file (or directory) path where the input data is saved. File path can be a local path or s3 path (when running with AWS cloud services) file_type : file format of the input data. Currently, we support csv, parquet or avro. Avro data source requires an external package to run, which can be configured with spark-submit options (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs (optional): Rest of the valid configurations can be passed through this argument in a dictionary format. All the key/value pairs written in this argument are passed as options to DataFrameReader, which is created using SparkSession.read.","title":"read_dataset"},{"location":"docs/anovos-modules-overview/data-ingest/#write_dataset","text":"This function saves Spark Dataframe in the user provided output path. Like read_dataset, this function is based on generic Save functionality of Spark SQL. It requires following arguments: idf : Spark Dataframe to be saved file_path : file (or directory) path where the output data is to be saved. File path can be a local path or s3 path (when running with AWS cloud services) file_type : file format of the input data. Currently, we support csv, parquet or avro. Avro data source requires an external package to run, which can be configured with spark-submit options (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs (optional): Rest of the valid configuration can be passed through this argument in a dictionary format e.g., repartition, mode, compression, header, delimiter etc. All the key/value pairs written in this argument are passed as options to DataFrameWriter is available using Dataset.write operator. If number of repartitions mentioned through this argument is less than the existing Dataframe partitions, then coalesce operation2 is used instead of repartition operation to make the execution efficient. This is because the coalesce operation doesn\u2019t require any shuffling like repartition which is known to be an expensive step.","title":"write_dataset"},{"location":"docs/anovos-modules-overview/data-ingest/#concatenate_dataset","text":"This function combines multiple dataframes into a single dataframe. To make the operation efficient, pairwise concatenation is done on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function is leveraging union3 functionality of Spark SQL. It requires following arguments: *idfs : Varying number of dataframes to be concatenated method_type : index or name. This argument needs to be entered as a keyword argument. \u201cindex\u201d method involves concatenating the dataframes by column index. In case, sequence of column is not fixed among the dataframe, this method should be avoided. \u201cname\u201d method involves concatenating by columns names. 1st dataframe passed under idfs will define the final columns in the concatenated dataframe and it will throw error if any column in 1st dataframe is not available in any of other dataframes.","title":"concatenate_dataset"},{"location":"docs/anovos-modules-overview/data-ingest/#join_dataset","text":"This function joins multiple dataframes into a single dataframe by a joining key column. To make the operation efficient, pairwise joining is done on the dataframes, instead of joining one dataframe at a time to the bigger dataframe. This function is leveraging join4 functionality of Spark SQL. It requires following arguments: *idfs : Varying number of all dataframes to be joined join_cols : Key column(s) to join all dataframes together. In case of multiple columns to join, they can be passed in a list format or a single text format where different column names are separated by pipe delimiter \u201c|\u201d join_type : \u201cinner\u201d, \u201cfull\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cleft_semi\u201d, \u201cleft_anti\u201d","title":"join_dataset"},{"location":"docs/anovos-modules-overview/data-ingest/#delete_column","text":"This function is used to delete specific columns from the input data. It is executed using drop functionality5 of Spark SQL. It is advisable to use this function if number of columns to delete are lesser than number of columns to select, otherwise it is recommended to use select_column. It requires following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are required to be deleted from the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d print_impact : This argument is to compare number of columns before and after the operation.","title":"delete_column"},{"location":"docs/anovos-modules-overview/data-ingest/#select_column","text":"This function is used to select specific columns from the input data. It is executed using select operation6 of spark dataframe. It is advisable to use this function if number of columns to select are lesser than number of columns to drop, otherwise it is recommended to use delete_column. It requires following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are required to be deleted from the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d print_impact : This argument is to compare number of columns before and after the operation.","title":"select_column"},{"location":"docs/anovos-modules-overview/data-ingest/#rename_column","text":"This function is used to rename columns of the input data. Multiple columns can be renamed, however, sequence in which they passed as argument is critical and needs to be consistent between list_of_cols and list_of_newcols. It requires following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are required to be renamed in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d list_of_newcols : This argument, in a list format, is used to specify the new column name i.e. first element in list_of_cols will be original column name and corresponding first column in list_of_newcols will be new column name. print_impact : This argument is to compare column names before and after the operation.","title":"rename_column"},{"location":"docs/anovos-modules-overview/data-ingest/#recast_column","text":"This function is used to modify the datatype of columns. Multiple columns can be casted, however, sequence in which they passed as argument is critical and needs to be consistent between list_of_cols and list_of_dtypes. It requires following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are required to be recast in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d list_of_dtypes : This argument, in a list format, is used to specify the datatype i.e. first element in list_of_cols will column name and corresponding element in list_of_dtypes will be new datatype such as float, integer, string, double, decimal etc. (case insensitive). print_impact : This argument is to compare schema before and after the operation.","title":"recast_column"},{"location":"docs/anovos-modules-overview/overview/","text":"Anovos modules are created in order to reflect the key components of Machine Learning (ML) pipeline. They are also built in a scalable manner using python API of Spark (PySpark) - the distributed computing framework. The key modules included in the alpha release are: Data Ingest : This module is ETL (Extract, transform, load) component of Anovos and helps in loading dataset(s) as Spark Dataframe. It also allows performing some basic preprocessing, such as selecting, deleting, renaming and recasting columns, to ensure that cleaner data is used in downstream data analysis. Data Analyzer : This data analysis module gives 360o view on the ingested data and helps with better understanding of the data quality and the transformations that may be required for the modelling purpose. This module is further divided into 3 submodules targeting specific needs of the data analysis process. a. Statistics Generator : This submodule generates all descriptive statistics related to the ingested data. Descriptive statistics is further broken down into different metric types such as Measures of Counts, Measures of Central Tendency, Measures of Cardinality, Measures of Dispersion (aka Measures of Spread in Statistics), Measures of Percentiles (aka Measures of Position), and Measures of Shape (aka Measures of Moments). b. Quality Checker : This submodule focusses on assessing the data quality at both row and column level. Also, there is an option to fix the identified issues with the right treatment method. The row level quality checks include duplicate detection and null detection (% columns which are missing for a row). On the other hand, the column level quality checks include outlier detection, null detection (% rows which are missing for a column), biasedness detection (checking if a column is biased towards one specific value), cardinality detection (checking if a categorical/discrete column have very high no. of unique values) and invalid entries detection which checks for suspicious patterns in the column values. c. Association Evaluator : This submodule focuses on understanding the interaction between different attributes (correlation, variable clustering) and/or the relationship between an attribute & the binary target variable (Information Gain, Information Value). Data Drift : In ML context, Data Drift is the change in distribution of the baseline dataset on which the model is trained (source distribution) and the ingested data (target distribution) on which prediction is to be made. Data Drift is one of the top reasons behind the poor performance of ML models over time. This module ensures the stability of the ingested dataset over time by analysing it with the baseline dataset (via computing drift statistics) and/or with historically ingested datasets (via computing stability index \u2013 currently supports only numerical features), if available. Identifying the data drift at early stage enables data scientists to be proactive and fix the root cause. Data Transformer : In the alpha release, data transformer module only includes some basic pre-processing functions such binning, encoding, to name a few. These functions were required to support computations of the above key modules. However, in the future releases, more exhaustive set of transformations can be expected. Data Report : This module is visualization component of Anovos. All the analysis done on the key modules are visualized via a html report for the user to get well-rounded understanding of the ingested dataset. The report contains executive summary, wiki for data dictionary & metric dictionary, a tab corresponding to key modules demonstrating the output. Note: Upcoming Modules - Feature Wiki, Feature store, Auto ML, ML Flow Integration","title":"Overview"},{"location":"docs/anovos-modules-overview/quality-checker/","text":"Module ANOVOS.quality_checker This submodule focus on assessing the data quality at both row level and column level and also provides an appropriate treatment option to fix those quality issues. Columns which are subjected to these analysis can be controlled by right combination of arguments - list_of_cols and drop_cols. All functions have following common arguments: idf: Input dataframe list_of_cols: This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols: This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. print_impact: This argument is to print out the statistics. At row level, the following checks are done: duplicate_detection nullRows_detection At column level, the following checks are done: nullColumns_detection outlier_detection IDness_detection biasedness_detection invalidEntries_detection duplicate_detection As the name suggests, this function detects duplication in the input dataset. This means, for a pair of duplicate rows, the values in each column coincide. Duplication check is confined to the list of columns passed in the arguments. As the part of treatment, duplicated rows are removed. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after deduplication (if treated) and 2nd dataframe is of schema \u2013 metric, value and contains total number of rows and number of unique rows. idf list_of_cols drop_cols treatment: This argument takes Boolean type input \u2013 True or False. If true, duplicate rows are removed from the input dataset. print_impact nullRows_detection This function inspects the row quality and computes number of columns which are missing for a row. This metric is further aggregated to check how many columns are missing for how many rows (also at % level). Intuition is if too many columns are missing for a row, removing it from the modelling may give better results than relying on its imputed values. Therefore as the part of treatment, rows with missing columns above the specified threshold are removed. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after filtering rows with high number of missing columns (if treated) and 2nd dataframe is of schema \u2013 null_cols_count, row_count, row_pct, flagged. |null_cols_count|row_count|row_pct|flagged| +---------------+---------+-------+-------+ | 5| 11| 3.0E-4| 0| | 7| 1306| 0.0401| 1| Interpretation: 1306 rows (4.01% of total rows) have 7 missing columns and flagged for removal because null_cols_count is above the threshold. idf list_of_cols drop_cols treatment: This argument takes Boolean type input \u2013 True or False. If true, rows with high null columns (defined by treatment_threshold argument) are removed from the input dataset. treatment_threshold: This argument takes value between 0 to 1 with default 0.8, which means 80% of columns allowed to be Null per row. If it is more than the threshold, then it is flagged and if treatment is True, then affected rows are removed. If threshold is 0, it means, rows with any missing value will be flagged. If threshold is 1, it means rows with all missing value will be flagged. print_impact nullColumns_detection This function inspects the column quality and computes number of rows which are missing for a column. This function also leverages statistics which were computed as the part of the State Generator module so that statistics are not computed twice if already available. As part of treatments, it currently supports 3 methods \u2013 Mean Median Mode (MMM), row_removal or column_removal (more methods to be added soon). MMM replaces null value by the measure of central tendency (mode for categorical features and mean/median for numerical features). row_removal removes all rows with any missing value (output of this treatment is same as nullRows_detection with treatment_threshold of 0). column_removal remove a column if %rows with missing value is above treatment_threshold. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after imputation (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, missing_count, missing_pct. idf list_of_cols : \"all\" can be passed to include all (non-array) columns for analysis. \"missing\" (default) can be passed to include only those columns with missing values. One of the use cases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, missing values are treated as per treatment_method argument treatment_method : MMM, row_removal or column_removal treatment_configs : This argument takes input in dictionary format with keys \u2013 \u2018treatment_threshold\u2019 for column_removal treatment, or all arguments corresponding to imputation_MMM function. stats_missing : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_counts function of stats generator stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator print_impact outlier_detection In Machine Learning, outlier detection is the identification of values that deviates drastically from the rest of the attribute values. An outlier may be caused simply by chance, measurement error or inherent heavy-tailed distribution. This function identify extreme values in both directions (or any direction provided by the user via detection_side argument). Outlier is identified by 3 different methodologies and tagged an outlier only if it is validated by at least 2 methodologies (can be changed by the user via min_validation under detection_configs argument). Percentile Method: In this methodology, a value higher than a certain (default 95th) percentile value is considered as an outlier. Similarly, a value lower than a certain (default 5th) percentile value is considered as an outlier. Standard Deviation Method: In this methodology, if a value is certain number of standard deviations (default 3) away from the mean, then it is identified as an outlier. Interquartile Range (IQR) Method: A value which is below Q1 \u2013 1.5 IQR or above Q3 + 1.5 IQR are identified as outliers, where Q1 is first quantile/25th percentile, Q3 is third quantile/75th percentile and IQR is difference between third quantile & first quantile. This function also leverages statistics which were computed as the part of the State Generator module so that statistics are not computed twice if already available. As part of treatments available, outlier values can be replaced by null so that it can be imputed by a reliable imputation methodology (null_replacement). It can also be replaced by maximum or minimum permissible by above methodologies (value_replacement). Lastly, rows can be removed if it is identified with any outlier (row_removal). This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating outlier (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, lower_outliers, upper_outliers. If outliers are checked only for upper end, then lower_outliers column will be shown all zero. Similarly if checked only for lower end, then upper_outliers will be zero for all attributes. idf list_of_cols : Any attribute with single value or all null values are not subjected to outlier detection even if it is selected under this argument. drop_cols detection_side : upper, lower, both detection_configs : This argument takes input in dictionary format with keys (representing upper and lower bound for different outlier identification methodologies) - pctile_lower (default 0.05), pctile_upper (default 0.95), stdev_lower (default 3.0), stdev_upper (default 3.0), IQR_lower (default 1.5), IQR_upper (default 1.5), min_validation (default 2) treatment : This argument takes Boolean type input \u2013 True or False. If true, specified treatment method is applied. treatment_method : null_replacement, row_removal, value_replacement pre_existing_model : This argument takes Boolean type input \u2013 True or False. True if the file with upper/lower permissible values exists already, False Otherwise. model_path : If pre_existing_model is True, this argument is path for pre-saved model file. If pre_existing_model is False, this field can be used for saving the model file. Default NA means there is neither pre-saved model file nor there is a need to save one. output_mode : replace or append. \u201creplace\u201d option replaces original columns with treated column, whereas \u201cappend\u201d option append treated column to the input dataset. All treated columns are appended with the naming convention - \"{original.column.name}_outliered\". stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator print_impact IDness_detection IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for categorical features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, , unique_values, IDness. idf list_of_cols drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, columns above IDness threshold are removed. treatment_threshold : This argument takes value between 0 to 1 with default 1.0. stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator. print_impact biasedness_detection This function flags column if they are biased or skewed towards one specific value and is equivalent to mode_pct computation from Measures of Central Tendency i.e. number of rows with mode value (most frequently seen value) divided by number of non-null values. It varies between 0 to 100% where biasedness of 100% means there is only a single value (other than null). The function flags a column if its biasedness is above a certain threshold. Such columns can be deleted from the modelling analysis, if required. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high biased columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, mode, mode_pct. idf list_of_cols drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, columns above biasedness threshold are removed. treatment_threshold : This argument takes value between 0 to 1 with default 1.0. stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator. print_impact 4.3.7 invalidEntries_detection This function checks for certain suspicious patterns in attributes\u2019 values. These suspicious values can be replaced as null and treated as missing. Patterns that are considered for this quality check: Missing Values: The function checks for all text strings which directly or indirectly indicate the missing value in an attribute. Currently, we check the following string values - '', ' ', 'nan', 'null', 'na', 'inf', 'n/a', 'not defined', 'none', 'undefined', 'blank'. The function also check for special characters such as ?, *, to name a few. Repetitive Characters: Certain attributes\u2019 values with repetitive characters may be default value or system error, rather than being a legit value etc xx, zzzzz, 99999 etc. Such values are flagged for the user to take an appropriate action. There may be certain false positive which are legit values. Consecutive Characters: Similar to repetitive characters, consecutive characters (at least 3 characters long) such as abc, 1234 etc may not be legit values, and hence flagged. There may be certain false positive which are legit values. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after replacing flagged values as null (or the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, invalid_entries, invalid_count, invalid_pct. All potential invalid values (separated by delimiter pipe \u201c|\u201d) are shown under invalid_entries column. Total number of rows impacted by these entries for each attribute is shown under invalid_count. invalid_pct is invalid_count divided by number of rows idf list_of_cols drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, columns above biasedness threshold are removed. output_mode : replace or append. \u201creplace\u201d option replaces original columns with treated column, whereas \u201cappend\u201d option append treated column to the input dataset. All treated columns are appended with the naming convention - \"{original.column.name}_cleaned\". print_impact","title":"Quality Checker"},{"location":"docs/anovos-modules-overview/quality-checker/#module-anovosquality_checker","text":"This submodule focus on assessing the data quality at both row level and column level and also provides an appropriate treatment option to fix those quality issues. Columns which are subjected to these analysis can be controlled by right combination of arguments - list_of_cols and drop_cols. All functions have following common arguments: idf: Input dataframe list_of_cols: This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols: This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. print_impact: This argument is to print out the statistics. At row level, the following checks are done: duplicate_detection nullRows_detection At column level, the following checks are done: nullColumns_detection outlier_detection IDness_detection biasedness_detection invalidEntries_detection","title":"Module ANOVOS.quality_checker"},{"location":"docs/anovos-modules-overview/quality-checker/#duplicate_detection","text":"As the name suggests, this function detects duplication in the input dataset. This means, for a pair of duplicate rows, the values in each column coincide. Duplication check is confined to the list of columns passed in the arguments. As the part of treatment, duplicated rows are removed. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after deduplication (if treated) and 2nd dataframe is of schema \u2013 metric, value and contains total number of rows and number of unique rows. idf list_of_cols drop_cols treatment: This argument takes Boolean type input \u2013 True or False. If true, duplicate rows are removed from the input dataset. print_impact","title":"duplicate_detection"},{"location":"docs/anovos-modules-overview/quality-checker/#nullrows_detection","text":"This function inspects the row quality and computes number of columns which are missing for a row. This metric is further aggregated to check how many columns are missing for how many rows (also at % level). Intuition is if too many columns are missing for a row, removing it from the modelling may give better results than relying on its imputed values. Therefore as the part of treatment, rows with missing columns above the specified threshold are removed. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after filtering rows with high number of missing columns (if treated) and 2nd dataframe is of schema \u2013 null_cols_count, row_count, row_pct, flagged. |null_cols_count|row_count|row_pct|flagged| +---------------+---------+-------+-------+ | 5| 11| 3.0E-4| 0| | 7| 1306| 0.0401| 1| Interpretation: 1306 rows (4.01% of total rows) have 7 missing columns and flagged for removal because null_cols_count is above the threshold. idf list_of_cols drop_cols treatment: This argument takes Boolean type input \u2013 True or False. If true, rows with high null columns (defined by treatment_threshold argument) are removed from the input dataset. treatment_threshold: This argument takes value between 0 to 1 with default 0.8, which means 80% of columns allowed to be Null per row. If it is more than the threshold, then it is flagged and if treatment is True, then affected rows are removed. If threshold is 0, it means, rows with any missing value will be flagged. If threshold is 1, it means rows with all missing value will be flagged. print_impact","title":"nullRows_detection"},{"location":"docs/anovos-modules-overview/quality-checker/#nullcolumns_detection","text":"This function inspects the column quality and computes number of rows which are missing for a column. This function also leverages statistics which were computed as the part of the State Generator module so that statistics are not computed twice if already available. As part of treatments, it currently supports 3 methods \u2013 Mean Median Mode (MMM), row_removal or column_removal (more methods to be added soon). MMM replaces null value by the measure of central tendency (mode for categorical features and mean/median for numerical features). row_removal removes all rows with any missing value (output of this treatment is same as nullRows_detection with treatment_threshold of 0). column_removal remove a column if %rows with missing value is above treatment_threshold. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after imputation (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, missing_count, missing_pct. idf list_of_cols : \"all\" can be passed to include all (non-array) columns for analysis. \"missing\" (default) can be passed to include only those columns with missing values. One of the use cases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, missing values are treated as per treatment_method argument treatment_method : MMM, row_removal or column_removal treatment_configs : This argument takes input in dictionary format with keys \u2013 \u2018treatment_threshold\u2019 for column_removal treatment, or all arguments corresponding to imputation_MMM function. stats_missing : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_counts function of stats generator stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator print_impact","title":"nullColumns_detection"},{"location":"docs/anovos-modules-overview/quality-checker/#outlier_detection","text":"In Machine Learning, outlier detection is the identification of values that deviates drastically from the rest of the attribute values. An outlier may be caused simply by chance, measurement error or inherent heavy-tailed distribution. This function identify extreme values in both directions (or any direction provided by the user via detection_side argument). Outlier is identified by 3 different methodologies and tagged an outlier only if it is validated by at least 2 methodologies (can be changed by the user via min_validation under detection_configs argument). Percentile Method: In this methodology, a value higher than a certain (default 95th) percentile value is considered as an outlier. Similarly, a value lower than a certain (default 5th) percentile value is considered as an outlier. Standard Deviation Method: In this methodology, if a value is certain number of standard deviations (default 3) away from the mean, then it is identified as an outlier. Interquartile Range (IQR) Method: A value which is below Q1 \u2013 1.5 IQR or above Q3 + 1.5 IQR are identified as outliers, where Q1 is first quantile/25th percentile, Q3 is third quantile/75th percentile and IQR is difference between third quantile & first quantile. This function also leverages statistics which were computed as the part of the State Generator module so that statistics are not computed twice if already available. As part of treatments available, outlier values can be replaced by null so that it can be imputed by a reliable imputation methodology (null_replacement). It can also be replaced by maximum or minimum permissible by above methodologies (value_replacement). Lastly, rows can be removed if it is identified with any outlier (row_removal). This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating outlier (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, lower_outliers, upper_outliers. If outliers are checked only for upper end, then lower_outliers column will be shown all zero. Similarly if checked only for lower end, then upper_outliers will be zero for all attributes. idf list_of_cols : Any attribute with single value or all null values are not subjected to outlier detection even if it is selected under this argument. drop_cols detection_side : upper, lower, both detection_configs : This argument takes input in dictionary format with keys (representing upper and lower bound for different outlier identification methodologies) - pctile_lower (default 0.05), pctile_upper (default 0.95), stdev_lower (default 3.0), stdev_upper (default 3.0), IQR_lower (default 1.5), IQR_upper (default 1.5), min_validation (default 2) treatment : This argument takes Boolean type input \u2013 True or False. If true, specified treatment method is applied. treatment_method : null_replacement, row_removal, value_replacement pre_existing_model : This argument takes Boolean type input \u2013 True or False. True if the file with upper/lower permissible values exists already, False Otherwise. model_path : If pre_existing_model is True, this argument is path for pre-saved model file. If pre_existing_model is False, this field can be used for saving the model file. Default NA means there is neither pre-saved model file nor there is a need to save one. output_mode : replace or append. \u201creplace\u201d option replaces original columns with treated column, whereas \u201cappend\u201d option append treated column to the input dataset. All treated columns are appended with the naming convention - \"{original.column.name}_outliered\". stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator print_impact","title":"outlier_detection"},{"location":"docs/anovos-modules-overview/quality-checker/#idness_detection","text":"IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for categorical features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, , unique_values, IDness. idf list_of_cols drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, columns above IDness threshold are removed. treatment_threshold : This argument takes value between 0 to 1 with default 1.0. stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator. print_impact","title":"IDness_detection"},{"location":"docs/anovos-modules-overview/quality-checker/#biasedness_detection","text":"This function flags column if they are biased or skewed towards one specific value and is equivalent to mode_pct computation from Measures of Central Tendency i.e. number of rows with mode value (most frequently seen value) divided by number of non-null values. It varies between 0 to 100% where biasedness of 100% means there is only a single value (other than null). The function flags a column if its biasedness is above a certain threshold. Such columns can be deleted from the modelling analysis, if required. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high biased columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, mode, mode_pct. idf list_of_cols drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, columns above biasedness threshold are removed. treatment_threshold : This argument takes value between 0 to 1 with default 1.0. stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator. print_impact 4.3.7 invalidEntries_detection This function checks for certain suspicious patterns in attributes\u2019 values. These suspicious values can be replaced as null and treated as missing. Patterns that are considered for this quality check: Missing Values: The function checks for all text strings which directly or indirectly indicate the missing value in an attribute. Currently, we check the following string values - '', ' ', 'nan', 'null', 'na', 'inf', 'n/a', 'not defined', 'none', 'undefined', 'blank'. The function also check for special characters such as ?, *, to name a few. Repetitive Characters: Certain attributes\u2019 values with repetitive characters may be default value or system error, rather than being a legit value etc xx, zzzzz, 99999 etc. Such values are flagged for the user to take an appropriate action. There may be certain false positive which are legit values. Consecutive Characters: Similar to repetitive characters, consecutive characters (at least 3 characters long) such as abc, 1234 etc may not be legit values, and hence flagged. There may be certain false positive which are legit values. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after replacing flagged values as null (or the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, invalid_entries, invalid_count, invalid_pct. All potential invalid values (separated by delimiter pipe \u201c|\u201d) are shown under invalid_entries column. Total number of rows impacted by these entries for each attribute is shown under invalid_count. invalid_pct is invalid_count divided by number of rows idf list_of_cols drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, columns above biasedness threshold are removed. output_mode : replace or append. \u201creplace\u201d option replaces original columns with treated column, whereas \u201cappend\u201d option append treated column to the input dataset. All treated columns are appended with the naming convention - \"{original.column.name}_cleaned\". print_impact","title":"biasedness_detection"},{"location":"docs/anovos-potential-workflow/","text":"Anovos is designed to support the following end to end machine learning workflow. It can be used to support an organization\u2019s end to end machine learning workflow in two ways, either as a full execution by Anovos or use as per the need of current pipeline set up. For example an organization might have an end to end workflow without some key components provided by Anovos. Those features can be incorporated by simply calling the respective API functions out of Anovos. The following workflow diagram shows the potential ways to use Anovos in an end to end workflow settings.","title":"Anovos Potential Workflow"},{"location":"docs/anovos-roadmap/","text":"Anovosis built out and been released as an open source based on the experience we gained in using massive data sets to produce predictive features. At Mobilewalla1, we process tera bytes of Mobile engagement signals daily to mine the consumer behaviour. Those features were then used in building distributed machine learning models to solve the respective business problems. In this journey, we faced lots of challenges by not having a comprehensive and scalable libraries. After realizing the unavailability of such libraries we design and implemented an open source library (ANOVOS) for every Data Scientists\u2019 use. The Roadmap As can be seen above, Anovos will be fully functional in 3 major releases namely alpha, beta and V1.0 respectively. In the alpha release of Anovos will have the basic Data ingestion and comprehensive data analysis functionalities, in addition to the data pre-processing & cleaning mechanisms. Alpha release of Anovos also will have some key differentiating functionalities such as data drift and stability computations which in turn are very crucial in deciding the need for model refresh/tweaking options. Another key feature of Anovos is that cool visualization component which is very dynamic and can be configured as per every data ingestion pipelines. Every data metric computed out of Anovos ingestion process can be visualized and can be used for CXO level decision making. Beta Release In the Beta release of Anovos, it will support ingesting from other cloud service providers like MS Azure as well will have mechanisms to read/write different file formats such as avro & nested Json. Further it will also support to ingest various data types (see figure for the details). Key differentiating functionality of beta release would be the \u201cFeature wiki\u201d for Data Scientists/end users to resolve their cold-start problems, which will immensely reduce their literature search time. In addition, beta release will also have another key functionality to explain the each attributes\u2019 contribution in feature building which we call as \u201cAttribute-> Feature\u201d mapper Full Release The complete functional Anovos will be released in March 2022 with the functionalities to support the end to end machine learning workflow. It will have the feasibility to store the generated features in an open source feature stores like Feast. Also it will support running open source based Auto ML models and ML workflow integration. Another critical feature which would be available in the March 2022 release is, to have a mechanism to explain the model behaviour by having the respective shapely values.","title":"Anovos Product Roadmap"},{"location":"docs/anovos-roadmap/#the-roadmap","text":"As can be seen above, Anovos will be fully functional in 3 major releases namely alpha, beta and V1.0 respectively. In the alpha release of Anovos will have the basic Data ingestion and comprehensive data analysis functionalities, in addition to the data pre-processing & cleaning mechanisms. Alpha release of Anovos also will have some key differentiating functionalities such as data drift and stability computations which in turn are very crucial in deciding the need for model refresh/tweaking options. Another key feature of Anovos is that cool visualization component which is very dynamic and can be configured as per every data ingestion pipelines. Every data metric computed out of Anovos ingestion process can be visualized and can be used for CXO level decision making.","title":"The Roadmap"},{"location":"docs/anovos-roadmap/#beta-release","text":"In the Beta release of Anovos, it will support ingesting from other cloud service providers like MS Azure as well will have mechanisms to read/write different file formats such as avro & nested Json. Further it will also support to ingest various data types (see figure for the details). Key differentiating functionality of beta release would be the \u201cFeature wiki\u201d for Data Scientists/end users to resolve their cold-start problems, which will immensely reduce their literature search time. In addition, beta release will also have another key functionality to explain the each attributes\u2019 contribution in feature building which we call as \u201cAttribute-> Feature\u201d mapper","title":"Beta Release"},{"location":"docs/anovos-roadmap/#full-release","text":"The complete functional Anovos will be released in March 2022 with the functionalities to support the end to end machine learning workflow. It will have the feasibility to store the generated features in an open source feature stores like Feast. Also it will support running open source based Auto ML models and ML workflow integration. Another critical feature which would be available in the March 2022 release is, to have a mechanism to explain the model behaviour by having the respective shapely values.","title":"Full Release"},{"location":"getting-started/","text":"Steps to Install ANOVOS Installation via public registry To install ANOVOS package via pypi, please execute the following commands: pip3 install anovos Installation via Git To install ANOVOS package via git, please execute the following commands: pip3 install \"git+https://github.com/anovos/anovos.git\" Get Started Once installed, packages can be imported and the required functionality can be called in the user flow in any application or notebook. Refer to the following links to get started : quick-start guide example notebooks documentation Steps to Run ANOVOS using spark-submit After checking out via Git clone, please follow the below instructions to run the E2E ML Anovos Package on the sample income dataset: First execute the following command to clean folder, build the latest modules: make clean build There are 2 ways to run using spark-submit after this: Follow A, if you have a working environment already and would just like to use the same configs. (Note: version dependencies are to be ensured by user) Follow B, if you want to run via dockers A. Running via User's local environment Check the pre-requisites - ANOVOS requires Spark (2.4.x), Python (3.7.*), Java(8). Check version using the following commands: spark-submit --version python --version java -version Set environment variables - $JAVA_HOME , $SPARK_HOME , $PYSPARK_PYTHON , and $PATH Ensure spark-submit and pyspark is working without any issues. Execute the following commands to run the end to end pipeline: cd dist/ nohup ./spark-submit.sh > run.txt & Check result of end to end run tail -f run.txt B. Running via Dockers Note: Kindly ensure the machine has ~15 GB free space atleast when running using Dockers Install docker on your machine (https://docs.docker.com/get-docker/) Set docker settings to use atleast 8GB memory and 4+ cores. Below image shows setting docker settings on Docker Desktop: Ensure dockers is successfully installed by executing the following commands to check docker image and docker container respectively: docker image ls docker ps Create docker image with the following command: (Note: Step #1 should have copied a \"Dockerfile\" to the directory the following command is being executed in) docker build -t mw_ds_feature_machine:0.1 . Check if docker image was successfully created: docker image ls Run the docker container using the following command: docker run -t -i -v $(PWD):/temp mw_ds_feature_machine:0.1 Check if docker container is successfully running: docker ps -a To explore the generated output folder, execute the following commands: docker exec -it <container_id> bash Once run has completed please exit the docker run by sending SIGINT - ^C (CTRL + C) Running on custom dataset The above E2E ANOVOS package run executed for the sample income dataset can be customized and run for any user dataset. This can be done configuring the config/config.yaml file and by defining the flow of run in src/main.py file. ANOVOS Package Documentation Please find the detailed documentation of ANOVOS package here. Reference Links Setting up pyspark and installing on Windows https://towardsdatascience.com/installing-apache-pyspark-on-windows-10-f5f0c506bea1","title":"Getting Started"},{"location":"getting-started/#steps-to-install-anovos","text":"","title":"Steps to Install ANOVOS"},{"location":"getting-started/#installation-via-public-registry","text":"To install ANOVOS package via pypi, please execute the following commands: pip3 install anovos","title":"Installation via public registry"},{"location":"getting-started/#installation-via-git","text":"To install ANOVOS package via git, please execute the following commands: pip3 install \"git+https://github.com/anovos/anovos.git\"","title":"Installation via Git"},{"location":"getting-started/#get-started","text":"Once installed, packages can be imported and the required functionality can be called in the user flow in any application or notebook. Refer to the following links to get started : quick-start guide example notebooks documentation","title":"Get Started"},{"location":"getting-started/#steps-to-run-anovos-using-spark-submit","text":"After checking out via Git clone, please follow the below instructions to run the E2E ML Anovos Package on the sample income dataset: First execute the following command to clean folder, build the latest modules: make clean build There are 2 ways to run using spark-submit after this: Follow A, if you have a working environment already and would just like to use the same configs. (Note: version dependencies are to be ensured by user) Follow B, if you want to run via dockers","title":"Steps to Run ANOVOS using spark-submit"},{"location":"getting-started/#a-running-via-users-local-environment","text":"Check the pre-requisites - ANOVOS requires Spark (2.4.x), Python (3.7.*), Java(8). Check version using the following commands: spark-submit --version python --version java -version Set environment variables - $JAVA_HOME , $SPARK_HOME , $PYSPARK_PYTHON , and $PATH Ensure spark-submit and pyspark is working without any issues. Execute the following commands to run the end to end pipeline: cd dist/ nohup ./spark-submit.sh > run.txt & Check result of end to end run tail -f run.txt","title":"A. Running via User's local environment"},{"location":"getting-started/#b-running-via-dockers","text":"Note: Kindly ensure the machine has ~15 GB free space atleast when running using Dockers Install docker on your machine (https://docs.docker.com/get-docker/) Set docker settings to use atleast 8GB memory and 4+ cores. Below image shows setting docker settings on Docker Desktop: Ensure dockers is successfully installed by executing the following commands to check docker image and docker container respectively: docker image ls docker ps Create docker image with the following command: (Note: Step #1 should have copied a \"Dockerfile\" to the directory the following command is being executed in) docker build -t mw_ds_feature_machine:0.1 . Check if docker image was successfully created: docker image ls Run the docker container using the following command: docker run -t -i -v $(PWD):/temp mw_ds_feature_machine:0.1 Check if docker container is successfully running: docker ps -a To explore the generated output folder, execute the following commands: docker exec -it <container_id> bash Once run has completed please exit the docker run by sending SIGINT - ^C (CTRL + C)","title":"B. Running via Dockers"},{"location":"getting-started/#running-on-custom-dataset","text":"The above E2E ANOVOS package run executed for the sample income dataset can be customized and run for any user dataset. This can be done configuring the config/config.yaml file and by defining the flow of run in src/main.py file.","title":"Running on custom dataset"},{"location":"getting-started/#anovos-package-documentation","text":"Please find the detailed documentation of ANOVOS package here.","title":"ANOVOS Package Documentation"},{"location":"getting-started/#reference-links","text":"Setting up pyspark and installing on Windows https://towardsdatascience.com/installing-apache-pyspark-on-windows-10-f5f0c506bea1","title":"Reference Links"}]}