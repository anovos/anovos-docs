{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Anovos Docs Everything you need to know about Anovos and its major component microservices. \u200b What is Anovos? Anovos is a an open source project, built by data scientist for the data science community, that brings automation to the feature engineering process. By rethinking ingestion and transformation, and including deeper analytics, drift identification, and stability analysis, Anovos improves productivity and helps data scientists build more resilient, higher performing models. Interested in contributing? Check out our Contributors' Page . What's Powering Anovos? Anovos is built on a curated collection of open source libraries, including: Apache Spark Pandas scripy scikit-learn oblib boto3 pyarrow sparkpickle s3path statsmodels pybind11 popmon seaborn varclushi pytest datapane Get Help Need a little help? We're here! Check out the FAQs - When there are questions, we document the answers. Join the MLOps Commuhity . Look for the #oss-anovos channel. Submit an issue on Github .","title":"Home"},{"location":"#the-anovos-docs","text":"Everything you need to know about Anovos and its major component microservices. \u200b","title":"The Anovos Docs"},{"location":"#what-is-anovos","text":"Anovos is a an open source project, built by data scientist for the data science community, that brings automation to the feature engineering process. By rethinking ingestion and transformation, and including deeper analytics, drift identification, and stability analysis, Anovos improves productivity and helps data scientists build more resilient, higher performing models. Interested in contributing? Check out our Contributors' Page .","title":"What is Anovos?"},{"location":"#whats-powering-anovos","text":"Anovos is built on a curated collection of open source libraries, including: Apache Spark Pandas scripy scikit-learn oblib boto3 pyarrow sparkpickle s3path statsmodels pybind11 popmon seaborn varclushi pytest datapane","title":"What's Powering Anovos?"},{"location":"#get-help","text":"Need a little help? We're here! Check out the FAQs - When there are questions, we document the answers. Join the MLOps Commuhity . Look for the #oss-anovos channel. Submit an issue on Github .","title":"Get Help"},{"location":"about/","text":"Introducing Anovos Data Science teams spend up to 80% of their time on feature engineering and still end up building models with poor resilience. Anovos seeks to address both these issues (lack of modeler productivity and insufficient model resilience) by enabling data scientists to understand the stability properties of the data they work with and then creating a transformation methodology that allows the building of features anchored by stable data, which in turn produce resilient models that break less often when deployed. Unlike current feature engineering workflows, which are ad hoc, error-prone, and modeler-driven, Anovos seeks to inject process-driven efficiency into feature creation based on innovations in understanding the stability of data and how data items impact the features they anchor. With Anovos, ML models will be more consistent, more accurate, and deliver results faster. The process of building models is more automated and procedural, saving time and decreasing cost. The Team Behind Anovos The data scientists at Mobilewalla built Anovos, a team of highly talented and experienced data scientists who have years of experience in machine learning (ML) and AI and are applying ML techniques to some of the most extensive consumer data sets available. This experience has brought to light an internal need to create tools to simplify and speed up the feature engineering process, increasing efficiency. They've been able to replicate and now open source these tools, bringing our learnings to the community. The Mobilewalla Feature Mart currently offers teams supplemental data to make their ML models more accurate and predictive. Anovos, a set of open source libraries, enables them to take that data, enhancing their internal data sets and making their modeling more systematic and procedural, creating cost and time efficiencies, and building more predictive models.","title":"About"},{"location":"about/#introducing-anovos","text":"Data Science teams spend up to 80% of their time on feature engineering and still end up building models with poor resilience. Anovos seeks to address both these issues (lack of modeler productivity and insufficient model resilience) by enabling data scientists to understand the stability properties of the data they work with and then creating a transformation methodology that allows the building of features anchored by stable data, which in turn produce resilient models that break less often when deployed. Unlike current feature engineering workflows, which are ad hoc, error-prone, and modeler-driven, Anovos seeks to inject process-driven efficiency into feature creation based on innovations in understanding the stability of data and how data items impact the features they anchor. With Anovos, ML models will be more consistent, more accurate, and deliver results faster. The process of building models is more automated and procedural, saving time and decreasing cost.","title":"Introducing Anovos"},{"location":"about/#the-team-behind-anovos","text":"The data scientists at Mobilewalla built Anovos, a team of highly talented and experienced data scientists who have years of experience in machine learning (ML) and AI and are applying ML techniques to some of the most extensive consumer data sets available. This experience has brought to light an internal need to create tools to simplify and speed up the feature engineering process, increasing efficiency. They've been able to replicate and now open source these tools, bringing our learnings to the community. The Mobilewalla Feature Mart currently offers teams supplemental data to make their ML models more accurate and predictive. Anovos, a set of open source libraries, enables them to take that data, enhancing their internal data sets and making their modeling more systematic and procedural, creating cost and time efficiencies, and building more predictive models.","title":"The Team Behind Anovos"},{"location":"community/code-of-conduct/","text":"Contributor Covenant Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at coc@anovos.ai. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of Conduct"},{"location":"community/code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"community/code-of-conduct/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"community/code-of-conduct/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"community/code-of-conduct/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"community/code-of-conduct/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"community/code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at coc@anovos.ai. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"community/code-of-conduct/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"Enforcement Guidelines"},{"location":"community/code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"community/communication/","text":"There are different ways and channels that you can use as a community member or maintainer to raise questions, collaborate or to have some fun! Slack The community has a channel on the ML-Ops Community Slack called #oss-anovos. Feel free to join us. This channel is the perfect place to have your first point of contact with the community. The maintainers and some contributors hang out on it. GitHub Issues The Anovos repository has issues enabled. If you need some help or have questions or ideas you can open an issue in this repository. If you have any questions about raising an issue, feel free to drop a message on Slack. Contributors Mailing List If you want to follow discussions about the Anovos development or you want to become a contributor there is a mailing list about that. This is not right place for general questions related to how Anovos works. The focus is 100% on ongoing or future development. Google Group: anovos-contributors anovos-contributors@googlegroups.com We have a monthly meeting to sync ongoing work or discuss future development efforts around the various components. You will be added to it automatically when joining the mailing list. There is an agenda available via Google Docs; feel free to add your item in there before the meeting.","title":"Communication"},{"location":"community/communication/#slack","text":"The community has a channel on the ML-Ops Community Slack called #oss-anovos. Feel free to join us. This channel is the perfect place to have your first point of contact with the community. The maintainers and some contributors hang out on it.","title":"Slack"},{"location":"community/communication/#github-issues","text":"The Anovos repository has issues enabled. If you need some help or have questions or ideas you can open an issue in this repository. If you have any questions about raising an issue, feel free to drop a message on Slack.","title":"GitHub Issues"},{"location":"community/communication/#contributors-mailing-list","text":"If you want to follow discussions about the Anovos development or you want to become a contributor there is a mailing list about that. This is not right place for general questions related to how Anovos works. The focus is 100% on ongoing or future development. Google Group: anovos-contributors anovos-contributors@googlegroups.com We have a monthly meeting to sync ongoing work or discuss future development efforts around the various components. You will be added to it automatically when joining the mailing list. There is an agenda available via Google Docs; feel free to add your item in there before the meeting.","title":"Contributors Mailing List"},{"location":"community/contributing/","text":"Welcome to the Anovos Community! We're excited to bring the immense experience of the Mobilewalla team to the data science community, and we'd love to have you join us! Here you'll find all the details you need to get started as an Anovos contributor. Getting Started with Anovos Anovos is an open source project that brings automation to the feature engineering process. To get Anovos up and running on your local machine, follow the Getting Started Guide . The Anovos Github Organization contains all repos, sample data, and notebooks you need. How To Get Involved We love your input! We want to make contributing to this project as easy and transparent as possible, whether it's: Reporting a bug Discussing the current state of the code Submitting a fix Proposing new features Testing out new features Contributing to the docs Giving talks about Anovos at Meetups, conferences, and webinars For the latest information about interacting with the project maintainers and the broader community, please visit the COMMUNICATION.md file in Github. Contribute to Anovos Pull requests are the best way to propose changes to the codebase. We use GitHub flow, and everything happens through pull requests. We welcome your pull requests; to make it simple, here are the steps to contribute: Fork the repo you're updating and create your branch from main. If you've added code that should be tested, add tests. If you've changed APIs, update the documentation. Ensure the test suite passes. Issue that pull request! Any contributions you make will be under the Apache Software License. In short, when you submit code changes, your submissions are understood to be under the same Apache License that covers the project. Feel free to contact the maintainers if that's a concern. Write Thorough Commit Messages Help reviewers know what you're contributing by writing good commit messages. The first line of the commit message is the subject; this should be followed by a blank line and then a message describing the intent and purpose of the commit. We based these guidelines on a post by Chris Beams . When you commit, you are accepting our DCO: Developer Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 1 Letterman Drive Suite D4700 San Francisco, CA, 94129 Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. When you run git commit make sure you sign-off the commit by typing git commit --signoff or git commit -s The commit subject-line should start with an uppercase letter The commit subject-line should not exceed 72 characters in length The commit subject-line should not end with punctuation (., etc) Note: please do not use the GitHub suggestions feature, since it will not allow your commits to be signed-off. When giving a commit body, be sure to: Leave a blank line after the subject-line Make sure all lines are wrapped to 72 characters Here's an example that would be accepted: Add luke to the contributors' _index.md file We need to add luke to the contributors' _index.md file as a contributor. Signed-off-by: Hans <hans@anovos.ai> Some invalid examples: (feat) Add page about X to documentation This example does not follow the convention by adding a custom scheme of (feat) Update the documentation for page X so including fixing A, B, C and D and F. This example will be truncated in the GitHub UI and via git log --oneline If you would like to amend your commit, follow this guide: Git: Rewriting History Report bugs using GitHub's issues The team tracks all bugs by using Github issues. If you find something that needs to be addressed, open a new issue; it's easy! License By contributing, you agree that your contributions will be licensed under its Apache License, adhere to the Developer Certificate of Origin, and adhere to our code of conduct.","title":"Contributing to Anovos"},{"location":"community/contributing/#getting-started-with-anovos","text":"Anovos is an open source project that brings automation to the feature engineering process. To get Anovos up and running on your local machine, follow the Getting Started Guide . The Anovos Github Organization contains all repos, sample data, and notebooks you need.","title":"Getting Started with Anovos"},{"location":"community/contributing/#how-to-get-involved","text":"We love your input! We want to make contributing to this project as easy and transparent as possible, whether it's: Reporting a bug Discussing the current state of the code Submitting a fix Proposing new features Testing out new features Contributing to the docs Giving talks about Anovos at Meetups, conferences, and webinars For the latest information about interacting with the project maintainers and the broader community, please visit the COMMUNICATION.md file in Github.","title":"How To Get Involved"},{"location":"community/contributing/#contribute-to-anovos","text":"Pull requests are the best way to propose changes to the codebase. We use GitHub flow, and everything happens through pull requests. We welcome your pull requests; to make it simple, here are the steps to contribute: Fork the repo you're updating and create your branch from main. If you've added code that should be tested, add tests. If you've changed APIs, update the documentation. Ensure the test suite passes. Issue that pull request! Any contributions you make will be under the Apache Software License. In short, when you submit code changes, your submissions are understood to be under the same Apache License that covers the project. Feel free to contact the maintainers if that's a concern.","title":"Contribute to Anovos"},{"location":"community/contributing/#write-thorough-commit-messages","text":"Help reviewers know what you're contributing by writing good commit messages. The first line of the commit message is the subject; this should be followed by a blank line and then a message describing the intent and purpose of the commit. We based these guidelines on a post by Chris Beams . When you commit, you are accepting our DCO: Developer Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 1 Letterman Drive Suite D4700 San Francisco, CA, 94129 Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. When you run git commit make sure you sign-off the commit by typing git commit --signoff or git commit -s The commit subject-line should start with an uppercase letter The commit subject-line should not exceed 72 characters in length The commit subject-line should not end with punctuation (., etc) Note: please do not use the GitHub suggestions feature, since it will not allow your commits to be signed-off. When giving a commit body, be sure to: Leave a blank line after the subject-line Make sure all lines are wrapped to 72 characters Here's an example that would be accepted: Add luke to the contributors' _index.md file We need to add luke to the contributors' _index.md file as a contributor. Signed-off-by: Hans <hans@anovos.ai> Some invalid examples: (feat) Add page about X to documentation This example does not follow the convention by adding a custom scheme of (feat) Update the documentation for page X so including fixing A, B, C and D and F. This example will be truncated in the GitHub UI and via git log --oneline If you would like to amend your commit, follow this guide: Git: Rewriting History","title":"Write Thorough Commit Messages"},{"location":"community/contributing/#report-bugs-using-githubs-issues","text":"The team tracks all bugs by using Github issues. If you find something that needs to be addressed, open a new issue; it's easy!","title":"Report bugs using GitHub's issues"},{"location":"community/contributing/#license","text":"By contributing, you agree that your contributions will be licensed under its Apache License, adhere to the Developer Certificate of Origin, and adhere to our code of conduct.","title":"License"},{"location":"community/license/","text":"License.md Copyright 2021 Anovos Developers Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS Third-Party License overview of included 3rd party libraries The Anovos project is licensed under the terms of the Apache 2.0 License . However, Anovos includes several third-party Open-Source libraries, which are licensed under their own respective Open-Source licenses. Pandas License: BSD 3-Clause License https://github.com/pandas-dev/pandas/blob/master/LICENSE scripy License: BSD 3-Clause License https://github.com/scipy/scipy/blob/master/LICENSE.txt scikit-learn License: BSD 3-Clause License https://github.com/scikit-learn/scikit-learn/blob/main/COPYING joblib License: BSD 3-Clause License https://github.com/joblib/joblib/blob/master/LICENSE.txt boto3 License: Apache 2.0 License https://github.com/boto/boto3/blob/develop/LICENSE pyarrow License: Apache 2.0 License https://github.com/apache/arrow/blob/master/LICENSE.txt sparkpickle License: Apache 2.0 License https://github.com/src-d/sparkpickle/blob/master/LICENSE s3path License: Apache-2.0 License https://github.com/liormizr/s3path/blob/master/LICENSE statsmodels License: BSD 3-Clause License https://github.com/statsmodels/statsmodels/blob/main/LICENSE.txt pybind11 License: BSD 3-Clause License https://github.com/pybind/pybind11/blob/master/LICENSE popmon License: MIT License https://github.com/ing-bank/popmon/blob/master/LICENSE seaborn License: BSD 3-Clause License https://github.com/mwaskom/seaborn/blob/master/LICENSE varclushi License: GPL 3.0 License https://github.com/jingtt/varclushi/blob/master/LICENSE pytest Licsnse: MIT License https://docs.pytest.org/en/latest/license.html datapane License: Apache 2.0 License https://github.com/datapane/datapane/blob/master/LICENSE","title":"License"},{"location":"community/license/#licensemd","text":"Copyright 2021 Anovos Developers Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS","title":"License.md"},{"location":"community/license/#third-party-license-overview-of-included-3rd-party-libraries","text":"The Anovos project is licensed under the terms of the Apache 2.0 License . However, Anovos includes several third-party Open-Source libraries, which are licensed under their own respective Open-Source licenses.","title":"Third-Party License overview of included 3rd party libraries"},{"location":"community/license/#pandas","text":"License: BSD 3-Clause License https://github.com/pandas-dev/pandas/blob/master/LICENSE","title":"Pandas"},{"location":"community/license/#scripy","text":"License: BSD 3-Clause License https://github.com/scipy/scipy/blob/master/LICENSE.txt","title":"scripy"},{"location":"community/license/#scikit-learn","text":"License: BSD 3-Clause License https://github.com/scikit-learn/scikit-learn/blob/main/COPYING","title":"scikit-learn"},{"location":"community/license/#joblib","text":"License: BSD 3-Clause License https://github.com/joblib/joblib/blob/master/LICENSE.txt","title":"joblib"},{"location":"community/license/#boto3","text":"License: Apache 2.0 License https://github.com/boto/boto3/blob/develop/LICENSE","title":"boto3"},{"location":"community/license/#pyarrow","text":"License: Apache 2.0 License https://github.com/apache/arrow/blob/master/LICENSE.txt","title":"pyarrow"},{"location":"community/license/#sparkpickle","text":"License: Apache 2.0 License https://github.com/src-d/sparkpickle/blob/master/LICENSE","title":"sparkpickle"},{"location":"community/license/#s3path","text":"License: Apache-2.0 License https://github.com/liormizr/s3path/blob/master/LICENSE","title":"s3path"},{"location":"community/license/#statsmodels","text":"License: BSD 3-Clause License https://github.com/statsmodels/statsmodels/blob/main/LICENSE.txt","title":"statsmodels"},{"location":"community/license/#pybind11","text":"License: BSD 3-Clause License https://github.com/pybind/pybind11/blob/master/LICENSE","title":"pybind11"},{"location":"community/license/#popmon","text":"License: MIT License https://github.com/ing-bank/popmon/blob/master/LICENSE","title":"popmon"},{"location":"community/license/#seaborn","text":"License: BSD 3-Clause License https://github.com/mwaskom/seaborn/blob/master/LICENSE","title":"seaborn"},{"location":"community/license/#varclushi","text":"License: GPL 3.0 License https://github.com/jingtt/varclushi/blob/master/LICENSE","title":"varclushi"},{"location":"community/license/#pytest","text":"Licsnse: MIT License https://docs.pytest.org/en/latest/license.html","title":"pytest"},{"location":"community/license/#datapane","text":"License: Apache 2.0 License https://github.com/datapane/datapane/blob/master/LICENSE","title":"datapane"},{"location":"docs/anovos-intro/","text":"Introducing Anovos Data Science teams spend up to 80% of their time on feature engineering and still end up building models with poor resilience. Anovos seeks to address both these issues (lack of modeler productivity and insufficient model resilience) by enabling data scientists to understand the stability properties of the data they work with and then creating a transformation methodology that allows the building of features anchored by stable data, which in turn produce resilient models that break less often when deployed. Unlike current feature engineering workflows, which are ad hoc, error-prone, and modeler-driven, Anovos seeks to inject process-driven efficiency into feature creation based on innovations in understanding the stability of data and how data items impact the features they anchor. With Anovos, ML models will be more consistent, more accurate, and deliver results faster. The process of building models is more automated and procedural, saving time and decreasing cost. The Team Behind Anovos The data scientists at Mobilewalla built Anovos, a team of highly talented and experienced data scientists who have years of experience in machine learning (ML) and AI and are applying ML techniques to some of the most extensive consumer data sets available. This experience has brought to light an internal need to create tools to simplify and speed up the feature engineering process, increasing efficiency. They've been able to replicate and now open source these tools, bringing our learnings to the community. The Mobilewalla Feature Mart currently offers teams supplemental data to make their ML models more accurate and predictive. Anovos, a set of open source libraries, enables them to take that data, enhancing their internal data sets and making their modeling more systematic and procedural, creating cost and time efficiencies, and building more predictive models.","title":"Anovos Intro"},{"location":"docs/anovos-intro/#introducing-anovos","text":"Data Science teams spend up to 80% of their time on feature engineering and still end up building models with poor resilience. Anovos seeks to address both these issues (lack of modeler productivity and insufficient model resilience) by enabling data scientists to understand the stability properties of the data they work with and then creating a transformation methodology that allows the building of features anchored by stable data, which in turn produce resilient models that break less often when deployed. Unlike current feature engineering workflows, which are ad hoc, error-prone, and modeler-driven, Anovos seeks to inject process-driven efficiency into feature creation based on innovations in understanding the stability of data and how data items impact the features they anchor. With Anovos, ML models will be more consistent, more accurate, and deliver results faster. The process of building models is more automated and procedural, saving time and decreasing cost.","title":"Introducing Anovos"},{"location":"docs/anovos-intro/#the-team-behind-anovos","text":"The data scientists at Mobilewalla built Anovos, a team of highly talented and experienced data scientists who have years of experience in machine learning (ML) and AI and are applying ML techniques to some of the most extensive consumer data sets available. This experience has brought to light an internal need to create tools to simplify and speed up the feature engineering process, increasing efficiency. They've been able to replicate and now open source these tools, bringing our learnings to the community. The Mobilewalla Feature Mart currently offers teams supplemental data to make their ML models more accurate and predictive. Anovos, a set of open source libraries, enables them to take that data, enhancing their internal data sets and making their modeling more systematic and procedural, creating cost and time efficiencies, and building more predictive models.","title":"The Team Behind Anovos"},{"location":"docs/anovos-modules-overview/association-evaluator/","text":"Module ANOVOS.association_evaluator This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target variable. Association between attributes is measured by: correlation_matrix variable_clustering Association between an attribute and binary target is measured by: IV_calculation IG_calculation Columns which are subjected to these analysis can be controlled by right combination of arguments - list_of_cols and drop_cols. All functions have following common arguments: idf : Input dataframe ist_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols : This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. print_impact : This argument is to print out the statistics. correlation_matrix This function calculates correlation coefficient statistical, which measures the strength of the relationship between the relative movements of two attributes. Pearson\u2019s correlation coefficient is a standard approach of measuring correlation between two variables. However, it has some drawbacks: a) It works only with continuous variables, b) It only accounts for a linear relationship between variables, and c) It is sensitive to outliers. To avoid these issues, we are computing Phik (\ud835\udf19k), which is a new and practical correlation coefficient that works consistently between categorical, ordinal and interval variables, captures non-linear dependency and reverts to the Pearson correlation coefficient in case of a bivariate normal input distribution. The correlation coefficient is calculated for every pair of attributes and its value lies between 0 and 1, where 0 means there is no correlation between the two attributes and 1 means strong correlation. However, this methodology have drawbacks of its own as it is found to be more computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). Further, there is no indication of the direction of the relationship. More detail can be referred from the source paper . This function returns a correlation matrix dataframe of schema \u2013 attribute, . Correlation between attribute X and Y can be found at intersection of a) row with value X in \u2018attribute\u2019 column and b) column \u2018Y\u2019 (or row with value Y in \u2018attribute\u2019 column and column \u2018X\u2019). idf list_of_cols drop_cols stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator. print_impact variable_clustering Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. The function is leveraging VarClusHi library to do variable clustering; however, this library is not implemented in a scalable manner due to which the analysis is done on a sample dataset. Further, it is found to be a bit computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). It returns a Spark Dataframe with schema \u2013 Cluster, Attribute, RS_Ratio. The attribute with the lowest (1 \u2014 RS_Ratio) can be chosen as a representative of the cluster while discarding the other attributes from that cluster. This can also help in achieving the dimension reduction, if required. idf list_of_cols drop_cols sample_size : Sample size used for performing variable cluster. Default is 100,000. stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator. This is used to remove single value columns from the analysis purpose. stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator. This is used for MMM imputation as Variable Clustering doesn\u2019t work with missing values. print_impact IV_calculation Information Value (IV) is simple and powerful technique to conduct attribute relevance analysis. It measures how well an attribute is able to distinguish between a binary target variable i.e. label 0 from label 1, and hence helps in ranking attributes on the basis of their importance. In the heart of IV methodology are groups (bins) of observations. For categorical attributes, usually each category is a bin while numerical attributes need to be split into categories. IV = \u2211 (% of non-events - % of events) * WOE where: WOE = In(% of non-events \u2797 % of events) % of event = % label 1 in a bin % of non-event = % label 0 in a bin General rule of thumb while creating the bins are that a) each bin should have at least 5% of the observations, b) the WOE should be monotonic, i.e. either growing or decreasing with the bins, and c) missing values should be binned separately. An article from listendata.com can be referred for good understanding of IV & WOE concepts. idf list_of_cols drop_cols label_col : Name of label or target column in the input dataset event_label : Value of event (label 1) in the label column encoding_configs : This argument takes input in dictionary format with keys related to binning operation - 'bin_method' (default 'equal_frequency'), 'bin_size' (default 10) and 'monotonicity_check' (default 0). monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature and can be expensive operation. print_impact IG_calculation Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event log\u2061(%event)-(1-%event) log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i ) log\u2061(1-%\u3016event\u3017_i) idf list_of_cols drop_cols label_col : Name of label or target column in the input dataset event_label : Value of event (label 1) in the label column encoding_configs : This argument takes input in dictionary format with keys related to binning operation - 'bin_method' (default 'equal_frequency'), 'bin_size' (default 10) and 'monotonicity_check' (default 0). monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature and can be expensive operation. print_impact","title":"Association Evaluator"},{"location":"docs/anovos-modules-overview/association-evaluator/#module-anovosassociation_evaluator","text":"This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target variable. Association between attributes is measured by: correlation_matrix variable_clustering Association between an attribute and binary target is measured by: IV_calculation IG_calculation Columns which are subjected to these analysis can be controlled by right combination of arguments - list_of_cols and drop_cols. All functions have following common arguments: idf : Input dataframe ist_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols : This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. print_impact : This argument is to print out the statistics.","title":"Module ANOVOS.association_evaluator"},{"location":"docs/anovos-modules-overview/association-evaluator/#correlation_matrix","text":"This function calculates correlation coefficient statistical, which measures the strength of the relationship between the relative movements of two attributes. Pearson\u2019s correlation coefficient is a standard approach of measuring correlation between two variables. However, it has some drawbacks: a) It works only with continuous variables, b) It only accounts for a linear relationship between variables, and c) It is sensitive to outliers. To avoid these issues, we are computing Phik (\ud835\udf19k), which is a new and practical correlation coefficient that works consistently between categorical, ordinal and interval variables, captures non-linear dependency and reverts to the Pearson correlation coefficient in case of a bivariate normal input distribution. The correlation coefficient is calculated for every pair of attributes and its value lies between 0 and 1, where 0 means there is no correlation between the two attributes and 1 means strong correlation. However, this methodology have drawbacks of its own as it is found to be more computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). Further, there is no indication of the direction of the relationship. More detail can be referred from the source paper . This function returns a correlation matrix dataframe of schema \u2013 attribute, . Correlation between attribute X and Y can be found at intersection of a) row with value X in \u2018attribute\u2019 column and b) column \u2018Y\u2019 (or row with value Y in \u2018attribute\u2019 column and column \u2018X\u2019). idf list_of_cols drop_cols stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator. print_impact","title":"correlation_matrix"},{"location":"docs/anovos-modules-overview/association-evaluator/#variable_clustering","text":"Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. The function is leveraging VarClusHi library to do variable clustering; however, this library is not implemented in a scalable manner due to which the analysis is done on a sample dataset. Further, it is found to be a bit computational expensive especially when number of columns in the input dataset is on higher side (number of pairs to analyse increases exponentially with number of columns). It returns a Spark Dataframe with schema \u2013 Cluster, Attribute, RS_Ratio. The attribute with the lowest (1 \u2014 RS_Ratio) can be chosen as a representative of the cluster while discarding the other attributes from that cluster. This can also help in achieving the dimension reduction, if required. idf list_of_cols drop_cols sample_size : Sample size used for performing variable cluster. Default is 100,000. stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator. This is used to remove single value columns from the analysis purpose. stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator. This is used for MMM imputation as Variable Clustering doesn\u2019t work with missing values. print_impact","title":"variable_clustering"},{"location":"docs/anovos-modules-overview/association-evaluator/#iv_calculation","text":"Information Value (IV) is simple and powerful technique to conduct attribute relevance analysis. It measures how well an attribute is able to distinguish between a binary target variable i.e. label 0 from label 1, and hence helps in ranking attributes on the basis of their importance. In the heart of IV methodology are groups (bins) of observations. For categorical attributes, usually each category is a bin while numerical attributes need to be split into categories. IV = \u2211 (% of non-events - % of events) * WOE where: WOE = In(% of non-events \u2797 % of events) % of event = % label 1 in a bin % of non-event = % label 0 in a bin General rule of thumb while creating the bins are that a) each bin should have at least 5% of the observations, b) the WOE should be monotonic, i.e. either growing or decreasing with the bins, and c) missing values should be binned separately. An article from listendata.com can be referred for good understanding of IV & WOE concepts. idf list_of_cols drop_cols label_col : Name of label or target column in the input dataset event_label : Value of event (label 1) in the label column encoding_configs : This argument takes input in dictionary format with keys related to binning operation - 'bin_method' (default 'equal_frequency'), 'bin_size' (default 10) and 'monotonicity_check' (default 0). monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature and can be expensive operation. print_impact","title":"IV_calculation"},{"location":"docs/anovos-modules-overview/association-evaluator/#ig_calculation","text":"Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event log\u2061(%event)-(1-%event) log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i ) log\u2061(1-%\u3016event\u3017_i) idf list_of_cols drop_cols label_col : Name of label or target column in the input dataset event_label : Value of event (label 1) in the label column encoding_configs : This argument takes input in dictionary format with keys related to binning operation - 'bin_method' (default 'equal_frequency'), 'bin_size' (default 10) and 'monotonicity_check' (default 0). monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature and can be expensive operation. print_impact","title":"IG_calculation"},{"location":"docs/anovos-modules-overview/data-analyzer/","text":"Module ANOVOS.stats_generator This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are broken down into different metric types, and each function corresponds to one metric type. - global_summary - measures_of_counts - measures_of_centralTendency - measures_of_cardinality - measures_of_dispersion - measures_of_percentiles - measures_of_shape Columns subjected to this analysis can be controlled by the right combination of arguments - list_of_cols and drop_cols. All the above functions require the following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols : In a list format, this argument is used to specify the columns that need to be dropped from list_of_cols. Instead of a list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. print_impact : This argument is to print out the statistics. global_summary The global summary function computes the following universal statistics/metrics and returns a Spark DataFrame with schema \u2013 metric, value. - No. of rows - No. of columns - No. of categorical columns along with column names - No. of numerical columns along with the column names - No. of non-numerical non-categorical columns such as date type, array type etc. along with column names measures_of_counts The Measures of Counts function computes different count metrics for each column (interchangeably called an attribute in the document). It returns a Spark DataFrame with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. - Fill Count/Rate is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary7 functionality of Spark SQL. - Missing Count/Rate is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. - Non Zero Count/Rate is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a subfunction nonzeroCount_computation, which is later called under measures_of_counts. Under the hood, it leverage Multivariate Statistical Summary8 of Spark MLlib. measures_of_centralTendency The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark DataFrame with schema \u2013 attribute, mean, median, mode, mode_pct. Mean is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Median is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Mode is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns) Mode Pct is defined as % of rows seen with Mode value. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns) measures_of_counts The Measures of Counts function computes different count metric for each column (interchangeably called as attribute in the document). It returns a Spark DataFrame with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. Fill Count/Rate is defined as the number of rows with non-null values in a column in terms of absolute count and its proportion to row count. It leverages count statistics from the summary7 functionality of Spark SQL. Missing Count/Rate is defined as null (or missing) values seen in a column in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. Non Zero Count/Rate is defined as non-zero values seen in a numerical column in terms of absolute count and its proportion to row count. For categorical columns, it will show a null value. Also, it uses a subfunction nonzeroCount_computation, which is later called under measures_of_counts. Under the hood, it leverages Multivariate Statistical Summary8 of Spark MLlib. measures_of_centralTendency The Measures of Central Tendency function provides summary statistics representing the attribute's center point or most likely value. It returns a Spark DataFrame with schema \u2013 attribute, mean, median, mode, mode_pct. Mean is the arithmetic average of a column i.e., the sum of all values seen in the column divided by the number of rows. It leverage mean statistic from the summary functionality of Spark SQL. Median is the 50th percentile or the middle value in a column when the values are arranged in ascending or descending order. It leverage '50%' statistic from the summary functionality of Spark SQL. Mode is the most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns). Mode Pct is defined as % of rows seen with Mode value. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns) measures_of_cardinality The Measures of Cardinality function provides statistics that are related to unique values seen in an attribute. These statistics are calculated only for discrete columns (categorical + Integer/Long columns). It returns a Spark Dataframe with schema \u2013 attribute, unique_values, IDness. Unique Value is defined as a distinct value count of a column. It relies on a subfunction uniqueCount_computation for its computation and leverages the countDistinct9 functionality of Spark SQL. IDness is calculated as Unique Values divided by non-null values seen in a column. Non-null values count is used instead of total count because too many null values can give misleading results even if the column have all unique values (except null). It uses subfunctions - uniqueCount_computation and missingCount_computation. measures_of_dispersion The Measures of Dispersion function provides statistics that describe the spread of a numerical attribute. Alternatively, these statistics are also known as measures of spread. It returns a Spark DataFrame with schema \u2013 attribute, stddev, variance, cov, IQR, range. Standard Deviation (stddev) measures how concentrated an attribute is around the mean or average and mathematically computed as below. It leverages \u2018stddev\u2019 statistic from summary functionality of Spark SQL. s= X- X2n -1 where: ` `X is an attribute value X is attribute mean n is no. of rows Variance is the squared value of Standard Deviation. Coefficient of Variance (cov) is computed as ratio of Standard Deviation & Mean. It leverages \u2018stddev\u2019 and \u2018mean\u2019 statistic from the summary functionality of Spark SQL. Interquartile Range (IQR): It describes the difference between the third quartile (75th percentile) and the first quartile (25th percentile), telling us about the range where middle half values are seen. It leverage \u201825%\u2019 and \u201875%\u2019 statistics from the summary functionality of Spark SQL. Range is simply the difference between the maximum value and the minimum value. It leverage \u2018min\u2019 and \u2018max\u2019 statistics from the summary functionality of Spark SQL. measures_of_percentiles he Measures of Percentiles function provides statistics at different percentiles. Nth percentile can be interpreted as N% of rows having values lesser than or equal to Nth percentile value. It is prominently used for quick detection of skewness or outlier. Alternatively, these statistics are also known as measures of position. These statistics are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max. It leverage \u2018N%\u2019 statistics from summary functionality of Spark SQL where N is 0 for min and 100 for max. measures_of_shape The Measures of Shapes function provides statistics related to the shape of an attribute's distribution. Alternatively, these statistics are also known as measures of the moment and are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, skewness, kurtosis. Skewness describes how much-skewed values are relative to a perfect bell curve observed in normal distribution and the direction of skew. If the majority of the values are at the left and the right tail is longer, we say that the distribution is skewed right or positively skewed; if the peak is toward the right and the left tail is longer, we say that the distribution is skewed left or negatively skewed. It leverage skewness10 functionality of Spark SQL. (Excess) Kurtosis describes how tall and sharp the central peak is relative to a perfect bell curve observed in the normal distribution. The reference standard is a normal distribution, which has a kurtosis of 3. In token of this, often, the excess kurtosis is presented: excess kurtosis is simply kurtosis\u22123. Higher (positive) values indicate a higher, sharper peak; lower (negative) values indicate a less distinct peak. It leverages kurtosis11 functionality of Spark SQL.","title":"Data Analyzer"},{"location":"docs/anovos-modules-overview/data-analyzer/#module-anovosstats_generator","text":"This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are broken down into different metric types, and each function corresponds to one metric type. - global_summary - measures_of_counts - measures_of_centralTendency - measures_of_cardinality - measures_of_dispersion - measures_of_percentiles - measures_of_shape Columns subjected to this analysis can be controlled by the right combination of arguments - list_of_cols and drop_cols. All the above functions require the following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols : In a list format, this argument is used to specify the columns that need to be dropped from list_of_cols. Instead of a list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. print_impact : This argument is to print out the statistics.","title":"Module ANOVOS.stats_generator"},{"location":"docs/anovos-modules-overview/data-analyzer/#global_summary","text":"The global summary function computes the following universal statistics/metrics and returns a Spark DataFrame with schema \u2013 metric, value. - No. of rows - No. of columns - No. of categorical columns along with column names - No. of numerical columns along with the column names - No. of non-numerical non-categorical columns such as date type, array type etc. along with column names","title":"global_summary"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_counts","text":"The Measures of Counts function computes different count metrics for each column (interchangeably called an attribute in the document). It returns a Spark DataFrame with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. - Fill Count/Rate is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary7 functionality of Spark SQL. - Missing Count/Rate is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. - Non Zero Count/Rate is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a subfunction nonzeroCount_computation, which is later called under measures_of_counts. Under the hood, it leverage Multivariate Statistical Summary8 of Spark MLlib.","title":"measures_of_counts"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_centraltendency","text":"The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark DataFrame with schema \u2013 attribute, mean, median, mode, mode_pct. Mean is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Median is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Mode is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns) Mode Pct is defined as % of rows seen with Mode value. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns)","title":"measures_of_centralTendency"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_counts_1","text":"The Measures of Counts function computes different count metric for each column (interchangeably called as attribute in the document). It returns a Spark DataFrame with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. Fill Count/Rate is defined as the number of rows with non-null values in a column in terms of absolute count and its proportion to row count. It leverages count statistics from the summary7 functionality of Spark SQL. Missing Count/Rate is defined as null (or missing) values seen in a column in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. Non Zero Count/Rate is defined as non-zero values seen in a numerical column in terms of absolute count and its proportion to row count. For categorical columns, it will show a null value. Also, it uses a subfunction nonzeroCount_computation, which is later called under measures_of_counts. Under the hood, it leverages Multivariate Statistical Summary8 of Spark MLlib.","title":"measures_of_counts"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_centraltendency_1","text":"The Measures of Central Tendency function provides summary statistics representing the attribute's center point or most likely value. It returns a Spark DataFrame with schema \u2013 attribute, mean, median, mode, mode_pct. Mean is the arithmetic average of a column i.e., the sum of all values seen in the column divided by the number of rows. It leverage mean statistic from the summary functionality of Spark SQL. Median is the 50th percentile or the middle value in a column when the values are arranged in ascending or descending order. It leverage '50%' statistic from the summary functionality of Spark SQL. Mode is the most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns). Mode Pct is defined as % of rows seen with Mode value. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns)","title":"measures_of_centralTendency"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_cardinality","text":"The Measures of Cardinality function provides statistics that are related to unique values seen in an attribute. These statistics are calculated only for discrete columns (categorical + Integer/Long columns). It returns a Spark Dataframe with schema \u2013 attribute, unique_values, IDness. Unique Value is defined as a distinct value count of a column. It relies on a subfunction uniqueCount_computation for its computation and leverages the countDistinct9 functionality of Spark SQL. IDness is calculated as Unique Values divided by non-null values seen in a column. Non-null values count is used instead of total count because too many null values can give misleading results even if the column have all unique values (except null). It uses subfunctions - uniqueCount_computation and missingCount_computation.","title":"measures_of_cardinality"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_dispersion","text":"The Measures of Dispersion function provides statistics that describe the spread of a numerical attribute. Alternatively, these statistics are also known as measures of spread. It returns a Spark DataFrame with schema \u2013 attribute, stddev, variance, cov, IQR, range. Standard Deviation (stddev) measures how concentrated an attribute is around the mean or average and mathematically computed as below. It leverages \u2018stddev\u2019 statistic from summary functionality of Spark SQL. s= X- X2n -1 where: ` `X is an attribute value X is attribute mean n is no. of rows Variance is the squared value of Standard Deviation. Coefficient of Variance (cov) is computed as ratio of Standard Deviation & Mean. It leverages \u2018stddev\u2019 and \u2018mean\u2019 statistic from the summary functionality of Spark SQL. Interquartile Range (IQR): It describes the difference between the third quartile (75th percentile) and the first quartile (25th percentile), telling us about the range where middle half values are seen. It leverage \u201825%\u2019 and \u201875%\u2019 statistics from the summary functionality of Spark SQL. Range is simply the difference between the maximum value and the minimum value. It leverage \u2018min\u2019 and \u2018max\u2019 statistics from the summary functionality of Spark SQL.","title":"measures_of_dispersion"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_percentiles","text":"he Measures of Percentiles function provides statistics at different percentiles. Nth percentile can be interpreted as N% of rows having values lesser than or equal to Nth percentile value. It is prominently used for quick detection of skewness or outlier. Alternatively, these statistics are also known as measures of position. These statistics are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max. It leverage \u2018N%\u2019 statistics from summary functionality of Spark SQL where N is 0 for min and 100 for max.","title":"measures_of_percentiles"},{"location":"docs/anovos-modules-overview/data-analyzer/#measures_of_shape","text":"The Measures of Shapes function provides statistics related to the shape of an attribute's distribution. Alternatively, these statistics are also known as measures of the moment and are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, skewness, kurtosis. Skewness describes how much-skewed values are relative to a perfect bell curve observed in normal distribution and the direction of skew. If the majority of the values are at the left and the right tail is longer, we say that the distribution is skewed right or positively skewed; if the peak is toward the right and the left tail is longer, we say that the distribution is skewed left or negatively skewed. It leverage skewness10 functionality of Spark SQL. (Excess) Kurtosis describes how tall and sharp the central peak is relative to a perfect bell curve observed in the normal distribution. The reference standard is a normal distribution, which has a kurtosis of 3. In token of this, often, the excess kurtosis is presented: excess kurtosis is simply kurtosis\u22123. Higher (positive) values indicate a higher, sharper peak; lower (negative) values indicate a less distinct peak. It leverages kurtosis11 functionality of Spark SQL.","title":"measures_of_shape"},{"location":"docs/anovos-modules-overview/data-ingest/","text":"Module ANOVOS.data_ingest This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic ETL actions such as selecting, deleting, renaming and/or recasting columns. List of functions included in this module are: - read_dataset - write_dataset - concatenate_dataset - join_dataset - delete_column - select_column - rename_column - recast_column read_dataset This function reads the input data path and return a Spark DataFrame. Under the hood, this function is based on generic Load functionality of Spark SQL1. It requires following arguments: file_path : file (or directory) path where the input data is saved. File path can be a local path or s3 path (when running with AWS cloud services) file_type : file format of the input data. Currently, we support CSV, Parquet or Avro. Avro data source requires an external package to run, which can be configured with spark-submit options (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs (optional): Rest of the valid configurations can be passed through this argument in a dictionary format. All the key/value pairs written in this argument are passed as options to DataFrameReader, which is created using SparkSession.read. write_dataset This function saves the Spark DataFrame in the user-provided output path. Like read_dataset, this function is based on the generic Save functionality of Spark SQL. It requires the following arguments: idf : Spark DataFrame to be saved file_path : file (or directory) path where the output data is to be saved. File path can be a local path or s3 path (when running with AWS cloud services) file_type : file format of the input data. Currently, we support CSV, Parquet or Avro. The Avro data source requires an external package to run, which can be configured with spark-submit options (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs (optional): The rest of the valid configuration can be passed through this argument in a dictionary format, e.g., repartition, mode, compression, header, delimiter, etc. All the key/value pairs written in this argument are passed as options to DataFrameWriter is available using Dataset.write operator. If the number of repartitions mentioned through this argument is less than the existing DataFrame partitions, then the coalesce operation2 is used instead of the repartition operation to make the execution work. This is because the coalesce operation doesn\u2019t concatenate_dataset This function combines multiple dataframes into a single dataframe. A pairwise concatenation is performed on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function leverages union3 functionality of Spark SQL. It requires the following arguments: *idfs : Varying number of dataframes to be concatenated method_type : index or name. This argument needs to be entered as a keyword argument. The \u201cindex\u201d method involves concatenating the dataframes by the column index. IF the sequence of column is not fixed among the dataframe, this method should be avoided. The \u201cname\u201d method involves concatenating by columns names. The 1st dataframe passed under idfs will define the final columns in the concatenated dataframe. It will throw an error if any column in the 1st dataframe is not available in any of other dataframes. join_dataset This function joins multiple dataframes into a single dataframe by a joining key column. Pairwise joining is done on the dataframes, instead of joining individual dataframes to the bigger dataframe. This function leverages join4 functionality of Spark SQL. It requires the following arguments: *idfs : Varying number of all dataframes to be joined join_cols : Key column(s) to join all dataframes together. In case of multiple columns to join, they can be passed in a list format or a single text format where different column names are separated by pipe delimiter \u201c|\u201d join_type : \u201cinner\u201d, \u201cfull\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cleft_semi\u201d, \u201cleft_anti\u201d delete_column This function is used to delete specific columns from the input data. It is executed using drop functionality5 of Spark SQL. It is advisable to use this function if the number of columns to delete is lesser than the number of columns to select; otherwise, it is recommended to use select_column. It requires the following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, specifies the columns required to be deleted from the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d print_impact : This argument is to compare number of columns before and after the operation. select_column This function is used to select specific columns from the input data. It is executed using select operation6 of spark dataframe. It is advisable to use this function if the number of columns to select is lesser than the number of columns to drop; otherwise, it is recommended to use delete_column. It requires the following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, specifies the columns required to be deleted from the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d print_impact : This argument is to compare number of columns before and after the operation. rename_column This function is used to rename the columns of the input data. Multiple columns can be renamed; however, the sequence they passed as an argument is critical and must be consistent between list_of_cols and list_of_newcols. It requires the following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns required to be renamed in the input dataframe. Alternatively, instead of a list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d list_of_newcols : This argument, in a list format, is used to specify the new column name, i.e. the first element in list_of_cols will be the original column name, and the corresponding first column in list_of_newcols will be the new column name. print_impact : This argument is to compare column names before and after the operation. recast_column This function is used to modify the datatype of columns. Multiple columns can be cast; however, the sequence they passed as argument is critical and needs to be consistent between list_of_cols and list_of_dtypes. It requires the following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns required to be recast in the input dataframe. Alternatively, instead of a list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d list_of_dtypes : This argument, in a list format, is used to specify the datatype, i.e. the first element in list_of_cols will column name, and the corresponding element in list_of_dtypes will be new datatype such as float, integer, string, double, decimal, etc. (case insensitive). print_impact : This argument is to compare schema before and after the operation.","title":"Data Ingest"},{"location":"docs/anovos-modules-overview/data-ingest/#module-anovosdata_ingest","text":"This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic ETL actions such as selecting, deleting, renaming and/or recasting columns. List of functions included in this module are: - read_dataset - write_dataset - concatenate_dataset - join_dataset - delete_column - select_column - rename_column - recast_column","title":"Module ANOVOS.data_ingest"},{"location":"docs/anovos-modules-overview/data-ingest/#read_dataset","text":"This function reads the input data path and return a Spark DataFrame. Under the hood, this function is based on generic Load functionality of Spark SQL1. It requires following arguments: file_path : file (or directory) path where the input data is saved. File path can be a local path or s3 path (when running with AWS cloud services) file_type : file format of the input data. Currently, we support CSV, Parquet or Avro. Avro data source requires an external package to run, which can be configured with spark-submit options (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs (optional): Rest of the valid configurations can be passed through this argument in a dictionary format. All the key/value pairs written in this argument are passed as options to DataFrameReader, which is created using SparkSession.read.","title":"read_dataset"},{"location":"docs/anovos-modules-overview/data-ingest/#write_dataset","text":"This function saves the Spark DataFrame in the user-provided output path. Like read_dataset, this function is based on the generic Save functionality of Spark SQL. It requires the following arguments: idf : Spark DataFrame to be saved file_path : file (or directory) path where the output data is to be saved. File path can be a local path or s3 path (when running with AWS cloud services) file_type : file format of the input data. Currently, we support CSV, Parquet or Avro. The Avro data source requires an external package to run, which can be configured with spark-submit options (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs (optional): The rest of the valid configuration can be passed through this argument in a dictionary format, e.g., repartition, mode, compression, header, delimiter, etc. All the key/value pairs written in this argument are passed as options to DataFrameWriter is available using Dataset.write operator. If the number of repartitions mentioned through this argument is less than the existing DataFrame partitions, then the coalesce operation2 is used instead of the repartition operation to make the execution work. This is because the coalesce operation doesn\u2019t","title":"write_dataset"},{"location":"docs/anovos-modules-overview/data-ingest/#concatenate_dataset","text":"This function combines multiple dataframes into a single dataframe. A pairwise concatenation is performed on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function leverages union3 functionality of Spark SQL. It requires the following arguments: *idfs : Varying number of dataframes to be concatenated method_type : index or name. This argument needs to be entered as a keyword argument. The \u201cindex\u201d method involves concatenating the dataframes by the column index. IF the sequence of column is not fixed among the dataframe, this method should be avoided. The \u201cname\u201d method involves concatenating by columns names. The 1st dataframe passed under idfs will define the final columns in the concatenated dataframe. It will throw an error if any column in the 1st dataframe is not available in any of other dataframes.","title":"concatenate_dataset"},{"location":"docs/anovos-modules-overview/data-ingest/#join_dataset","text":"This function joins multiple dataframes into a single dataframe by a joining key column. Pairwise joining is done on the dataframes, instead of joining individual dataframes to the bigger dataframe. This function leverages join4 functionality of Spark SQL. It requires the following arguments: *idfs : Varying number of all dataframes to be joined join_cols : Key column(s) to join all dataframes together. In case of multiple columns to join, they can be passed in a list format or a single text format where different column names are separated by pipe delimiter \u201c|\u201d join_type : \u201cinner\u201d, \u201cfull\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cleft_semi\u201d, \u201cleft_anti\u201d","title":"join_dataset"},{"location":"docs/anovos-modules-overview/data-ingest/#delete_column","text":"This function is used to delete specific columns from the input data. It is executed using drop functionality5 of Spark SQL. It is advisable to use this function if the number of columns to delete is lesser than the number of columns to select; otherwise, it is recommended to use select_column. It requires the following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, specifies the columns required to be deleted from the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d print_impact : This argument is to compare number of columns before and after the operation.","title":"delete_column"},{"location":"docs/anovos-modules-overview/data-ingest/#select_column","text":"This function is used to select specific columns from the input data. It is executed using select operation6 of spark dataframe. It is advisable to use this function if the number of columns to select is lesser than the number of columns to drop; otherwise, it is recommended to use delete_column. It requires the following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, specifies the columns required to be deleted from the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d print_impact : This argument is to compare number of columns before and after the operation.","title":"select_column"},{"location":"docs/anovos-modules-overview/data-ingest/#rename_column","text":"This function is used to rename the columns of the input data. Multiple columns can be renamed; however, the sequence they passed as an argument is critical and must be consistent between list_of_cols and list_of_newcols. It requires the following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns required to be renamed in the input dataframe. Alternatively, instead of a list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d list_of_newcols : This argument, in a list format, is used to specify the new column name, i.e. the first element in list_of_cols will be the original column name, and the corresponding first column in list_of_newcols will be the new column name. print_impact : This argument is to compare column names before and after the operation.","title":"rename_column"},{"location":"docs/anovos-modules-overview/data-ingest/#recast_column","text":"This function is used to modify the datatype of columns. Multiple columns can be cast; however, the sequence they passed as argument is critical and needs to be consistent between list_of_cols and list_of_dtypes. It requires the following arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns required to be recast in the input dataframe. Alternatively, instead of a list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d list_of_dtypes : This argument, in a list format, is used to specify the datatype, i.e. the first element in list_of_cols will column name, and the corresponding element in list_of_dtypes will be new datatype such as float, integer, string, double, decimal, etc. (case insensitive). print_impact : This argument is to compare schema before and after the operation.","title":"recast_column"},{"location":"docs/anovos-modules-overview/data_drift_and_stability_index/","text":"Module ANOVOS.drift_detector drift_statistics When the performance of a deployed machine learning model degrades in production, one potential reason is that the data used in training and prediction are not following the same distribution. Data drift mainly includes the following manifestations: Covariate shift: training and test data follow different distributions. For example, An algorithm predicting income that is trained on younger population but tested on older population. Prior probability shift: change of prior probability. For example in a spam classification problem, the proportion of spam emails changes from 0.2 in training data to 0.6 in testing data. Concept shift: the distribution of the target variable changes given fixed input values. For example in the same spam classification problem, emails tagged as spam in training data are more likely to be tagged as non-spam in testing data. In our module, we mainly focus on covariate shift detection. In summary, given 2 datasets, source and target datasets, we would like to quantify the drift of some numerical attributes from source to target datasets. The whole process can be broken down into 2 steps: (1) convert each attribute of interest in source and target datasets into source and target probability distributions. (2) calculate the statistical distance between source and target distributions for each attribute. In the first step, attribute_binning is firstly performed to bin the numerical attributes of the source dataset, which requires two input variables: bin_method and bin_size. The same binning method is applied on the target dataset to align two results. The probability distributions are computed by dividing the frequency of each bin by the total frequency. In the second step, 4 choices of statistical metrics are provided to measure the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). They are calculated as below: For two discrete probability distributions P=(p_1,\u2026,p_k) and Q=(q_1,\u2026,q_k), PSI(P,Q)= \u2211_(i=1)^k\u2592(\u3016(p\u3017_i-q_i)*log\u2061(p_i/q_i ))^2 , JSD(P,Q)=(KL_divergence(p,(p+q)/2)+ KL_divergence(q,(p+q)/2))/2, where KL_divergence(P,Q)=\u2211_(i=1)^k\u2592(p_i*log\u2061(p_i/q_i )) , HD(P,Q) = 1/\u221a2 \u221a(\u2211_(i=1)^k\u2592(\u221a(p_i )-\u221a(q_i ))^2 ) KS(P,Q)=\u3016sup \u3017_i (\u3016|F\u3017_P (i)-F_Q (i)|) , A threshold can be set to flag out drifted attributes. If multiple statistical metrics have been calculated, an attribute will be marked as drifted if any of its statistical metric is larger than the threshold. This function can be used in many scenarios. For example: Attribute level data drift can be analysed together with the attribute importance of a machine learning model. The more important an attribute is, the more attention it needs to be given if drift presents. To analyse data drift over time, one can treat one dataset as the source / baseline dataset and multiple datasets as the target datasets. Drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. idf_target : Input target Dataframe idf_source : Input source Dataframe list_of_cols : List of columns to check drift (list or string of col names separated by |). Use \u2018all\u2019 - to include all non-array columns (excluding drop_cols). drop_cols : List of columns to be dropped (list or string of col names separated by |) method: PSI, JSD, HD, KS (list or string of methods separated by |). Use \u2018all\u2019 - to calculate all metrics. bin_method : equal_frequency or equal_range bin_size : 10 - 20 (recommended for PSI), >100 (other method types) threshold : To flag attributes meeting drift threshold pre_existing_source : True if binning model & frequency counts/attribute exists already, False Otherwise. source_path : If pre_existing_source is True, this argument is path for the source dataset details - drift_statistics folder. drift_statistics folder must contain attribute_binning & frequency_counts folders. If pre_existing_source is False, this argument can be used for saving the details. Default \"NA\" for temporarily saving source dataset attribute_binning folder stabilityIndex_computation The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 6 datasets collected in 6 consecutive time periods (D1, D2, \u2026, D6), data stability index of an attribute measures how stable the attribute is from D1 to D6. The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. In summary, given N datasets representing different time periods, we would like to measure the stability of some numerical attributes from the first to the N-th dataset. The whole process can be broken down into 2 steps: (1) Choose a few statistical metrics to describe the distribution of each attribute at each time period. (2) Compute attribute level stability by combining the stability of each statistical metric over time periods. In the first step, we choose mean, standard deviation and kurtosis as the statistical metrics in our implementation. Intuitively, they represent different aspects of a distribution: mean measures central tendency, standard deviation measures dispersion and kurtosis measures shape of a distribution. Reasons of selecting those 3 metrics will be explained in a later section. With mean, standard deviation and kurtosis computed for each attribute at each time interval, we can form 3 arrays of size N for each attribute. In the second step, Coefficient of Variation (CV) is used to measure the stability of each metric. CV represents the ratio of the standard deviation to the mean, which is a unitless statistic to compare the relative variation from one array to another. Considering the wide potential range of CV, the absolute value of CV is then mapped to an integer between 0 and 4 according to the table below, where 0 indicates highly unstable and 4 indicates highly stable. We call this integer a metric stability index. abs(CV) Interval Metric Stability Index [0, 0.03) 4 [0.03, 0.1) 3 [0.1, 0.2) 2 [0.2, 0.5) 1 [0.5, +inf) 0 Finally, the attribute stability index (SI) is a weighted sum of 3 metric stability indexes, where we assign 50% for mean, 30% for standard deviation and 20% for kurtosis. The final output is a float between 0 and 4 and an attribute can be classified as one of the following categories: very unstable (0\u2264SI<1), unstable (1\u2264SI<2), marginally stable (2\u2264SI<3), stable (3\u2264SI<3.5) and very stable (3.5\u2264SI\u22644). For example, there are 6 samples of attribute X from T1 to T6. For each sample, we have computed the statistical metrics of X from T1 to T6: idx Mean Standard deviation Kurtosis 1 11 2 3.9 2 12 1 4.2 3 15 3 4.0 4 10 2 4.1 5 11 1 4.2 6 13 0.5 4.0 Then we calculate the Coefficient of Variation for each array: CV of mean = CV([11, 12, 15, 10, 11, 13]) = 0.136 CV of standard deviation = CV([2, 1, 3, 2, 1, 0.5]) = 0.529 CV of kurtosis = CV([3.9, 4.2, 4.0, 4.1, 4.2, 4.0]) = 0.027 Metric stability indexes are then computed by mapping each CV value to an integer accordingly. As a result, metric stability index is 2 for mean, 0 for standard deviation and 4 for kurtosis. Why mean is chosen over median? Dummy variables which take only the value 0 or 1 are frequently seen in machine learning features. Mean of a dummy variable represents the proportion of value 1 and median of a dummy variable is either 0 or 1 whichever is more frequent. However, CV may not work well when 0 appears in the array or the array contains both positive and negative values. For example, intuitively [0,0,0,0,0,1,0,0,0] is a stable array but its CV is 2.83 which is extremely high, but cv of [0.45,0.44,0.48,0.49,0.42,0.52,0.49,0.47,0.48] is 0.06 which is much more reasonable. Thus we decided to use mean instead of median. Although median is considered as a more robust choice, outlier treatment can be applied prior to data stability analysis to handle this issue. Why kurtosis is chosen over skewness? Kurtosis is a positive value (note that we use kurtosis instead of excess kurtosis which) but skewness can range from \u2013inf to +inf. Usually, if skewness is between -0.5 and 0.5, the distribution is approximately symmetric. Thus, if the skewness fluctuates around 0, the CV is highly likely to be high or invalid because the mean will be close to 0. Stability index is preferred in the following scenario: Pairwise drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. However this can be time-consuming especially when the number of target dataset is large. In this case, measuring data stability instead of data drift would be a much faster alternative and the source/baseline dataset is not required as well Troubleshooting If the attribute stability index appears to be nan, it may due to one of the following reasons: One metric (likely to be kurtosis) is nan. For example, the kurtosis of a sample is nan If its standard deviation is 0. The mean of a metric from the first to the N-th dataset is zero, causing the denominator of CV to be 0. For example, when mean of attribute X is always zero for all datasets, its stability index would be nan. Limitation Limitation of CV: CV may not work well when 0 appears in the array or the array contains both positive and negative values. idfs : Input Dataframes (flexible) list_of_cols : Numerical columns (in list format or string separated by |). Use \u2018all\u2019 - to include all numerical columns (excluding drop_cols). drop_cols : List of columns to be dropped (list or string of col names separated by |) metric_weightages : A dictionary with key being the metric name (mean,stdev,kurtosis) and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. existing_metric_path : this argument is path for pre-existing metrics of historical datasets . idx is index number of historical datasets assigned in chronological order appended_metric_path : this argument is path for saving input dataframes metrics after appending to the historical datasets' metrics. threshold : To flag unstable attributes meeting the threshold","title":"Data Drift and Stability Index"},{"location":"docs/anovos-modules-overview/data_drift_and_stability_index/#drift_statistics","text":"When the performance of a deployed machine learning model degrades in production, one potential reason is that the data used in training and prediction are not following the same distribution. Data drift mainly includes the following manifestations: Covariate shift: training and test data follow different distributions. For example, An algorithm predicting income that is trained on younger population but tested on older population. Prior probability shift: change of prior probability. For example in a spam classification problem, the proportion of spam emails changes from 0.2 in training data to 0.6 in testing data. Concept shift: the distribution of the target variable changes given fixed input values. For example in the same spam classification problem, emails tagged as spam in training data are more likely to be tagged as non-spam in testing data. In our module, we mainly focus on covariate shift detection. In summary, given 2 datasets, source and target datasets, we would like to quantify the drift of some numerical attributes from source to target datasets. The whole process can be broken down into 2 steps: (1) convert each attribute of interest in source and target datasets into source and target probability distributions. (2) calculate the statistical distance between source and target distributions for each attribute. In the first step, attribute_binning is firstly performed to bin the numerical attributes of the source dataset, which requires two input variables: bin_method and bin_size. The same binning method is applied on the target dataset to align two results. The probability distributions are computed by dividing the frequency of each bin by the total frequency. In the second step, 4 choices of statistical metrics are provided to measure the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). They are calculated as below: For two discrete probability distributions P=(p_1,\u2026,p_k) and Q=(q_1,\u2026,q_k), PSI(P,Q)= \u2211_(i=1)^k\u2592(\u3016(p\u3017_i-q_i)*log\u2061(p_i/q_i ))^2 , JSD(P,Q)=(KL_divergence(p,(p+q)/2)+ KL_divergence(q,(p+q)/2))/2, where KL_divergence(P,Q)=\u2211_(i=1)^k\u2592(p_i*log\u2061(p_i/q_i )) , HD(P,Q) = 1/\u221a2 \u221a(\u2211_(i=1)^k\u2592(\u221a(p_i )-\u221a(q_i ))^2 ) KS(P,Q)=\u3016sup \u3017_i (\u3016|F\u3017_P (i)-F_Q (i)|) , A threshold can be set to flag out drifted attributes. If multiple statistical metrics have been calculated, an attribute will be marked as drifted if any of its statistical metric is larger than the threshold. This function can be used in many scenarios. For example: Attribute level data drift can be analysed together with the attribute importance of a machine learning model. The more important an attribute is, the more attention it needs to be given if drift presents. To analyse data drift over time, one can treat one dataset as the source / baseline dataset and multiple datasets as the target datasets. Drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. idf_target : Input target Dataframe idf_source : Input source Dataframe list_of_cols : List of columns to check drift (list or string of col names separated by |). Use \u2018all\u2019 - to include all non-array columns (excluding drop_cols). drop_cols : List of columns to be dropped (list or string of col names separated by |) method: PSI, JSD, HD, KS (list or string of methods separated by |). Use \u2018all\u2019 - to calculate all metrics. bin_method : equal_frequency or equal_range bin_size : 10 - 20 (recommended for PSI), >100 (other method types) threshold : To flag attributes meeting drift threshold pre_existing_source : True if binning model & frequency counts/attribute exists already, False Otherwise. source_path : If pre_existing_source is True, this argument is path for the source dataset details - drift_statistics folder. drift_statistics folder must contain attribute_binning & frequency_counts folders. If pre_existing_source is False, this argument can be used for saving the details. Default \"NA\" for temporarily saving source dataset attribute_binning folder","title":"drift_statistics"},{"location":"docs/anovos-modules-overview/data_drift_and_stability_index/#stabilityindex_computation","text":"The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 6 datasets collected in 6 consecutive time periods (D1, D2, \u2026, D6), data stability index of an attribute measures how stable the attribute is from D1 to D6. The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. In summary, given N datasets representing different time periods, we would like to measure the stability of some numerical attributes from the first to the N-th dataset. The whole process can be broken down into 2 steps: (1) Choose a few statistical metrics to describe the distribution of each attribute at each time period. (2) Compute attribute level stability by combining the stability of each statistical metric over time periods. In the first step, we choose mean, standard deviation and kurtosis as the statistical metrics in our implementation. Intuitively, they represent different aspects of a distribution: mean measures central tendency, standard deviation measures dispersion and kurtosis measures shape of a distribution. Reasons of selecting those 3 metrics will be explained in a later section. With mean, standard deviation and kurtosis computed for each attribute at each time interval, we can form 3 arrays of size N for each attribute. In the second step, Coefficient of Variation (CV) is used to measure the stability of each metric. CV represents the ratio of the standard deviation to the mean, which is a unitless statistic to compare the relative variation from one array to another. Considering the wide potential range of CV, the absolute value of CV is then mapped to an integer between 0 and 4 according to the table below, where 0 indicates highly unstable and 4 indicates highly stable. We call this integer a metric stability index. abs(CV) Interval Metric Stability Index [0, 0.03) 4 [0.03, 0.1) 3 [0.1, 0.2) 2 [0.2, 0.5) 1 [0.5, +inf) 0 Finally, the attribute stability index (SI) is a weighted sum of 3 metric stability indexes, where we assign 50% for mean, 30% for standard deviation and 20% for kurtosis. The final output is a float between 0 and 4 and an attribute can be classified as one of the following categories: very unstable (0\u2264SI<1), unstable (1\u2264SI<2), marginally stable (2\u2264SI<3), stable (3\u2264SI<3.5) and very stable (3.5\u2264SI\u22644). For example, there are 6 samples of attribute X from T1 to T6. For each sample, we have computed the statistical metrics of X from T1 to T6: idx Mean Standard deviation Kurtosis 1 11 2 3.9 2 12 1 4.2 3 15 3 4.0 4 10 2 4.1 5 11 1 4.2 6 13 0.5 4.0 Then we calculate the Coefficient of Variation for each array: CV of mean = CV([11, 12, 15, 10, 11, 13]) = 0.136 CV of standard deviation = CV([2, 1, 3, 2, 1, 0.5]) = 0.529 CV of kurtosis = CV([3.9, 4.2, 4.0, 4.1, 4.2, 4.0]) = 0.027 Metric stability indexes are then computed by mapping each CV value to an integer accordingly. As a result, metric stability index is 2 for mean, 0 for standard deviation and 4 for kurtosis. Why mean is chosen over median? Dummy variables which take only the value 0 or 1 are frequently seen in machine learning features. Mean of a dummy variable represents the proportion of value 1 and median of a dummy variable is either 0 or 1 whichever is more frequent. However, CV may not work well when 0 appears in the array or the array contains both positive and negative values. For example, intuitively [0,0,0,0,0,1,0,0,0] is a stable array but its CV is 2.83 which is extremely high, but cv of [0.45,0.44,0.48,0.49,0.42,0.52,0.49,0.47,0.48] is 0.06 which is much more reasonable. Thus we decided to use mean instead of median. Although median is considered as a more robust choice, outlier treatment can be applied prior to data stability analysis to handle this issue. Why kurtosis is chosen over skewness? Kurtosis is a positive value (note that we use kurtosis instead of excess kurtosis which) but skewness can range from \u2013inf to +inf. Usually, if skewness is between -0.5 and 0.5, the distribution is approximately symmetric. Thus, if the skewness fluctuates around 0, the CV is highly likely to be high or invalid because the mean will be close to 0. Stability index is preferred in the following scenario: Pairwise drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. However this can be time-consuming especially when the number of target dataset is large. In this case, measuring data stability instead of data drift would be a much faster alternative and the source/baseline dataset is not required as well Troubleshooting If the attribute stability index appears to be nan, it may due to one of the following reasons: One metric (likely to be kurtosis) is nan. For example, the kurtosis of a sample is nan If its standard deviation is 0. The mean of a metric from the first to the N-th dataset is zero, causing the denominator of CV to be 0. For example, when mean of attribute X is always zero for all datasets, its stability index would be nan. Limitation Limitation of CV: CV may not work well when 0 appears in the array or the array contains both positive and negative values. idfs : Input Dataframes (flexible) list_of_cols : Numerical columns (in list format or string separated by |). Use \u2018all\u2019 - to include all numerical columns (excluding drop_cols). drop_cols : List of columns to be dropped (list or string of col names separated by |) metric_weightages : A dictionary with key being the metric name (mean,stdev,kurtosis) and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. existing_metric_path : this argument is path for pre-existing metrics of historical datasets . idx is index number of historical datasets assigned in chronological order appended_metric_path : this argument is path for saving input dataframes metrics after appending to the historical datasets' metrics. threshold : To flag unstable attributes meeting the threshold","title":"stabilityIndex_computation"},{"location":"docs/anovos-modules-overview/data_transformer/","text":"Module ANOVOS.transformers In this release, data transformer module supports selected pre-processing functions such binning, encoding, to name a few, which were required for statistics generation and quality checks. However, in the future releases, more exhaustive transformations will be included. List of functions included in this modules are: attribute_binning monotonic_binning cat_to_num_unsupervised imputation_MMM outlier_categories Columns which are subjected to these analysis can be controlled by right combination of arguments - list_of_cols and drop_cols. All functions have following common arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all valid columns. This is super useful instead of specifying all column names manually. drop_cols : This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. output_mode : replace or append. \u201creplace\u201d option replaces original columns with transformed column, whereas \u201cappend\u201d option append transformed column to the input dataset. print_impact : This argument is to print out the statistics. attribute_binning Attribute binning (or discretization) is a method of numerical attribute into discrete (integer or categorical values) using pre-defined number of bins. This data pre-processing technique is used to reduce the effects of minor observation errors. Also, Binning introduces non-linearity and tends to improve the performance of the model. In this function, we are focussing on unsupervised way of binning i.e. without considering the target variable into account - Equal Range Binning, Equal Frequency Binning. In Equal Range method, each bin is of equal size/width and computed as: w = max- min / no. of bins bins cutoff=[min, min+w,min+2w\u2026..,max-w,max] whereas in Equal Frequency binning method, bins are created in such a way that each bin has equal no. of rows, though the width of bins may vary from each other. w = 1 / no. of bins bins cutoff=[min, wthpctile, 2wthpctile\u2026.,max ] idf list_of_cols: If \u2018all\u2019 is passed for this argument, then only numerical attributes are selected. drop_cols method_type: equal_frequency, equal_range bin_size: Number of bins bin_dtype: numerical, categorical . Original value is replaced with Integer (1,2,\u2026) with \u2018numerical\u2019 input and replaced with string describing min and max value observed in the bin (\"minval-maxval\") pre_existing_model: This argument takes Boolean type input \u2013 True or False. True if the file with bin cutoff values exists already, False Otherwise. model_path: If pre_existing_model is True, this argument is path for pre-saved model file. If pre_existing_model is False, this field can be used for saving the model file. Default NA means there is neither pre-saved model file nor there is a need to save one. output_mode: All transformed columns are appended with the naming convention - \"{original.column.name}_binned\". print_impact monotonic_binning This function constitutes supervised way of binning the numerical attribute into discrete (integer or categorical values) attribute. Instead of pre-defined fixed number of bins, number of bins are computed dynamically ensuring the monotonic nature of bins i.e. % event should increase or decrease with the bin. Monotonic nature of bins is evaluated by looking at spearman rank correlation, which should be either +1 or -1, between the bin index and % event. In case, the monotonic nature is not attained, user defined fixed number of bins are used for the binning. idf list_of_cols: If 'all' is passed for this argument, then only numerical attributes are selected. drop_cols label_col : Name of label or target column in the input dataset event_label : Value of event (label 1) in the label column method_type : equal_frequency, equal_range_ bin_size : Number of bins_ bin_dtype: numerical, categorical_._ Original value is replaced with Integer (1,2,\u2026) with 'numerical' input and replaced with string describing min and max value observed in the bin '<bin_cutoffi >- <bin_cutoffi+1>' output_mode: All transformed columns are appended with the naming convention - \"{original.column.name}_binned\". cat_to_num_unsupervised This is unsupervised method of converting a categorical attribute into numerical attribute(s). This is among the most important transformations required for any modelling exercise, as most of the machine learning algorithms cannot process categorical values. It covers two popular encoding techniques \u2013 label encoding & one-hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available \u2013 can be selected by index_order argument). One of the pitfalls of using this technique is that the model may learn some spurious relationship, which doesn't exist or make logical sense in the real world. In one-hot encoding, every unique value in the attribute will be added as a feature in a form of dummy/binary attribute. However, using this method on high cardinality attributes can further aggravate the dimensionality issue. idf list_of_cols: If 'all' is passed for this argument, then only categorical attributes are selected. drop_cols method_type: 1 (for Label Encoding) or 0 (for One hot encoding) index_order: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc (Valid only for Label Encoding) onehot_dropLast: This argument takes Boolean type input \u2013 True or False. if True, it drops one last column in one hot encoding pre_existing_model: This argument takes Boolean type input \u2013 True or False. True if the encoding models exist already, False Otherwise. model_path: If pre_existing_model is True, this argument is path for pre-saved model. If pre_existing_model is False, this field can be used for saving the mode. Default NA means there is neither pre-saved model nor there is a need to save one. output_mode: All transformed columns are appended with the naming convention - \"{original.column.name} index\" for label encoding. & \"{original.column.name} {n}\" for one hot encoding, n varies from 0 to unique value count. print_impact imputation_MMM This function handles missing value related issues by substituting null values by the measure of central tendency (mode for categorical features and mean/median for numerical features). For numerical attributes, it leverages Imputer functionality of Spark MLlib. Though, Imputer can be used for categorical attributes but this feature is available only in Spark3.x, therefore for categorical features, we compute mode or leverage mode computation from Measures of Central Tendency. idf list_of_cols : 'missing' can be used for this argument, in which case, it will analyse only those columns with any missing value. drop_cols method_type : median (default), mean. Valid only for Numerical attributes. pre_existing_model : This argument takes Boolean type input \u2013 True or False. True if the encoding models exist already, False Otherwise. model_path : If pre_existing_model is True, this argument is path for pre-saved model. If pre_existing_model is False, this field can be used for saving the mode. Default NA means there is neither pre-saved model nor there is a need to save one. output_mode : All transformed columns are appended with the naming convention - \"{original.column.name}_imputed\". stats_missing : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_counts function of stats generator stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator print_impact outlier_categories This function replaces less frequently seen values (called as outlier values in the current context) in a categorical column by 'others'. Outlier values can be defined in two ways \u2013 a) Max N categories, where N is used defined value. In this method, top N-1 frequently seen categories are considered and rest are clubbed under single category 'others'. or Alternatively, b) Coverage \u2013 top frequently seen categories are considered till it covers minimum N% of rows and rest lesser seen values are mapped to mapped to others. Even if the Coverage is less, maximum category constraint is given priority. Further, there is a caveat that when multiple categories have same rank. Then, number of categorical values can be more than max_category defined by the user. idf list_of_cols : 'missing' can be used for this argument, in which case, it will analyse only those columns with any missing value. drop_cols coverage : Minimum % of rows mapped to actual category name and rest will be mapped to others max_category : Maximum number of categories allowed pre_existing_model : This argument takes Boolean type input \u2013 True or False. True if the file with outlier values exist already for each attribute, False Otherwise. model_path : If pre_existing_model is True, this argument is path for pre-saved model file. If pre_existing_model is False, this field can be used for saving the model file. Default NA means there is neither pre-saved model nor there is a need to save one. output_mode : All transformed columns are appended with the naming convention - \"{original.column.name}_ outliered \". print_impact","title":"Data Transformers"},{"location":"docs/anovos-modules-overview/data_transformer/#module-anovostransformers","text":"In this release, data transformer module supports selected pre-processing functions such binning, encoding, to name a few, which were required for statistics generation and quality checks. However, in the future releases, more exhaustive transformations will be included. List of functions included in this modules are: attribute_binning monotonic_binning cat_to_num_unsupervised imputation_MMM outlier_categories Columns which are subjected to these analysis can be controlled by right combination of arguments - list_of_cols and drop_cols. All functions have following common arguments: idf : Input dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all valid columns. This is super useful instead of specifying all column names manually. drop_cols : This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. output_mode : replace or append. \u201creplace\u201d option replaces original columns with transformed column, whereas \u201cappend\u201d option append transformed column to the input dataset. print_impact : This argument is to print out the statistics.","title":"Module ANOVOS.transformers"},{"location":"docs/anovos-modules-overview/data_transformer/#attribute_binning","text":"Attribute binning (or discretization) is a method of numerical attribute into discrete (integer or categorical values) using pre-defined number of bins. This data pre-processing technique is used to reduce the effects of minor observation errors. Also, Binning introduces non-linearity and tends to improve the performance of the model. In this function, we are focussing on unsupervised way of binning i.e. without considering the target variable into account - Equal Range Binning, Equal Frequency Binning. In Equal Range method, each bin is of equal size/width and computed as: w = max- min / no. of bins bins cutoff=[min, min+w,min+2w\u2026..,max-w,max] whereas in Equal Frequency binning method, bins are created in such a way that each bin has equal no. of rows, though the width of bins may vary from each other. w = 1 / no. of bins bins cutoff=[min, wthpctile, 2wthpctile\u2026.,max ] idf list_of_cols: If \u2018all\u2019 is passed for this argument, then only numerical attributes are selected. drop_cols method_type: equal_frequency, equal_range bin_size: Number of bins bin_dtype: numerical, categorical . Original value is replaced with Integer (1,2,\u2026) with \u2018numerical\u2019 input and replaced with string describing min and max value observed in the bin (\"minval-maxval\") pre_existing_model: This argument takes Boolean type input \u2013 True or False. True if the file with bin cutoff values exists already, False Otherwise. model_path: If pre_existing_model is True, this argument is path for pre-saved model file. If pre_existing_model is False, this field can be used for saving the model file. Default NA means there is neither pre-saved model file nor there is a need to save one. output_mode: All transformed columns are appended with the naming convention - \"{original.column.name}_binned\". print_impact","title":"attribute_binning"},{"location":"docs/anovos-modules-overview/data_transformer/#monotonic_binning","text":"This function constitutes supervised way of binning the numerical attribute into discrete (integer or categorical values) attribute. Instead of pre-defined fixed number of bins, number of bins are computed dynamically ensuring the monotonic nature of bins i.e. % event should increase or decrease with the bin. Monotonic nature of bins is evaluated by looking at spearman rank correlation, which should be either +1 or -1, between the bin index and % event. In case, the monotonic nature is not attained, user defined fixed number of bins are used for the binning. idf list_of_cols: If 'all' is passed for this argument, then only numerical attributes are selected. drop_cols label_col : Name of label or target column in the input dataset event_label : Value of event (label 1) in the label column method_type : equal_frequency, equal_range_ bin_size : Number of bins_ bin_dtype: numerical, categorical_._ Original value is replaced with Integer (1,2,\u2026) with 'numerical' input and replaced with string describing min and max value observed in the bin '<bin_cutoffi >- <bin_cutoffi+1>' output_mode: All transformed columns are appended with the naming convention - \"{original.column.name}_binned\".","title":"monotonic_binning"},{"location":"docs/anovos-modules-overview/data_transformer/#cat_to_num_unsupervised","text":"This is unsupervised method of converting a categorical attribute into numerical attribute(s). This is among the most important transformations required for any modelling exercise, as most of the machine learning algorithms cannot process categorical values. It covers two popular encoding techniques \u2013 label encoding & one-hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available \u2013 can be selected by index_order argument). One of the pitfalls of using this technique is that the model may learn some spurious relationship, which doesn't exist or make logical sense in the real world. In one-hot encoding, every unique value in the attribute will be added as a feature in a form of dummy/binary attribute. However, using this method on high cardinality attributes can further aggravate the dimensionality issue. idf list_of_cols: If 'all' is passed for this argument, then only categorical attributes are selected. drop_cols method_type: 1 (for Label Encoding) or 0 (for One hot encoding) index_order: frequencyDesc, frequencyAsc, alphabetDesc, alphabetAsc (Valid only for Label Encoding) onehot_dropLast: This argument takes Boolean type input \u2013 True or False. if True, it drops one last column in one hot encoding pre_existing_model: This argument takes Boolean type input \u2013 True or False. True if the encoding models exist already, False Otherwise. model_path: If pre_existing_model is True, this argument is path for pre-saved model. If pre_existing_model is False, this field can be used for saving the mode. Default NA means there is neither pre-saved model nor there is a need to save one. output_mode: All transformed columns are appended with the naming convention - \"{original.column.name} index\" for label encoding. & \"{original.column.name} {n}\" for one hot encoding, n varies from 0 to unique value count. print_impact","title":"cat_to_num_unsupervised"},{"location":"docs/anovos-modules-overview/data_transformer/#imputation_mmm","text":"This function handles missing value related issues by substituting null values by the measure of central tendency (mode for categorical features and mean/median for numerical features). For numerical attributes, it leverages Imputer functionality of Spark MLlib. Though, Imputer can be used for categorical attributes but this feature is available only in Spark3.x, therefore for categorical features, we compute mode or leverage mode computation from Measures of Central Tendency. idf list_of_cols : 'missing' can be used for this argument, in which case, it will analyse only those columns with any missing value. drop_cols method_type : median (default), mean. Valid only for Numerical attributes. pre_existing_model : This argument takes Boolean type input \u2013 True or False. True if the encoding models exist already, False Otherwise. model_path : If pre_existing_model is True, this argument is path for pre-saved model. If pre_existing_model is False, this field can be used for saving the mode. Default NA means there is neither pre-saved model nor there is a need to save one. output_mode : All transformed columns are appended with the naming convention - \"{original.column.name}_imputed\". stats_missing : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_counts function of stats generator stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator print_impact","title":"imputation_MMM"},{"location":"docs/anovos-modules-overview/data_transformer/#outlier_categories","text":"This function replaces less frequently seen values (called as outlier values in the current context) in a categorical column by 'others'. Outlier values can be defined in two ways \u2013 a) Max N categories, where N is used defined value. In this method, top N-1 frequently seen categories are considered and rest are clubbed under single category 'others'. or Alternatively, b) Coverage \u2013 top frequently seen categories are considered till it covers minimum N% of rows and rest lesser seen values are mapped to mapped to others. Even if the Coverage is less, maximum category constraint is given priority. Further, there is a caveat that when multiple categories have same rank. Then, number of categorical values can be more than max_category defined by the user. idf list_of_cols : 'missing' can be used for this argument, in which case, it will analyse only those columns with any missing value. drop_cols coverage : Minimum % of rows mapped to actual category name and rest will be mapped to others max_category : Maximum number of categories allowed pre_existing_model : This argument takes Boolean type input \u2013 True or False. True if the file with outlier values exist already for each attribute, False Otherwise. model_path : If pre_existing_model is True, this argument is path for pre-saved model file. If pre_existing_model is False, this field can be used for saving the model file. Default NA means there is neither pre-saved model nor there is a need to save one. output_mode : All transformed columns are appended with the naming convention - \"{original.column.name}_ outliered \". print_impact","title":"outlier_categories"},{"location":"docs/anovos-modules-overview/overview/","text":"Anovos modules reflect the key components of the Machine Learning (ML) pipeline and are scalable using python API of Spark (PySpark) - the distributed computing framework. The key modules included in the alpha release are: Data Ingest : This module is an ETL (Extract, transform, load) component of Anovos and helps load dataset(s) as Spark Dataframe. It also allows performing some basic pre-processing, like selecting, deleting, renaming, and recasting columns to ensure cleaner data is used in downstream data analysis. Data Analyzer : This data analysis module gives a 360\u00ba view of the ingested data. It helps provide a better understanding of the data quality and the transformations required for the modeling purpose. There are three submodules of this module targeting specific needs of the data analysis process. a. Statistics Generator : This submodule generates all descriptive statistics related to the ingested data. The descriptive statistics are further broken down into different metric types such as Measures of Counts, Measures of Central Tendency, Measures of Cardinality, Measures of Dispersion (aka Measures of Spread in Statistics), Measures of Percentiles (aka Measures of Position), and Measures of Shape (aka Measures of Moments). b. Quality Checker : This submodule focuses on assessing the data quality at both row and column levels. It includes an option to fix identified issues with the correct treatment method. The row-level quality checks include duplicate detection and null detection (% columns that are missing for a row). The column level quality checks include outlier detection, null detection (% rows which are missing for a column), biasedness detection (checking if a column is biased towards one specific value), cardinality detection (checking if a categorical/discrete column have very high no. of unique values) and invalid entries detection which checks for suspicious patterns in the column values. c. Association Evaluator : This submodule focuses on understanding the interaction between different attributes (correlation, variable clustering) and/or the relationship between an attribute & the binary target variable (Information Gain, Information Value). Data Drift : In an ML context, data drift is the change in the distribution of the baseline dataset that trained the model (source distribution) and the ingested data (target distribution) that makes the prediction. Data drift is one of the primary causes of poor performance of ML models over time. This module ensures the stability of the ingested dataset over time by analyzing it with the baseline dataset (via computing drift statistics) and/or with historically ingested datasets (via computing stability index \u2013 currently supports only numerical features), if available. Identifying the data drift at an early stage enables data scientists to be proactive and fix the root cause. Data Transformer : In the alpha release, the data transformer module only includes some basic pre-processing functions like binning, encoding, to name a few. These functions were required to support computations of the above key modules. A more exhaustive set of transformations can be expected in future releases. Data Report : This module is a visualization component of Anovos. All the analysis on the key modules is visualized via an HTML report to get a well-rounded understanding of the ingested dataset. The report contains an executive summary, wiki for data dictionary & metric dictionary, a tab corresponding to key modules demonstrating the output. Note: Upcoming Modules - Feature Wiki, Feature store, Auto ML, ML Flow Integration","title":"Overview"},{"location":"docs/anovos-modules-overview/quality-checker/","text":"Module ANOVOS.quality_checker This submodule focus on assessing the data quality at both row level and column level and also provides an appropriate treatment option to fix those quality issues. Columns which are subjected to these analysis can be controlled by right combination of arguments - list_of_cols and drop_cols. All functions have following common arguments: idf: Input dataframe list_of_cols: This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols: This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. print_impact: This argument is to print out the statistics. At row level, the following checks are done: duplicate_detection nullRows_detection At column level, the following checks are done: nullColumns_detection outlier_detection IDness_detection biasedness_detection invalidEntries_detection duplicate_detection As the name suggests, this function detects duplication in the input dataset. This means, for a pair of duplicate rows, the values in each column coincide. Duplication check is confined to the list of columns passed in the arguments. As the part of treatment, duplicated rows are removed. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after deduplication (if treated) and 2nd dataframe is of schema \u2013 metric, value and contains total number of rows and number of unique rows. idf list_of_cols drop_cols treatment: This argument takes Boolean type input \u2013 True or False. If true, duplicate rows are removed from the input dataset. print_impact nullRows_detection This function inspects the row quality and computes number of columns which are missing for a row. This metric is further aggregated to check how many columns are missing for how many rows (also at % level). Intuition is if too many columns are missing for a row, removing it from the modelling may give better results than relying on its imputed values. Therefore as the part of treatment, rows with missing columns above the specified threshold are removed. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after filtering rows with high number of missing columns (if treated) and 2nd dataframe is of schema \u2013 null_cols_count, row_count, row_pct, flagged. null_cols_count row_count row_pct flagged 5 11 3.0E-4 0 7 1306 0.0401 1 Interpretation: 1306 rows (4.01% of total rows) have 7 missing columns and flagged for removal because null_cols_count is above the threshold. idf list_of_cols drop_cols treatment: This argument takes Boolean type input \u2013 True or False. If true, rows with high null columns (defined by treatment_threshold argument) are removed from the input dataset. treatment_threshold: This argument takes value between 0 to 1 with default 0.8, which means 80% of columns allowed to be Null per row. If it is more than the threshold, then it is flagged and if treatment is True, then affected rows are removed. If threshold is 0, it means, rows with any missing value will be flagged. If threshold is 1, it means rows with all missing value will be flagged. print_impact nullColumns_detection This function inspects the column quality and computes number of rows which are missing for a column. This function also leverages statistics which were computed as the part of the State Generator module so that statistics are not computed twice if already available. As part of treatments, it currently supports 3 methods \u2013 Mean Median Mode (MMM), row_removal or column_removal (more methods to be added soon). MMM replaces null value by the measure of central tendency (mode for categorical features and mean/median for numerical features). row_removal removes all rows with any missing value (output of this treatment is same as nullRows_detection with treatment_threshold of 0). column_removal remove a column if %rows with missing value is above treatment_threshold. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after imputation (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, missing_count, missing_pct. idf list_of_cols : \"all\" can be passed to include all (non-array) columns for analysis. \"missing\" (default) can be passed to include only those columns with missing values. One of the use cases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, missing values are treated as per treatment_method argument treatment_method : MMM, row_removal or column_removal treatment_configs : This argument takes input in dictionary format with keys \u2013 \u2018treatment_threshold\u2019 for column_removal treatment, or all arguments corresponding to imputation_MMM function. stats_missing : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_counts function of stats generator stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator print_impact outlier_detection In Machine Learning, outlier detection is the identification of values that deviates drastically from the rest of the attribute values. An outlier may be caused simply by chance, measurement error or inherent heavy-tailed distribution. This function identify extreme values in both directions (or any direction provided by the user via detection_side argument). Outlier is identified by 3 different methodologies and tagged an outlier only if it is validated by at least 2 methodologies (can be changed by the user via min_validation under detection_configs argument). Percentile Method: In this methodology, a value higher than a certain (default 95th) percentile value is considered as an outlier. Similarly, a value lower than a certain (default 5th) percentile value is considered as an outlier. Standard Deviation Method: In this methodology, if a value is certain number of standard deviations (default 3) away from the mean, then it is identified as an outlier. Interquartile Range (IQR) Method: A value which is below Q1 \u2013 1.5 IQR or above Q3 + 1.5 IQR are identified as outliers, where Q1 is first quantile/25th percentile, Q3 is third quantile/75th percentile and IQR is difference between third quantile & first quantile. This function also leverages statistics which were computed as the part of the State Generator module so that statistics are not computed twice if already available. As part of treatments available, outlier values can be replaced by null so that it can be imputed by a reliable imputation methodology (null_replacement). It can also be replaced by maximum or minimum permissible by above methodologies (value_replacement). Lastly, rows can be removed if it is identified with any outlier (row_removal). This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating outlier (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, lower_outliers, upper_outliers. If outliers are checked only for upper end, then lower_outliers column will be shown all zero. Similarly if checked only for lower end, then upper_outliers will be zero for all attributes. idf list_of_cols : Any attribute with single value or all null values are not subjected to outlier detection even if it is selected under this argument. drop_cols detection_side : upper, lower, both detection_configs : This argument takes input in dictionary format with keys (representing upper and lower bound for different outlier identification methodologies) - pctile_lower (default 0.05), pctile_upper (default 0.95), stdev_lower (default 3.0), stdev_upper (default 3.0), IQR_lower (default 1.5), IQR_upper (default 1.5), min_validation (default 2) treatment : This argument takes Boolean type input \u2013 True or False. If true, specified treatment method is applied. treatment_method : null_replacement, row_removal, value_replacement pre_existing_model : This argument takes Boolean type input \u2013 True or False. True if the file with upper/lower permissible values exists already, False Otherwise. model_path : If pre_existing_model is True, this argument is path for pre-saved model file. If pre_existing_model is False, this field can be used for saving the model file. Default NA means there is neither pre-saved model file nor there is a need to save one. output_mode : replace or append. \u201creplace\u201d option replaces original columns with treated column, whereas \u201cappend\u201d option append treated column to the input dataset. All treated columns are appended with the naming convention - \"{original.column.name}_outliered\". stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator print_impact IDness_detection IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for categorical features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, , unique_values, IDness. idf list_of_cols drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, columns above IDness threshold are removed. treatment_threshold : This argument takes value between 0 to 1 with default 1.0. stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator. print_impact biasedness_detection This function flags column if they are biased or skewed towards one specific value and is equivalent to mode_pct computation from Measures of Central Tendency i.e. number of rows with mode value (most frequently seen value) divided by number of non-null values. It varies between 0 to 100% where biasedness of 100% means there is only a single value (other than null). The function flags a column if its biasedness is above a certain threshold. Such columns can be deleted from the modelling analysis, if required. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high biased columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, mode, mode_pct. idf list_of_cols drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, columns above biasedness threshold are removed. treatment_threshold : This argument takes value between 0 to 1 with default 1.0. stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator. print_impact invalidEntries_detection This function checks for certain suspicious patterns in attributes\u2019 values. These suspicious values can be replaced as null and treated as missing. Patterns that are considered for this quality check: Missing Values: The function checks for all text strings which directly or indirectly indicate the missing value in an attribute. Currently, we check the following string values - '', ' ', 'nan', 'null', 'na', 'inf', 'n/a', 'not defined', 'none', 'undefined', 'blank'. The function also check for special characters such as ?, *, to name a few. Repetitive Characters: Certain attributes\u2019 values with repetitive characters may be default value or system error, rather than being a legit value etc xx, zzzzz, 99999 etc. Such values are flagged for the user to take an appropriate action. There may be certain false positive which are legit values. Consecutive Characters: Similar to repetitive characters, consecutive characters (at least 3 characters long) such as abc, 1234 etc may not be legit values, and hence flagged. There may be certain false positive which are legit values. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after replacing flagged values as null (or the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, invalid_entries, invalid_count, invalid_pct. All potential invalid values (separated by delimiter pipe \u201c|\u201d) are shown under invalid_entries column. Total number of rows impacted by these entries for each attribute is shown under invalid_count. invalid_pct is invalid_count divided by number of rows idf list_of_cols drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, columns above biasedness threshold are removed. output_mode : replace or append. \u201creplace\u201d option replaces original columns with treated column, whereas \u201cappend\u201d option append treated column to the input dataset. All treated columns are appended with the naming convention - \"{original.column.name}_cleaned\". print_impact","title":"Quality Checker"},{"location":"docs/anovos-modules-overview/quality-checker/#module-anovosquality_checker","text":"This submodule focus on assessing the data quality at both row level and column level and also provides an appropriate treatment option to fix those quality issues. Columns which are subjected to these analysis can be controlled by right combination of arguments - list_of_cols and drop_cols. All functions have following common arguments: idf: Input dataframe list_of_cols: This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols: This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. print_impact: This argument is to print out the statistics. At row level, the following checks are done: duplicate_detection nullRows_detection At column level, the following checks are done: nullColumns_detection outlier_detection IDness_detection biasedness_detection invalidEntries_detection","title":"Module ANOVOS.quality_checker"},{"location":"docs/anovos-modules-overview/quality-checker/#duplicate_detection","text":"As the name suggests, this function detects duplication in the input dataset. This means, for a pair of duplicate rows, the values in each column coincide. Duplication check is confined to the list of columns passed in the arguments. As the part of treatment, duplicated rows are removed. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after deduplication (if treated) and 2nd dataframe is of schema \u2013 metric, value and contains total number of rows and number of unique rows. idf list_of_cols drop_cols treatment: This argument takes Boolean type input \u2013 True or False. If true, duplicate rows are removed from the input dataset. print_impact","title":"duplicate_detection"},{"location":"docs/anovos-modules-overview/quality-checker/#nullrows_detection","text":"This function inspects the row quality and computes number of columns which are missing for a row. This metric is further aggregated to check how many columns are missing for how many rows (also at % level). Intuition is if too many columns are missing for a row, removing it from the modelling may give better results than relying on its imputed values. Therefore as the part of treatment, rows with missing columns above the specified threshold are removed. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after filtering rows with high number of missing columns (if treated) and 2nd dataframe is of schema \u2013 null_cols_count, row_count, row_pct, flagged. null_cols_count row_count row_pct flagged 5 11 3.0E-4 0 7 1306 0.0401 1 Interpretation: 1306 rows (4.01% of total rows) have 7 missing columns and flagged for removal because null_cols_count is above the threshold. idf list_of_cols drop_cols treatment: This argument takes Boolean type input \u2013 True or False. If true, rows with high null columns (defined by treatment_threshold argument) are removed from the input dataset. treatment_threshold: This argument takes value between 0 to 1 with default 0.8, which means 80% of columns allowed to be Null per row. If it is more than the threshold, then it is flagged and if treatment is True, then affected rows are removed. If threshold is 0, it means, rows with any missing value will be flagged. If threshold is 1, it means rows with all missing value will be flagged. print_impact","title":"nullRows_detection"},{"location":"docs/anovos-modules-overview/quality-checker/#nullcolumns_detection","text":"This function inspects the column quality and computes number of rows which are missing for a column. This function also leverages statistics which were computed as the part of the State Generator module so that statistics are not computed twice if already available. As part of treatments, it currently supports 3 methods \u2013 Mean Median Mode (MMM), row_removal or column_removal (more methods to be added soon). MMM replaces null value by the measure of central tendency (mode for categorical features and mean/median for numerical features). row_removal removes all rows with any missing value (output of this treatment is same as nullRows_detection with treatment_threshold of 0). column_removal remove a column if %rows with missing value is above treatment_threshold. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after imputation (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, missing_count, missing_pct. idf list_of_cols : \"all\" can be passed to include all (non-array) columns for analysis. \"missing\" (default) can be passed to include only those columns with missing values. One of the use cases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, missing values are treated as per treatment_method argument treatment_method : MMM, row_removal or column_removal treatment_configs : This argument takes input in dictionary format with keys \u2013 \u2018treatment_threshold\u2019 for column_removal treatment, or all arguments corresponding to imputation_MMM function. stats_missing : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_counts function of stats generator stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator print_impact","title":"nullColumns_detection"},{"location":"docs/anovos-modules-overview/quality-checker/#outlier_detection","text":"In Machine Learning, outlier detection is the identification of values that deviates drastically from the rest of the attribute values. An outlier may be caused simply by chance, measurement error or inherent heavy-tailed distribution. This function identify extreme values in both directions (or any direction provided by the user via detection_side argument). Outlier is identified by 3 different methodologies and tagged an outlier only if it is validated by at least 2 methodologies (can be changed by the user via min_validation under detection_configs argument). Percentile Method: In this methodology, a value higher than a certain (default 95th) percentile value is considered as an outlier. Similarly, a value lower than a certain (default 5th) percentile value is considered as an outlier. Standard Deviation Method: In this methodology, if a value is certain number of standard deviations (default 3) away from the mean, then it is identified as an outlier. Interquartile Range (IQR) Method: A value which is below Q1 \u2013 1.5 IQR or above Q3 + 1.5 IQR are identified as outliers, where Q1 is first quantile/25th percentile, Q3 is third quantile/75th percentile and IQR is difference between third quantile & first quantile. This function also leverages statistics which were computed as the part of the State Generator module so that statistics are not computed twice if already available. As part of treatments available, outlier values can be replaced by null so that it can be imputed by a reliable imputation methodology (null_replacement). It can also be replaced by maximum or minimum permissible by above methodologies (value_replacement). Lastly, rows can be removed if it is identified with any outlier (row_removal). This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating outlier (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, lower_outliers, upper_outliers. If outliers are checked only for upper end, then lower_outliers column will be shown all zero. Similarly if checked only for lower end, then upper_outliers will be zero for all attributes. idf list_of_cols : Any attribute with single value or all null values are not subjected to outlier detection even if it is selected under this argument. drop_cols detection_side : upper, lower, both detection_configs : This argument takes input in dictionary format with keys (representing upper and lower bound for different outlier identification methodologies) - pctile_lower (default 0.05), pctile_upper (default 0.95), stdev_lower (default 3.0), stdev_upper (default 3.0), IQR_lower (default 1.5), IQR_upper (default 1.5), min_validation (default 2) treatment : This argument takes Boolean type input \u2013 True or False. If true, specified treatment method is applied. treatment_method : null_replacement, row_removal, value_replacement pre_existing_model : This argument takes Boolean type input \u2013 True or False. True if the file with upper/lower permissible values exists already, False Otherwise. model_path : If pre_existing_model is True, this argument is path for pre-saved model file. If pre_existing_model is False, this field can be used for saving the model file. Default NA means there is neither pre-saved model file nor there is a need to save one. output_mode : replace or append. \u201creplace\u201d option replaces original columns with treated column, whereas \u201cappend\u201d option append treated column to the input dataset. All treated columns are appended with the naming convention - \"{original.column.name}_outliered\". stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator print_impact","title":"outlier_detection"},{"location":"docs/anovos-modules-overview/quality-checker/#idness_detection","text":"IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for categorical features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, , unique_values, IDness. idf list_of_cols drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, columns above IDness threshold are removed. treatment_threshold : This argument takes value between 0 to 1 with default 1.0. stats_unique : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_cardinality function of stats generator. print_impact","title":"IDness_detection"},{"location":"docs/anovos-modules-overview/quality-checker/#biasedness_detection","text":"This function flags column if they are biased or skewed towards one specific value and is equivalent to mode_pct computation from Measures of Central Tendency i.e. number of rows with mode value (most frequently seen value) divided by number of non-null values. It varies between 0 to 100% where biasedness of 100% means there is only a single value (other than null). The function flags a column if its biasedness is above a certain threshold. Such columns can be deleted from the modelling analysis, if required. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high biased columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, mode, mode_pct. idf list_of_cols drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, columns above biasedness threshold are removed. treatment_threshold : This argument takes value between 0 to 1 with default 1.0. stats_mode : Arguments corresponding to read_dataset function in dictionary format, to read output from measures_of_centralTendency function of stats generator. print_impact","title":"biasedness_detection"},{"location":"docs/anovos-modules-overview/quality-checker/#invalidentries_detection","text":"This function checks for certain suspicious patterns in attributes\u2019 values. These suspicious values can be replaced as null and treated as missing. Patterns that are considered for this quality check: Missing Values: The function checks for all text strings which directly or indirectly indicate the missing value in an attribute. Currently, we check the following string values - '', ' ', 'nan', 'null', 'na', 'inf', 'n/a', 'not defined', 'none', 'undefined', 'blank'. The function also check for special characters such as ?, *, to name a few. Repetitive Characters: Certain attributes\u2019 values with repetitive characters may be default value or system error, rather than being a legit value etc xx, zzzzz, 99999 etc. Such values are flagged for the user to take an appropriate action. There may be certain false positive which are legit values. Consecutive Characters: Similar to repetitive characters, consecutive characters (at least 3 characters long) such as abc, 1234 etc may not be legit values, and hence flagged. There may be certain false positive which are legit values. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after replacing flagged values as null (or the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, invalid_entries, invalid_count, invalid_pct. All potential invalid values (separated by delimiter pipe \u201c|\u201d) are shown under invalid_entries column. Total number of rows impacted by these entries for each attribute is shown under invalid_count. invalid_pct is invalid_count divided by number of rows idf list_of_cols drop_cols treatment : This argument takes Boolean type input \u2013 True or False. If true, columns above biasedness threshold are removed. output_mode : replace or append. \u201creplace\u201d option replaces original columns with treated column, whereas \u201cappend\u201d option append treated column to the input dataset. All treated columns are appended with the naming convention - \"{original.column.name}_cleaned\". print_impact","title":"invalidEntries_detection"},{"location":"docs/anovos-on-aws-emr/getting-started/","text":"Step 1: Installing/Downloading Anovos Clone the Anovos Repository on local environment using command: git clone https://github.com/anovos/anovos.git or pip3 install \"git+https://github.com/anovos/anovos.git\" After cloning, go to anovos directory and execute the following command to clean and build the latest modules in dist folder: make clean build Step 2: Copy necessary files to AWS S3 Copy the following files to AWS S3: dist/anovos.zip This file contains all Anovos modules Zipped version is mandatory for running importing the modules as \u2013py-files dist/income_dataset (optional) This folder contains our demo dataset dist/main.py This is sample script to show how different functions from Anovos module can be stitched together to create a workflow. The users can create their own workflow script by importing the necessary functions. This script takes input from a yaml configuration file dist/configs.yaml This is the sample yaml configuration file which sets the argument for all functions. Update configs.yaml for all input & output s3 paths. All other changes depends upon the dataset being used. bin/req_packages_anovos.sh This shell script is used to install all required packages to run Anovos on EMR AWS copy command: aws s3 cp --recursive <local file path> <s3 path> --profile <profile name> Step 3: Creating Cluster Software Configuration Emr-5.33.0 Hadoop-2.10.1 Spark-2.4.7 Hive-2.3.7 Spark Submit Details Deploy mode client Spark-submit options --num-executors 1000 --executor-cores 2 --executor-memory 20g --driver-memory 20G --driver-cores 4 --conf spark.driver.maxResultSize=15g --conf spark.yarn.am.memoryOverhead=1000m --conf spark.executor.memoryOverhead=2000m --conf spark.kryo.referenceTracking=false --conf spark.network.timeout=18000s --conf spark.executor.heartbeatInterval=12000s --conf spark.dynamicAllocation.executorIdleTimeout=12000s --conf spark.rpc.message.maxSize=1024 --conf spark.yarn.maxAppAttempts=1 --conf spark.speculation=false --conf spark.kryoserializer.buffer.max=1024 --conf spark.executor.extraJavaOptions=-XX:+UseG1GC --conf spark.driver.extraJavaOptions=-XX:+UseG1GC --packages org.apache.spark:spark-avro_2.11:2.4.0 --jars s3://mw.com.ds.kajanan/Vishnu/ml_ingest/jars/histogrammar-sparksql_2.11-1.0.20.jar,s3://mw.com.ds.kajanan/Vishnu/ml_ingest/jars/histogrammar_2.11-1.0.20.jar --py-files s3://mw.com.ds.kajanan/Sumit/Anovos_testing/income_data_emr/com.zip Application location*: s3 path of main.py file Arguments: , - Bootstrap Actions script location : path of bootstrap shell script (req_packages_anovos.sh)","title":"Anovos on AWS EMR"},{"location":"docs/anovos-on-aws-emr/getting-started/#step-1-installingdownloading-anovos","text":"Clone the Anovos Repository on local environment using command: git clone https://github.com/anovos/anovos.git or pip3 install \"git+https://github.com/anovos/anovos.git\" After cloning, go to anovos directory and execute the following command to clean and build the latest modules in dist folder: make clean build","title":"Step 1: Installing/Downloading Anovos"},{"location":"docs/anovos-on-aws-emr/getting-started/#step-2-copy-necessary-files-to-aws-s3","text":"Copy the following files to AWS S3: dist/anovos.zip This file contains all Anovos modules Zipped version is mandatory for running importing the modules as \u2013py-files dist/income_dataset (optional) This folder contains our demo dataset dist/main.py This is sample script to show how different functions from Anovos module can be stitched together to create a workflow. The users can create their own workflow script by importing the necessary functions. This script takes input from a yaml configuration file dist/configs.yaml This is the sample yaml configuration file which sets the argument for all functions. Update configs.yaml for all input & output s3 paths. All other changes depends upon the dataset being used. bin/req_packages_anovos.sh This shell script is used to install all required packages to run Anovos on EMR AWS copy command: aws s3 cp --recursive <local file path> <s3 path> --profile <profile name>","title":"Step 2: Copy necessary files to AWS S3"},{"location":"docs/anovos-on-aws-emr/getting-started/#step-3-creating-cluster","text":"Software Configuration Emr-5.33.0 Hadoop-2.10.1 Spark-2.4.7 Hive-2.3.7 Spark Submit Details Deploy mode client Spark-submit options --num-executors 1000 --executor-cores 2 --executor-memory 20g --driver-memory 20G --driver-cores 4 --conf spark.driver.maxResultSize=15g --conf spark.yarn.am.memoryOverhead=1000m --conf spark.executor.memoryOverhead=2000m --conf spark.kryo.referenceTracking=false --conf spark.network.timeout=18000s --conf spark.executor.heartbeatInterval=12000s --conf spark.dynamicAllocation.executorIdleTimeout=12000s --conf spark.rpc.message.maxSize=1024 --conf spark.yarn.maxAppAttempts=1 --conf spark.speculation=false --conf spark.kryoserializer.buffer.max=1024 --conf spark.executor.extraJavaOptions=-XX:+UseG1GC --conf spark.driver.extraJavaOptions=-XX:+UseG1GC --packages org.apache.spark:spark-avro_2.11:2.4.0 --jars s3://mw.com.ds.kajanan/Vishnu/ml_ingest/jars/histogrammar-sparksql_2.11-1.0.20.jar,s3://mw.com.ds.kajanan/Vishnu/ml_ingest/jars/histogrammar_2.11-1.0.20.jar --py-files s3://mw.com.ds.kajanan/Sumit/Anovos_testing/income_data_emr/com.zip Application location*: s3 path of main.py file Arguments: , - Bootstrap Actions script location : path of bootstrap shell script (req_packages_anovos.sh)","title":"Step 3: Creating Cluster"},{"location":"docs/anovos-potential-workflow/","text":"We designed Anovos to support the following end-to-end machine learning workflow. A team or organization can do this in one of two ways, either as a complete execution by Anovos or use as part of an existing pipeline. For example, an organization might have an end-to-end workflow that lacks components offered by Anovos. The organization can incorporate additional components by a simple API call for a function from Anovos. The following workflow diagram shows the potential ways to use Anovos in an end-to-end workflow settings.","title":"Anovos Potential Workflow"},{"location":"docs/anovos-roadmap/","text":"Anovos is built and released as an open source project based on our experience using massive data sets to produce predictive features. At Mobilewalla, we process terabytes of mobile engagement signals daily to mine consumer behavior and use features from that data to build distributed machine learning models to solve the respective business problems. In this journey, we faced lots of challenges by not having a comprehensive and scalable library. After realizing the unavailability of such libraries, we designed and implemented an open source library (ANOVOS) for every Data Scientists\u2019 use. The Roadmap As you can see, Anovos will be fully functional in 3 major releases, namely alpha, beta, and V1.0, respectively. The alpha release of Anovos has the essential data ingestion and comprehensive data analysis functionalities, as well as the data pre-processing and cleaning mechanisms. It also has some key differentiating functionalities, like data drift and stability computations, which are crucial in deciding the need for model refreshing/tweaking options. Another benefit of Anovos is a dynamic visualization component configured based on data ingestion pipelines. Every data metric computed from the Anovos ingestion process can be visualized and utilized for CXO level decision-making. Beta Release In the Beta release of Anovos, it will support ingesting from other cloud service providers like MS Azure and will have mechanisms to read/write different file formats such as avro & nested Json. It will also enable ingesting various data types (see the above figure for the details). The key differentiating functionality of beta release would be the \u201cFeature Wiki\u201d for data scientists and end-users to resolve their cold-start problems, which will immensely reduce their literature search time. The beta release will also have another explain each attributes\u2019 contribution in feature building which we call the \u201cAttribute-> Feature\u201d mapper. Full Release We'll release the GA release of Anovos in March 2022 with the functionalities to support an end-to-end machine learning workflow. It will be able to store the generated features in an open source feature store, like Feast. It will also support running open source based Auto ML models and ML workflow integration. This release will also include a mechanism to explain the model behavior by having the respective shapely values.","title":"Anovos Product Roadmap"},{"location":"docs/anovos-roadmap/#the-roadmap","text":"As you can see, Anovos will be fully functional in 3 major releases, namely alpha, beta, and V1.0, respectively. The alpha release of Anovos has the essential data ingestion and comprehensive data analysis functionalities, as well as the data pre-processing and cleaning mechanisms. It also has some key differentiating functionalities, like data drift and stability computations, which are crucial in deciding the need for model refreshing/tweaking options. Another benefit of Anovos is a dynamic visualization component configured based on data ingestion pipelines. Every data metric computed from the Anovos ingestion process can be visualized and utilized for CXO level decision-making.","title":"The Roadmap"},{"location":"docs/anovos-roadmap/#beta-release","text":"In the Beta release of Anovos, it will support ingesting from other cloud service providers like MS Azure and will have mechanisms to read/write different file formats such as avro & nested Json. It will also enable ingesting various data types (see the above figure for the details). The key differentiating functionality of beta release would be the \u201cFeature Wiki\u201d for data scientists and end-users to resolve their cold-start problems, which will immensely reduce their literature search time. The beta release will also have another explain each attributes\u2019 contribution in feature building which we call the \u201cAttribute-> Feature\u201d mapper.","title":"Beta Release"},{"location":"docs/anovos-roadmap/#full-release","text":"We'll release the GA release of Anovos in March 2022 with the functionalities to support an end-to-end machine learning workflow. It will be able to store the generated features in an open source feature store, like Feast. It will also support running open source based Auto ML models and ML workflow integration. This release will also include a mechanism to explain the model behavior by having the respective shapely values.","title":"Full Release"},{"location":"docs/anovos-scale-testing/overview/","text":"Anovos Scale Testing We tested Anovos modules on a Mobilewalla bid stream data with the following attributes: Data Size 50 GB No. of Rows 384,694,946 No. of Columns 35 No. of Numerical Columns 4 No. of Categorical Columns 31 The entire pipeline was optimized so computed statistics can be reused by other functions e.g. mode (most frequently seen value) computed under statistics generator ( measures_of_centralTendency ) were also used for imputation while treating null values in a column ( nullColumns_detection ) or detecting biasedness in a column ( biasedness_detection ). The time taken by a function in a pipeline may differ significantly from the time taken by the same function on running in solitary. Also, Spark does its own set of optimizations transformations under the hood while running multiple functions together, which further adds to the time difference. For further detail, please refer to the main.py script in the Github. Function Time (mins) global_summary 1 measures_of_counts 6 measures_of_centralTendency 38 measures_of_cardinality 65 measures_of_percentiles 2 measures_of_dispersion 2 measures_of_shape 3 duplicate_detection 6 nullRows_detection 3 invalidEntries_detection 17 IDness_detection 2 biasedness_detection 2 outlier_detection 4 nullColumns_detection 2 variable_clustering 2 IV Calculation* 16 IG Calculation* 12 * A binary categorical column was selected as a target variable to test this function. Limitations: - Computing mode and/or distinct count are most expensive operations in Anovos. We aim to further optimize them in the upcoming releases. - Correlation Matrix may throw memory issues if very high cardinality categorical features are involved \u2013 a limitation that was propagated from phik library.","title":"Anovos Scale Testing"},{"location":"docs/anovos-scale-testing/overview/#anovos-scale-testing","text":"We tested Anovos modules on a Mobilewalla bid stream data with the following attributes: Data Size 50 GB No. of Rows 384,694,946 No. of Columns 35 No. of Numerical Columns 4 No. of Categorical Columns 31 The entire pipeline was optimized so computed statistics can be reused by other functions e.g. mode (most frequently seen value) computed under statistics generator ( measures_of_centralTendency ) were also used for imputation while treating null values in a column ( nullColumns_detection ) or detecting biasedness in a column ( biasedness_detection ). The time taken by a function in a pipeline may differ significantly from the time taken by the same function on running in solitary. Also, Spark does its own set of optimizations transformations under the hood while running multiple functions together, which further adds to the time difference. For further detail, please refer to the main.py script in the Github. Function Time (mins) global_summary 1 measures_of_counts 6 measures_of_centralTendency 38 measures_of_cardinality 65 measures_of_percentiles 2 measures_of_dispersion 2 measures_of_shape 3 duplicate_detection 6 nullRows_detection 3 invalidEntries_detection 17 IDness_detection 2 biasedness_detection 2 outlier_detection 4 nullColumns_detection 2 variable_clustering 2 IV Calculation* 16 IG Calculation* 12 * A binary categorical column was selected as a target variable to test this function. Limitations: - Computing mode and/or distinct count are most expensive operations in Anovos. We aim to further optimize them in the upcoming releases. - Correlation Matrix may throw memory issues if very high cardinality categorical features are involved \u2013 a limitation that was propagated from phik library.","title":"Anovos Scale Testing"},{"location":"docs/data-reports/final-report-generation/","text":"This section covers the final execution part where primarily the output generated by the previous step is being fetched upon and structured in the desirable UI layout. The primary function dealt here is anovos_report which caters to the: reading of available data produced from data analyzer module and chart objects as produced by the report preprocessing module computation of additional charts based on available data populating the reporting layer leveraging an open-sourced python package called datapane. capability of producing stand alone reports for individual sections (Descriptive Statistics, Quality Check, Attribute Associations, Data Drift & Stability) The following parameters are specified in the function anovos_report : master_path : The path which contains the data of intermediate output in terms of json chart objects, csv file (pandas df) id_col : The ID column is accepted to ensure & restrict unnecessary analysis to be performed on the same label_col : Name of label or target column in the input dataset. By default, the label_col is set as blank. corr_threshold : The threshold chosen beyond which the attributes are found to be redundant iv_threshold : The threshold beyond which the attributes are found to be significant in terms of model. drift_threshold_model: The threshold beyond which the attribute can be flagged as 1 or drifted as measured across different drift metrices specified by the user dataDict_path : The path containing the exact name, definition mapping of the attributes. This is eventually used to populate at the report for easy referencing metricDict_path : The path containing the metric dictionary **run_type: Option to specify whether the execution happen locally or in EMR way final_report_path : Path where the final report needs to be saved","title":"Final Report Generation"},{"location":"docs/data-reports/html-report/","text":"The final output is generated in form of HTML report. This has 6 sections viz. Executive Summary, Wiki, Descriptive Statistics, Quality Check, Attribute Associations, Data Drift & Data Stability at most which can be seen basis user input. We\u2019ve tried to detail each section based on the analysis performed referring to a publicly available dataset. Executive Summary The \"Executive Summary\" gives an overall summary of the key statistics from the analyzed data. 1 & 2 specifies about the dimensions of data & nature of use case whether target variable is involved or not. 3 & 4 covers the overall view of the data in a nutshell across some of the key metrices. Wiki The \"Wiki\" section has two different sections consisting of: Data Dictionary : This section contains the details of the attributes present in the data frame. The user if specifies the attribute wise definition at a specific path, then the details of the same will be populated along with the data type else only the attribute wise datatype will be seen. Metric Dictionary : Details about the different sections under the report. This could be a quick reference for the user. Descriptive Statistics The Descriptive Statistics gives specific information about the data elements and their individual. Descriptive Statistics consists of the following modules: Global Summary : Details about the data dimensions and the attribute wise information. Statistics By Metric Type includes the following modules: Measures of Counts : Details about the attribute wise count, fill rate, etc. Measures of Central Tendency : Details about the measurement of central tendency in terms of mean, median, mode. Measures of Cardinality : Details about the uniqueness in categories for each attribute. Measures of Percentiles : Indicates the different attribute value associated against the range of percentile cut offs. This helps to understand the spread of attributes. Measures of Dispersion : Explains how much the data is dispersed through metrics like Standard Deviation, Variance, Covariance, IQR and range for each attribute Measures of Shape : Describe the tail ness of distribution (or pattern) for different attributes through skewness and kurtosis. Attribute Visuations includes the following modules: Numeric : Visualize the distributions of Numerical attributes using Histograms - Categorical - Visualize the distributions of Categorical attributes using Barplot Quality Check The Quality Check seciont conissts of a qualitative inspection of the data at a row & columnar level. The Quality Check consists of the following modules: Column Level Null Columns Detections \u2013 Detect the sparsity of the datasets, e.g., count and percentage of missing value of attributes Outlier Detection \u2013 Used to detect and visualize the outlier present in numerical attributes of the datasets Violin Plot - Displays the spread of numerical attributes IDness Detection Biasedness Detection Invalid Entries Detection Row Level Duplicate Detection \u2013 Measure number of rows in the datasets that have same value for each attribute NullRows Detection - Measure the count/percentage of rows which have missing/null attributes Attribute Associations Association analysis done for Attributes basis different statistical checks Association Matrix & Plot is a Correlation Measure of the strength of relationship among each attribute by finding correlation coefficient having range -1.0 to 1.0. Visualization is shown through heat map to describe the strength of relationship among attributes. Information Value Computation is used to rank variables on the basis of their importance. Greater the value of IV higher rthe attribute importance. IV less than 0.02 is not useful for prediction. Bar plot is used to show the significance in descending order. Information Gain Computation measures the reduction in entropy by splitting a dataset according to given value of a attribute. Bar plot is used to show the significance in descending order. Variable Clustering divides the numerical attributes into disjoint or hierarchical clusters based on linear relationship of attributes. Attribute to Target Association determines the event rate trend across different attribute categories Numeric Categorical Data Drift & Data Stability Data Drift Analysis Overall Data Health Data Stability Analysis","title":"HTML Report"},{"location":"docs/data-reports/html-report/#executive-summary","text":"The \"Executive Summary\" gives an overall summary of the key statistics from the analyzed data. 1 & 2 specifies about the dimensions of data & nature of use case whether target variable is involved or not. 3 & 4 covers the overall view of the data in a nutshell across some of the key metrices.","title":"Executive Summary"},{"location":"docs/data-reports/html-report/#wiki","text":"The \"Wiki\" section has two different sections consisting of: Data Dictionary : This section contains the details of the attributes present in the data frame. The user if specifies the attribute wise definition at a specific path, then the details of the same will be populated along with the data type else only the attribute wise datatype will be seen. Metric Dictionary : Details about the different sections under the report. This could be a quick reference for the user.","title":"Wiki"},{"location":"docs/data-reports/html-report/#descriptive-statistics","text":"The Descriptive Statistics gives specific information about the data elements and their individual. Descriptive Statistics consists of the following modules: Global Summary : Details about the data dimensions and the attribute wise information. Statistics By Metric Type includes the following modules: Measures of Counts : Details about the attribute wise count, fill rate, etc. Measures of Central Tendency : Details about the measurement of central tendency in terms of mean, median, mode. Measures of Cardinality : Details about the uniqueness in categories for each attribute. Measures of Percentiles : Indicates the different attribute value associated against the range of percentile cut offs. This helps to understand the spread of attributes. Measures of Dispersion : Explains how much the data is dispersed through metrics like Standard Deviation, Variance, Covariance, IQR and range for each attribute Measures of Shape : Describe the tail ness of distribution (or pattern) for different attributes through skewness and kurtosis. Attribute Visuations includes the following modules: Numeric : Visualize the distributions of Numerical attributes using Histograms - Categorical - Visualize the distributions of Categorical attributes using Barplot","title":"Descriptive Statistics"},{"location":"docs/data-reports/html-report/#quality-check","text":"The Quality Check seciont conissts of a qualitative inspection of the data at a row & columnar level. The Quality Check consists of the following modules: Column Level Null Columns Detections \u2013 Detect the sparsity of the datasets, e.g., count and percentage of missing value of attributes Outlier Detection \u2013 Used to detect and visualize the outlier present in numerical attributes of the datasets Violin Plot - Displays the spread of numerical attributes IDness Detection Biasedness Detection Invalid Entries Detection Row Level Duplicate Detection \u2013 Measure number of rows in the datasets that have same value for each attribute NullRows Detection - Measure the count/percentage of rows which have missing/null attributes","title":"Quality Check"},{"location":"docs/data-reports/html-report/#attribute-associations","text":"Association analysis done for Attributes basis different statistical checks Association Matrix & Plot is a Correlation Measure of the strength of relationship among each attribute by finding correlation coefficient having range -1.0 to 1.0. Visualization is shown through heat map to describe the strength of relationship among attributes. Information Value Computation is used to rank variables on the basis of their importance. Greater the value of IV higher rthe attribute importance. IV less than 0.02 is not useful for prediction. Bar plot is used to show the significance in descending order. Information Gain Computation measures the reduction in entropy by splitting a dataset according to given value of a attribute. Bar plot is used to show the significance in descending order. Variable Clustering divides the numerical attributes into disjoint or hierarchical clusters based on linear relationship of attributes. Attribute to Target Association determines the event rate trend across different attribute categories Numeric Categorical","title":"Attribute Associations"},{"location":"docs/data-reports/html-report/#data-drift-data-stability","text":"Data Drift Analysis Overall Data Health Data Stability Analysis","title":"Data Drift &amp; Data Stability"},{"location":"docs/data-reports/intermediate-report-data-generation/","text":"This section largely covers the data pre\u2013processing. The primary function which is used to address all the subsequent modules is charts_to_objects . It precisely helps in saving the chart data in form of objects, which is eventually read by the final report generation script. The objects saved are specifically used at the modules shown at the Report based on the user input. Wide variations of chart are used for showcasing the data trends through Bar Plot, Histogram, Violin Plot, Heat Map, Gauge Chart, Line Chart, etc. Following arguments are specified in the primary function charts_to_objects : spark : Spark session idf : Input Dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. * drop_cols 8: This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. label_col : Name of label or target column in the input dataset. By default, the label_col is set as None to accommodate unsupervised use case. event_label : Value of event (label 1) in the label column. By default, the event_label is kept as 1 unless otherwise specified explicitly. bin_method : equal_frequency or equal_range. The bin method is set as \u201cequal_range\u201d and is being further referred to the attribute binning function where the necessary aggregation / binning is done. bin_size : The maximum number of categories which the user wants to retain is to be set here. By default the size is kept as 10 beyond which remaining records would be grouped under \u201cothers\u201d. coverage : Minimum % of rows mapped to actual category name and rest will be mapped to others. The default value kept is 1.0 which is the maximum at 100%. drift_detector : This argument takes Boolean type input \u2013 True or False. It indicates whether the drift component is already analyzed or not. By default it is kept as False. source_path : The source data path which is needed for drift analysis. If it\u2019s not computed / out of scope, the default value of \"NA\" is considered. master_path : The path which will contain the data of intermediate output in terms of json chart objects, csv file (pandas df) run_type : local or EMR. Option to specify whether the execution happen locally or in EMR way as it requires reading & writing to s3. The two form of output generated from this are chart objects and data frame . There are some secondary functions used alongside as a part of charts_to_objects processing.","title":"Intermediate Report Data Generation"},{"location":"docs/data-reports/overview/","text":"The data report module is composed of two sections details of which is described further. The primary utility of keeping the two modules is to decouple the two steps in a way that one happens at a distributed way involving the intermediate report data generation while the other is restricted to only the pre-processing and generation of the Anovos report .","title":"Overview"},{"location":"docs/setting-up-your-environment/anovos-on-aws/","text":"Step 1: Installing/Downloading Anovos Clone the Anovos repository on you local environment using command: git clone https://github.com/anovos/anovos.git or pip3 install \"git+https://github.com/anovos/anovos.git\" After cloning, go to the anovos directory and execute the following command to clean and build the latest modules in dist folder: make clean build Step 2: Copy necessary files to AWS S3 Copy the following files to AWS S3: dist/anovos.zip This file contains all Anovos modules Zipped version is mandatory for running importing the modules as \u2013py-files dist/income_dataset (optional) This folder contains our demo dataset dist/main.py This is sample script to show how different functions from Anovos module can be stitched together to create a workflow. The users can create their own workflow script by importing the necessary functions. This script takes input from a yaml configuration file dist/configs.yaml This is the sample yaml configuration file that sets the argument for all functions. Update configs.yaml for all input & output s3 paths. All other changes depend on the dataset used. bin/req_packages_anovos.sh This shell script is used to install all required packages to run Anovos on EMR AWS copy command: aws s3 cp --recursive <local file path> <s3 path> --profile <profile name> Step 3: Creating Cluster Software Configuration Emr-5.33.0 Hadoop-2.10.1 Spark-2.4.7 Hive-2.3.7 Spark Submit Details Deploy mode client Spark-submit options --num-executors 1000 --executor-cores 4 --executor-memory 20g --driver-memory 20G --driver-cores 4 --conf spark.driver.maxResultSize=15g --conf spark.yarn.am.memoryOverhead=1000m --conf spark.executor.memoryOverhead=2000m --conf spark.kryo.referenceTracking=false --conf spark.network.timeout=18000s --conf spark.executor.heartbeatInterval=12000s --conf spark.dynamicAllocation.executorIdleTimeout=12000s --conf spark.rpc.message.maxSize=1024 --conf spark.yarn.maxAppAttempts=1 --conf spark.speculation=false --conf spark.kryoserializer.buffer.max=1024 --conf spark.executor.extraJavaOptions=-XX:+UseG1GC --conf spark.driver.extraJavaOptions=-XX:+UseG1GC --packages org.apache.spark:spark-avro_2.11:2.4.0 --jars s3://mw.com.ds.kajanan/Vishnu/ml_ingest/jars/histogrammar-sparksql_2.11-1.0.20.jar,s3://mw.com.ds.kajanan/Vishnu/ml_ingest/jars/histogrammar_2.11-1.0.20.jar --py-files s3://mw.com.ds.kajanan/Sumit/Anovos_testing/income_data_emr/com.zip Application location*: s3 path of main.py file Arguments: , - Bootstrap Actions script location : path of bootstrap shell script (req_packages_anovos.sh)","title":"Anovos on AWS"},{"location":"docs/setting-up-your-environment/anovos-on-aws/#step-1-installingdownloading-anovos","text":"Clone the Anovos repository on you local environment using command: git clone https://github.com/anovos/anovos.git or pip3 install \"git+https://github.com/anovos/anovos.git\" After cloning, go to the anovos directory and execute the following command to clean and build the latest modules in dist folder: make clean build","title":"Step 1: Installing/Downloading Anovos"},{"location":"docs/setting-up-your-environment/anovos-on-aws/#step-2-copy-necessary-files-to-aws-s3","text":"Copy the following files to AWS S3: dist/anovos.zip This file contains all Anovos modules Zipped version is mandatory for running importing the modules as \u2013py-files dist/income_dataset (optional) This folder contains our demo dataset dist/main.py This is sample script to show how different functions from Anovos module can be stitched together to create a workflow. The users can create their own workflow script by importing the necessary functions. This script takes input from a yaml configuration file dist/configs.yaml This is the sample yaml configuration file that sets the argument for all functions. Update configs.yaml for all input & output s3 paths. All other changes depend on the dataset used. bin/req_packages_anovos.sh This shell script is used to install all required packages to run Anovos on EMR AWS copy command: aws s3 cp --recursive <local file path> <s3 path> --profile <profile name>","title":"Step 2: Copy necessary files to AWS S3"},{"location":"docs/setting-up-your-environment/anovos-on-aws/#step-3-creating-cluster","text":"Software Configuration Emr-5.33.0 Hadoop-2.10.1 Spark-2.4.7 Hive-2.3.7 Spark Submit Details Deploy mode client Spark-submit options --num-executors 1000 --executor-cores 4 --executor-memory 20g --driver-memory 20G --driver-cores 4 --conf spark.driver.maxResultSize=15g --conf spark.yarn.am.memoryOverhead=1000m --conf spark.executor.memoryOverhead=2000m --conf spark.kryo.referenceTracking=false --conf spark.network.timeout=18000s --conf spark.executor.heartbeatInterval=12000s --conf spark.dynamicAllocation.executorIdleTimeout=12000s --conf spark.rpc.message.maxSize=1024 --conf spark.yarn.maxAppAttempts=1 --conf spark.speculation=false --conf spark.kryoserializer.buffer.max=1024 --conf spark.executor.extraJavaOptions=-XX:+UseG1GC --conf spark.driver.extraJavaOptions=-XX:+UseG1GC --packages org.apache.spark:spark-avro_2.11:2.4.0 --jars s3://mw.com.ds.kajanan/Vishnu/ml_ingest/jars/histogrammar-sparksql_2.11-1.0.20.jar,s3://mw.com.ds.kajanan/Vishnu/ml_ingest/jars/histogrammar_2.11-1.0.20.jar --py-files s3://mw.com.ds.kajanan/Sumit/Anovos_testing/income_data_emr/com.zip Application location*: s3 path of main.py file Arguments: , - Bootstrap Actions script location : path of bootstrap shell script (req_packages_anovos.sh)","title":"Step 3: Creating Cluster"},{"location":"docs/setting-up-your-environment/anovos-on-local/","text":"Step 1: Software Prerequisites Anovos requires Spark (2.4.x), Python (3.7.*) and Java(8) to be set up to run in a local environment. The following links are useful for setting up Spark, Python and Java in local: Spark 2.4.8 Python 3.7 Java 8 Installing Apache on Mac OSX Using PySpark on Windows Step2 : Running on local Anovos can run locally in one of two ways: By cloning the repo and running it via spark-submit Clone the Anovos repository in your local environment using the command: git clone https://github.com/anovos/anovos.git After cloning, go to the Anovos directory and execute the following command to clean and build the latest modules in dist folder: make clean build Then go to dist/ folder and Update configs.yaml for all input & output paths. All other changes depend on the dataset being used. Also, update configs.yaml for other threshold settings for different functionalities. Update main.py - This sample script demonstrates how different functions from Anovos module can be stitched together to create a workflow. Update spark-submit.sh \u2013 This sample shell script runs the spark application via spark-submit. Run using spark submit shell script nohup ./spark-submit.sh > run.txt & Check stdout logs while running tail -f run.txt Once the run completes, the script will automatically open the final generated report\u202f\"report_stats/ml_anovos_report.html\"\u202fon the browser. By installing Anovos and importing it as you need it Install anovos on local using: pip3 install \"git+https://github.com/anovos/anovos.git\" or pip3 install anovos Import Anovos as a package in your spark application import anovos Ensure dependent external packages are included in SparkSession If using your own SparkSession, include the following dependent packages while initializing: \"io.github.histogrammar:histogrammar_2.11:1.0.20\", \"io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20\", \"org.apache.spark:spark-avro_2.11:2.4.0\" Dependent Package JAR links : https://repo1.maven.org/maven2/io/github/histogrammar/histogrammar-sparksql_2.11/1.0.20/ https://repo1.maven.org/maven2/io/github/histogrammar/histogrammar_2.11/1.0.20/ https://mvnrepository.com/artifact/org.apache.spark/spark-avro_2.11/2.4 Alternatively, if creating a new SparkSession please use the pre-configured\u202fSparkSession\u202finstance provided by anovos.shared.spark: from anovos.shared.spark import spark","title":"Anovos On Local"},{"location":"docs/setting-up-your-environment/anovos-on-local/#step-1-software-prerequisites","text":"Anovos requires Spark (2.4.x), Python (3.7.*) and Java(8) to be set up to run in a local environment. The following links are useful for setting up Spark, Python and Java in local: Spark 2.4.8 Python 3.7 Java 8 Installing Apache on Mac OSX Using PySpark on Windows","title":"Step 1: Software Prerequisites"},{"location":"docs/setting-up-your-environment/anovos-on-local/#step2-running-on-local","text":"Anovos can run locally in one of two ways: By cloning the repo and running it via spark-submit Clone the Anovos repository in your local environment using the command: git clone https://github.com/anovos/anovos.git After cloning, go to the Anovos directory and execute the following command to clean and build the latest modules in dist folder: make clean build Then go to dist/ folder and Update configs.yaml for all input & output paths. All other changes depend on the dataset being used. Also, update configs.yaml for other threshold settings for different functionalities. Update main.py - This sample script demonstrates how different functions from Anovos module can be stitched together to create a workflow. Update spark-submit.sh \u2013 This sample shell script runs the spark application via spark-submit. Run using spark submit shell script nohup ./spark-submit.sh > run.txt & Check stdout logs while running tail -f run.txt Once the run completes, the script will automatically open the final generated report\u202f\"report_stats/ml_anovos_report.html\"\u202fon the browser. By installing Anovos and importing it as you need it Install anovos on local using: pip3 install \"git+https://github.com/anovos/anovos.git\" or pip3 install anovos Import Anovos as a package in your spark application import anovos Ensure dependent external packages are included in SparkSession If using your own SparkSession, include the following dependent packages while initializing: \"io.github.histogrammar:histogrammar_2.11:1.0.20\", \"io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20\", \"org.apache.spark:spark-avro_2.11:2.4.0\" Dependent Package JAR links : https://repo1.maven.org/maven2/io/github/histogrammar/histogrammar-sparksql_2.11/1.0.20/ https://repo1.maven.org/maven2/io/github/histogrammar/histogrammar_2.11/1.0.20/ https://mvnrepository.com/artifact/org.apache.spark/spark-avro_2.11/2.4 Alternatively, if creating a new SparkSession please use the pre-configured\u202fSparkSession\u202finstance provided by anovos.shared.spark: from anovos.shared.spark import spark","title":"Step2 : Running on local"},{"location":"getting-started/","text":"Getting Started with Anovos Anovos provides data scientists and ML engineers with powerful and versatile tools for feature engineering. In this guide, you will learn how to set up Anovos and get to know what it can do. Setting up and verifying the Python and Spark environment Anovos builds on Apache Spark , a highly scalable engine for data engineering, so an installation of Spark is required to run any Anovos code. The Python bindings for Spark (known as pyspark) also need to be installed in a compatible version. If you are first starting out with Anovos and are not yet familiar with Spark, we recommend you execute this guide through the provided anovos-demo Docker container, which provides a full Spark setup along with compatible versions of Python and a Jupyter notebook environment. Setting up Anovos on Local The currently version of Anovos is specifically built for Spark 2.4.x, Python 3.7.x, and Java 8 (OpenJDK 1.8.x). You can verify that you're running the correct versions by executing the following lines: !python --version !spark-submit --version Let's also check that a compatible version of pyspark is available within our Python environment: import pyspark pyspark.__version__ If you haven't done so already, let's install Anovos into the currently active Python environment: !pip install anovos Since Anovos relies on Spark behind the scenes for most heavy lifting, we need to pass an instantiated SparkSession to many of the function calls. Setting up Spark Session for Anovos For the purposes of this guide, we'll use the pre-configured SparkSession instance provided by anovos.shared.spark : from anovos.shared.spark import spark (Don't worry if this import takes some time and prints a lot of output. You should see that settings are loaded, dependencies are added, and the logger is configured.) Loading data Data ingestion is the first step in any feature engineering project. Anovos builds on Spark's data loading capabilites and can handle different common file formats such as CSV, Parquet, and Avro. Anovos data_ingest module provides all data ingestion functionality. It includes functions to merge multiple datasets as well as to select subsets of the loaded data. Let's load the classic Adult Income dataset in CSV format, which we'll use throughout this guide: from anovos.data_ingest.data_ingest import read_dataset df = read_dataset( spark, # Remember: The first argument of Anovos functions is always an instantiated SparkSession file_path='../data/income_dataset/csv', file_type='csv', file_configs={'header': 'True', 'delimiter': ',', 'inferSchema': 'True'} ) Note that df is a standard Spark DataFrame : type(df) Thus, you can use all the built-in methods you might be familiar with, e.g. df.printSchema() The Adult Income dataset's more than 48k entries each describes a person along with the information whether they earn more than $50k per year. In this guide, we will work with just a few of its columns: from anovos.data_ingest.data_ingest import select_column df = select_column(df, list_of_cols=['age', 'education', 'education-num', 'occupation', 'hours-per-week', 'income']) df = df.withColumn('income', (df['income'] == '>50K').cast('integer')) # convert label to integer df.printSchema() Learning about the data Before we can start to engineer features, we need to understand the data. For example, we need to verify that the data has sufficient quality or if there are missing values. Anovos ' data_analyzer module provides three submodules for this purpose: The functions of data_analyzer.quality_checker allow us to detect and fix issues like empty rows or duplicate entries data_analyzer.stats_generator offers functions to caculate various statistical properties data_analyzer.association_evaluator enables us to examine a dataset for correlations between columns Assess the data quality Once a new dataset is loaded, its quality should be assessed. Does the dataset contain all the columns we expect? Do we have duplicate values? Did we ingest the expected number of unique data points? Anovos ' data_analyzer.quality_checker module provides convenient functions to answer these questions. For time's sake, we'll assume that our dataset does not exhibit any of these problems. Instead, we'll move on to more advanced quality assessments and check for outliers in the data. Outliers are data points that deviate significantly from the others. These points can be problematic when training ML models or during inference because there is very little information about the ranges in the dataset, leading to a high degree of uncertainty. We immediately see that most people work a standard 40-hour week, while a significant minority reports anywhere between 20 and 60 hours. However, some individuals work very few hours, and others reported almost 100 hours per week, more than twice the median. These are the groups detected by the outlier detector. We cannot only detect that there are outliers, but we can also deal with them right away. For example, let's remove the rows where individuals reported an excessive amount of hours worked per week: How we deal with outliers depends on their origin and the application context. from anovos.data_analyzer.quality_checker import outlier_detection output_df, metric_df = outlier_detection(spark, df, detection_side='both', print_impact=True) This tells us that we have little data for older people, which might be an issue later when we are trying to predict the income of this group, so we should keep this in mind. We can ignore the value for education-num , a categorical column for which the calculations performed by the outlier_detection function are meaningless: In its standard configuration, it checks for values that fulfill at least two of the following criteria: They belong to the smallest or largest 5% of values, they deviate from the mean by more than 3 standard deviations, or they lie below Q1 - 1.5*IQR or above Q3 + 1.5*IQR , where Q1 and Q3 are the first and third quartile, respectively, and IQR is the Interquartile Range . (For more details and information on utilizing additional methods for outlier detection, see the documentation . To better understand this, let's examine the hours worked per week reported by the surveyed people: import plotly.express as px px.histogram(df.toPandas(), x='hours-per-week') We immediately see that the vast majority of people works a standard 40-hour week, while a significant minority reports anywhere between 20 and 60 hours. However, there are also individuals that work very few hours as well as people that reported almost 100 hours per week, more than twice the median. These are the groups detected by the outlier detector. We cannot only detect that there are outliers, we can also deal with them right away. For example, let's remove the rows where individuals reported an excessive amount of hours worked per week: df, metric_df = outlier_detection(spark, df, detection_side='both', list_of_cols=['hours-per-week'], treatment=True, treatment_method='row_removal') Let's check that we have indeed reduced the dataset to entries where the number of hours worked per week lies within a common range: px.histogram(df.toPandas(), x='hours-per-week') Understand how your data is distributed When familiarizing ourselves with a dataset, one of the first steps is to understand the ranges of values each column contains and how the values are distributed. At the very least, we should learn the minimal and maximal values and examine the distribution within that range (e.g., by computing the mean, median, and standard deviation). This information is vital to know. For many popular ML models, each feature column should be scaled to the same order of magnitude. Further, ML models will generally only work well for ranges of feature values trained on, so we should check that the data points they see during inference lie within the ranges we find. Anovos ' data_analyzer.stats_generator module provides an easy way to quickly calculate a set of common properties for the columns in a dataset: from anovos.data_analyzer.stats_generator import global_summary global_summary(spark, df).toPandas() In the real world, incomplete datasets are a common problem for data scientists. For example, customer records might be missing values because we have not had a chance to ask the customer about them. There might have been a broken sensor in other cases, leading to missing values in a certain time period. In any case, we should know which columns in our dataset might exhibit such issues to consider during feature selection and modeling. We can use the data_analyzer.stats_generator module to check this: from anovos.data_analyzer.stats_generator import measures_of_counts measures_of_counts(spark, df).toPandas() There are various ways to deal with missing or unknown values. We could replace missing entries in our dataset with the mean or the median of the respective column. If a column contains mostly null values, it might also be an option to drop it entirely. If a dataset contains a lot of unknown values across all of its columns, it will likely be necessary to design a model that can explicitly handle this situation. However you choose to handle these situations, Anovos offers convenient functions in the data_transformer module which we will look at later in this guide. Detect correlations within the dataset In machine learning, we are often interested in predicting a class or value (the label ) from a set of features . To decide which features to use in a specific scenario, it is often helpful to determine which columns in a dataset are correlated with the label column. In other words, we would like to find out which columns hold \"predictive power.\" Anovos ' data_analyzer.association_evaluator provides several functions for this purpose. A commonly used tool is a correlation matrix. It visualizes the degree of pairwise correlation between multiple columns at once. from anovos.data_analyzer.association_evaluator import correlation_matrix correlation_matrix(spark, df, list_of_cols=['age', 'education-num', 'income']).toPandas() The matrix shows that age and education correlate with income: The older or, the higher educated a person, the higher the likelihood of earning above $50k. However, education and income exhibit a higher degree of correlation than age and income. Examine Drift Anovos has drift detection built-in, solving one problem for machine learning models. Model performance might degrade if the distribution changes over time compared to the training, validation, and test data. For a more in-depth look into data drift, reference this article . Anovos provides an entire module dedicated to detecting various kinds of data drift. We recommend checking how the data you're planning to use evolves before beginning feature engineering. As we only have one dataset, we will artificially create a dataset that has drift by duplicating our dataset and shifting the age column, artificially aging the population an entire decade: df_shifted = df.withColumn('age', df['age']+10) With the drift_detector.drift_statistics function we can compare a given dataset to a baseline. Let's try this: from anovos.data_drift.drift_detector import drift_statistics drift_statistics(spark, df_shifted, df).toPandas() We see that the drift detector has flagged the age column as exhibiting drift. The calculated Population Stability Index signals that the column's value distribution differs significantly from the baseline. Transform the data So far, we have only analyzed the dataset and removed entire entries based on these analyses. Now it's time to apply changes to the dataset to prepare it for future ML model training. We discovered that there are missing values in all five feature columns of the dataset. If we choose to use this dataset in an ML model that cannot handle missing values, we should consider replacing missing values with the feature column's average value. Anovos ' data_transformer module provides a handy utility function for this and similar transformations: from anovos.data_transformer.transformers import imputation_MMM transformed_df = imputation_MMM(spark, df, list_of_cols=['age', 'hours-per-week'], method_type='mean') Let's confirm that we replaced all missing values in the age and hours-per-week columns: measures_of_counts(spark, transformed_df).toPandas() Note that the data transformation capabilities of Anovos are currently limited. We have plans for improved capabilities, like auto-encoders and methods for dimensionality reduction. For more information, see the Anovos Product Roadmap . Generate a report Documenting datasets is an essential component of any data governance strategy, so Anovos integrates datapane , a library to create interactive reports. You can generate a basic report with just one line of code: from anovos.data_report.basic_report_generation import anovos_basic_report anovos_basic_report(spark, transformed_df, output_path='./report') Once the report generation is finished, you can download and view the generated basic_report.html stored in ./report in any browser. Please note that due to Jupyter's security settings, it is currently not possible to view it directly from within the Jupyter environment spun up by the Docker container. Chances are the format and content of the basic report will not be work for your teams's specific needs, so Anovos allows you to conveniently configure and create custom reports using the data_report.report_generation submodule. Store the data Once we've prepared and documented the data, it's time to store it so we can use it to train and evaluate ML models. Similar to data ingestion, Anovos handles data storage through Spark's versatile capabilities . Using the data_ingest.write_dataset function, we can write our processed DataFrame to disk: from anovos.data_ingest.data_ingest import write_dataset write_dataset(transformed_df, file_path=\"./export.csv\", file_type=\"csv\", file_configs={\"header\": \"True\", \"delimiter\": \",\"}) As of now, this final step of data preparation (and this introductory guide) is where Anovos' capabilities end. However, upcoming releases will extend Anovos to include adapters for popular AutoML solutions and Feature Stores, allowing you to seamlessly move to model training and serving as well as data monitoring For more details and to see what else is ahead, see the Anovos Product Roadmap . What's next? In this guide, you've had a glimpse at the different capabilities offered by Anovos . However, we've just scratched the surface, and there is much more to see and explore: The Anovos documentation is a great place to get an overview of the available functionality. To see how different parts of Anovos can be used in practice, have a look at the Jupyter notebooks our team has prepared for each of the modules. Finally, to understand how Anovos can be integrated into your Spark ecosystem, see these hints","title":"Getting Started"},{"location":"getting-started/#getting-started-with-anovos","text":"Anovos provides data scientists and ML engineers with powerful and versatile tools for feature engineering. In this guide, you will learn how to set up Anovos and get to know what it can do.","title":"Getting Started with Anovos"},{"location":"getting-started/#setting-up-and-verifying-the-python-and-spark-environment","text":"Anovos builds on Apache Spark , a highly scalable engine for data engineering, so an installation of Spark is required to run any Anovos code. The Python bindings for Spark (known as pyspark) also need to be installed in a compatible version. If you are first starting out with Anovos and are not yet familiar with Spark, we recommend you execute this guide through the provided anovos-demo Docker container, which provides a full Spark setup along with compatible versions of Python and a Jupyter notebook environment. Setting up Anovos on Local The currently version of Anovos is specifically built for Spark 2.4.x, Python 3.7.x, and Java 8 (OpenJDK 1.8.x). You can verify that you're running the correct versions by executing the following lines: !python --version !spark-submit --version Let's also check that a compatible version of pyspark is available within our Python environment: import pyspark pyspark.__version__ If you haven't done so already, let's install Anovos into the currently active Python environment: !pip install anovos Since Anovos relies on Spark behind the scenes for most heavy lifting, we need to pass an instantiated SparkSession to many of the function calls. Setting up Spark Session for Anovos For the purposes of this guide, we'll use the pre-configured SparkSession instance provided by anovos.shared.spark : from anovos.shared.spark import spark (Don't worry if this import takes some time and prints a lot of output. You should see that settings are loaded, dependencies are added, and the logger is configured.)","title":"Setting up and verifying the Python and Spark environment"},{"location":"getting-started/#loading-data","text":"Data ingestion is the first step in any feature engineering project. Anovos builds on Spark's data loading capabilites and can handle different common file formats such as CSV, Parquet, and Avro. Anovos data_ingest module provides all data ingestion functionality. It includes functions to merge multiple datasets as well as to select subsets of the loaded data. Let's load the classic Adult Income dataset in CSV format, which we'll use throughout this guide: from anovos.data_ingest.data_ingest import read_dataset df = read_dataset( spark, # Remember: The first argument of Anovos functions is always an instantiated SparkSession file_path='../data/income_dataset/csv', file_type='csv', file_configs={'header': 'True', 'delimiter': ',', 'inferSchema': 'True'} ) Note that df is a standard Spark DataFrame : type(df) Thus, you can use all the built-in methods you might be familiar with, e.g. df.printSchema() The Adult Income dataset's more than 48k entries each describes a person along with the information whether they earn more than $50k per year. In this guide, we will work with just a few of its columns: from anovos.data_ingest.data_ingest import select_column df = select_column(df, list_of_cols=['age', 'education', 'education-num', 'occupation', 'hours-per-week', 'income']) df = df.withColumn('income', (df['income'] == '>50K').cast('integer')) # convert label to integer df.printSchema()","title":"Loading data"},{"location":"getting-started/#learning-about-the-data","text":"Before we can start to engineer features, we need to understand the data. For example, we need to verify that the data has sufficient quality or if there are missing values. Anovos ' data_analyzer module provides three submodules for this purpose: The functions of data_analyzer.quality_checker allow us to detect and fix issues like empty rows or duplicate entries data_analyzer.stats_generator offers functions to caculate various statistical properties data_analyzer.association_evaluator enables us to examine a dataset for correlations between columns","title":"Learning about the data"},{"location":"getting-started/#assess-the-data-quality","text":"Once a new dataset is loaded, its quality should be assessed. Does the dataset contain all the columns we expect? Do we have duplicate values? Did we ingest the expected number of unique data points? Anovos ' data_analyzer.quality_checker module provides convenient functions to answer these questions. For time's sake, we'll assume that our dataset does not exhibit any of these problems. Instead, we'll move on to more advanced quality assessments and check for outliers in the data. Outliers are data points that deviate significantly from the others. These points can be problematic when training ML models or during inference because there is very little information about the ranges in the dataset, leading to a high degree of uncertainty. We immediately see that most people work a standard 40-hour week, while a significant minority reports anywhere between 20 and 60 hours. However, some individuals work very few hours, and others reported almost 100 hours per week, more than twice the median. These are the groups detected by the outlier detector. We cannot only detect that there are outliers, but we can also deal with them right away. For example, let's remove the rows where individuals reported an excessive amount of hours worked per week: How we deal with outliers depends on their origin and the application context. from anovos.data_analyzer.quality_checker import outlier_detection output_df, metric_df = outlier_detection(spark, df, detection_side='both', print_impact=True) This tells us that we have little data for older people, which might be an issue later when we are trying to predict the income of this group, so we should keep this in mind. We can ignore the value for education-num , a categorical column for which the calculations performed by the outlier_detection function are meaningless: In its standard configuration, it checks for values that fulfill at least two of the following criteria: They belong to the smallest or largest 5% of values, they deviate from the mean by more than 3 standard deviations, or they lie below Q1 - 1.5*IQR or above Q3 + 1.5*IQR , where Q1 and Q3 are the first and third quartile, respectively, and IQR is the Interquartile Range . (For more details and information on utilizing additional methods for outlier detection, see the documentation . To better understand this, let's examine the hours worked per week reported by the surveyed people: import plotly.express as px px.histogram(df.toPandas(), x='hours-per-week') We immediately see that the vast majority of people works a standard 40-hour week, while a significant minority reports anywhere between 20 and 60 hours. However, there are also individuals that work very few hours as well as people that reported almost 100 hours per week, more than twice the median. These are the groups detected by the outlier detector. We cannot only detect that there are outliers, we can also deal with them right away. For example, let's remove the rows where individuals reported an excessive amount of hours worked per week: df, metric_df = outlier_detection(spark, df, detection_side='both', list_of_cols=['hours-per-week'], treatment=True, treatment_method='row_removal') Let's check that we have indeed reduced the dataset to entries where the number of hours worked per week lies within a common range: px.histogram(df.toPandas(), x='hours-per-week')","title":"Assess the data quality"},{"location":"getting-started/#understand-how-your-data-is-distributed","text":"When familiarizing ourselves with a dataset, one of the first steps is to understand the ranges of values each column contains and how the values are distributed. At the very least, we should learn the minimal and maximal values and examine the distribution within that range (e.g., by computing the mean, median, and standard deviation). This information is vital to know. For many popular ML models, each feature column should be scaled to the same order of magnitude. Further, ML models will generally only work well for ranges of feature values trained on, so we should check that the data points they see during inference lie within the ranges we find. Anovos ' data_analyzer.stats_generator module provides an easy way to quickly calculate a set of common properties for the columns in a dataset: from anovos.data_analyzer.stats_generator import global_summary global_summary(spark, df).toPandas() In the real world, incomplete datasets are a common problem for data scientists. For example, customer records might be missing values because we have not had a chance to ask the customer about them. There might have been a broken sensor in other cases, leading to missing values in a certain time period. In any case, we should know which columns in our dataset might exhibit such issues to consider during feature selection and modeling. We can use the data_analyzer.stats_generator module to check this: from anovos.data_analyzer.stats_generator import measures_of_counts measures_of_counts(spark, df).toPandas() There are various ways to deal with missing or unknown values. We could replace missing entries in our dataset with the mean or the median of the respective column. If a column contains mostly null values, it might also be an option to drop it entirely. If a dataset contains a lot of unknown values across all of its columns, it will likely be necessary to design a model that can explicitly handle this situation. However you choose to handle these situations, Anovos offers convenient functions in the data_transformer module which we will look at later in this guide.","title":"Understand how your data is distributed"},{"location":"getting-started/#detect-correlations-within-the-dataset","text":"In machine learning, we are often interested in predicting a class or value (the label ) from a set of features . To decide which features to use in a specific scenario, it is often helpful to determine which columns in a dataset are correlated with the label column. In other words, we would like to find out which columns hold \"predictive power.\" Anovos ' data_analyzer.association_evaluator provides several functions for this purpose. A commonly used tool is a correlation matrix. It visualizes the degree of pairwise correlation between multiple columns at once. from anovos.data_analyzer.association_evaluator import correlation_matrix correlation_matrix(spark, df, list_of_cols=['age', 'education-num', 'income']).toPandas() The matrix shows that age and education correlate with income: The older or, the higher educated a person, the higher the likelihood of earning above $50k. However, education and income exhibit a higher degree of correlation than age and income.","title":"Detect correlations within the dataset"},{"location":"getting-started/#examine-drift","text":"Anovos has drift detection built-in, solving one problem for machine learning models. Model performance might degrade if the distribution changes over time compared to the training, validation, and test data. For a more in-depth look into data drift, reference this article . Anovos provides an entire module dedicated to detecting various kinds of data drift. We recommend checking how the data you're planning to use evolves before beginning feature engineering. As we only have one dataset, we will artificially create a dataset that has drift by duplicating our dataset and shifting the age column, artificially aging the population an entire decade: df_shifted = df.withColumn('age', df['age']+10) With the drift_detector.drift_statistics function we can compare a given dataset to a baseline. Let's try this: from anovos.data_drift.drift_detector import drift_statistics drift_statistics(spark, df_shifted, df).toPandas() We see that the drift detector has flagged the age column as exhibiting drift. The calculated Population Stability Index signals that the column's value distribution differs significantly from the baseline.","title":"Examine Drift"},{"location":"getting-started/#transform-the-data","text":"So far, we have only analyzed the dataset and removed entire entries based on these analyses. Now it's time to apply changes to the dataset to prepare it for future ML model training. We discovered that there are missing values in all five feature columns of the dataset. If we choose to use this dataset in an ML model that cannot handle missing values, we should consider replacing missing values with the feature column's average value. Anovos ' data_transformer module provides a handy utility function for this and similar transformations: from anovos.data_transformer.transformers import imputation_MMM transformed_df = imputation_MMM(spark, df, list_of_cols=['age', 'hours-per-week'], method_type='mean') Let's confirm that we replaced all missing values in the age and hours-per-week columns: measures_of_counts(spark, transformed_df).toPandas() Note that the data transformation capabilities of Anovos are currently limited. We have plans for improved capabilities, like auto-encoders and methods for dimensionality reduction. For more information, see the Anovos Product Roadmap .","title":"Transform the data"},{"location":"getting-started/#generate-a-report","text":"Documenting datasets is an essential component of any data governance strategy, so Anovos integrates datapane , a library to create interactive reports. You can generate a basic report with just one line of code: from anovos.data_report.basic_report_generation import anovos_basic_report anovos_basic_report(spark, transformed_df, output_path='./report') Once the report generation is finished, you can download and view the generated basic_report.html stored in ./report in any browser. Please note that due to Jupyter's security settings, it is currently not possible to view it directly from within the Jupyter environment spun up by the Docker container. Chances are the format and content of the basic report will not be work for your teams's specific needs, so Anovos allows you to conveniently configure and create custom reports using the data_report.report_generation submodule.","title":"Generate a report"},{"location":"getting-started/#store-the-data","text":"Once we've prepared and documented the data, it's time to store it so we can use it to train and evaluate ML models. Similar to data ingestion, Anovos handles data storage through Spark's versatile capabilities . Using the data_ingest.write_dataset function, we can write our processed DataFrame to disk: from anovos.data_ingest.data_ingest import write_dataset write_dataset(transformed_df, file_path=\"./export.csv\", file_type=\"csv\", file_configs={\"header\": \"True\", \"delimiter\": \",\"}) As of now, this final step of data preparation (and this introductory guide) is where Anovos' capabilities end. However, upcoming releases will extend Anovos to include adapters for popular AutoML solutions and Feature Stores, allowing you to seamlessly move to model training and serving as well as data monitoring For more details and to see what else is ahead, see the Anovos Product Roadmap .","title":"Store the data"},{"location":"getting-started/#whats-next","text":"In this guide, you've had a glimpse at the different capabilities offered by Anovos . However, we've just scratched the surface, and there is much more to see and explore: The Anovos documentation is a great place to get an overview of the available functionality. To see how different parts of Anovos can be used in practice, have a look at the Jupyter notebooks our team has prepared for each of the modules. Finally, to understand how Anovos can be integrated into your Spark ecosystem, see these hints","title":"What's next?"}]}