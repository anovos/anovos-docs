{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to Anovos Here you'll find everything you need to know about Anovos ! \ud83e\udded What is Anovos? Anovos is an open source project, built by data scientist for the data science community, that brings automation to the feature engineering process. By rethinking ingestion and transformation, and including deeper analytics, drift identification, and stability analysis, Anovos improves productivity and helps data scientists build more resilient, higher performing models. \ud83d\ude80 Getting Started To get a first impression of Anovos ' capabilities, check out our interactive Getting Started guide . \u2753 Need help? Need a little help? We're here! This documentation provides a thorough introduction to Anovos and includes a comprehensive API documentation . If you have any questions on how to use Anovos or would like to suggest future enhancements, here's how to reach out . \ud83d\udee0 Contributing We're happy to welcome you as an Anovos contributor! Check out our Contributors' page for more information. \ud83d\udd0b What's Powering Anovos? Anovos is built on a curated collection of powerful open source libraries, including: Apache Spark datapane findspark numpy loguru Pandas plotly popmon pyarrow py4j pyaml s3fs seaborn sentence-transformers scipy scikit-learn statsmodels varclushi","title":"Home"},{"location":"index.html#welcome-to-anovos","text":"Here you'll find everything you need to know about Anovos !","title":"Welcome to Anovos"},{"location":"index.html#what-is-anovos","text":"Anovos is an open source project, built by data scientist for the data science community, that brings automation to the feature engineering process. By rethinking ingestion and transformation, and including deeper analytics, drift identification, and stability analysis, Anovos improves productivity and helps data scientists build more resilient, higher performing models.","title":"\ud83e\udded What is Anovos?"},{"location":"index.html#getting-started","text":"To get a first impression of Anovos ' capabilities, check out our interactive Getting Started guide .","title":"\ud83d\ude80 Getting Started"},{"location":"index.html#need-help","text":"Need a little help? We're here! This documentation provides a thorough introduction to Anovos and includes a comprehensive API documentation . If you have any questions on how to use Anovos or would like to suggest future enhancements, here's how to reach out .","title":"\u2753 Need help?"},{"location":"index.html#contributing","text":"We're happy to welcome you as an Anovos contributor! Check out our Contributors' page for more information.","title":"\ud83d\udee0 Contributing"},{"location":"index.html#whats-powering-anovos","text":"Anovos is built on a curated collection of powerful open source libraries, including: Apache Spark datapane findspark numpy loguru Pandas plotly popmon pyarrow py4j pyaml s3fs seaborn sentence-transformers scipy scikit-learn statsmodels varclushi","title":"\ud83d\udd0b What's Powering Anovos?"},{"location":"about.html","text":"About Anovos \ud83e\uddd0 Why Anovos? Data science teams spend up to 80% of their time on feature engineering and still end up building models with poor resilience. Anovos seeks to address both these issues (lack of modeler productivity and insufficient model resilience) by enabling data scientists to understand the data they work with and to engineer reproducible and robust features. In turn, these serve as the foundation for the training of resilient models that perform reliably and consistently when deployed. Unlike current feature engineering workflows, which are ad hoc, error-prone, and modeler-driven, Anovos seeks to inject process-driven efficiency into feature creation based on innovations in understanding the stability of data and how data items impact the features they anchor. With Anovos , ML models become more consistent, more accurate, and deliver results faster. At the same time, the process of building models becomes more projectable, saving time and decreasing cost. \ud83d\udc65 Who's behind Anovos? Anovos is built by a team of highly talented and experienced data scientists at Mobilewalla with years of experience in applying ML techniques to some of the most extensive consumer data sets available. Frequently, the team found a need to create novel tools to simplify and speed up the feature engineering process to increase efficiency. Through open sourcing these tools, we share our learnings with the community.","title":"About"},{"location":"about.html#about-anovos","text":"","title":"About Anovos"},{"location":"about.html#why-anovos","text":"Data science teams spend up to 80% of their time on feature engineering and still end up building models with poor resilience. Anovos seeks to address both these issues (lack of modeler productivity and insufficient model resilience) by enabling data scientists to understand the data they work with and to engineer reproducible and robust features. In turn, these serve as the foundation for the training of resilient models that perform reliably and consistently when deployed. Unlike current feature engineering workflows, which are ad hoc, error-prone, and modeler-driven, Anovos seeks to inject process-driven efficiency into feature creation based on innovations in understanding the stability of data and how data items impact the features they anchor. With Anovos , ML models become more consistent, more accurate, and deliver results faster. At the same time, the process of building models becomes more projectable, saving time and decreasing cost.","title":"\ud83e\uddd0 Why Anovos?"},{"location":"about.html#whos-behind-anovos","text":"Anovos is built by a team of highly talented and experienced data scientists at Mobilewalla with years of experience in applying ML techniques to some of the most extensive consumer data sets available. Frequently, the team found a need to create novel tools to simplify and speed up the feature engineering process to increase efficiency. Through open sourcing these tools, we share our learnings with the community.","title":"\ud83d\udc65 Who's behind Anovos?"},{"location":"getting-started.html","text":"Getting Started with Anovos \ud83d\ude80 Anovos provides data scientists and ML engineers with powerful and versatile tools for feature engineering. To get you started quickly, we have prepared an interactive Getting Started Guide . All you need is Docker (see here for instructions how to install it on your machine). Once you've finished the guide, the best way to explore what Anovos has to offer is through the provided examples that are included in the anovos-examples Docker image as well. To launch an anovos-examples Docker container that contains both the guide and the examples, execute the following command: docker run -p 8888 :8888 anovos/anovos-examples-3.2.2:latest \u26a0\ufe0f Please be aware that this will download several GB of data from Docker Hub. \ud83d\udca1 Depending on your Docker setup, you might encounter an error message Couldn't connect to Docker daemon at ... . Quite often, this means that your user account lacks permission to access it, and you need to prefix the call to Docker with sudo , i.e., execute sudo docker run -p 8888:8888 anovos/anovos-examples-3.2.2:latest . The numbers in the image's name specify the Spark version. To see all available image versions, head over to our Docker Hub profile . To reach the Jupyter environment, open the link to http://127.0.0.1:8888/?token... generated by the Jupyter NotebookApp. If you're not familiar with Anovos or feature engineering, the Getting Started with Anovos guide is a good place to begin your journey. You can find it in the /guides folder within the Jupyter environment. For more detailed instructions on how to install Docker and how to troubleshoot potential issues, see the examples README .","title":"Getting Started \ud83d\ude80"},{"location":"getting-started.html#getting-started-with-anovos","text":"Anovos provides data scientists and ML engineers with powerful and versatile tools for feature engineering. To get you started quickly, we have prepared an interactive Getting Started Guide . All you need is Docker (see here for instructions how to install it on your machine). Once you've finished the guide, the best way to explore what Anovos has to offer is through the provided examples that are included in the anovos-examples Docker image as well. To launch an anovos-examples Docker container that contains both the guide and the examples, execute the following command: docker run -p 8888 :8888 anovos/anovos-examples-3.2.2:latest \u26a0\ufe0f Please be aware that this will download several GB of data from Docker Hub. \ud83d\udca1 Depending on your Docker setup, you might encounter an error message Couldn't connect to Docker daemon at ... . Quite often, this means that your user account lacks permission to access it, and you need to prefix the call to Docker with sudo , i.e., execute sudo docker run -p 8888:8888 anovos/anovos-examples-3.2.2:latest . The numbers in the image's name specify the Spark version. To see all available image versions, head over to our Docker Hub profile . To reach the Jupyter environment, open the link to http://127.0.0.1:8888/?token... generated by the Jupyter NotebookApp. If you're not familiar with Anovos or feature engineering, the Getting Started with Anovos guide is a good place to begin your journey. You can find it in the /guides folder within the Jupyter environment. For more detailed instructions on how to install Docker and how to troubleshoot potential issues, see the examples README .","title":"Getting Started with Anovos \ud83d\ude80"},{"location":"license.html","text":"License Copyright 2021-2022 Anovos Developers Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS Licenses of third-party dependencies The Anovos project is licensed under the terms of the Apache 2.0 License . However, Anovos builds on several third-party open source libraries that are made available under the following licenses: Java dependencies Apache Spark Apache License Version 2.0, see https://github.com/apache/spark/blob/master/LICENSE Histogrammar Apache License Version 2.0, see https://github.com/histogrammar/histogrammar-scala/blob/master/LICENSE Histogrammar-SparkSQL Apache License Version 2.0, see https://github.com/histogrammar/histogrammar-scala/blob/master/LICENSE Python packages black MIT License, see https://github.com/psf/black/blob/main/LICENSE boto3 Apache License Version 2.0, see https://github.com/boto/boto3/blob/develop/LICENSE cython Apache License Version 2.0, see https://github.com/cython/cython/blob/master/LICENSE.txt datapane Apache License Version 2.0, see https://github.com/datapane/datapane/blob/master/LICENSE findspark BSD 3-Clause License, see https://github.com/minrk/findspark/blob/main/LICENSE.md fsspec BSD 3-Clause License, see https://github.com/fsspec/filesystem_spec/blob/master/LICENSE isort MIT License, see https://github.com/PyCQA/isort/blob/main/LICENSE loguru MIT License, see https://github.com/Delgan/loguru/blob/master/LICENSE matplotlib License agreement for matplotlib versions 1.3.0 and later, see https://github.com/matplotlib/matplotlib/blob/main/LICENSE/LICENSE numpy BSD 3-Clause License, see https://github.com/numpy/numpy/blob/main/LICENSE.txt pandas BSD 3-Clause License, see https://github.com/pandas-dev/pandas/blob/master/LICENSE plotly MIT License, see https://github.com/plotly/plotly.py/blob/master/LICENSE.txt popmon MIT License, see https://github.com/ing-bank/popmon/blob/master/LICENSE py4j BSD 3-Clause License, see https://github.com/py4j/py4j/blob/master/LICENSE.txt pyaml MIT License, see https://github.com/yaml/pyyaml/blob/master/LICENSE pyarrow Apache License Version 2.0, see https://github.com/apache/arrow/blob/master/LICENSE.txt pybind11 BSD 3-Clause License, see https://github.com/pybind/pybind11/blob/master/LICENSE reverse_geocoder GNU Lesser General Public License Version 2.1, see https://github.com/thampiman/reverse-geocoder/blob/master/LICENSE s3fs BSD 3-Clause License, see https://github.com/fsspec/s3fs/blob/main/LICENSE.txt s3path Apache License Version 2.0, see https://github.com/liormizr/s3path/blob/master/LICENSE scikit-learn BSD 3-Clause License, see https://github.com/scikit-learn/scikit-learn/blob/main/COPYING scipy BSD 3-Clause License, see https://github.com/scipy/scipy/blob/master/LICENSE.txt sentence-transformers Apache License Version 2.0, see https://github.com/UKPLab/sentence-transformers/blob/master/LICENSE soupsieve MIT License, see https://github.com/facelessuser/soupsieve/blob/main/LICENSE.md statsmodels BSD 3-Clause License, see https://github.com/statsmodels/statsmodels/blob/main/LICENSE.txt sympy BSD 3-Clause License, see https://github.com/sympy/sympy/blob/master/LICENSE Note that some submodules are published under different Licenses. tensorflow Apache License Version 2.0, see https://github.com/tensorflow/tensorflow/blob/master/LICENSE varclushi GNU General Public License 3.0, see https://github.com/jingtt/varclushi/blob/master/LICENSE","title":"License"},{"location":"license.html#license","text":"Copyright 2021-2022 Anovos Developers Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at: http://www.apache.org/licenses/LICENSE-2.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS","title":"License"},{"location":"license.html#licenses-of-third-party-dependencies","text":"The Anovos project is licensed under the terms of the Apache 2.0 License . However, Anovos builds on several third-party open source libraries that are made available under the following licenses:","title":"Licenses of third-party dependencies"},{"location":"license.html#java-dependencies","text":"","title":"Java dependencies"},{"location":"license.html#apache-spark","text":"Apache License Version 2.0, see https://github.com/apache/spark/blob/master/LICENSE","title":"Apache Spark"},{"location":"license.html#histogrammar","text":"Apache License Version 2.0, see https://github.com/histogrammar/histogrammar-scala/blob/master/LICENSE","title":"Histogrammar"},{"location":"license.html#histogrammar-sparksql","text":"Apache License Version 2.0, see https://github.com/histogrammar/histogrammar-scala/blob/master/LICENSE","title":"Histogrammar-SparkSQL"},{"location":"license.html#python-packages","text":"","title":"Python packages"},{"location":"license.html#black","text":"MIT License, see https://github.com/psf/black/blob/main/LICENSE","title":"black"},{"location":"license.html#boto3","text":"Apache License Version 2.0, see https://github.com/boto/boto3/blob/develop/LICENSE","title":"boto3"},{"location":"license.html#cython","text":"Apache License Version 2.0, see https://github.com/cython/cython/blob/master/LICENSE.txt","title":"cython"},{"location":"license.html#datapane","text":"Apache License Version 2.0, see https://github.com/datapane/datapane/blob/master/LICENSE","title":"datapane"},{"location":"license.html#findspark","text":"BSD 3-Clause License, see https://github.com/minrk/findspark/blob/main/LICENSE.md","title":"findspark"},{"location":"license.html#fsspec","text":"BSD 3-Clause License, see https://github.com/fsspec/filesystem_spec/blob/master/LICENSE","title":"fsspec"},{"location":"license.html#isort","text":"MIT License, see https://github.com/PyCQA/isort/blob/main/LICENSE","title":"isort"},{"location":"license.html#loguru","text":"MIT License, see https://github.com/Delgan/loguru/blob/master/LICENSE","title":"loguru"},{"location":"license.html#matplotlib","text":"License agreement for matplotlib versions 1.3.0 and later, see https://github.com/matplotlib/matplotlib/blob/main/LICENSE/LICENSE","title":"matplotlib"},{"location":"license.html#numpy","text":"BSD 3-Clause License, see https://github.com/numpy/numpy/blob/main/LICENSE.txt","title":"numpy"},{"location":"license.html#pandas","text":"BSD 3-Clause License, see https://github.com/pandas-dev/pandas/blob/master/LICENSE","title":"pandas"},{"location":"license.html#plotly","text":"MIT License, see https://github.com/plotly/plotly.py/blob/master/LICENSE.txt","title":"plotly"},{"location":"license.html#popmon","text":"MIT License, see https://github.com/ing-bank/popmon/blob/master/LICENSE","title":"popmon"},{"location":"license.html#py4j","text":"BSD 3-Clause License, see https://github.com/py4j/py4j/blob/master/LICENSE.txt","title":"py4j"},{"location":"license.html#pyaml","text":"MIT License, see https://github.com/yaml/pyyaml/blob/master/LICENSE","title":"pyaml"},{"location":"license.html#pyarrow","text":"Apache License Version 2.0, see https://github.com/apache/arrow/blob/master/LICENSE.txt","title":"pyarrow"},{"location":"license.html#pybind11","text":"BSD 3-Clause License, see https://github.com/pybind/pybind11/blob/master/LICENSE","title":"pybind11"},{"location":"license.html#reverse_geocoder","text":"GNU Lesser General Public License Version 2.1, see https://github.com/thampiman/reverse-geocoder/blob/master/LICENSE","title":"reverse_geocoder"},{"location":"license.html#s3fs","text":"BSD 3-Clause License, see https://github.com/fsspec/s3fs/blob/main/LICENSE.txt","title":"s3fs"},{"location":"license.html#s3path","text":"Apache License Version 2.0, see https://github.com/liormizr/s3path/blob/master/LICENSE","title":"s3path"},{"location":"license.html#scikit-learn","text":"BSD 3-Clause License, see https://github.com/scikit-learn/scikit-learn/blob/main/COPYING","title":"scikit-learn"},{"location":"license.html#scipy","text":"BSD 3-Clause License, see https://github.com/scipy/scipy/blob/master/LICENSE.txt","title":"scipy"},{"location":"license.html#sentence-transformers","text":"Apache License Version 2.0, see https://github.com/UKPLab/sentence-transformers/blob/master/LICENSE","title":"sentence-transformers"},{"location":"license.html#soupsieve","text":"MIT License, see https://github.com/facelessuser/soupsieve/blob/main/LICENSE.md","title":"soupsieve"},{"location":"license.html#statsmodels","text":"BSD 3-Clause License, see https://github.com/statsmodels/statsmodels/blob/main/LICENSE.txt","title":"statsmodels"},{"location":"license.html#sympy","text":"BSD 3-Clause License, see https://github.com/sympy/sympy/blob/master/LICENSE Note that some submodules are published under different Licenses.","title":"sympy"},{"location":"license.html#tensorflow","text":"Apache License Version 2.0, see https://github.com/tensorflow/tensorflow/blob/master/LICENSE","title":"tensorflow"},{"location":"license.html#varclushi","text":"GNU General Public License 3.0, see https://github.com/jingtt/varclushi/blob/master/LICENSE","title":"varclushi"},{"location":"api/_index.html","text":"Overview Anovos modules reflect the key components of the Machine Learning (ML) pipeline and are scalable using python API of Spark (PySpark) - the distributed computing framework. The key modules included in the alpha release are: Data Ingest : This module is an ETL (Extract, transform, load) component of Anovos and helps load dataset(s) as Spark Dataframe. It also allows performing some basic pre-processing, like selecting, deleting, renaming, and recasting columns to ensure cleaner data is used in downstream data analysis. Data Analyzer : This data analysis module gives a 360\u00ba view of the ingested data. It helps provide a better understanding of the data quality and the transformations required for the modeling purpose. There are three submodules of this module targeting specific needs of the data analysis process. a. Statistics Generator : This submodule generates all descriptive statistics related to the ingested data. The descriptive statistics are further broken down into different metric types such as Measures of Counts, Measures of Central Tendency, Measures of Cardinality, Measures of Dispersion (aka Measures of Spread in Statistics), Measures of Percentiles (aka Measures of Position), and Measures of Shape (aka Measures of Moments). b. Quality Checker : This submodule focuses on assessing the data quality at both row and column levels. It includes an option to fix identified issues with the correct treatment method. The row-level quality checks include duplicate detection and null detection (% columns that are missing for a row). The column level quality checks include outlier detection, null detection (% rows which are missing for a column), biasedness detection ( checking if a column is biased towards one specific value), cardinality detection (checking if a categorical/discrete column have very high no. of unique values) and invalid entries detection which checks for suspicious patterns in the column values. c. Association Evaluator : This submodule focuses on understanding the interaction between different attributes (correlation, variable clustering) and/or the relationship between an attribute & the binary target variable ( Information Gain, Information Value). Data Drift & Data Stability Computation : In an ML context, data drift is the change in the distribution of the baseline dataset that trained the model (source distribution) and the ingested data (target distribution) that makes the prediction. Data drift is one of the primary causes of poor performance of ML models over time. This module ensures the stability of the ingested dataset over time by analyzing it with the baseline dataset (via computing drift statistics) and/or with historically ingested datasets (via computing stability index for existing attributes or estimating for newly composed features \u2013 currently supports only numerical features), if available. Identifying the data drift at an early stage enables data scientists to be proactive and fix the root cause. Data Transformer : In the alpha release, the data transformer module only includes some basic pre-processing functions like binning, encoding, to name a few. These functions were required to support computations of the above key modules. A more exhaustive set of transformations can be expected in future releases. Data Report : This module is a visualization component of Anovos. All the analysis on the key modules is visualized via an HTML report to get a well-rounded understanding of the ingested dataset. The report contains an executive summary, wiki for data dictionary & metric dictionary, a tab corresponding to key modules demonstrating the output. Note: Upcoming Modules - Feature Wiki, Feature store, Auto ML, ML Flow Integration Expand source code \"\"\"Anovos modules reflect the key components of the Machine Learning (ML) pipeline and are scalable using python API of Spark (PySpark) - the distributed computing framework. The key modules included in the alpha release are: 1. **Data Ingest**: This module is an ETL (Extract, transform, load) component of Anovos and helps load dataset(s) as Spark Dataframe. It also allows performing some basic pre-processing, like selecting, deleting, renaming, and recasting columns to ensure cleaner data is used in downstream data analysis. 2. **Data Analyzer**: This data analysis module gives a 360\u00ba view of the ingested data. It helps provide a better understanding of the data quality and the transformations required for the modeling purpose. There are three submodules of this module targeting specific needs of the data analysis process. a. *Statistics Generator*: This submodule generates all descriptive statistics related to the ingested data. The descriptive statistics are further broken down into different metric types such as Measures of Counts, Measures of Central Tendency, Measures of Cardinality, Measures of Dispersion (aka Measures of Spread in Statistics), Measures of Percentiles (aka Measures of Position), and Measures of Shape (aka Measures of Moments). b. *Quality Checker*: This submodule focuses on assessing the data quality at both row and column levels. It includes an option to fix identified issues with the correct treatment method. The row-level quality checks include duplicate detection and null detection (% columns that are missing for a row). The column level quality checks include outlier detection, null detection (% rows which are missing for a column), biasedness detection ( checking if a column is biased towards one specific value), cardinality detection (checking if a categorical/discrete column have very high no. of unique values) and invalid entries detection which checks for suspicious patterns in the column values. c. *Association Evaluator*: This submodule focuses on understanding the interaction between different attributes (correlation, variable clustering) and/or the relationship between an attribute & the binary target variable ( Information Gain, Information Value). 3. **Data Drift & Data Stability Computation**: In an ML context, data drift is the change in the distribution of the baseline dataset that trained the model (source distribution) and the ingested data (target distribution) that makes the prediction. Data drift is one of the primary causes of poor performance of ML models over time. This module ensures the stability of the ingested dataset over time by analyzing it with the baseline dataset (via computing drift statistics) and/or with historically ingested datasets (via computing stability index for existing attributes or estimating for newly composed features \u2013 currently supports only numerical features), if available. Identifying the data drift at an early stage enables data scientists to be proactive and fix the root cause. 4. **Data Transformer**: In the alpha release, the data transformer module only includes some basic pre-processing functions like binning, encoding, to name a few. These functions were required to support computations of the above key modules. A more exhaustive set of transformations can be expected in future releases. 5. **Data Report**: This module is a visualization component of Anovos. All the analysis on the key modules is visualized via an HTML report to get a well-rounded understanding of the ingested dataset. The report contains an executive summary, wiki for data dictionary & metric dictionary, a tab corresponding to key modules demonstrating the output. Note: Upcoming Modules - Feature Wiki, Feature store, Auto ML, ML Flow Integration \"\"\" from .version import __version__ Sub-modules anovos.data_analyzer anovos.data_ingest anovos.data_report anovos.data_transformer anovos.drift_stability anovos.feature_recommender anovos.feature_store anovos.shared anovos.version anovos.workflow","title":"Overview"},{"location":"api/_index.html#overview","text":"Anovos modules reflect the key components of the Machine Learning (ML) pipeline and are scalable using python API of Spark (PySpark) - the distributed computing framework. The key modules included in the alpha release are: Data Ingest : This module is an ETL (Extract, transform, load) component of Anovos and helps load dataset(s) as Spark Dataframe. It also allows performing some basic pre-processing, like selecting, deleting, renaming, and recasting columns to ensure cleaner data is used in downstream data analysis. Data Analyzer : This data analysis module gives a 360\u00ba view of the ingested data. It helps provide a better understanding of the data quality and the transformations required for the modeling purpose. There are three submodules of this module targeting specific needs of the data analysis process. a. Statistics Generator : This submodule generates all descriptive statistics related to the ingested data. The descriptive statistics are further broken down into different metric types such as Measures of Counts, Measures of Central Tendency, Measures of Cardinality, Measures of Dispersion (aka Measures of Spread in Statistics), Measures of Percentiles (aka Measures of Position), and Measures of Shape (aka Measures of Moments). b. Quality Checker : This submodule focuses on assessing the data quality at both row and column levels. It includes an option to fix identified issues with the correct treatment method. The row-level quality checks include duplicate detection and null detection (% columns that are missing for a row). The column level quality checks include outlier detection, null detection (% rows which are missing for a column), biasedness detection ( checking if a column is biased towards one specific value), cardinality detection (checking if a categorical/discrete column have very high no. of unique values) and invalid entries detection which checks for suspicious patterns in the column values. c. Association Evaluator : This submodule focuses on understanding the interaction between different attributes (correlation, variable clustering) and/or the relationship between an attribute & the binary target variable ( Information Gain, Information Value). Data Drift & Data Stability Computation : In an ML context, data drift is the change in the distribution of the baseline dataset that trained the model (source distribution) and the ingested data (target distribution) that makes the prediction. Data drift is one of the primary causes of poor performance of ML models over time. This module ensures the stability of the ingested dataset over time by analyzing it with the baseline dataset (via computing drift statistics) and/or with historically ingested datasets (via computing stability index for existing attributes or estimating for newly composed features \u2013 currently supports only numerical features), if available. Identifying the data drift at an early stage enables data scientists to be proactive and fix the root cause. Data Transformer : In the alpha release, the data transformer module only includes some basic pre-processing functions like binning, encoding, to name a few. These functions were required to support computations of the above key modules. A more exhaustive set of transformations can be expected in future releases. Data Report : This module is a visualization component of Anovos. All the analysis on the key modules is visualized via an HTML report to get a well-rounded understanding of the ingested dataset. The report contains an executive summary, wiki for data dictionary & metric dictionary, a tab corresponding to key modules demonstrating the output. Note: Upcoming Modules - Feature Wiki, Feature store, Auto ML, ML Flow Integration Expand source code \"\"\"Anovos modules reflect the key components of the Machine Learning (ML) pipeline and are scalable using python API of Spark (PySpark) - the distributed computing framework. The key modules included in the alpha release are: 1. **Data Ingest**: This module is an ETL (Extract, transform, load) component of Anovos and helps load dataset(s) as Spark Dataframe. It also allows performing some basic pre-processing, like selecting, deleting, renaming, and recasting columns to ensure cleaner data is used in downstream data analysis. 2. **Data Analyzer**: This data analysis module gives a 360\u00ba view of the ingested data. It helps provide a better understanding of the data quality and the transformations required for the modeling purpose. There are three submodules of this module targeting specific needs of the data analysis process. a. *Statistics Generator*: This submodule generates all descriptive statistics related to the ingested data. The descriptive statistics are further broken down into different metric types such as Measures of Counts, Measures of Central Tendency, Measures of Cardinality, Measures of Dispersion (aka Measures of Spread in Statistics), Measures of Percentiles (aka Measures of Position), and Measures of Shape (aka Measures of Moments). b. *Quality Checker*: This submodule focuses on assessing the data quality at both row and column levels. It includes an option to fix identified issues with the correct treatment method. The row-level quality checks include duplicate detection and null detection (% columns that are missing for a row). The column level quality checks include outlier detection, null detection (% rows which are missing for a column), biasedness detection ( checking if a column is biased towards one specific value), cardinality detection (checking if a categorical/discrete column have very high no. of unique values) and invalid entries detection which checks for suspicious patterns in the column values. c. *Association Evaluator*: This submodule focuses on understanding the interaction between different attributes (correlation, variable clustering) and/or the relationship between an attribute & the binary target variable ( Information Gain, Information Value). 3. **Data Drift & Data Stability Computation**: In an ML context, data drift is the change in the distribution of the baseline dataset that trained the model (source distribution) and the ingested data (target distribution) that makes the prediction. Data drift is one of the primary causes of poor performance of ML models over time. This module ensures the stability of the ingested dataset over time by analyzing it with the baseline dataset (via computing drift statistics) and/or with historically ingested datasets (via computing stability index for existing attributes or estimating for newly composed features \u2013 currently supports only numerical features), if available. Identifying the data drift at an early stage enables data scientists to be proactive and fix the root cause. 4. **Data Transformer**: In the alpha release, the data transformer module only includes some basic pre-processing functions like binning, encoding, to name a few. These functions were required to support computations of the above key modules. A more exhaustive set of transformations can be expected in future releases. 5. **Data Report**: This module is a visualization component of Anovos. All the analysis on the key modules is visualized via an HTML report to get a well-rounded understanding of the ingested dataset. The report contains an executive summary, wiki for data dictionary & metric dictionary, a tab corresponding to key modules demonstrating the output. Note: Upcoming Modules - Feature Wiki, Feature store, Auto ML, ML Flow Integration \"\"\" from .version import __version__","title":"Overview"},{"location":"api/_index.html#sub-modules","text":"anovos.data_analyzer anovos.data_ingest anovos.data_report anovos.data_transformer anovos.drift_stability anovos.feature_recommender anovos.feature_store anovos.shared anovos.version anovos.workflow","title":"Sub-modules"},{"location":"api/workflow.html","text":"workflow Expand source code import contextlib import copy import glob import os import subprocess import timeit import mlflow import yaml from loguru import logger from anovos.data_analyzer import association_evaluator , quality_checker , stats_generator from anovos.data_analyzer.geospatial_analyzer import geospatial_autodetection from anovos.data_analyzer.ts_analyzer import ts_analyzer from anovos.data_ingest import data_ingest from anovos.data_ingest.ts_auto_detection import ts_preprocess from anovos.data_report import report_preprocessing from anovos.data_report.basic_report_generation import anovos_basic_report from anovos.data_report.report_generation import anovos_report from anovos.data_report.report_preprocessing import save_stats from anovos.data_transformer import transformers from anovos.data_transformer.geospatial import ( centroid , geo_format_geohash , geo_format_latlon , location_in_country , rog_calculation , ) from anovos.drift_stability import drift_detector as ddetector from anovos.drift_stability import stability as dstability from anovos.feature_store import feast_exporter from anovos.shared.spark import spark mapbox_list = [ \"open-street-map\" , \"white-bg\" , \"carto-positron\" , \"carto-darkmatter\" , \"stamen-terrain\" , \"stamen-toner\" , \"stamen-watercolor\" , ] def ETL ( args ): f = getattr ( data_ingest , \"read_dataset\" ) read_args = args . get ( \"read_dataset\" , None ) if read_args : df = f ( spark , ** read_args ) else : raise TypeError ( \"Invalid input for reading dataset\" ) for key , value in args . items (): if key != \"read_dataset\" : if value is not None : f = getattr ( data_ingest , key ) if isinstance ( value , dict ): df = f ( df , ** value ) else : df = f ( df , value ) return df def save ( data , write_configs , folder_name , reread = False ): if write_configs : if \"file_path\" not in write_configs : raise TypeError ( \"file path missing for writing data\" ) write = copy . deepcopy ( write_configs ) run_id = write . pop ( \"mlflow_run_id\" , \"\" ) log_mlflow = write . pop ( \"log_mlflow\" , False ) write [ \"file_path\" ] = write [ \"file_path\" ] + \"/\" + folder_name + \"/\" + str ( run_id ) data_ingest . write_dataset ( data , ** write ) if log_mlflow : mlflow . log_artifacts ( local_dir = write [ \"file_path\" ], artifact_path = folder_name ) if reread : read = copy . deepcopy ( write ) if \"file_configs\" in read : read [ \"file_configs\" ] . pop ( \"repartition\" , None ) read [ \"file_configs\" ] . pop ( \"mode\" , None ) data = data_ingest . read_dataset ( spark , ** read ) return data def stats_args ( all_configs , func ): stats_configs = all_configs . get ( \"stats_generator\" , None ) write_configs = all_configs . get ( \"write_stats\" , None ) report_input_path = \"\" report_configs = all_configs . get ( \"report_preprocessing\" , None ) if report_configs is not None : if \"master_path\" not in report_configs : raise TypeError ( \"Master path missing for saving report statistics\" ) else : report_input_path = report_configs . get ( \"master_path\" ) result = {} if stats_configs : mainfunc_to_args = { \"biasedness_detection\" : [ \"stats_mode\" ], \"IDness_detection\" : [ \"stats_unique\" ], \"nullColumns_detection\" : [ \"stats_unique\" , \"stats_mode\" , \"stats_missing\" ], \"variable_clustering\" : [ \"stats_mode\" ], \"charts_to_objects\" : [ \"stats_unique\" ], \"cat_to_num_unsupervised\" : [ \"stats_unique\" ], \"PCA_latentFeatures\" : [ \"stats_missing\" ], \"autoencoder_latentFeatures\" : [ \"stats_missing\" ], } args_to_statsfunc = { \"stats_unique\" : \"measures_of_cardinality\" , \"stats_mode\" : \"measures_of_centralTendency\" , \"stats_missing\" : \"measures_of_counts\" , } for arg in mainfunc_to_args . get ( func , []): if not report_input_path : if write_configs : read = copy . deepcopy ( write_configs ) if \"file_configs\" in read : read [ \"file_configs\" ] . pop ( \"repartition\" , None ) read [ \"file_configs\" ] . pop ( \"mode\" , None ) if read [ \"file_type\" ] == \"csv\" : read [ \"file_configs\" ][ \"inferSchema\" ] = True read [ \"file_path\" ] = ( read [ \"file_path\" ] + \"/data_analyzer/stats_generator/\" + args_to_statsfunc [ arg ] ) result [ arg ] = read else : result [ arg ] = { \"file_path\" : ( report_input_path + \"/\" + args_to_statsfunc [ arg ] + \".csv\" ), \"file_type\" : \"csv\" , \"file_configs\" : { \"header\" : True , \"inferSchema\" : True }, } return result def main ( all_configs , run_type , auth_key_val = {}): if run_type == \"ak8s\" : conf = spark . sparkContext . _jsc . hadoopConfiguration () conf . set ( \"fs.wasbs.impl\" , \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\" ) # Set credentials using auth_key_val for key , value in auth_key_val . items (): spark . conf . set ( key , value ) auth_key = value else : auth_key = \"NA\" start_main = timeit . default_timer () df = ETL ( all_configs . get ( \"input_dataset\" )) write_main = all_configs . get ( \"write_main\" , None ) write_intermediate = all_configs . get ( \"write_intermediate\" , None ) write_stats = all_configs . get ( \"write_stats\" , None ) write_feast_features = all_configs . get ( \"write_feast_features\" , None ) if write_intermediate and run_type == \"ak8s\" : default_root_path = write_intermediate . get ( \"file_path\" , None ) else : default_root_path = None if write_feast_features is not None : repartition_count = ( write_main [ \"file_configs\" ][ \"repartition\" ] if \"file_configs\" in write_main and \"repartition\" in write_main [ \"file_configs\" ] else - 1 ) feast_exporter . check_feast_configuration ( write_feast_features , repartition_count ) mlflow_config = all_configs . get ( \"mlflow\" , None ) if mlflow_config is not None : mlflow . set_tracking_uri ( mlflow_config [ \"tracking_uri\" ]) mlflow . set_experiment ( mlflow_config [ \"experiment\" ]) mlflow_run = ( mlflow . start_run () if mlflow_config is not None else contextlib . nullcontext () ) with mlflow_run : if mlflow_config is not None : mlflow_config [ \"run_id\" ] = mlflow_run . info . run_id start_main = timeit . default_timer () df = ETL ( all_configs . get ( \"input_dataset\" )) write_main = all_configs . get ( \"write_main\" , None ) write_intermediate = all_configs . get ( \"write_intermediate\" , None ) write_stats = all_configs . get ( \"write_stats\" , None ) if mlflow_config : if write_main : write_main [ \"mlflow_run_id\" ] = mlflow_run . info . run_id write_main [ \"log_mlflow\" ] = mlflow_config [ \"track_output\" ] if write_intermediate : write_intermediate [ \"mlflow_run_id\" ] = mlflow_run . info . run_id write_intermediate [ \"log_mlflow\" ] = mlflow_config [ \"track_intermediates\" ] if write_stats : write_stats [ \"mlflow_run_id\" ] = mlflow_run . info . run_id write_stats [ \"log_mlflow\" ] = mlflow_config [ \"track_reports\" ] report_input_path = \"\" report_configs = all_configs . get ( \"report_preprocessing\" , None ) if report_configs is not None : if \"master_path\" not in report_configs : raise TypeError ( \"Master path missing for saving report statistics\" ) else : report_input_path = report_configs . get ( \"master_path\" ) for key , args in all_configs . items (): if ( key == \"concatenate_dataset\" ) & ( args is not None ): start = timeit . default_timer () idfs = [ df ] for k in [ e for e in args . keys () if e not in ( \"method\" )]: tmp = ETL ( args . get ( k )) idfs . append ( tmp ) df = data_ingest . concatenate_dataset ( * idfs , method_type = args . get ( \"method\" ) ) df = save ( df , write_intermediate , folder_name = \"data_ingest/concatenate_dataset\" , reread = True , ) end = timeit . default_timer () logger . info ( f \" { key } : execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if ( key == \"join_dataset\" ) & ( args is not None ): start = timeit . default_timer () idfs = [ df ] for k in [ e for e in args . keys () if e not in ( \"join_type\" , \"join_cols\" ) ]: tmp = ETL ( args . get ( k )) idfs . append ( tmp ) df = data_ingest . join_dataset ( * idfs , join_cols = args . get ( \"join_cols\" ), join_type = args . get ( \"join_type\" ), ) df = save ( df , write_intermediate , folder_name = \"data_ingest/join_dataset\" , reread = True , ) end = timeit . default_timer () logger . info ( f \" { key } : execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if ( key == \"geospatial_controller\" ) & ( args is not None ): start = timeit . default_timer () auto_detection_analyzer_flag = args . get ( \"geospatial_analyzer\" ) . get ( \"auto_detection_analyzer\" , False ) geo_transformations = args . get ( \"geo_transformations\" , False ) id_col = args . get ( \"geospatial_analyzer\" ) . get ( \"id_col\" , None ) max_analysis_records = args . get ( \"geospatial_analyzer\" ) . get ( \"max_analysis_records\" , None ) top_geo_records = args . get ( \"geospatial_analyzer\" ) . get ( \"top_geo_records\" , None ) max_cluster = args . get ( \"geospatial_analyzer\" ) . get ( \"max_cluster\" , None ) eps = args . get ( \"geospatial_analyzer\" ) . get ( \"eps\" , None ) min_samples = args . get ( \"geospatial_analyzer\" ) . get ( \"min_samples\" , None ) try : global_map_box_val = mapbox_list . index ( args . get ( \"geospatial_analyzer\" , None ) . get ( \"global_map_box_val\" , None ) ) except : global_map_box_val = 0 if auto_detection_analyzer_flag : start = timeit . default_timer () lat_cols , long_cols , gh_cols = geospatial_autodetection ( df , id_col , report_input_path , max_analysis_records , top_geo_records , max_cluster , eps , min_samples , global_map_box_val , run_type , auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , auto_detection_geospatial: execution time (in secs) = { round ( end - start , 4 ) } \" ) if geo_transformations : country_val = args . get ( \"geo_transformations\" ) . get ( \"country\" , None ) country_shapefile_path = args . get ( \"geo_transformations\" ) . get ( \"country_shapefile_path\" , None ) method_type = args . get ( \"geo_transformations\" ) . get ( \"method_type\" , None ) result_prefix = args . get ( \"geo_transformations\" ) . get ( \"result_prefix\" , None ) loc_input_format = args . get ( \"geo_transformations\" ) . get ( \"loc_input_format\" , None ) loc_output_format = args . get ( \"geo_transformations\" ) . get ( \"loc_output_format\" , None ) result_prefix_lat_lon = args . get ( \"geo_transformations\" ) . get ( \"result_prefix_lat_lon\" , None ) result_prefix_geo = args . get ( \"geo_transformations\" ) . get ( \"result_prefix_geo\" , None ) id_col = args . get ( \"geo_transformations\" ) . get ( \"id_col\" , None ) if (( lat_cols == []) & ( long_cols == [])) or ( gh_cols == []): lat_cols = args . get ( \"geo_transformations\" ) . get ( \"list_of_lat\" , None ) long_cols = args . get ( \"geo_transformations\" ) . get ( \"list_of_lon\" , None ) gh_cols = args . get ( \"geo_transformations\" ) . get ( \"list_of_geohash\" , None ) if args . get ( \"geo_transformations\" ) . get ( \"location_in_country_detection\" ): df = location_in_country ( spark , df , lat_cols , long_cols , country_val , country_shapefile_path , method_type , result_prefix_lat_lon , ) if args . get ( \"geo_transformations\" ) . get ( \"geo_format_conversion\" ): if len ( lat_cols ) >= 1 : df = geo_format_latlon ( df , lat_cols , long_cols , loc_input_format , loc_output_format , result_prefix_geo , ) if ( len ( lat_cols ) >= 1 ) & ( len ( gh_cols ) >= 1 ): logger . info ( f \"Transformation of Latitude and Longitude columns have been done. Transformation of Geohash columns will be skipped.\" ) elif len ( gh_cols ) >= 1 : df = geo_format_geohash ( df , gh_cols , loc_output_format , result_prefix_lat_lon ) if args . get ( \"geo_transformations\" ) . get ( \"centroid_calculation\" ): for idx , i in enumerate ( lat_cols ): df_ = centroid ( df , lat_cols [ idx ], long_cols [ idx ], id_col ) df = df . join ( df_ , id_col , \"inner\" ) if args . get ( \"geo_transformations\" ) . get ( \"rog_calculation\" ): for idx , i in enumerate ( lat_cols ): cols_drop = [ lat_cols [ idx ] + \"_centroid\" , long_cols [ idx ] + \"_centroid\" , ] df_ = rog_calculation ( df . drop ( * cols_drop ), lat_cols [ idx ], long_cols [ idx ], id_col , ) df = df . join ( df_ , id_col , \"inner\" ) if ( not auto_detection_analyzer_flag ) & ( not geo_transformations ): lat_cols , long_cols , gh_cols = [], [], [] continue if ( key == \"timeseries_analyzer\" ) & ( args is not None ): auto_detection_flag = args . get ( \"auto_detection\" , False ) id_col = args . get ( \"id_col\" , None ) tz_val = args . get ( \"tz_offset\" , None ) inspection_flag = args . get ( \"inspection\" , False ) analysis_level = args . get ( \"analysis_level\" , None ) max_days_limit = args . get ( \"max_days\" , None ) if auto_detection_flag : start = timeit . default_timer () df = ts_preprocess ( spark , df , id_col , output_path = report_input_path , tz_offset = tz_val , run_type = run_type , mlflow_config = mlflow_config , auth_key = auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , auto_detection: execution time (in secs) = { round ( end - start , 4 ) } \" ) if inspection_flag : start = timeit . default_timer () ts_analyzer ( spark , df , id_col , max_days = max_days_limit , output_path = report_input_path , output_type = analysis_level , tz_offset = tz_val , run_type = run_type , auth_key = auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , inspection: execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if ( ( key == \"anovos_basic_report\" ) & ( args is not None ) & args . get ( \"basic_report\" , False ) ): start = timeit . default_timer () anovos_basic_report ( spark , df , ** args . get ( \"report_args\" , {}), run_type = run_type , auth_key = auth_key , mlflow_config = mlflow_config , ) end = timeit . default_timer () logger . info ( f \"Basic Report: execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if not all_configs . get ( \"anovos_basic_report\" , {}) . get ( \"basic_report\" , False ): if ( key == \"stats_generator\" ) & ( args is not None ): for m in args [ \"metric\" ]: start = timeit . default_timer () print ( \" \\n \" + m + \": \\n \" ) f = getattr ( stats_generator , m ) df_stats = f ( spark , df , ** args [ \"metric_args\" ], print_impact = False ) if report_input_path : save_stats ( spark , df_stats , report_input_path , m , reread = True , run_type = run_type , auth_key = auth_key , mlflow_config = mlflow_config , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"data_analyzer/stats_generator/\" + m , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { m } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"quality_checker\" ) & ( args is not None ): for subkey , value in args . items (): if value is not None : start = timeit . default_timer () print ( \" \\n \" + subkey + \": \\n \" ) f = getattr ( quality_checker , subkey ) extra_args = stats_args ( all_configs , subkey ) if subkey == \"nullColumns_detection\" : if \"invalidEntries_detection\" in args . keys (): if args . get ( \"invalidEntries_detection\" ) . get ( \"treatment\" , None ): extra_args [ \"stats_missing\" ] = {} if \"outlier_detection\" in args . keys (): if args . get ( \"outlier_detection\" ) . get ( \"treatment\" , None ): if ( args . get ( \"outlier_detection\" ) . get ( \"treatment_method\" , None ) == \"null_replacement\" ): extra_args [ \"stats_missing\" ] = {} if subkey in [ \"outlier_detection\" , \"duplicate_detection\" ]: extra_args [ \"print_impact\" ] = True else : extra_args [ \"print_impact\" ] = False df , df_stats = f ( spark , df , ** value , ** extra_args ) df = save ( df , write_intermediate , folder_name = \"data_analyzer/quality_checker/\" + subkey + \"/dataset\" , reread = True , ) if report_input_path : df_stats = save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) else : df_stats = save ( df_stats , write_stats , folder_name = \"data_analyzer/quality_checker/\" + subkey , reread = True , ) if subkey != \"outlier_detection\" : df_stats . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"association_evaluator\" ) & ( args is not None ): for subkey , value in args . items (): if value is not None : start = timeit . default_timer () print ( \" \\n \" + subkey + \": \\n \" ) if subkey == \"correlation_matrix\" : f = getattr ( association_evaluator , subkey ) extra_args = stats_args ( all_configs , subkey ) cat_to_num_trans_params = all_configs . get ( \"cat_to_num_transformer\" , None ) df_trans_corr = transformers . cat_to_num_transformer ( spark , df , ** cat_to_num_trans_params ) df_stats = f ( spark , df_trans_corr , ** value , ** extra_args , print_impact = False , ) else : f = getattr ( association_evaluator , subkey ) extra_args = stats_args ( all_configs , subkey ) df_stats = f ( spark , df , ** value , ** extra_args , print_impact = False ) if report_input_path : save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"data_analyzer/association_evaluator/\" + subkey , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"drift_detector\" ) & ( args is not None ): for subkey , value in args . items (): if ( subkey == \"drift_statistics\" ) & ( value is not None ): start = timeit . default_timer () if not value [ \"configs\" ][ \"pre_existing_source\" ]: source = ETL ( value . get ( \"source_dataset\" )) else : source = None logger . info ( f \"running drift statistics detector using { value [ 'configs' ] } \" ) df_stats = ddetector . statistics ( spark , df , source , ** value [ \"configs\" ], print_impact = False , ) if report_input_path : save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"drift_detector/drift_statistics\" , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( subkey == \"stability_index\" ) & ( value is not None ): start = timeit . default_timer () idfs = [] for k in [ e for e in value . keys () if e not in ( \"configs\" )]: tmp = ETL ( value . get ( k )) idfs . append ( tmp ) df_stats = dstability . stability_index_computation ( spark , idfs , ** value [ \"configs\" ], print_impact = False ) if report_input_path : save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) appended_metric_path = value [ \"configs\" ] . get ( \"appended_metric_path\" , \"\" ) if appended_metric_path : df_metrics = data_ingest . read_dataset ( spark , file_path = appended_metric_path , file_type = \"csv\" , file_configs = { \"header\" : True , \"mode\" : \"overwrite\" , }, ) save_stats ( spark , df_metrics , report_input_path , \"stabilityIndex_metrics\" , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"drift_detector/stability_index\" , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) logger . info ( f \"execution time w/o report (in sec) = { round ( end - start_main , 4 ) } \" ) if ( key == \"transformers\" ) & ( args is not None ): for subkey , value in args . items (): if value is not None : for subkey2 , value2 in value . items (): if value2 is not None : start = timeit . default_timer () print ( \" \\n \" + subkey2 + \": \\n \" ) f = getattr ( transformers , subkey2 ) extra_args = stats_args ( all_configs , subkey2 ) if subkey2 in ( \"imputation_sklearn\" , \"autoencoder_latentFeatures\" , \"auto_imputation\" , \"PCA_latentFeatures\" , ): extra_args [ \"run_type\" ] = run_type extra_args [ \"auth_key\" ] = auth_key if subkey2 == \"cat_to_num_supervised\" : if ( \"model_path\" not in value2 . keys () and default_root_path ): extra_args [ \"model_path\" ] = ( default_root_path + \"/intermediate_model\" ) if subkey2 in ( \"normalization\" , \"feature_transformation\" , \"boxcox_transformation\" , \"expression_parser\" , ): df_transformed = f ( df , ** value2 , ** extra_args , print_impact = True , ) elif subkey2 in \"imputation_sklearn\" : df_transformed = f ( spark , df , ** value2 , ** extra_args , print_impact = False , ) else : df_transformed = f ( spark , df , ** value2 , ** extra_args , print_impact = True , ) df = save ( df_transformed , write_intermediate , folder_name = \"data_transformer/transformers/\" + subkey2 , reread = True , ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey2 } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"report_preprocessing\" ) & ( args is not None ): for subkey , value in args . items (): if ( subkey == \"charts_to_objects\" ) & ( value is not None ): start = timeit . default_timer () f = getattr ( report_preprocessing , subkey ) extra_args = stats_args ( all_configs , subkey ) f ( spark , df , ** value , ** extra_args , master_path = report_input_path , run_type = run_type , auth_key = auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"report_generation\" ) & ( args is not None ): start = timeit . default_timer () timeseries_analyzer = all_configs . get ( \"timeseries_analyzer\" , None ) if timeseries_analyzer : analysis_level = timeseries_analyzer . get ( \"analysis_level\" , None ) else : analysis_level = None geospatial_controller = all_configs . get ( \"geospatial_controller\" , None ) if not geospatial_controller : lat_cols , long_cols , gh_cols = [], [], [] max_analysis_records , top_geo_records = None , None anovos_report ( ** args , run_type = run_type , output_type = analysis_level , lat_cols = lat_cols , long_cols = long_cols , gh_cols = gh_cols , max_records = max_analysis_records , top_geo_records = top_geo_records , auth_key = auth_key , mlflow_config = mlflow_config , ) end = timeit . default_timer () logger . info ( f \" { key } , full_report: execution time (in secs) = { round ( end - start , 4 ) } \" ) if write_feast_features is not None : file_source_config = write_feast_features [ \"file_source\" ] df = feast_exporter . add_timestamp_columns ( df , file_source_config ) save ( df , write_main , folder_name = \"final_dataset\" , reread = False ) if write_feast_features is not None : if \"file_path\" not in write_feast_features : raise ValueError ( \"File path missing for saving feature_store feature descriptions\" ) else : path = os . path . join ( write_main [ \"file_path\" ], \"final_dataset\" , \"part*\" ) filename = glob . glob ( path )[ 0 ] feast_exporter . generate_feature_description ( df . dtypes , write_feast_features , filename ) def run ( config_path , run_type , auth_key_val = {}): if run_type in ( \"local\" , \"databricks\" , \"ak8s\" ): config_file = config_path elif run_type == \"emr\" : bash_cmd = \"aws s3 cp \" + config_path + \" config.yaml\" _ = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) config_file = \"config.yaml\" else : raise ValueError ( \"Invalid run_type\" ) if run_type == \"ak8s\" and auth_key_val == {}: raise ValueError ( \"Invalid auth key for run_type\" ) with open ( config_file , \"r\" ) as f : all_configs = yaml . load ( f , yaml . SafeLoader ) main ( all_configs , run_type , auth_key_val ) Functions def ETL ( args) Expand source code def ETL ( args ): f = getattr ( data_ingest , \"read_dataset\" ) read_args = args . get ( \"read_dataset\" , None ) if read_args : df = f ( spark , ** read_args ) else : raise TypeError ( \"Invalid input for reading dataset\" ) for key , value in args . items (): if key != \"read_dataset\" : if value is not None : f = getattr ( data_ingest , key ) if isinstance ( value , dict ): df = f ( df , ** value ) else : df = f ( df , value ) return df def main ( all_configs, run_type, auth_key_val={}) Expand source code def main ( all_configs , run_type , auth_key_val = {}): if run_type == \"ak8s\" : conf = spark . sparkContext . _jsc . hadoopConfiguration () conf . set ( \"fs.wasbs.impl\" , \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\" ) # Set credentials using auth_key_val for key , value in auth_key_val . items (): spark . conf . set ( key , value ) auth_key = value else : auth_key = \"NA\" start_main = timeit . default_timer () df = ETL ( all_configs . get ( \"input_dataset\" )) write_main = all_configs . get ( \"write_main\" , None ) write_intermediate = all_configs . get ( \"write_intermediate\" , None ) write_stats = all_configs . get ( \"write_stats\" , None ) write_feast_features = all_configs . get ( \"write_feast_features\" , None ) if write_intermediate and run_type == \"ak8s\" : default_root_path = write_intermediate . get ( \"file_path\" , None ) else : default_root_path = None if write_feast_features is not None : repartition_count = ( write_main [ \"file_configs\" ][ \"repartition\" ] if \"file_configs\" in write_main and \"repartition\" in write_main [ \"file_configs\" ] else - 1 ) feast_exporter . check_feast_configuration ( write_feast_features , repartition_count ) mlflow_config = all_configs . get ( \"mlflow\" , None ) if mlflow_config is not None : mlflow . set_tracking_uri ( mlflow_config [ \"tracking_uri\" ]) mlflow . set_experiment ( mlflow_config [ \"experiment\" ]) mlflow_run = ( mlflow . start_run () if mlflow_config is not None else contextlib . nullcontext () ) with mlflow_run : if mlflow_config is not None : mlflow_config [ \"run_id\" ] = mlflow_run . info . run_id start_main = timeit . default_timer () df = ETL ( all_configs . get ( \"input_dataset\" )) write_main = all_configs . get ( \"write_main\" , None ) write_intermediate = all_configs . get ( \"write_intermediate\" , None ) write_stats = all_configs . get ( \"write_stats\" , None ) if mlflow_config : if write_main : write_main [ \"mlflow_run_id\" ] = mlflow_run . info . run_id write_main [ \"log_mlflow\" ] = mlflow_config [ \"track_output\" ] if write_intermediate : write_intermediate [ \"mlflow_run_id\" ] = mlflow_run . info . run_id write_intermediate [ \"log_mlflow\" ] = mlflow_config [ \"track_intermediates\" ] if write_stats : write_stats [ \"mlflow_run_id\" ] = mlflow_run . info . run_id write_stats [ \"log_mlflow\" ] = mlflow_config [ \"track_reports\" ] report_input_path = \"\" report_configs = all_configs . get ( \"report_preprocessing\" , None ) if report_configs is not None : if \"master_path\" not in report_configs : raise TypeError ( \"Master path missing for saving report statistics\" ) else : report_input_path = report_configs . get ( \"master_path\" ) for key , args in all_configs . items (): if ( key == \"concatenate_dataset\" ) & ( args is not None ): start = timeit . default_timer () idfs = [ df ] for k in [ e for e in args . keys () if e not in ( \"method\" )]: tmp = ETL ( args . get ( k )) idfs . append ( tmp ) df = data_ingest . concatenate_dataset ( * idfs , method_type = args . get ( \"method\" ) ) df = save ( df , write_intermediate , folder_name = \"data_ingest/concatenate_dataset\" , reread = True , ) end = timeit . default_timer () logger . info ( f \" { key } : execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if ( key == \"join_dataset\" ) & ( args is not None ): start = timeit . default_timer () idfs = [ df ] for k in [ e for e in args . keys () if e not in ( \"join_type\" , \"join_cols\" ) ]: tmp = ETL ( args . get ( k )) idfs . append ( tmp ) df = data_ingest . join_dataset ( * idfs , join_cols = args . get ( \"join_cols\" ), join_type = args . get ( \"join_type\" ), ) df = save ( df , write_intermediate , folder_name = \"data_ingest/join_dataset\" , reread = True , ) end = timeit . default_timer () logger . info ( f \" { key } : execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if ( key == \"geospatial_controller\" ) & ( args is not None ): start = timeit . default_timer () auto_detection_analyzer_flag = args . get ( \"geospatial_analyzer\" ) . get ( \"auto_detection_analyzer\" , False ) geo_transformations = args . get ( \"geo_transformations\" , False ) id_col = args . get ( \"geospatial_analyzer\" ) . get ( \"id_col\" , None ) max_analysis_records = args . get ( \"geospatial_analyzer\" ) . get ( \"max_analysis_records\" , None ) top_geo_records = args . get ( \"geospatial_analyzer\" ) . get ( \"top_geo_records\" , None ) max_cluster = args . get ( \"geospatial_analyzer\" ) . get ( \"max_cluster\" , None ) eps = args . get ( \"geospatial_analyzer\" ) . get ( \"eps\" , None ) min_samples = args . get ( \"geospatial_analyzer\" ) . get ( \"min_samples\" , None ) try : global_map_box_val = mapbox_list . index ( args . get ( \"geospatial_analyzer\" , None ) . get ( \"global_map_box_val\" , None ) ) except : global_map_box_val = 0 if auto_detection_analyzer_flag : start = timeit . default_timer () lat_cols , long_cols , gh_cols = geospatial_autodetection ( df , id_col , report_input_path , max_analysis_records , top_geo_records , max_cluster , eps , min_samples , global_map_box_val , run_type , auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , auto_detection_geospatial: execution time (in secs) = { round ( end - start , 4 ) } \" ) if geo_transformations : country_val = args . get ( \"geo_transformations\" ) . get ( \"country\" , None ) country_shapefile_path = args . get ( \"geo_transformations\" ) . get ( \"country_shapefile_path\" , None ) method_type = args . get ( \"geo_transformations\" ) . get ( \"method_type\" , None ) result_prefix = args . get ( \"geo_transformations\" ) . get ( \"result_prefix\" , None ) loc_input_format = args . get ( \"geo_transformations\" ) . get ( \"loc_input_format\" , None ) loc_output_format = args . get ( \"geo_transformations\" ) . get ( \"loc_output_format\" , None ) result_prefix_lat_lon = args . get ( \"geo_transformations\" ) . get ( \"result_prefix_lat_lon\" , None ) result_prefix_geo = args . get ( \"geo_transformations\" ) . get ( \"result_prefix_geo\" , None ) id_col = args . get ( \"geo_transformations\" ) . get ( \"id_col\" , None ) if (( lat_cols == []) & ( long_cols == [])) or ( gh_cols == []): lat_cols = args . get ( \"geo_transformations\" ) . get ( \"list_of_lat\" , None ) long_cols = args . get ( \"geo_transformations\" ) . get ( \"list_of_lon\" , None ) gh_cols = args . get ( \"geo_transformations\" ) . get ( \"list_of_geohash\" , None ) if args . get ( \"geo_transformations\" ) . get ( \"location_in_country_detection\" ): df = location_in_country ( spark , df , lat_cols , long_cols , country_val , country_shapefile_path , method_type , result_prefix_lat_lon , ) if args . get ( \"geo_transformations\" ) . get ( \"geo_format_conversion\" ): if len ( lat_cols ) >= 1 : df = geo_format_latlon ( df , lat_cols , long_cols , loc_input_format , loc_output_format , result_prefix_geo , ) if ( len ( lat_cols ) >= 1 ) & ( len ( gh_cols ) >= 1 ): logger . info ( f \"Transformation of Latitude and Longitude columns have been done. Transformation of Geohash columns will be skipped.\" ) elif len ( gh_cols ) >= 1 : df = geo_format_geohash ( df , gh_cols , loc_output_format , result_prefix_lat_lon ) if args . get ( \"geo_transformations\" ) . get ( \"centroid_calculation\" ): for idx , i in enumerate ( lat_cols ): df_ = centroid ( df , lat_cols [ idx ], long_cols [ idx ], id_col ) df = df . join ( df_ , id_col , \"inner\" ) if args . get ( \"geo_transformations\" ) . get ( \"rog_calculation\" ): for idx , i in enumerate ( lat_cols ): cols_drop = [ lat_cols [ idx ] + \"_centroid\" , long_cols [ idx ] + \"_centroid\" , ] df_ = rog_calculation ( df . drop ( * cols_drop ), lat_cols [ idx ], long_cols [ idx ], id_col , ) df = df . join ( df_ , id_col , \"inner\" ) if ( not auto_detection_analyzer_flag ) & ( not geo_transformations ): lat_cols , long_cols , gh_cols = [], [], [] continue if ( key == \"timeseries_analyzer\" ) & ( args is not None ): auto_detection_flag = args . get ( \"auto_detection\" , False ) id_col = args . get ( \"id_col\" , None ) tz_val = args . get ( \"tz_offset\" , None ) inspection_flag = args . get ( \"inspection\" , False ) analysis_level = args . get ( \"analysis_level\" , None ) max_days_limit = args . get ( \"max_days\" , None ) if auto_detection_flag : start = timeit . default_timer () df = ts_preprocess ( spark , df , id_col , output_path = report_input_path , tz_offset = tz_val , run_type = run_type , mlflow_config = mlflow_config , auth_key = auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , auto_detection: execution time (in secs) = { round ( end - start , 4 ) } \" ) if inspection_flag : start = timeit . default_timer () ts_analyzer ( spark , df , id_col , max_days = max_days_limit , output_path = report_input_path , output_type = analysis_level , tz_offset = tz_val , run_type = run_type , auth_key = auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , inspection: execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if ( ( key == \"anovos_basic_report\" ) & ( args is not None ) & args . get ( \"basic_report\" , False ) ): start = timeit . default_timer () anovos_basic_report ( spark , df , ** args . get ( \"report_args\" , {}), run_type = run_type , auth_key = auth_key , mlflow_config = mlflow_config , ) end = timeit . default_timer () logger . info ( f \"Basic Report: execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if not all_configs . get ( \"anovos_basic_report\" , {}) . get ( \"basic_report\" , False ): if ( key == \"stats_generator\" ) & ( args is not None ): for m in args [ \"metric\" ]: start = timeit . default_timer () print ( \" \\n \" + m + \": \\n \" ) f = getattr ( stats_generator , m ) df_stats = f ( spark , df , ** args [ \"metric_args\" ], print_impact = False ) if report_input_path : save_stats ( spark , df_stats , report_input_path , m , reread = True , run_type = run_type , auth_key = auth_key , mlflow_config = mlflow_config , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"data_analyzer/stats_generator/\" + m , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { m } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"quality_checker\" ) & ( args is not None ): for subkey , value in args . items (): if value is not None : start = timeit . default_timer () print ( \" \\n \" + subkey + \": \\n \" ) f = getattr ( quality_checker , subkey ) extra_args = stats_args ( all_configs , subkey ) if subkey == \"nullColumns_detection\" : if \"invalidEntries_detection\" in args . keys (): if args . get ( \"invalidEntries_detection\" ) . get ( \"treatment\" , None ): extra_args [ \"stats_missing\" ] = {} if \"outlier_detection\" in args . keys (): if args . get ( \"outlier_detection\" ) . get ( \"treatment\" , None ): if ( args . get ( \"outlier_detection\" ) . get ( \"treatment_method\" , None ) == \"null_replacement\" ): extra_args [ \"stats_missing\" ] = {} if subkey in [ \"outlier_detection\" , \"duplicate_detection\" ]: extra_args [ \"print_impact\" ] = True else : extra_args [ \"print_impact\" ] = False df , df_stats = f ( spark , df , ** value , ** extra_args ) df = save ( df , write_intermediate , folder_name = \"data_analyzer/quality_checker/\" + subkey + \"/dataset\" , reread = True , ) if report_input_path : df_stats = save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) else : df_stats = save ( df_stats , write_stats , folder_name = \"data_analyzer/quality_checker/\" + subkey , reread = True , ) if subkey != \"outlier_detection\" : df_stats . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"association_evaluator\" ) & ( args is not None ): for subkey , value in args . items (): if value is not None : start = timeit . default_timer () print ( \" \\n \" + subkey + \": \\n \" ) if subkey == \"correlation_matrix\" : f = getattr ( association_evaluator , subkey ) extra_args = stats_args ( all_configs , subkey ) cat_to_num_trans_params = all_configs . get ( \"cat_to_num_transformer\" , None ) df_trans_corr = transformers . cat_to_num_transformer ( spark , df , ** cat_to_num_trans_params ) df_stats = f ( spark , df_trans_corr , ** value , ** extra_args , print_impact = False , ) else : f = getattr ( association_evaluator , subkey ) extra_args = stats_args ( all_configs , subkey ) df_stats = f ( spark , df , ** value , ** extra_args , print_impact = False ) if report_input_path : save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"data_analyzer/association_evaluator/\" + subkey , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"drift_detector\" ) & ( args is not None ): for subkey , value in args . items (): if ( subkey == \"drift_statistics\" ) & ( value is not None ): start = timeit . default_timer () if not value [ \"configs\" ][ \"pre_existing_source\" ]: source = ETL ( value . get ( \"source_dataset\" )) else : source = None logger . info ( f \"running drift statistics detector using { value [ 'configs' ] } \" ) df_stats = ddetector . statistics ( spark , df , source , ** value [ \"configs\" ], print_impact = False , ) if report_input_path : save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"drift_detector/drift_statistics\" , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( subkey == \"stability_index\" ) & ( value is not None ): start = timeit . default_timer () idfs = [] for k in [ e for e in value . keys () if e not in ( \"configs\" )]: tmp = ETL ( value . get ( k )) idfs . append ( tmp ) df_stats = dstability . stability_index_computation ( spark , idfs , ** value [ \"configs\" ], print_impact = False ) if report_input_path : save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) appended_metric_path = value [ \"configs\" ] . get ( \"appended_metric_path\" , \"\" ) if appended_metric_path : df_metrics = data_ingest . read_dataset ( spark , file_path = appended_metric_path , file_type = \"csv\" , file_configs = { \"header\" : True , \"mode\" : \"overwrite\" , }, ) save_stats ( spark , df_metrics , report_input_path , \"stabilityIndex_metrics\" , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"drift_detector/stability_index\" , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) logger . info ( f \"execution time w/o report (in sec) = { round ( end - start_main , 4 ) } \" ) if ( key == \"transformers\" ) & ( args is not None ): for subkey , value in args . items (): if value is not None : for subkey2 , value2 in value . items (): if value2 is not None : start = timeit . default_timer () print ( \" \\n \" + subkey2 + \": \\n \" ) f = getattr ( transformers , subkey2 ) extra_args = stats_args ( all_configs , subkey2 ) if subkey2 in ( \"imputation_sklearn\" , \"autoencoder_latentFeatures\" , \"auto_imputation\" , \"PCA_latentFeatures\" , ): extra_args [ \"run_type\" ] = run_type extra_args [ \"auth_key\" ] = auth_key if subkey2 == \"cat_to_num_supervised\" : if ( \"model_path\" not in value2 . keys () and default_root_path ): extra_args [ \"model_path\" ] = ( default_root_path + \"/intermediate_model\" ) if subkey2 in ( \"normalization\" , \"feature_transformation\" , \"boxcox_transformation\" , \"expression_parser\" , ): df_transformed = f ( df , ** value2 , ** extra_args , print_impact = True , ) elif subkey2 in \"imputation_sklearn\" : df_transformed = f ( spark , df , ** value2 , ** extra_args , print_impact = False , ) else : df_transformed = f ( spark , df , ** value2 , ** extra_args , print_impact = True , ) df = save ( df_transformed , write_intermediate , folder_name = \"data_transformer/transformers/\" + subkey2 , reread = True , ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey2 } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"report_preprocessing\" ) & ( args is not None ): for subkey , value in args . items (): if ( subkey == \"charts_to_objects\" ) & ( value is not None ): start = timeit . default_timer () f = getattr ( report_preprocessing , subkey ) extra_args = stats_args ( all_configs , subkey ) f ( spark , df , ** value , ** extra_args , master_path = report_input_path , run_type = run_type , auth_key = auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"report_generation\" ) & ( args is not None ): start = timeit . default_timer () timeseries_analyzer = all_configs . get ( \"timeseries_analyzer\" , None ) if timeseries_analyzer : analysis_level = timeseries_analyzer . get ( \"analysis_level\" , None ) else : analysis_level = None geospatial_controller = all_configs . get ( \"geospatial_controller\" , None ) if not geospatial_controller : lat_cols , long_cols , gh_cols = [], [], [] max_analysis_records , top_geo_records = None , None anovos_report ( ** args , run_type = run_type , output_type = analysis_level , lat_cols = lat_cols , long_cols = long_cols , gh_cols = gh_cols , max_records = max_analysis_records , top_geo_records = top_geo_records , auth_key = auth_key , mlflow_config = mlflow_config , ) end = timeit . default_timer () logger . info ( f \" { key } , full_report: execution time (in secs) = { round ( end - start , 4 ) } \" ) if write_feast_features is not None : file_source_config = write_feast_features [ \"file_source\" ] df = feast_exporter . add_timestamp_columns ( df , file_source_config ) save ( df , write_main , folder_name = \"final_dataset\" , reread = False ) if write_feast_features is not None : if \"file_path\" not in write_feast_features : raise ValueError ( \"File path missing for saving feature_store feature descriptions\" ) else : path = os . path . join ( write_main [ \"file_path\" ], \"final_dataset\" , \"part*\" ) filename = glob . glob ( path )[ 0 ] feast_exporter . generate_feature_description ( df . dtypes , write_feast_features , filename ) def run ( config_path, run_type, auth_key_val={}) Expand source code def run ( config_path , run_type , auth_key_val = {}): if run_type in ( \"local\" , \"databricks\" , \"ak8s\" ): config_file = config_path elif run_type == \"emr\" : bash_cmd = \"aws s3 cp \" + config_path + \" config.yaml\" _ = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) config_file = \"config.yaml\" else : raise ValueError ( \"Invalid run_type\" ) if run_type == \"ak8s\" and auth_key_val == {}: raise ValueError ( \"Invalid auth key for run_type\" ) with open ( config_file , \"r\" ) as f : all_configs = yaml . load ( f , yaml . SafeLoader ) main ( all_configs , run_type , auth_key_val ) def save ( data, write_configs, folder_name, reread=False) Expand source code def save ( data , write_configs , folder_name , reread = False ): if write_configs : if \"file_path\" not in write_configs : raise TypeError ( \"file path missing for writing data\" ) write = copy . deepcopy ( write_configs ) run_id = write . pop ( \"mlflow_run_id\" , \"\" ) log_mlflow = write . pop ( \"log_mlflow\" , False ) write [ \"file_path\" ] = write [ \"file_path\" ] + \"/\" + folder_name + \"/\" + str ( run_id ) data_ingest . write_dataset ( data , ** write ) if log_mlflow : mlflow . log_artifacts ( local_dir = write [ \"file_path\" ], artifact_path = folder_name ) if reread : read = copy . deepcopy ( write ) if \"file_configs\" in read : read [ \"file_configs\" ] . pop ( \"repartition\" , None ) read [ \"file_configs\" ] . pop ( \"mode\" , None ) data = data_ingest . read_dataset ( spark , ** read ) return data def stats_args ( all_configs, func) Expand source code def stats_args ( all_configs , func ): stats_configs = all_configs . get ( \"stats_generator\" , None ) write_configs = all_configs . get ( \"write_stats\" , None ) report_input_path = \"\" report_configs = all_configs . get ( \"report_preprocessing\" , None ) if report_configs is not None : if \"master_path\" not in report_configs : raise TypeError ( \"Master path missing for saving report statistics\" ) else : report_input_path = report_configs . get ( \"master_path\" ) result = {} if stats_configs : mainfunc_to_args = { \"biasedness_detection\" : [ \"stats_mode\" ], \"IDness_detection\" : [ \"stats_unique\" ], \"nullColumns_detection\" : [ \"stats_unique\" , \"stats_mode\" , \"stats_missing\" ], \"variable_clustering\" : [ \"stats_mode\" ], \"charts_to_objects\" : [ \"stats_unique\" ], \"cat_to_num_unsupervised\" : [ \"stats_unique\" ], \"PCA_latentFeatures\" : [ \"stats_missing\" ], \"autoencoder_latentFeatures\" : [ \"stats_missing\" ], } args_to_statsfunc = { \"stats_unique\" : \"measures_of_cardinality\" , \"stats_mode\" : \"measures_of_centralTendency\" , \"stats_missing\" : \"measures_of_counts\" , } for arg in mainfunc_to_args . get ( func , []): if not report_input_path : if write_configs : read = copy . deepcopy ( write_configs ) if \"file_configs\" in read : read [ \"file_configs\" ] . pop ( \"repartition\" , None ) read [ \"file_configs\" ] . pop ( \"mode\" , None ) if read [ \"file_type\" ] == \"csv\" : read [ \"file_configs\" ][ \"inferSchema\" ] = True read [ \"file_path\" ] = ( read [ \"file_path\" ] + \"/data_analyzer/stats_generator/\" + args_to_statsfunc [ arg ] ) result [ arg ] = read else : result [ arg ] = { \"file_path\" : ( report_input_path + \"/\" + args_to_statsfunc [ arg ] + \".csv\" ), \"file_type\" : \"csv\" , \"file_configs\" : { \"header\" : True , \"inferSchema\" : True }, } return result","title":"<code>workflow</code>"},{"location":"api/workflow.html#workflow","text":"Expand source code import contextlib import copy import glob import os import subprocess import timeit import mlflow import yaml from loguru import logger from anovos.data_analyzer import association_evaluator , quality_checker , stats_generator from anovos.data_analyzer.geospatial_analyzer import geospatial_autodetection from anovos.data_analyzer.ts_analyzer import ts_analyzer from anovos.data_ingest import data_ingest from anovos.data_ingest.ts_auto_detection import ts_preprocess from anovos.data_report import report_preprocessing from anovos.data_report.basic_report_generation import anovos_basic_report from anovos.data_report.report_generation import anovos_report from anovos.data_report.report_preprocessing import save_stats from anovos.data_transformer import transformers from anovos.data_transformer.geospatial import ( centroid , geo_format_geohash , geo_format_latlon , location_in_country , rog_calculation , ) from anovos.drift_stability import drift_detector as ddetector from anovos.drift_stability import stability as dstability from anovos.feature_store import feast_exporter from anovos.shared.spark import spark mapbox_list = [ \"open-street-map\" , \"white-bg\" , \"carto-positron\" , \"carto-darkmatter\" , \"stamen-terrain\" , \"stamen-toner\" , \"stamen-watercolor\" , ] def ETL ( args ): f = getattr ( data_ingest , \"read_dataset\" ) read_args = args . get ( \"read_dataset\" , None ) if read_args : df = f ( spark , ** read_args ) else : raise TypeError ( \"Invalid input for reading dataset\" ) for key , value in args . items (): if key != \"read_dataset\" : if value is not None : f = getattr ( data_ingest , key ) if isinstance ( value , dict ): df = f ( df , ** value ) else : df = f ( df , value ) return df def save ( data , write_configs , folder_name , reread = False ): if write_configs : if \"file_path\" not in write_configs : raise TypeError ( \"file path missing for writing data\" ) write = copy . deepcopy ( write_configs ) run_id = write . pop ( \"mlflow_run_id\" , \"\" ) log_mlflow = write . pop ( \"log_mlflow\" , False ) write [ \"file_path\" ] = write [ \"file_path\" ] + \"/\" + folder_name + \"/\" + str ( run_id ) data_ingest . write_dataset ( data , ** write ) if log_mlflow : mlflow . log_artifacts ( local_dir = write [ \"file_path\" ], artifact_path = folder_name ) if reread : read = copy . deepcopy ( write ) if \"file_configs\" in read : read [ \"file_configs\" ] . pop ( \"repartition\" , None ) read [ \"file_configs\" ] . pop ( \"mode\" , None ) data = data_ingest . read_dataset ( spark , ** read ) return data def stats_args ( all_configs , func ): stats_configs = all_configs . get ( \"stats_generator\" , None ) write_configs = all_configs . get ( \"write_stats\" , None ) report_input_path = \"\" report_configs = all_configs . get ( \"report_preprocessing\" , None ) if report_configs is not None : if \"master_path\" not in report_configs : raise TypeError ( \"Master path missing for saving report statistics\" ) else : report_input_path = report_configs . get ( \"master_path\" ) result = {} if stats_configs : mainfunc_to_args = { \"biasedness_detection\" : [ \"stats_mode\" ], \"IDness_detection\" : [ \"stats_unique\" ], \"nullColumns_detection\" : [ \"stats_unique\" , \"stats_mode\" , \"stats_missing\" ], \"variable_clustering\" : [ \"stats_mode\" ], \"charts_to_objects\" : [ \"stats_unique\" ], \"cat_to_num_unsupervised\" : [ \"stats_unique\" ], \"PCA_latentFeatures\" : [ \"stats_missing\" ], \"autoencoder_latentFeatures\" : [ \"stats_missing\" ], } args_to_statsfunc = { \"stats_unique\" : \"measures_of_cardinality\" , \"stats_mode\" : \"measures_of_centralTendency\" , \"stats_missing\" : \"measures_of_counts\" , } for arg in mainfunc_to_args . get ( func , []): if not report_input_path : if write_configs : read = copy . deepcopy ( write_configs ) if \"file_configs\" in read : read [ \"file_configs\" ] . pop ( \"repartition\" , None ) read [ \"file_configs\" ] . pop ( \"mode\" , None ) if read [ \"file_type\" ] == \"csv\" : read [ \"file_configs\" ][ \"inferSchema\" ] = True read [ \"file_path\" ] = ( read [ \"file_path\" ] + \"/data_analyzer/stats_generator/\" + args_to_statsfunc [ arg ] ) result [ arg ] = read else : result [ arg ] = { \"file_path\" : ( report_input_path + \"/\" + args_to_statsfunc [ arg ] + \".csv\" ), \"file_type\" : \"csv\" , \"file_configs\" : { \"header\" : True , \"inferSchema\" : True }, } return result def main ( all_configs , run_type , auth_key_val = {}): if run_type == \"ak8s\" : conf = spark . sparkContext . _jsc . hadoopConfiguration () conf . set ( \"fs.wasbs.impl\" , \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\" ) # Set credentials using auth_key_val for key , value in auth_key_val . items (): spark . conf . set ( key , value ) auth_key = value else : auth_key = \"NA\" start_main = timeit . default_timer () df = ETL ( all_configs . get ( \"input_dataset\" )) write_main = all_configs . get ( \"write_main\" , None ) write_intermediate = all_configs . get ( \"write_intermediate\" , None ) write_stats = all_configs . get ( \"write_stats\" , None ) write_feast_features = all_configs . get ( \"write_feast_features\" , None ) if write_intermediate and run_type == \"ak8s\" : default_root_path = write_intermediate . get ( \"file_path\" , None ) else : default_root_path = None if write_feast_features is not None : repartition_count = ( write_main [ \"file_configs\" ][ \"repartition\" ] if \"file_configs\" in write_main and \"repartition\" in write_main [ \"file_configs\" ] else - 1 ) feast_exporter . check_feast_configuration ( write_feast_features , repartition_count ) mlflow_config = all_configs . get ( \"mlflow\" , None ) if mlflow_config is not None : mlflow . set_tracking_uri ( mlflow_config [ \"tracking_uri\" ]) mlflow . set_experiment ( mlflow_config [ \"experiment\" ]) mlflow_run = ( mlflow . start_run () if mlflow_config is not None else contextlib . nullcontext () ) with mlflow_run : if mlflow_config is not None : mlflow_config [ \"run_id\" ] = mlflow_run . info . run_id start_main = timeit . default_timer () df = ETL ( all_configs . get ( \"input_dataset\" )) write_main = all_configs . get ( \"write_main\" , None ) write_intermediate = all_configs . get ( \"write_intermediate\" , None ) write_stats = all_configs . get ( \"write_stats\" , None ) if mlflow_config : if write_main : write_main [ \"mlflow_run_id\" ] = mlflow_run . info . run_id write_main [ \"log_mlflow\" ] = mlflow_config [ \"track_output\" ] if write_intermediate : write_intermediate [ \"mlflow_run_id\" ] = mlflow_run . info . run_id write_intermediate [ \"log_mlflow\" ] = mlflow_config [ \"track_intermediates\" ] if write_stats : write_stats [ \"mlflow_run_id\" ] = mlflow_run . info . run_id write_stats [ \"log_mlflow\" ] = mlflow_config [ \"track_reports\" ] report_input_path = \"\" report_configs = all_configs . get ( \"report_preprocessing\" , None ) if report_configs is not None : if \"master_path\" not in report_configs : raise TypeError ( \"Master path missing for saving report statistics\" ) else : report_input_path = report_configs . get ( \"master_path\" ) for key , args in all_configs . items (): if ( key == \"concatenate_dataset\" ) & ( args is not None ): start = timeit . default_timer () idfs = [ df ] for k in [ e for e in args . keys () if e not in ( \"method\" )]: tmp = ETL ( args . get ( k )) idfs . append ( tmp ) df = data_ingest . concatenate_dataset ( * idfs , method_type = args . get ( \"method\" ) ) df = save ( df , write_intermediate , folder_name = \"data_ingest/concatenate_dataset\" , reread = True , ) end = timeit . default_timer () logger . info ( f \" { key } : execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if ( key == \"join_dataset\" ) & ( args is not None ): start = timeit . default_timer () idfs = [ df ] for k in [ e for e in args . keys () if e not in ( \"join_type\" , \"join_cols\" ) ]: tmp = ETL ( args . get ( k )) idfs . append ( tmp ) df = data_ingest . join_dataset ( * idfs , join_cols = args . get ( \"join_cols\" ), join_type = args . get ( \"join_type\" ), ) df = save ( df , write_intermediate , folder_name = \"data_ingest/join_dataset\" , reread = True , ) end = timeit . default_timer () logger . info ( f \" { key } : execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if ( key == \"geospatial_controller\" ) & ( args is not None ): start = timeit . default_timer () auto_detection_analyzer_flag = args . get ( \"geospatial_analyzer\" ) . get ( \"auto_detection_analyzer\" , False ) geo_transformations = args . get ( \"geo_transformations\" , False ) id_col = args . get ( \"geospatial_analyzer\" ) . get ( \"id_col\" , None ) max_analysis_records = args . get ( \"geospatial_analyzer\" ) . get ( \"max_analysis_records\" , None ) top_geo_records = args . get ( \"geospatial_analyzer\" ) . get ( \"top_geo_records\" , None ) max_cluster = args . get ( \"geospatial_analyzer\" ) . get ( \"max_cluster\" , None ) eps = args . get ( \"geospatial_analyzer\" ) . get ( \"eps\" , None ) min_samples = args . get ( \"geospatial_analyzer\" ) . get ( \"min_samples\" , None ) try : global_map_box_val = mapbox_list . index ( args . get ( \"geospatial_analyzer\" , None ) . get ( \"global_map_box_val\" , None ) ) except : global_map_box_val = 0 if auto_detection_analyzer_flag : start = timeit . default_timer () lat_cols , long_cols , gh_cols = geospatial_autodetection ( df , id_col , report_input_path , max_analysis_records , top_geo_records , max_cluster , eps , min_samples , global_map_box_val , run_type , auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , auto_detection_geospatial: execution time (in secs) = { round ( end - start , 4 ) } \" ) if geo_transformations : country_val = args . get ( \"geo_transformations\" ) . get ( \"country\" , None ) country_shapefile_path = args . get ( \"geo_transformations\" ) . get ( \"country_shapefile_path\" , None ) method_type = args . get ( \"geo_transformations\" ) . get ( \"method_type\" , None ) result_prefix = args . get ( \"geo_transformations\" ) . get ( \"result_prefix\" , None ) loc_input_format = args . get ( \"geo_transformations\" ) . get ( \"loc_input_format\" , None ) loc_output_format = args . get ( \"geo_transformations\" ) . get ( \"loc_output_format\" , None ) result_prefix_lat_lon = args . get ( \"geo_transformations\" ) . get ( \"result_prefix_lat_lon\" , None ) result_prefix_geo = args . get ( \"geo_transformations\" ) . get ( \"result_prefix_geo\" , None ) id_col = args . get ( \"geo_transformations\" ) . get ( \"id_col\" , None ) if (( lat_cols == []) & ( long_cols == [])) or ( gh_cols == []): lat_cols = args . get ( \"geo_transformations\" ) . get ( \"list_of_lat\" , None ) long_cols = args . get ( \"geo_transformations\" ) . get ( \"list_of_lon\" , None ) gh_cols = args . get ( \"geo_transformations\" ) . get ( \"list_of_geohash\" , None ) if args . get ( \"geo_transformations\" ) . get ( \"location_in_country_detection\" ): df = location_in_country ( spark , df , lat_cols , long_cols , country_val , country_shapefile_path , method_type , result_prefix_lat_lon , ) if args . get ( \"geo_transformations\" ) . get ( \"geo_format_conversion\" ): if len ( lat_cols ) >= 1 : df = geo_format_latlon ( df , lat_cols , long_cols , loc_input_format , loc_output_format , result_prefix_geo , ) if ( len ( lat_cols ) >= 1 ) & ( len ( gh_cols ) >= 1 ): logger . info ( f \"Transformation of Latitude and Longitude columns have been done. Transformation of Geohash columns will be skipped.\" ) elif len ( gh_cols ) >= 1 : df = geo_format_geohash ( df , gh_cols , loc_output_format , result_prefix_lat_lon ) if args . get ( \"geo_transformations\" ) . get ( \"centroid_calculation\" ): for idx , i in enumerate ( lat_cols ): df_ = centroid ( df , lat_cols [ idx ], long_cols [ idx ], id_col ) df = df . join ( df_ , id_col , \"inner\" ) if args . get ( \"geo_transformations\" ) . get ( \"rog_calculation\" ): for idx , i in enumerate ( lat_cols ): cols_drop = [ lat_cols [ idx ] + \"_centroid\" , long_cols [ idx ] + \"_centroid\" , ] df_ = rog_calculation ( df . drop ( * cols_drop ), lat_cols [ idx ], long_cols [ idx ], id_col , ) df = df . join ( df_ , id_col , \"inner\" ) if ( not auto_detection_analyzer_flag ) & ( not geo_transformations ): lat_cols , long_cols , gh_cols = [], [], [] continue if ( key == \"timeseries_analyzer\" ) & ( args is not None ): auto_detection_flag = args . get ( \"auto_detection\" , False ) id_col = args . get ( \"id_col\" , None ) tz_val = args . get ( \"tz_offset\" , None ) inspection_flag = args . get ( \"inspection\" , False ) analysis_level = args . get ( \"analysis_level\" , None ) max_days_limit = args . get ( \"max_days\" , None ) if auto_detection_flag : start = timeit . default_timer () df = ts_preprocess ( spark , df , id_col , output_path = report_input_path , tz_offset = tz_val , run_type = run_type , mlflow_config = mlflow_config , auth_key = auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , auto_detection: execution time (in secs) = { round ( end - start , 4 ) } \" ) if inspection_flag : start = timeit . default_timer () ts_analyzer ( spark , df , id_col , max_days = max_days_limit , output_path = report_input_path , output_type = analysis_level , tz_offset = tz_val , run_type = run_type , auth_key = auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , inspection: execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if ( ( key == \"anovos_basic_report\" ) & ( args is not None ) & args . get ( \"basic_report\" , False ) ): start = timeit . default_timer () anovos_basic_report ( spark , df , ** args . get ( \"report_args\" , {}), run_type = run_type , auth_key = auth_key , mlflow_config = mlflow_config , ) end = timeit . default_timer () logger . info ( f \"Basic Report: execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if not all_configs . get ( \"anovos_basic_report\" , {}) . get ( \"basic_report\" , False ): if ( key == \"stats_generator\" ) & ( args is not None ): for m in args [ \"metric\" ]: start = timeit . default_timer () print ( \" \\n \" + m + \": \\n \" ) f = getattr ( stats_generator , m ) df_stats = f ( spark , df , ** args [ \"metric_args\" ], print_impact = False ) if report_input_path : save_stats ( spark , df_stats , report_input_path , m , reread = True , run_type = run_type , auth_key = auth_key , mlflow_config = mlflow_config , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"data_analyzer/stats_generator/\" + m , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { m } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"quality_checker\" ) & ( args is not None ): for subkey , value in args . items (): if value is not None : start = timeit . default_timer () print ( \" \\n \" + subkey + \": \\n \" ) f = getattr ( quality_checker , subkey ) extra_args = stats_args ( all_configs , subkey ) if subkey == \"nullColumns_detection\" : if \"invalidEntries_detection\" in args . keys (): if args . get ( \"invalidEntries_detection\" ) . get ( \"treatment\" , None ): extra_args [ \"stats_missing\" ] = {} if \"outlier_detection\" in args . keys (): if args . get ( \"outlier_detection\" ) . get ( \"treatment\" , None ): if ( args . get ( \"outlier_detection\" ) . get ( \"treatment_method\" , None ) == \"null_replacement\" ): extra_args [ \"stats_missing\" ] = {} if subkey in [ \"outlier_detection\" , \"duplicate_detection\" ]: extra_args [ \"print_impact\" ] = True else : extra_args [ \"print_impact\" ] = False df , df_stats = f ( spark , df , ** value , ** extra_args ) df = save ( df , write_intermediate , folder_name = \"data_analyzer/quality_checker/\" + subkey + \"/dataset\" , reread = True , ) if report_input_path : df_stats = save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) else : df_stats = save ( df_stats , write_stats , folder_name = \"data_analyzer/quality_checker/\" + subkey , reread = True , ) if subkey != \"outlier_detection\" : df_stats . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"association_evaluator\" ) & ( args is not None ): for subkey , value in args . items (): if value is not None : start = timeit . default_timer () print ( \" \\n \" + subkey + \": \\n \" ) if subkey == \"correlation_matrix\" : f = getattr ( association_evaluator , subkey ) extra_args = stats_args ( all_configs , subkey ) cat_to_num_trans_params = all_configs . get ( \"cat_to_num_transformer\" , None ) df_trans_corr = transformers . cat_to_num_transformer ( spark , df , ** cat_to_num_trans_params ) df_stats = f ( spark , df_trans_corr , ** value , ** extra_args , print_impact = False , ) else : f = getattr ( association_evaluator , subkey ) extra_args = stats_args ( all_configs , subkey ) df_stats = f ( spark , df , ** value , ** extra_args , print_impact = False ) if report_input_path : save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"data_analyzer/association_evaluator/\" + subkey , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"drift_detector\" ) & ( args is not None ): for subkey , value in args . items (): if ( subkey == \"drift_statistics\" ) & ( value is not None ): start = timeit . default_timer () if not value [ \"configs\" ][ \"pre_existing_source\" ]: source = ETL ( value . get ( \"source_dataset\" )) else : source = None logger . info ( f \"running drift statistics detector using { value [ 'configs' ] } \" ) df_stats = ddetector . statistics ( spark , df , source , ** value [ \"configs\" ], print_impact = False , ) if report_input_path : save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"drift_detector/drift_statistics\" , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( subkey == \"stability_index\" ) & ( value is not None ): start = timeit . default_timer () idfs = [] for k in [ e for e in value . keys () if e not in ( \"configs\" )]: tmp = ETL ( value . get ( k )) idfs . append ( tmp ) df_stats = dstability . stability_index_computation ( spark , idfs , ** value [ \"configs\" ], print_impact = False ) if report_input_path : save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) appended_metric_path = value [ \"configs\" ] . get ( \"appended_metric_path\" , \"\" ) if appended_metric_path : df_metrics = data_ingest . read_dataset ( spark , file_path = appended_metric_path , file_type = \"csv\" , file_configs = { \"header\" : True , \"mode\" : \"overwrite\" , }, ) save_stats ( spark , df_metrics , report_input_path , \"stabilityIndex_metrics\" , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"drift_detector/stability_index\" , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) logger . info ( f \"execution time w/o report (in sec) = { round ( end - start_main , 4 ) } \" ) if ( key == \"transformers\" ) & ( args is not None ): for subkey , value in args . items (): if value is not None : for subkey2 , value2 in value . items (): if value2 is not None : start = timeit . default_timer () print ( \" \\n \" + subkey2 + \": \\n \" ) f = getattr ( transformers , subkey2 ) extra_args = stats_args ( all_configs , subkey2 ) if subkey2 in ( \"imputation_sklearn\" , \"autoencoder_latentFeatures\" , \"auto_imputation\" , \"PCA_latentFeatures\" , ): extra_args [ \"run_type\" ] = run_type extra_args [ \"auth_key\" ] = auth_key if subkey2 == \"cat_to_num_supervised\" : if ( \"model_path\" not in value2 . keys () and default_root_path ): extra_args [ \"model_path\" ] = ( default_root_path + \"/intermediate_model\" ) if subkey2 in ( \"normalization\" , \"feature_transformation\" , \"boxcox_transformation\" , \"expression_parser\" , ): df_transformed = f ( df , ** value2 , ** extra_args , print_impact = True , ) elif subkey2 in \"imputation_sklearn\" : df_transformed = f ( spark , df , ** value2 , ** extra_args , print_impact = False , ) else : df_transformed = f ( spark , df , ** value2 , ** extra_args , print_impact = True , ) df = save ( df_transformed , write_intermediate , folder_name = \"data_transformer/transformers/\" + subkey2 , reread = True , ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey2 } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"report_preprocessing\" ) & ( args is not None ): for subkey , value in args . items (): if ( subkey == \"charts_to_objects\" ) & ( value is not None ): start = timeit . default_timer () f = getattr ( report_preprocessing , subkey ) extra_args = stats_args ( all_configs , subkey ) f ( spark , df , ** value , ** extra_args , master_path = report_input_path , run_type = run_type , auth_key = auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"report_generation\" ) & ( args is not None ): start = timeit . default_timer () timeseries_analyzer = all_configs . get ( \"timeseries_analyzer\" , None ) if timeseries_analyzer : analysis_level = timeseries_analyzer . get ( \"analysis_level\" , None ) else : analysis_level = None geospatial_controller = all_configs . get ( \"geospatial_controller\" , None ) if not geospatial_controller : lat_cols , long_cols , gh_cols = [], [], [] max_analysis_records , top_geo_records = None , None anovos_report ( ** args , run_type = run_type , output_type = analysis_level , lat_cols = lat_cols , long_cols = long_cols , gh_cols = gh_cols , max_records = max_analysis_records , top_geo_records = top_geo_records , auth_key = auth_key , mlflow_config = mlflow_config , ) end = timeit . default_timer () logger . info ( f \" { key } , full_report: execution time (in secs) = { round ( end - start , 4 ) } \" ) if write_feast_features is not None : file_source_config = write_feast_features [ \"file_source\" ] df = feast_exporter . add_timestamp_columns ( df , file_source_config ) save ( df , write_main , folder_name = \"final_dataset\" , reread = False ) if write_feast_features is not None : if \"file_path\" not in write_feast_features : raise ValueError ( \"File path missing for saving feature_store feature descriptions\" ) else : path = os . path . join ( write_main [ \"file_path\" ], \"final_dataset\" , \"part*\" ) filename = glob . glob ( path )[ 0 ] feast_exporter . generate_feature_description ( df . dtypes , write_feast_features , filename ) def run ( config_path , run_type , auth_key_val = {}): if run_type in ( \"local\" , \"databricks\" , \"ak8s\" ): config_file = config_path elif run_type == \"emr\" : bash_cmd = \"aws s3 cp \" + config_path + \" config.yaml\" _ = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) config_file = \"config.yaml\" else : raise ValueError ( \"Invalid run_type\" ) if run_type == \"ak8s\" and auth_key_val == {}: raise ValueError ( \"Invalid auth key for run_type\" ) with open ( config_file , \"r\" ) as f : all_configs = yaml . load ( f , yaml . SafeLoader ) main ( all_configs , run_type , auth_key_val )","title":"workflow"},{"location":"api/workflow.html#functions","text":"def ETL ( args) Expand source code def ETL ( args ): f = getattr ( data_ingest , \"read_dataset\" ) read_args = args . get ( \"read_dataset\" , None ) if read_args : df = f ( spark , ** read_args ) else : raise TypeError ( \"Invalid input for reading dataset\" ) for key , value in args . items (): if key != \"read_dataset\" : if value is not None : f = getattr ( data_ingest , key ) if isinstance ( value , dict ): df = f ( df , ** value ) else : df = f ( df , value ) return df def main ( all_configs, run_type, auth_key_val={}) Expand source code def main ( all_configs , run_type , auth_key_val = {}): if run_type == \"ak8s\" : conf = spark . sparkContext . _jsc . hadoopConfiguration () conf . set ( \"fs.wasbs.impl\" , \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\" ) # Set credentials using auth_key_val for key , value in auth_key_val . items (): spark . conf . set ( key , value ) auth_key = value else : auth_key = \"NA\" start_main = timeit . default_timer () df = ETL ( all_configs . get ( \"input_dataset\" )) write_main = all_configs . get ( \"write_main\" , None ) write_intermediate = all_configs . get ( \"write_intermediate\" , None ) write_stats = all_configs . get ( \"write_stats\" , None ) write_feast_features = all_configs . get ( \"write_feast_features\" , None ) if write_intermediate and run_type == \"ak8s\" : default_root_path = write_intermediate . get ( \"file_path\" , None ) else : default_root_path = None if write_feast_features is not None : repartition_count = ( write_main [ \"file_configs\" ][ \"repartition\" ] if \"file_configs\" in write_main and \"repartition\" in write_main [ \"file_configs\" ] else - 1 ) feast_exporter . check_feast_configuration ( write_feast_features , repartition_count ) mlflow_config = all_configs . get ( \"mlflow\" , None ) if mlflow_config is not None : mlflow . set_tracking_uri ( mlflow_config [ \"tracking_uri\" ]) mlflow . set_experiment ( mlflow_config [ \"experiment\" ]) mlflow_run = ( mlflow . start_run () if mlflow_config is not None else contextlib . nullcontext () ) with mlflow_run : if mlflow_config is not None : mlflow_config [ \"run_id\" ] = mlflow_run . info . run_id start_main = timeit . default_timer () df = ETL ( all_configs . get ( \"input_dataset\" )) write_main = all_configs . get ( \"write_main\" , None ) write_intermediate = all_configs . get ( \"write_intermediate\" , None ) write_stats = all_configs . get ( \"write_stats\" , None ) if mlflow_config : if write_main : write_main [ \"mlflow_run_id\" ] = mlflow_run . info . run_id write_main [ \"log_mlflow\" ] = mlflow_config [ \"track_output\" ] if write_intermediate : write_intermediate [ \"mlflow_run_id\" ] = mlflow_run . info . run_id write_intermediate [ \"log_mlflow\" ] = mlflow_config [ \"track_intermediates\" ] if write_stats : write_stats [ \"mlflow_run_id\" ] = mlflow_run . info . run_id write_stats [ \"log_mlflow\" ] = mlflow_config [ \"track_reports\" ] report_input_path = \"\" report_configs = all_configs . get ( \"report_preprocessing\" , None ) if report_configs is not None : if \"master_path\" not in report_configs : raise TypeError ( \"Master path missing for saving report statistics\" ) else : report_input_path = report_configs . get ( \"master_path\" ) for key , args in all_configs . items (): if ( key == \"concatenate_dataset\" ) & ( args is not None ): start = timeit . default_timer () idfs = [ df ] for k in [ e for e in args . keys () if e not in ( \"method\" )]: tmp = ETL ( args . get ( k )) idfs . append ( tmp ) df = data_ingest . concatenate_dataset ( * idfs , method_type = args . get ( \"method\" ) ) df = save ( df , write_intermediate , folder_name = \"data_ingest/concatenate_dataset\" , reread = True , ) end = timeit . default_timer () logger . info ( f \" { key } : execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if ( key == \"join_dataset\" ) & ( args is not None ): start = timeit . default_timer () idfs = [ df ] for k in [ e for e in args . keys () if e not in ( \"join_type\" , \"join_cols\" ) ]: tmp = ETL ( args . get ( k )) idfs . append ( tmp ) df = data_ingest . join_dataset ( * idfs , join_cols = args . get ( \"join_cols\" ), join_type = args . get ( \"join_type\" ), ) df = save ( df , write_intermediate , folder_name = \"data_ingest/join_dataset\" , reread = True , ) end = timeit . default_timer () logger . info ( f \" { key } : execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if ( key == \"geospatial_controller\" ) & ( args is not None ): start = timeit . default_timer () auto_detection_analyzer_flag = args . get ( \"geospatial_analyzer\" ) . get ( \"auto_detection_analyzer\" , False ) geo_transformations = args . get ( \"geo_transformations\" , False ) id_col = args . get ( \"geospatial_analyzer\" ) . get ( \"id_col\" , None ) max_analysis_records = args . get ( \"geospatial_analyzer\" ) . get ( \"max_analysis_records\" , None ) top_geo_records = args . get ( \"geospatial_analyzer\" ) . get ( \"top_geo_records\" , None ) max_cluster = args . get ( \"geospatial_analyzer\" ) . get ( \"max_cluster\" , None ) eps = args . get ( \"geospatial_analyzer\" ) . get ( \"eps\" , None ) min_samples = args . get ( \"geospatial_analyzer\" ) . get ( \"min_samples\" , None ) try : global_map_box_val = mapbox_list . index ( args . get ( \"geospatial_analyzer\" , None ) . get ( \"global_map_box_val\" , None ) ) except : global_map_box_val = 0 if auto_detection_analyzer_flag : start = timeit . default_timer () lat_cols , long_cols , gh_cols = geospatial_autodetection ( df , id_col , report_input_path , max_analysis_records , top_geo_records , max_cluster , eps , min_samples , global_map_box_val , run_type , auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , auto_detection_geospatial: execution time (in secs) = { round ( end - start , 4 ) } \" ) if geo_transformations : country_val = args . get ( \"geo_transformations\" ) . get ( \"country\" , None ) country_shapefile_path = args . get ( \"geo_transformations\" ) . get ( \"country_shapefile_path\" , None ) method_type = args . get ( \"geo_transformations\" ) . get ( \"method_type\" , None ) result_prefix = args . get ( \"geo_transformations\" ) . get ( \"result_prefix\" , None ) loc_input_format = args . get ( \"geo_transformations\" ) . get ( \"loc_input_format\" , None ) loc_output_format = args . get ( \"geo_transformations\" ) . get ( \"loc_output_format\" , None ) result_prefix_lat_lon = args . get ( \"geo_transformations\" ) . get ( \"result_prefix_lat_lon\" , None ) result_prefix_geo = args . get ( \"geo_transformations\" ) . get ( \"result_prefix_geo\" , None ) id_col = args . get ( \"geo_transformations\" ) . get ( \"id_col\" , None ) if (( lat_cols == []) & ( long_cols == [])) or ( gh_cols == []): lat_cols = args . get ( \"geo_transformations\" ) . get ( \"list_of_lat\" , None ) long_cols = args . get ( \"geo_transformations\" ) . get ( \"list_of_lon\" , None ) gh_cols = args . get ( \"geo_transformations\" ) . get ( \"list_of_geohash\" , None ) if args . get ( \"geo_transformations\" ) . get ( \"location_in_country_detection\" ): df = location_in_country ( spark , df , lat_cols , long_cols , country_val , country_shapefile_path , method_type , result_prefix_lat_lon , ) if args . get ( \"geo_transformations\" ) . get ( \"geo_format_conversion\" ): if len ( lat_cols ) >= 1 : df = geo_format_latlon ( df , lat_cols , long_cols , loc_input_format , loc_output_format , result_prefix_geo , ) if ( len ( lat_cols ) >= 1 ) & ( len ( gh_cols ) >= 1 ): logger . info ( f \"Transformation of Latitude and Longitude columns have been done. Transformation of Geohash columns will be skipped.\" ) elif len ( gh_cols ) >= 1 : df = geo_format_geohash ( df , gh_cols , loc_output_format , result_prefix_lat_lon ) if args . get ( \"geo_transformations\" ) . get ( \"centroid_calculation\" ): for idx , i in enumerate ( lat_cols ): df_ = centroid ( df , lat_cols [ idx ], long_cols [ idx ], id_col ) df = df . join ( df_ , id_col , \"inner\" ) if args . get ( \"geo_transformations\" ) . get ( \"rog_calculation\" ): for idx , i in enumerate ( lat_cols ): cols_drop = [ lat_cols [ idx ] + \"_centroid\" , long_cols [ idx ] + \"_centroid\" , ] df_ = rog_calculation ( df . drop ( * cols_drop ), lat_cols [ idx ], long_cols [ idx ], id_col , ) df = df . join ( df_ , id_col , \"inner\" ) if ( not auto_detection_analyzer_flag ) & ( not geo_transformations ): lat_cols , long_cols , gh_cols = [], [], [] continue if ( key == \"timeseries_analyzer\" ) & ( args is not None ): auto_detection_flag = args . get ( \"auto_detection\" , False ) id_col = args . get ( \"id_col\" , None ) tz_val = args . get ( \"tz_offset\" , None ) inspection_flag = args . get ( \"inspection\" , False ) analysis_level = args . get ( \"analysis_level\" , None ) max_days_limit = args . get ( \"max_days\" , None ) if auto_detection_flag : start = timeit . default_timer () df = ts_preprocess ( spark , df , id_col , output_path = report_input_path , tz_offset = tz_val , run_type = run_type , mlflow_config = mlflow_config , auth_key = auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , auto_detection: execution time (in secs) = { round ( end - start , 4 ) } \" ) if inspection_flag : start = timeit . default_timer () ts_analyzer ( spark , df , id_col , max_days = max_days_limit , output_path = report_input_path , output_type = analysis_level , tz_offset = tz_val , run_type = run_type , auth_key = auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , inspection: execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if ( ( key == \"anovos_basic_report\" ) & ( args is not None ) & args . get ( \"basic_report\" , False ) ): start = timeit . default_timer () anovos_basic_report ( spark , df , ** args . get ( \"report_args\" , {}), run_type = run_type , auth_key = auth_key , mlflow_config = mlflow_config , ) end = timeit . default_timer () logger . info ( f \"Basic Report: execution time (in secs) = { round ( end - start , 4 ) } \" ) continue if not all_configs . get ( \"anovos_basic_report\" , {}) . get ( \"basic_report\" , False ): if ( key == \"stats_generator\" ) & ( args is not None ): for m in args [ \"metric\" ]: start = timeit . default_timer () print ( \" \\n \" + m + \": \\n \" ) f = getattr ( stats_generator , m ) df_stats = f ( spark , df , ** args [ \"metric_args\" ], print_impact = False ) if report_input_path : save_stats ( spark , df_stats , report_input_path , m , reread = True , run_type = run_type , auth_key = auth_key , mlflow_config = mlflow_config , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"data_analyzer/stats_generator/\" + m , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { m } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"quality_checker\" ) & ( args is not None ): for subkey , value in args . items (): if value is not None : start = timeit . default_timer () print ( \" \\n \" + subkey + \": \\n \" ) f = getattr ( quality_checker , subkey ) extra_args = stats_args ( all_configs , subkey ) if subkey == \"nullColumns_detection\" : if \"invalidEntries_detection\" in args . keys (): if args . get ( \"invalidEntries_detection\" ) . get ( \"treatment\" , None ): extra_args [ \"stats_missing\" ] = {} if \"outlier_detection\" in args . keys (): if args . get ( \"outlier_detection\" ) . get ( \"treatment\" , None ): if ( args . get ( \"outlier_detection\" ) . get ( \"treatment_method\" , None ) == \"null_replacement\" ): extra_args [ \"stats_missing\" ] = {} if subkey in [ \"outlier_detection\" , \"duplicate_detection\" ]: extra_args [ \"print_impact\" ] = True else : extra_args [ \"print_impact\" ] = False df , df_stats = f ( spark , df , ** value , ** extra_args ) df = save ( df , write_intermediate , folder_name = \"data_analyzer/quality_checker/\" + subkey + \"/dataset\" , reread = True , ) if report_input_path : df_stats = save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) else : df_stats = save ( df_stats , write_stats , folder_name = \"data_analyzer/quality_checker/\" + subkey , reread = True , ) if subkey != \"outlier_detection\" : df_stats . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"association_evaluator\" ) & ( args is not None ): for subkey , value in args . items (): if value is not None : start = timeit . default_timer () print ( \" \\n \" + subkey + \": \\n \" ) if subkey == \"correlation_matrix\" : f = getattr ( association_evaluator , subkey ) extra_args = stats_args ( all_configs , subkey ) cat_to_num_trans_params = all_configs . get ( \"cat_to_num_transformer\" , None ) df_trans_corr = transformers . cat_to_num_transformer ( spark , df , ** cat_to_num_trans_params ) df_stats = f ( spark , df_trans_corr , ** value , ** extra_args , print_impact = False , ) else : f = getattr ( association_evaluator , subkey ) extra_args = stats_args ( all_configs , subkey ) df_stats = f ( spark , df , ** value , ** extra_args , print_impact = False ) if report_input_path : save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"data_analyzer/association_evaluator/\" + subkey , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"drift_detector\" ) & ( args is not None ): for subkey , value in args . items (): if ( subkey == \"drift_statistics\" ) & ( value is not None ): start = timeit . default_timer () if not value [ \"configs\" ][ \"pre_existing_source\" ]: source = ETL ( value . get ( \"source_dataset\" )) else : source = None logger . info ( f \"running drift statistics detector using { value [ 'configs' ] } \" ) df_stats = ddetector . statistics ( spark , df , source , ** value [ \"configs\" ], print_impact = False , ) if report_input_path : save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"drift_detector/drift_statistics\" , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( subkey == \"stability_index\" ) & ( value is not None ): start = timeit . default_timer () idfs = [] for k in [ e for e in value . keys () if e not in ( \"configs\" )]: tmp = ETL ( value . get ( k )) idfs . append ( tmp ) df_stats = dstability . stability_index_computation ( spark , idfs , ** value [ \"configs\" ], print_impact = False ) if report_input_path : save_stats ( spark , df_stats , report_input_path , subkey , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) appended_metric_path = value [ \"configs\" ] . get ( \"appended_metric_path\" , \"\" ) if appended_metric_path : df_metrics = data_ingest . read_dataset ( spark , file_path = appended_metric_path , file_type = \"csv\" , file_configs = { \"header\" : True , \"mode\" : \"overwrite\" , }, ) save_stats ( spark , df_metrics , report_input_path , \"stabilityIndex_metrics\" , reread = True , run_type = run_type , auth_key = auth_key , ) . show ( 100 ) else : save ( df_stats , write_stats , folder_name = \"drift_detector/stability_index\" , reread = True , ) . show ( 100 ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) logger . info ( f \"execution time w/o report (in sec) = { round ( end - start_main , 4 ) } \" ) if ( key == \"transformers\" ) & ( args is not None ): for subkey , value in args . items (): if value is not None : for subkey2 , value2 in value . items (): if value2 is not None : start = timeit . default_timer () print ( \" \\n \" + subkey2 + \": \\n \" ) f = getattr ( transformers , subkey2 ) extra_args = stats_args ( all_configs , subkey2 ) if subkey2 in ( \"imputation_sklearn\" , \"autoencoder_latentFeatures\" , \"auto_imputation\" , \"PCA_latentFeatures\" , ): extra_args [ \"run_type\" ] = run_type extra_args [ \"auth_key\" ] = auth_key if subkey2 == \"cat_to_num_supervised\" : if ( \"model_path\" not in value2 . keys () and default_root_path ): extra_args [ \"model_path\" ] = ( default_root_path + \"/intermediate_model\" ) if subkey2 in ( \"normalization\" , \"feature_transformation\" , \"boxcox_transformation\" , \"expression_parser\" , ): df_transformed = f ( df , ** value2 , ** extra_args , print_impact = True , ) elif subkey2 in \"imputation_sklearn\" : df_transformed = f ( spark , df , ** value2 , ** extra_args , print_impact = False , ) else : df_transformed = f ( spark , df , ** value2 , ** extra_args , print_impact = True , ) df = save ( df_transformed , write_intermediate , folder_name = \"data_transformer/transformers/\" + subkey2 , reread = True , ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey2 } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"report_preprocessing\" ) & ( args is not None ): for subkey , value in args . items (): if ( subkey == \"charts_to_objects\" ) & ( value is not None ): start = timeit . default_timer () f = getattr ( report_preprocessing , subkey ) extra_args = stats_args ( all_configs , subkey ) f ( spark , df , ** value , ** extra_args , master_path = report_input_path , run_type = run_type , auth_key = auth_key , ) end = timeit . default_timer () logger . info ( f \" { key } , { subkey } : execution time (in secs) = { round ( end - start , 4 ) } \" ) if ( key == \"report_generation\" ) & ( args is not None ): start = timeit . default_timer () timeseries_analyzer = all_configs . get ( \"timeseries_analyzer\" , None ) if timeseries_analyzer : analysis_level = timeseries_analyzer . get ( \"analysis_level\" , None ) else : analysis_level = None geospatial_controller = all_configs . get ( \"geospatial_controller\" , None ) if not geospatial_controller : lat_cols , long_cols , gh_cols = [], [], [] max_analysis_records , top_geo_records = None , None anovos_report ( ** args , run_type = run_type , output_type = analysis_level , lat_cols = lat_cols , long_cols = long_cols , gh_cols = gh_cols , max_records = max_analysis_records , top_geo_records = top_geo_records , auth_key = auth_key , mlflow_config = mlflow_config , ) end = timeit . default_timer () logger . info ( f \" { key } , full_report: execution time (in secs) = { round ( end - start , 4 ) } \" ) if write_feast_features is not None : file_source_config = write_feast_features [ \"file_source\" ] df = feast_exporter . add_timestamp_columns ( df , file_source_config ) save ( df , write_main , folder_name = \"final_dataset\" , reread = False ) if write_feast_features is not None : if \"file_path\" not in write_feast_features : raise ValueError ( \"File path missing for saving feature_store feature descriptions\" ) else : path = os . path . join ( write_main [ \"file_path\" ], \"final_dataset\" , \"part*\" ) filename = glob . glob ( path )[ 0 ] feast_exporter . generate_feature_description ( df . dtypes , write_feast_features , filename ) def run ( config_path, run_type, auth_key_val={}) Expand source code def run ( config_path , run_type , auth_key_val = {}): if run_type in ( \"local\" , \"databricks\" , \"ak8s\" ): config_file = config_path elif run_type == \"emr\" : bash_cmd = \"aws s3 cp \" + config_path + \" config.yaml\" _ = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) config_file = \"config.yaml\" else : raise ValueError ( \"Invalid run_type\" ) if run_type == \"ak8s\" and auth_key_val == {}: raise ValueError ( \"Invalid auth key for run_type\" ) with open ( config_file , \"r\" ) as f : all_configs = yaml . load ( f , yaml . SafeLoader ) main ( all_configs , run_type , auth_key_val ) def save ( data, write_configs, folder_name, reread=False) Expand source code def save ( data , write_configs , folder_name , reread = False ): if write_configs : if \"file_path\" not in write_configs : raise TypeError ( \"file path missing for writing data\" ) write = copy . deepcopy ( write_configs ) run_id = write . pop ( \"mlflow_run_id\" , \"\" ) log_mlflow = write . pop ( \"log_mlflow\" , False ) write [ \"file_path\" ] = write [ \"file_path\" ] + \"/\" + folder_name + \"/\" + str ( run_id ) data_ingest . write_dataset ( data , ** write ) if log_mlflow : mlflow . log_artifacts ( local_dir = write [ \"file_path\" ], artifact_path = folder_name ) if reread : read = copy . deepcopy ( write ) if \"file_configs\" in read : read [ \"file_configs\" ] . pop ( \"repartition\" , None ) read [ \"file_configs\" ] . pop ( \"mode\" , None ) data = data_ingest . read_dataset ( spark , ** read ) return data def stats_args ( all_configs, func) Expand source code def stats_args ( all_configs , func ): stats_configs = all_configs . get ( \"stats_generator\" , None ) write_configs = all_configs . get ( \"write_stats\" , None ) report_input_path = \"\" report_configs = all_configs . get ( \"report_preprocessing\" , None ) if report_configs is not None : if \"master_path\" not in report_configs : raise TypeError ( \"Master path missing for saving report statistics\" ) else : report_input_path = report_configs . get ( \"master_path\" ) result = {} if stats_configs : mainfunc_to_args = { \"biasedness_detection\" : [ \"stats_mode\" ], \"IDness_detection\" : [ \"stats_unique\" ], \"nullColumns_detection\" : [ \"stats_unique\" , \"stats_mode\" , \"stats_missing\" ], \"variable_clustering\" : [ \"stats_mode\" ], \"charts_to_objects\" : [ \"stats_unique\" ], \"cat_to_num_unsupervised\" : [ \"stats_unique\" ], \"PCA_latentFeatures\" : [ \"stats_missing\" ], \"autoencoder_latentFeatures\" : [ \"stats_missing\" ], } args_to_statsfunc = { \"stats_unique\" : \"measures_of_cardinality\" , \"stats_mode\" : \"measures_of_centralTendency\" , \"stats_missing\" : \"measures_of_counts\" , } for arg in mainfunc_to_args . get ( func , []): if not report_input_path : if write_configs : read = copy . deepcopy ( write_configs ) if \"file_configs\" in read : read [ \"file_configs\" ] . pop ( \"repartition\" , None ) read [ \"file_configs\" ] . pop ( \"mode\" , None ) if read [ \"file_type\" ] == \"csv\" : read [ \"file_configs\" ][ \"inferSchema\" ] = True read [ \"file_path\" ] = ( read [ \"file_path\" ] + \"/data_analyzer/stats_generator/\" + args_to_statsfunc [ arg ] ) result [ arg ] = read else : result [ arg ] = { \"file_path\" : ( report_input_path + \"/\" + args_to_statsfunc [ arg ] + \".csv\" ), \"file_type\" : \"csv\" , \"file_configs\" : { \"header\" : True , \"inferSchema\" : True }, } return result","title":"Functions"},{"location":"api/data_analyzer/_index.html","text":"Overview Sub-modules anovos.data_analyzer.association_eval_varclus anovos.data_analyzer.association_evaluator This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target \u2026 anovos.data_analyzer.geospatial_analyzer This module help to analyze & summarize the geospatial related data fields which are identified through the auto-detection module. Additionally, it \u2026 anovos.data_analyzer.quality_checker This submodule focuses on assessing the data quality at both row-level and column-level and also provides an appropriate treatment option to fix \u2026 anovos.data_analyzer.stats_generator This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are split into different metric types, and \u2026 anovos.data_analyzer.ts_analyzer This module generates the intermediate output specific to the inspection of Time series analysis \u2026","title":"Overview"},{"location":"api/data_analyzer/_index.html#overview","text":"","title":"Overview"},{"location":"api/data_analyzer/_index.html#sub-modules","text":"anovos.data_analyzer.association_eval_varclus anovos.data_analyzer.association_evaluator This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target \u2026 anovos.data_analyzer.geospatial_analyzer This module help to analyze & summarize the geospatial related data fields which are identified through the auto-detection module. Additionally, it \u2026 anovos.data_analyzer.quality_checker This submodule focuses on assessing the data quality at both row-level and column-level and also provides an appropriate treatment option to fix \u2026 anovos.data_analyzer.stats_generator This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are split into different metric types, and \u2026 anovos.data_analyzer.ts_analyzer This module generates the intermediate output specific to the inspection of Time series analysis \u2026","title":"Sub-modules"},{"location":"api/data_analyzer/association_eval_varclus.html","text":"association_eval_varclus Expand source code from pyspark.ml.feature import VectorAssembler , StandardScaler from pyspark.mllib.linalg.distributed import RowMatrix , DenseMatrix import math import collections from factor_analyzer import Rotator import pandas as pd import numpy as np import random class VarClusHiSpark ( object ): \"\"\" This class is a scalable version of [VarClusHi] [2] library to perform variable clustering on PySpark dataframes with necessary optimizations, and sampling is not required. [2]: https://github.com/jingtt/varclushi \"VarCluShi\" Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. By default, it begins with all variables in a single cluster. It then repeats the following steps: 1. A cluster is chosen for splitting. This cluster has the largest eigenvalue associated with the 2nd PC 2. Find the first 2 PCs of the chosen cluster and split into 2 clusters, then perform an orthoblique rotation (raw quartimax rotation on the eigenvectors) 3. Variables are iteratively re-assigned to clusters to maximize the variance: (a) Nearest Component Sorting (NCS) Phase: In each iteration, the cluster components are computed. Each variable is assigned to the rotated component with which it has the highest squared correlation (b) Search Phase: Assign each variable into different cluster to see if this increase the variance explained. If variable is re-assigned here, the components of the clusters involved are recomputed before next variable is tested The procedure stops when the 2nd eigenvalue of each cluster is smaller than maxeigval2 parameter. \"\"\" def __init__ ( self , df , feat_list = None , maxeigval2 = 1 , maxclus = None , n_rs = 0 ): \"\"\" Parameters ---------- df PySpark Dataframe feat_list List of features to perform variable clustering e.g., [\"col1\",\"col2\"]. If feat_list is None, all columns of the input dataframe will be used. If feat_list is specified, only columns inside feat_list will be used. To run the algorithm successfully, df[feat_list] should only contain numerical values. (Default value = None) maxeigval2 Maximum value of second-largest eigenvalue e.g., 1 (Default value = 1) maxclus Maximum number of clusters e.g., 20 If maxclus is None, there will be no restrictions on total number of clusters. If maxclus is specified, the algorithm will stop splitting data when number of clusters reaches maxclus. (Default value = None) n_rs Number of random shuffles e.g., 0 This parameter controls the number of random shuffle iterations after re-assignment. If n_rs is 0, re-assignment of each feature will be conducted with no extra random-shuffling. If n_rs is not 0, extra random shuffling of the features and re-assignment will be conducted. (Default value = 0) \"\"\" if feat_list is None : self . df = df self . feat_list = df . columns else : self . df = df . select ( feat_list ) self . feat_list = feat_list self . maxeigval2 = maxeigval2 self . maxclus = maxclus self . n_rs = n_rs if len ( self . feat_list ) <= 1 : all_corr = [ len ( self . feat_list )] else : vecAssembler = VectorAssembler ( inputCols = self . feat_list , outputCol = \"features\" ) stream_df = vecAssembler . transform ( df ) scaler = StandardScaler ( inputCol = \"features\" , outputCol = \"scaledFeatures\" , withStd = True , withMean = True , ) scalerModel = scaler . fit ( stream_df ) scaled_df = scalerModel . transform ( stream_df ) rm = RowMatrix ( scaled_df . select ( \"scaledFeatures\" ) . rdd . map ( list )) all_corr = rm . computeCovariance () . toArray () self . corr_df = pd . DataFrame ( all_corr , columns = self . feat_list , index = self . feat_list ) def correig ( self , df , feat_list = None , n_pcs = 2 ): \"\"\" This function find the correlation matrix between feat_list, calculates the top n_pcs eigenvalues, eigenvectors, and gets the variance-proportion. Parameters ---------- df Spark dataframe feat_list list of features columns e.g. [\"col1\", \"col2\"] If feat_list is None, all columns of the input dataframe will be used. If feat_list is specified, only columns inside feat_list will be used. To run the algorithm successfully, df[feat_list] should only contain numerical values. (Default value = None) n_pcs number of PCs e.g. 2 This parameter controls the size of Principal Components. e.g. If n_pcs=2, only the first 2 eigenvalues, eigenvectors of the correlation matrix will be extracted. (Default value = 2) Returns ------- (top n_pcs) eigenvalues, associated eigenvectors, correlation matrix in Pandas dataframe and variance proportions eigvals Top n_pcs eigenvalues in a Numpy array e.g. [eigval1, eigval2] eigvecs The associated eigenvectors of the top n_pcs eigenvalues in a Numpy array e.g. [eigvec1, eigvec2] Each eigenvector is an array of size D, where D represents the number of columns of df[feat_list] corr_df Correlation matrix of stored in Pandas dataframe The size of this output is DxD where D represents the number of columns of df[feat_list]. The value at index i and column j indicates the pearson's correlation score between feat_list[i] and feat_list[j] varprops The variance proportion of the top n_pcs eigenvalues in a Numpy array e.g. [varprop1, varprop2] The order of this array is the same with eigvals, eigvecs, with the first value varprop1 indicating the variance proportion of the largest eigenvalue. \"\"\" if feat_list is None : feat_list = df . columns if len ( feat_list ) <= 1 : corr = [ len ( feat_list )] eigvals = [ len ( feat_list )] + [ 0 ] * ( n_pcs - 1 ) eigvecs = np . array ([[ len ( feat_list )]]) varprops = [ sum ( eigvals )] else : corr = self . corr_df . loc [ feat_list , feat_list ] . values raw_eigvals , raw_eigvecs = np . linalg . eigh ( corr ) idx = np . argsort ( raw_eigvals )[:: - 1 ] eigvals , eigvecs = raw_eigvals [ idx ], raw_eigvecs [:, idx ] eigvals , eigvecs = eigvals [: n_pcs ], eigvecs [:, : n_pcs ] varprops = eigvals / sum ( raw_eigvals ) corr_df = pd . DataFrame ( corr , columns = feat_list , index = feat_list ) return eigvals , eigvecs , corr_df , varprops def _calc_tot_var ( self , df , * clusters ): \"\"\" This function calculates the total variance explained of given clusters Parameters ---------- df Spark dataframe clusters list of clusters e.g. [clus1, clus2] Each cluster is a list of feature columns which are classified into this cluster. e.g. clus1 = [\"col1\", \"col2\"] Returns ------- tot_var, tot_prop tot_var: sum of the first eigenvalues among all clusters passed in. e.g. 0.5 tot_prop: weighted average of the \"variance for 1st PC\" for each cluster passed in. e.g. 0.4 \"\"\" tot_len , tot_var , tot_prop = ( 0 ,) * 3 for clus in clusters : if clus == []: continue c_len = len ( clus ) c_eigvals , _ , _ , c_varprops = self . correig ( df . select ( clus )) tot_var += c_eigvals [ 0 ] tot_prop = ( tot_prop * tot_len + c_varprops [ 0 ] * c_len ) / ( tot_len + c_len ) tot_len += c_len return tot_var , tot_prop def _reassign ( self , df , clus1 , clus2 , feat_list = None ): \"\"\" This function performs the re-assignment of each variable into different cluster. If the re-assignment could increase the variance explained, the components of the clusters involved are recomputed before next variable is tested. Parameters ---------- df Spark dataframe clus1 List of feature columns in first cluster e.g. [\"col1\", \"col2\"] clus2 List of feature columns in second cluster e.g. [\"col3\", \"col4\"] feat_list List of features to re-assign e.g. [\"feat1\", \"feat2\"] If feat_list is None, all features inside clus1 and clus2 will be re-assigned If feat_list is specified, it should only contain columns in clus1 and/or clus2, and only the specified columns will be re-assigned. (Default value = None) Returns ------- fin_clus1, fin_clus2, max_var fin_clus1: final cluster 1's list of feature columns e.g. [\"col1\", \"col2\"] fin_clus2: final cluster 2's list of feature columns e.g. [\"col3\", \"col4\"] max_var: the maximum variance achieved e.g. 0.9 \"\"\" if feat_list is None : feat_list = clus1 + clus2 init_var = self . _calc_tot_var ( df , clus1 , clus2 )[ 0 ] fin_clus1 , fin_clus2 = clus1 [:], clus2 [:] check_var , max_var = ( init_var ,) * 2 while True : for feat in feat_list : new_clus1 , new_clus2 = fin_clus1 [:], fin_clus2 [:] if feat in new_clus1 : new_clus1 . remove ( feat ) new_clus2 . append ( feat ) elif feat in new_clus2 : new_clus1 . append ( feat ) new_clus2 . remove ( feat ) else : continue new_var = self . _calc_tot_var ( df , new_clus1 , new_clus2 )[ 0 ] if new_var > check_var : check_var = new_var fin_clus1 , fin_clus2 = new_clus1 [:], new_clus2 [:] if max_var == check_var : break else : max_var = check_var return fin_clus1 , fin_clus2 , max_var def _reassign_rs ( self , df , clus1 , clus2 , n_rs = 0 ): \"\"\" This function should be called after _reassign, and it performs random shuffling of n_rs times to run the re-assignment Parameters ---------- df Spark dataframe clus1 List of feature columns in first cluster e.g. [\"col1\", \"col2\"] clus2 List of feature columns in second cluster e.g. [\"col3\", \"col4\"] n_rs Number of random shuffles e.g. 2 If n_rs is 0, random shuffling of the features after re-assignment will not be conducted. If n_rs is >0, random shuffling of n_rs times will be conducted to perform re-assignment. (Default value = 0) Returns ------- fin_rs_clus1, fin_rs_clus2, max_rs_var fin_rs_clus1: final cluster 1's list of feature columns after random shuffling e.g. [\"col1\", \"col2\"] fin_rs_clus2: final cluster 2's list of feature columns after random shuffling e.g. [\"col3\", \"col4\"] max_rs_var: the maximum variance achieved after random shuffling e.g. 0.9 \"\"\" feat_list = clus1 + clus2 fin_rs_clus1 , fin_rs_clus2 , max_rs_var = self . _reassign ( df , clus1 , clus2 ) for _ in range ( n_rs ): random . shuffle ( feat_list ) rs_clus1 , rs_clus2 , rs_var = self . _reassign ( df , clus1 , clus2 , feat_list ) if rs_var > max_rs_var : max_rs_var = rs_var fin_rs_clus1 , fin_rs_clus2 = rs_clus1 , rs_clus2 return fin_rs_clus1 , fin_rs_clus2 , max_rs_var def _varclusspark ( self , spark ): \"\"\" This function is the main function which performs variable clustering. By default, it begins with all variables in a single cluster. It then repeats the following steps: 1. A cluster is chosen for splitting. This cluster has the largest eigenvalue associated with the 2nd PC 2. Find the first 2 PCs of the chosen cluster and split into 2 clusters, then perform an orthoblique rotation (raw quartimax rotation on the eigenvectors) 3. Variables are iteratively re-assigned to clusters to maximize the variance (a) Nearest Component Sorting (NCS) Phase: In each iteration, the cluster components are computed. Each variable is assigned to the rotated component with which it has the highest squared correlation (b) Search Phase: Assign each variable into different cluster to see if this increase the variance explained. If variable is re-assigned here, the components of the clusters involved are recomputed before next variable is tested The procedure stops when the 2nd eigenvalue of each cluster is smaller than maxeigval2 parameter. Parameters ---------- spark Spark Session Returns ------- \"\"\" \"\"\"\"\"\" ClusInfo = collections . namedtuple ( \"ClusInfo\" , [ \"clus\" , \"eigval1\" , \"eigval2\" , \"eigvecs\" , \"varprop\" ] ) c_eigvals , c_eigvecs , c_corrs , c_varprops = self . correig ( self . df . select ( self . feat_list ) ) self . corrs = c_corrs clus0 = ClusInfo ( clus = self . feat_list , eigval1 = c_eigvals [ 0 ], eigval2 = c_eigvals [ 1 ], eigvecs = c_eigvecs , varprop = c_varprops [ 0 ], ) self . clusters = collections . OrderedDict ([( 0 , clus0 )]) while True : if self . maxclus is not None and len ( self . clusters ) >= self . maxclus : break idx = max ( self . clusters , key = lambda x : self . clusters . get ( x ) . eigval2 ) if self . clusters [ idx ] . eigval2 > self . maxeigval2 : split_clus = self . clusters [ idx ] . clus c_eigvals , c_eigvecs , split_corrs , _ = self . correig ( self . df . select ( split_clus ) ) else : break if c_eigvals [ 1 ] > self . maxeigval2 : clus1 , clus2 = [], [] rotator = Rotator ( method = \"quartimax\" ) r_eigvecs = rotator . fit_transform ( pd . DataFrame ( c_eigvecs )) num_rows , num_cols = r_eigvecs . shape r_eigvecs_dm = DenseMatrix ( num_rows , num_cols , r_eigvecs . ravel ( order = \"F\" ) ) r_eigvecs_rm = RowMatrix ( spark . sparkContext . parallelize ( r_eigvecs . T )) dm = DenseMatrix ( num_rows , num_rows , split_corrs . values . ravel ()) comb_sigmas = r_eigvecs_rm . multiply ( dm ) . multiply ( r_eigvecs_dm ) comb_sigmas = np . sqrt ( np . diag ( comb_sigmas . rows . map ( lambda row : np . array ( row )) . collect ()) ) comb_sigma1 = comb_sigmas [ 0 ] comb_sigma2 = comb_sigmas [ 1 ] for feat in split_clus : comb_cov1 = np . dot ( r_eigvecs [:, 0 ], split_corrs [ feat ] . values . T ) comb_cov2 = np . dot ( r_eigvecs [:, 1 ], split_corrs [ feat ] . values . T ) corr_pc1 = comb_cov1 / comb_sigma1 corr_pc2 = comb_cov2 / comb_sigma2 if abs ( corr_pc1 ) > abs ( corr_pc2 ): clus1 . append ( feat ) else : clus2 . append ( feat ) fin_clus1 , fin_clus2 , _ = self . _reassign_rs ( self . df , clus1 , clus2 , self . n_rs ) c1_eigvals , c1_eigvecs , _ , c1_varprops = self . correig ( self . df . select ( fin_clus1 ) ) c2_eigvals , c2_eigvecs , _ , c2_varprops = self . correig ( self . df . select ( fin_clus2 ) ) self . clusters [ idx ] = ClusInfo ( clus = fin_clus1 , eigval1 = c1_eigvals [ 0 ], eigval2 = c1_eigvals [ 1 ], eigvecs = c1_eigvecs , varprop = c1_varprops [ 0 ], ) self . clusters [ len ( self . clusters )] = ClusInfo ( clus = fin_clus2 , eigval1 = c2_eigvals [ 0 ], eigval2 = c2_eigvals [ 1 ], eigvecs = c2_eigvecs , varprop = c2_varprops [ 0 ], ) else : break return self def _rsquarespark ( self ): \"\"\" After variable clustering is done, this function calculates the square correlation of each feature with (1) its own cluster (2) the \"nearest cluster\" (3) RS-ratio using own cluster and \"nearest clutser\" RS_Own: Squared correlation between variable and its own cluster RS_NC: The largest squared correlation among correlations between variable and all other clusters RS_Ratio: (1-RS_Own)/(1-RS_NC) Returns ------- rs_table Pandas dataframe [Cluster, Variable, RS_Own, RS_NC, RS_Ratio] Cluster: integer-type column starting from 0 to maximum number of clusters Variable: string-type column. Each row represents a feature name RS_Own: float-type column indicating the squared correlation between Variable and its Cluster RS_NC: float-type column indicating the squared correlation between Variable and its \"nearest cluster\" RS_Ratio: float-type column (1-RS_Own)/(1-RS_NC) \"\"\" cols = [ \"Cluster\" , \"Variable\" , \"RS_Own\" , \"RS_NC\" , \"RS_Ratio\" ] rs_table = pd . DataFrame ( columns = cols ) sigmas = [] for _ , clusinfo in self . clusters . items (): c_eigvec = clusinfo . eigvecs [:, 0 ] c_sigma = math . sqrt ( np . dot ( np . dot ( c_eigvec , self . corr_df . loc [ clusinfo . clus , clusinfo . clus ] . values ), c_eigvec . T , ) ) sigmas . append ( c_sigma ) n_row = 0 for i , clus_own in self . clusters . items (): for feat in clus_own . clus : row = [ i , feat ] cov_own = np . dot ( clus_own . eigvecs [:, 0 ], self . corr_df . loc [ feat , clus_own . clus ] . values . T , ) if len ( clus_own . clus ) == 1 and feat == clus_own . clus [ 0 ]: rs_own = 1 else : rs_own = ( cov_own / sigmas [ i ]) ** 2 rs_others = [] for j , clus_other in self . clusters . items (): if j == i : continue cov_other = np . dot ( clus_other . eigvecs [:, 0 ], self . corr_df . loc [ feat , clus_other . clus ] . values . T , ) rs = ( cov_other / sigmas [ j ]) ** 2 rs_others . append ( rs ) rs_nc = max ( rs_others ) if len ( rs_others ) > 0 else 0 row += [ rs_own , rs_nc , ( 1 - rs_own ) / ( 1 - rs_nc )] rs_table . loc [ n_row ] = row n_row += 1 return rs_table Classes class VarClusHiSpark ( df, feat_list=None, maxeigval2=1, maxclus=None, n_rs=0) This class is a scalable version of VarClusHi library to perform variable clustering on PySpark dataframes with necessary optimizations, and sampling is not required. Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. By default, it begins with all variables in a single cluster. It then repeats the following steps: 1. A cluster is chosen for splitting. This cluster has the largest eigenvalue associated with the 2nd PC 2. Find the first 2 PCs of the chosen cluster and split into 2 clusters, then perform an orthoblique rotation (raw quartimax rotation on the eigenvectors) 3. Variables are iteratively re-assigned to clusters to maximize the variance: (a) Nearest Component Sorting (NCS) Phase: In each iteration, the cluster components are computed. Each variable is assigned to the rotated component with which it has the highest squared correlation (b) Search Phase: Assign each variable into different cluster to see if this increase the variance explained. If variable is re-assigned here, the components of the clusters involved are recomputed before next variable is tested The procedure stops when the 2nd eigenvalue of each cluster is smaller than maxeigval2 parameter. Parameters df PySpark Dataframe feat_list List of features to perform variable clustering e.g., [\"col1\",\"col2\"]. If feat_list is None, all columns of the input dataframe will be used. If feat_list is specified, only columns inside feat_list will be used. To run the algorithm successfully, df[feat_list] should only contain numerical values. (Default value = None) maxeigval2 Maximum value of second-largest eigenvalue e.g., 1 (Default value = 1) maxclus Maximum number of clusters e.g., 20 If maxclus is None, there will be no restrictions on total number of clusters. If maxclus is specified, the algorithm will stop splitting data when number of clusters reaches maxclus. (Default value = None) n_rs Number of random shuffles e.g., 0 This parameter controls the number of random shuffle iterations after re-assignment. If n_rs is 0, re-assignment of each feature will be conducted with no extra random-shuffling. If n_rs is not 0, extra random shuffling of the features and re-assignment will be conducted. (Default value = 0) Expand source code class VarClusHiSpark ( object ): \"\"\" This class is a scalable version of [VarClusHi] [2] library to perform variable clustering on PySpark dataframes with necessary optimizations, and sampling is not required. [2]: https://github.com/jingtt/varclushi \"VarCluShi\" Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. By default, it begins with all variables in a single cluster. It then repeats the following steps: 1. A cluster is chosen for splitting. This cluster has the largest eigenvalue associated with the 2nd PC 2. Find the first 2 PCs of the chosen cluster and split into 2 clusters, then perform an orthoblique rotation (raw quartimax rotation on the eigenvectors) 3. Variables are iteratively re-assigned to clusters to maximize the variance: (a) Nearest Component Sorting (NCS) Phase: In each iteration, the cluster components are computed. Each variable is assigned to the rotated component with which it has the highest squared correlation (b) Search Phase: Assign each variable into different cluster to see if this increase the variance explained. If variable is re-assigned here, the components of the clusters involved are recomputed before next variable is tested The procedure stops when the 2nd eigenvalue of each cluster is smaller than maxeigval2 parameter. \"\"\" def __init__ ( self , df , feat_list = None , maxeigval2 = 1 , maxclus = None , n_rs = 0 ): \"\"\" Parameters ---------- df PySpark Dataframe feat_list List of features to perform variable clustering e.g., [\"col1\",\"col2\"]. If feat_list is None, all columns of the input dataframe will be used. If feat_list is specified, only columns inside feat_list will be used. To run the algorithm successfully, df[feat_list] should only contain numerical values. (Default value = None) maxeigval2 Maximum value of second-largest eigenvalue e.g., 1 (Default value = 1) maxclus Maximum number of clusters e.g., 20 If maxclus is None, there will be no restrictions on total number of clusters. If maxclus is specified, the algorithm will stop splitting data when number of clusters reaches maxclus. (Default value = None) n_rs Number of random shuffles e.g., 0 This parameter controls the number of random shuffle iterations after re-assignment. If n_rs is 0, re-assignment of each feature will be conducted with no extra random-shuffling. If n_rs is not 0, extra random shuffling of the features and re-assignment will be conducted. (Default value = 0) \"\"\" if feat_list is None : self . df = df self . feat_list = df . columns else : self . df = df . select ( feat_list ) self . feat_list = feat_list self . maxeigval2 = maxeigval2 self . maxclus = maxclus self . n_rs = n_rs if len ( self . feat_list ) <= 1 : all_corr = [ len ( self . feat_list )] else : vecAssembler = VectorAssembler ( inputCols = self . feat_list , outputCol = \"features\" ) stream_df = vecAssembler . transform ( df ) scaler = StandardScaler ( inputCol = \"features\" , outputCol = \"scaledFeatures\" , withStd = True , withMean = True , ) scalerModel = scaler . fit ( stream_df ) scaled_df = scalerModel . transform ( stream_df ) rm = RowMatrix ( scaled_df . select ( \"scaledFeatures\" ) . rdd . map ( list )) all_corr = rm . computeCovariance () . toArray () self . corr_df = pd . DataFrame ( all_corr , columns = self . feat_list , index = self . feat_list ) def correig ( self , df , feat_list = None , n_pcs = 2 ): \"\"\" This function find the correlation matrix between feat_list, calculates the top n_pcs eigenvalues, eigenvectors, and gets the variance-proportion. Parameters ---------- df Spark dataframe feat_list list of features columns e.g. [\"col1\", \"col2\"] If feat_list is None, all columns of the input dataframe will be used. If feat_list is specified, only columns inside feat_list will be used. To run the algorithm successfully, df[feat_list] should only contain numerical values. (Default value = None) n_pcs number of PCs e.g. 2 This parameter controls the size of Principal Components. e.g. If n_pcs=2, only the first 2 eigenvalues, eigenvectors of the correlation matrix will be extracted. (Default value = 2) Returns ------- (top n_pcs) eigenvalues, associated eigenvectors, correlation matrix in Pandas dataframe and variance proportions eigvals Top n_pcs eigenvalues in a Numpy array e.g. [eigval1, eigval2] eigvecs The associated eigenvectors of the top n_pcs eigenvalues in a Numpy array e.g. [eigvec1, eigvec2] Each eigenvector is an array of size D, where D represents the number of columns of df[feat_list] corr_df Correlation matrix of stored in Pandas dataframe The size of this output is DxD where D represents the number of columns of df[feat_list]. The value at index i and column j indicates the pearson's correlation score between feat_list[i] and feat_list[j] varprops The variance proportion of the top n_pcs eigenvalues in a Numpy array e.g. [varprop1, varprop2] The order of this array is the same with eigvals, eigvecs, with the first value varprop1 indicating the variance proportion of the largest eigenvalue. \"\"\" if feat_list is None : feat_list = df . columns if len ( feat_list ) <= 1 : corr = [ len ( feat_list )] eigvals = [ len ( feat_list )] + [ 0 ] * ( n_pcs - 1 ) eigvecs = np . array ([[ len ( feat_list )]]) varprops = [ sum ( eigvals )] else : corr = self . corr_df . loc [ feat_list , feat_list ] . values raw_eigvals , raw_eigvecs = np . linalg . eigh ( corr ) idx = np . argsort ( raw_eigvals )[:: - 1 ] eigvals , eigvecs = raw_eigvals [ idx ], raw_eigvecs [:, idx ] eigvals , eigvecs = eigvals [: n_pcs ], eigvecs [:, : n_pcs ] varprops = eigvals / sum ( raw_eigvals ) corr_df = pd . DataFrame ( corr , columns = feat_list , index = feat_list ) return eigvals , eigvecs , corr_df , varprops def _calc_tot_var ( self , df , * clusters ): \"\"\" This function calculates the total variance explained of given clusters Parameters ---------- df Spark dataframe clusters list of clusters e.g. [clus1, clus2] Each cluster is a list of feature columns which are classified into this cluster. e.g. clus1 = [\"col1\", \"col2\"] Returns ------- tot_var, tot_prop tot_var: sum of the first eigenvalues among all clusters passed in. e.g. 0.5 tot_prop: weighted average of the \"variance for 1st PC\" for each cluster passed in. e.g. 0.4 \"\"\" tot_len , tot_var , tot_prop = ( 0 ,) * 3 for clus in clusters : if clus == []: continue c_len = len ( clus ) c_eigvals , _ , _ , c_varprops = self . correig ( df . select ( clus )) tot_var += c_eigvals [ 0 ] tot_prop = ( tot_prop * tot_len + c_varprops [ 0 ] * c_len ) / ( tot_len + c_len ) tot_len += c_len return tot_var , tot_prop def _reassign ( self , df , clus1 , clus2 , feat_list = None ): \"\"\" This function performs the re-assignment of each variable into different cluster. If the re-assignment could increase the variance explained, the components of the clusters involved are recomputed before next variable is tested. Parameters ---------- df Spark dataframe clus1 List of feature columns in first cluster e.g. [\"col1\", \"col2\"] clus2 List of feature columns in second cluster e.g. [\"col3\", \"col4\"] feat_list List of features to re-assign e.g. [\"feat1\", \"feat2\"] If feat_list is None, all features inside clus1 and clus2 will be re-assigned If feat_list is specified, it should only contain columns in clus1 and/or clus2, and only the specified columns will be re-assigned. (Default value = None) Returns ------- fin_clus1, fin_clus2, max_var fin_clus1: final cluster 1's list of feature columns e.g. [\"col1\", \"col2\"] fin_clus2: final cluster 2's list of feature columns e.g. [\"col3\", \"col4\"] max_var: the maximum variance achieved e.g. 0.9 \"\"\" if feat_list is None : feat_list = clus1 + clus2 init_var = self . _calc_tot_var ( df , clus1 , clus2 )[ 0 ] fin_clus1 , fin_clus2 = clus1 [:], clus2 [:] check_var , max_var = ( init_var ,) * 2 while True : for feat in feat_list : new_clus1 , new_clus2 = fin_clus1 [:], fin_clus2 [:] if feat in new_clus1 : new_clus1 . remove ( feat ) new_clus2 . append ( feat ) elif feat in new_clus2 : new_clus1 . append ( feat ) new_clus2 . remove ( feat ) else : continue new_var = self . _calc_tot_var ( df , new_clus1 , new_clus2 )[ 0 ] if new_var > check_var : check_var = new_var fin_clus1 , fin_clus2 = new_clus1 [:], new_clus2 [:] if max_var == check_var : break else : max_var = check_var return fin_clus1 , fin_clus2 , max_var def _reassign_rs ( self , df , clus1 , clus2 , n_rs = 0 ): \"\"\" This function should be called after _reassign, and it performs random shuffling of n_rs times to run the re-assignment Parameters ---------- df Spark dataframe clus1 List of feature columns in first cluster e.g. [\"col1\", \"col2\"] clus2 List of feature columns in second cluster e.g. [\"col3\", \"col4\"] n_rs Number of random shuffles e.g. 2 If n_rs is 0, random shuffling of the features after re-assignment will not be conducted. If n_rs is >0, random shuffling of n_rs times will be conducted to perform re-assignment. (Default value = 0) Returns ------- fin_rs_clus1, fin_rs_clus2, max_rs_var fin_rs_clus1: final cluster 1's list of feature columns after random shuffling e.g. [\"col1\", \"col2\"] fin_rs_clus2: final cluster 2's list of feature columns after random shuffling e.g. [\"col3\", \"col4\"] max_rs_var: the maximum variance achieved after random shuffling e.g. 0.9 \"\"\" feat_list = clus1 + clus2 fin_rs_clus1 , fin_rs_clus2 , max_rs_var = self . _reassign ( df , clus1 , clus2 ) for _ in range ( n_rs ): random . shuffle ( feat_list ) rs_clus1 , rs_clus2 , rs_var = self . _reassign ( df , clus1 , clus2 , feat_list ) if rs_var > max_rs_var : max_rs_var = rs_var fin_rs_clus1 , fin_rs_clus2 = rs_clus1 , rs_clus2 return fin_rs_clus1 , fin_rs_clus2 , max_rs_var def _varclusspark ( self , spark ): \"\"\" This function is the main function which performs variable clustering. By default, it begins with all variables in a single cluster. It then repeats the following steps: 1. A cluster is chosen for splitting. This cluster has the largest eigenvalue associated with the 2nd PC 2. Find the first 2 PCs of the chosen cluster and split into 2 clusters, then perform an orthoblique rotation (raw quartimax rotation on the eigenvectors) 3. Variables are iteratively re-assigned to clusters to maximize the variance (a) Nearest Component Sorting (NCS) Phase: In each iteration, the cluster components are computed. Each variable is assigned to the rotated component with which it has the highest squared correlation (b) Search Phase: Assign each variable into different cluster to see if this increase the variance explained. If variable is re-assigned here, the components of the clusters involved are recomputed before next variable is tested The procedure stops when the 2nd eigenvalue of each cluster is smaller than maxeigval2 parameter. Parameters ---------- spark Spark Session Returns ------- \"\"\" \"\"\"\"\"\" ClusInfo = collections . namedtuple ( \"ClusInfo\" , [ \"clus\" , \"eigval1\" , \"eigval2\" , \"eigvecs\" , \"varprop\" ] ) c_eigvals , c_eigvecs , c_corrs , c_varprops = self . correig ( self . df . select ( self . feat_list ) ) self . corrs = c_corrs clus0 = ClusInfo ( clus = self . feat_list , eigval1 = c_eigvals [ 0 ], eigval2 = c_eigvals [ 1 ], eigvecs = c_eigvecs , varprop = c_varprops [ 0 ], ) self . clusters = collections . OrderedDict ([( 0 , clus0 )]) while True : if self . maxclus is not None and len ( self . clusters ) >= self . maxclus : break idx = max ( self . clusters , key = lambda x : self . clusters . get ( x ) . eigval2 ) if self . clusters [ idx ] . eigval2 > self . maxeigval2 : split_clus = self . clusters [ idx ] . clus c_eigvals , c_eigvecs , split_corrs , _ = self . correig ( self . df . select ( split_clus ) ) else : break if c_eigvals [ 1 ] > self . maxeigval2 : clus1 , clus2 = [], [] rotator = Rotator ( method = \"quartimax\" ) r_eigvecs = rotator . fit_transform ( pd . DataFrame ( c_eigvecs )) num_rows , num_cols = r_eigvecs . shape r_eigvecs_dm = DenseMatrix ( num_rows , num_cols , r_eigvecs . ravel ( order = \"F\" ) ) r_eigvecs_rm = RowMatrix ( spark . sparkContext . parallelize ( r_eigvecs . T )) dm = DenseMatrix ( num_rows , num_rows , split_corrs . values . ravel ()) comb_sigmas = r_eigvecs_rm . multiply ( dm ) . multiply ( r_eigvecs_dm ) comb_sigmas = np . sqrt ( np . diag ( comb_sigmas . rows . map ( lambda row : np . array ( row )) . collect ()) ) comb_sigma1 = comb_sigmas [ 0 ] comb_sigma2 = comb_sigmas [ 1 ] for feat in split_clus : comb_cov1 = np . dot ( r_eigvecs [:, 0 ], split_corrs [ feat ] . values . T ) comb_cov2 = np . dot ( r_eigvecs [:, 1 ], split_corrs [ feat ] . values . T ) corr_pc1 = comb_cov1 / comb_sigma1 corr_pc2 = comb_cov2 / comb_sigma2 if abs ( corr_pc1 ) > abs ( corr_pc2 ): clus1 . append ( feat ) else : clus2 . append ( feat ) fin_clus1 , fin_clus2 , _ = self . _reassign_rs ( self . df , clus1 , clus2 , self . n_rs ) c1_eigvals , c1_eigvecs , _ , c1_varprops = self . correig ( self . df . select ( fin_clus1 ) ) c2_eigvals , c2_eigvecs , _ , c2_varprops = self . correig ( self . df . select ( fin_clus2 ) ) self . clusters [ idx ] = ClusInfo ( clus = fin_clus1 , eigval1 = c1_eigvals [ 0 ], eigval2 = c1_eigvals [ 1 ], eigvecs = c1_eigvecs , varprop = c1_varprops [ 0 ], ) self . clusters [ len ( self . clusters )] = ClusInfo ( clus = fin_clus2 , eigval1 = c2_eigvals [ 0 ], eigval2 = c2_eigvals [ 1 ], eigvecs = c2_eigvecs , varprop = c2_varprops [ 0 ], ) else : break return self def _rsquarespark ( self ): \"\"\" After variable clustering is done, this function calculates the square correlation of each feature with (1) its own cluster (2) the \"nearest cluster\" (3) RS-ratio using own cluster and \"nearest clutser\" RS_Own: Squared correlation between variable and its own cluster RS_NC: The largest squared correlation among correlations between variable and all other clusters RS_Ratio: (1-RS_Own)/(1-RS_NC) Returns ------- rs_table Pandas dataframe [Cluster, Variable, RS_Own, RS_NC, RS_Ratio] Cluster: integer-type column starting from 0 to maximum number of clusters Variable: string-type column. Each row represents a feature name RS_Own: float-type column indicating the squared correlation between Variable and its Cluster RS_NC: float-type column indicating the squared correlation between Variable and its \"nearest cluster\" RS_Ratio: float-type column (1-RS_Own)/(1-RS_NC) \"\"\" cols = [ \"Cluster\" , \"Variable\" , \"RS_Own\" , \"RS_NC\" , \"RS_Ratio\" ] rs_table = pd . DataFrame ( columns = cols ) sigmas = [] for _ , clusinfo in self . clusters . items (): c_eigvec = clusinfo . eigvecs [:, 0 ] c_sigma = math . sqrt ( np . dot ( np . dot ( c_eigvec , self . corr_df . loc [ clusinfo . clus , clusinfo . clus ] . values ), c_eigvec . T , ) ) sigmas . append ( c_sigma ) n_row = 0 for i , clus_own in self . clusters . items (): for feat in clus_own . clus : row = [ i , feat ] cov_own = np . dot ( clus_own . eigvecs [:, 0 ], self . corr_df . loc [ feat , clus_own . clus ] . values . T , ) if len ( clus_own . clus ) == 1 and feat == clus_own . clus [ 0 ]: rs_own = 1 else : rs_own = ( cov_own / sigmas [ i ]) ** 2 rs_others = [] for j , clus_other in self . clusters . items (): if j == i : continue cov_other = np . dot ( clus_other . eigvecs [:, 0 ], self . corr_df . loc [ feat , clus_other . clus ] . values . T , ) rs = ( cov_other / sigmas [ j ]) ** 2 rs_others . append ( rs ) rs_nc = max ( rs_others ) if len ( rs_others ) > 0 else 0 row += [ rs_own , rs_nc , ( 1 - rs_own ) / ( 1 - rs_nc )] rs_table . loc [ n_row ] = row n_row += 1 return rs_table Methods def correig ( self, df, feat_list=None, n_pcs=2) This function find the correlation matrix between feat_list, calculates the top n_pcs eigenvalues, eigenvectors, and gets the variance-proportion. Parameters df Spark dataframe feat_list list of features columns e.g. [\"col1\", \"col2\"] If feat_list is None, all columns of the input dataframe will be used. If feat_list is specified, only columns inside feat_list will be used. To run the algorithm successfully, df[feat_list] should only contain numerical values. (Default value = None) n_pcs number of PCs e.g. 2 This parameter controls the size of Principal Components. e.g. If n_pcs=2, only the first 2 eigenvalues, eigenvectors of the correlation matrix will be extracted. (Default value = 2) Returns (top n_pcs) eigenvalues, associated eigenvectors, correlation matrix in Pandas dataframe and variance proportions eigvals Top n_pcs eigenvalues in a Numpy array e.g. [eigval1, eigval2] eigvecs The associated eigenvectors of the top n_pcs eigenvalues in a Numpy array e.g. [eigvec1, eigvec2] Each eigenvector is an array of size D, where D represents the number of columns of df[feat_list] corr_df Correlation matrix of stored in Pandas dataframe The size of this output is DxD where D represents the number of columns of df[feat_list]. The value at index i and column j indicates the pearson's correlation score between feat_list[i] and feat_list[j] varprops The variance proportion of the top n_pcs eigenvalues in a Numpy array e.g. [varprop1, varprop2] The order of this array is the same with eigvals, eigvecs, with the first value varprop1 indicating the variance proportion of the largest eigenvalue. Expand source code def correig ( self , df , feat_list = None , n_pcs = 2 ): \"\"\" This function find the correlation matrix between feat_list, calculates the top n_pcs eigenvalues, eigenvectors, and gets the variance-proportion. Parameters ---------- df Spark dataframe feat_list list of features columns e.g. [\"col1\", \"col2\"] If feat_list is None, all columns of the input dataframe will be used. If feat_list is specified, only columns inside feat_list will be used. To run the algorithm successfully, df[feat_list] should only contain numerical values. (Default value = None) n_pcs number of PCs e.g. 2 This parameter controls the size of Principal Components. e.g. If n_pcs=2, only the first 2 eigenvalues, eigenvectors of the correlation matrix will be extracted. (Default value = 2) Returns ------- (top n_pcs) eigenvalues, associated eigenvectors, correlation matrix in Pandas dataframe and variance proportions eigvals Top n_pcs eigenvalues in a Numpy array e.g. [eigval1, eigval2] eigvecs The associated eigenvectors of the top n_pcs eigenvalues in a Numpy array e.g. [eigvec1, eigvec2] Each eigenvector is an array of size D, where D represents the number of columns of df[feat_list] corr_df Correlation matrix of stored in Pandas dataframe The size of this output is DxD where D represents the number of columns of df[feat_list]. The value at index i and column j indicates the pearson's correlation score between feat_list[i] and feat_list[j] varprops The variance proportion of the top n_pcs eigenvalues in a Numpy array e.g. [varprop1, varprop2] The order of this array is the same with eigvals, eigvecs, with the first value varprop1 indicating the variance proportion of the largest eigenvalue. \"\"\" if feat_list is None : feat_list = df . columns if len ( feat_list ) <= 1 : corr = [ len ( feat_list )] eigvals = [ len ( feat_list )] + [ 0 ] * ( n_pcs - 1 ) eigvecs = np . array ([[ len ( feat_list )]]) varprops = [ sum ( eigvals )] else : corr = self . corr_df . loc [ feat_list , feat_list ] . values raw_eigvals , raw_eigvecs = np . linalg . eigh ( corr ) idx = np . argsort ( raw_eigvals )[:: - 1 ] eigvals , eigvecs = raw_eigvals [ idx ], raw_eigvecs [:, idx ] eigvals , eigvecs = eigvals [: n_pcs ], eigvecs [:, : n_pcs ] varprops = eigvals / sum ( raw_eigvals ) corr_df = pd . DataFrame ( corr , columns = feat_list , index = feat_list ) return eigvals , eigvecs , corr_df , varprops","title":"<code>association_eval_varclus</code>"},{"location":"api/data_analyzer/association_eval_varclus.html#association_eval_varclus","text":"Expand source code from pyspark.ml.feature import VectorAssembler , StandardScaler from pyspark.mllib.linalg.distributed import RowMatrix , DenseMatrix import math import collections from factor_analyzer import Rotator import pandas as pd import numpy as np import random class VarClusHiSpark ( object ): \"\"\" This class is a scalable version of [VarClusHi] [2] library to perform variable clustering on PySpark dataframes with necessary optimizations, and sampling is not required. [2]: https://github.com/jingtt/varclushi \"VarCluShi\" Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. By default, it begins with all variables in a single cluster. It then repeats the following steps: 1. A cluster is chosen for splitting. This cluster has the largest eigenvalue associated with the 2nd PC 2. Find the first 2 PCs of the chosen cluster and split into 2 clusters, then perform an orthoblique rotation (raw quartimax rotation on the eigenvectors) 3. Variables are iteratively re-assigned to clusters to maximize the variance: (a) Nearest Component Sorting (NCS) Phase: In each iteration, the cluster components are computed. Each variable is assigned to the rotated component with which it has the highest squared correlation (b) Search Phase: Assign each variable into different cluster to see if this increase the variance explained. If variable is re-assigned here, the components of the clusters involved are recomputed before next variable is tested The procedure stops when the 2nd eigenvalue of each cluster is smaller than maxeigval2 parameter. \"\"\" def __init__ ( self , df , feat_list = None , maxeigval2 = 1 , maxclus = None , n_rs = 0 ): \"\"\" Parameters ---------- df PySpark Dataframe feat_list List of features to perform variable clustering e.g., [\"col1\",\"col2\"]. If feat_list is None, all columns of the input dataframe will be used. If feat_list is specified, only columns inside feat_list will be used. To run the algorithm successfully, df[feat_list] should only contain numerical values. (Default value = None) maxeigval2 Maximum value of second-largest eigenvalue e.g., 1 (Default value = 1) maxclus Maximum number of clusters e.g., 20 If maxclus is None, there will be no restrictions on total number of clusters. If maxclus is specified, the algorithm will stop splitting data when number of clusters reaches maxclus. (Default value = None) n_rs Number of random shuffles e.g., 0 This parameter controls the number of random shuffle iterations after re-assignment. If n_rs is 0, re-assignment of each feature will be conducted with no extra random-shuffling. If n_rs is not 0, extra random shuffling of the features and re-assignment will be conducted. (Default value = 0) \"\"\" if feat_list is None : self . df = df self . feat_list = df . columns else : self . df = df . select ( feat_list ) self . feat_list = feat_list self . maxeigval2 = maxeigval2 self . maxclus = maxclus self . n_rs = n_rs if len ( self . feat_list ) <= 1 : all_corr = [ len ( self . feat_list )] else : vecAssembler = VectorAssembler ( inputCols = self . feat_list , outputCol = \"features\" ) stream_df = vecAssembler . transform ( df ) scaler = StandardScaler ( inputCol = \"features\" , outputCol = \"scaledFeatures\" , withStd = True , withMean = True , ) scalerModel = scaler . fit ( stream_df ) scaled_df = scalerModel . transform ( stream_df ) rm = RowMatrix ( scaled_df . select ( \"scaledFeatures\" ) . rdd . map ( list )) all_corr = rm . computeCovariance () . toArray () self . corr_df = pd . DataFrame ( all_corr , columns = self . feat_list , index = self . feat_list ) def correig ( self , df , feat_list = None , n_pcs = 2 ): \"\"\" This function find the correlation matrix between feat_list, calculates the top n_pcs eigenvalues, eigenvectors, and gets the variance-proportion. Parameters ---------- df Spark dataframe feat_list list of features columns e.g. [\"col1\", \"col2\"] If feat_list is None, all columns of the input dataframe will be used. If feat_list is specified, only columns inside feat_list will be used. To run the algorithm successfully, df[feat_list] should only contain numerical values. (Default value = None) n_pcs number of PCs e.g. 2 This parameter controls the size of Principal Components. e.g. If n_pcs=2, only the first 2 eigenvalues, eigenvectors of the correlation matrix will be extracted. (Default value = 2) Returns ------- (top n_pcs) eigenvalues, associated eigenvectors, correlation matrix in Pandas dataframe and variance proportions eigvals Top n_pcs eigenvalues in a Numpy array e.g. [eigval1, eigval2] eigvecs The associated eigenvectors of the top n_pcs eigenvalues in a Numpy array e.g. [eigvec1, eigvec2] Each eigenvector is an array of size D, where D represents the number of columns of df[feat_list] corr_df Correlation matrix of stored in Pandas dataframe The size of this output is DxD where D represents the number of columns of df[feat_list]. The value at index i and column j indicates the pearson's correlation score between feat_list[i] and feat_list[j] varprops The variance proportion of the top n_pcs eigenvalues in a Numpy array e.g. [varprop1, varprop2] The order of this array is the same with eigvals, eigvecs, with the first value varprop1 indicating the variance proportion of the largest eigenvalue. \"\"\" if feat_list is None : feat_list = df . columns if len ( feat_list ) <= 1 : corr = [ len ( feat_list )] eigvals = [ len ( feat_list )] + [ 0 ] * ( n_pcs - 1 ) eigvecs = np . array ([[ len ( feat_list )]]) varprops = [ sum ( eigvals )] else : corr = self . corr_df . loc [ feat_list , feat_list ] . values raw_eigvals , raw_eigvecs = np . linalg . eigh ( corr ) idx = np . argsort ( raw_eigvals )[:: - 1 ] eigvals , eigvecs = raw_eigvals [ idx ], raw_eigvecs [:, idx ] eigvals , eigvecs = eigvals [: n_pcs ], eigvecs [:, : n_pcs ] varprops = eigvals / sum ( raw_eigvals ) corr_df = pd . DataFrame ( corr , columns = feat_list , index = feat_list ) return eigvals , eigvecs , corr_df , varprops def _calc_tot_var ( self , df , * clusters ): \"\"\" This function calculates the total variance explained of given clusters Parameters ---------- df Spark dataframe clusters list of clusters e.g. [clus1, clus2] Each cluster is a list of feature columns which are classified into this cluster. e.g. clus1 = [\"col1\", \"col2\"] Returns ------- tot_var, tot_prop tot_var: sum of the first eigenvalues among all clusters passed in. e.g. 0.5 tot_prop: weighted average of the \"variance for 1st PC\" for each cluster passed in. e.g. 0.4 \"\"\" tot_len , tot_var , tot_prop = ( 0 ,) * 3 for clus in clusters : if clus == []: continue c_len = len ( clus ) c_eigvals , _ , _ , c_varprops = self . correig ( df . select ( clus )) tot_var += c_eigvals [ 0 ] tot_prop = ( tot_prop * tot_len + c_varprops [ 0 ] * c_len ) / ( tot_len + c_len ) tot_len += c_len return tot_var , tot_prop def _reassign ( self , df , clus1 , clus2 , feat_list = None ): \"\"\" This function performs the re-assignment of each variable into different cluster. If the re-assignment could increase the variance explained, the components of the clusters involved are recomputed before next variable is tested. Parameters ---------- df Spark dataframe clus1 List of feature columns in first cluster e.g. [\"col1\", \"col2\"] clus2 List of feature columns in second cluster e.g. [\"col3\", \"col4\"] feat_list List of features to re-assign e.g. [\"feat1\", \"feat2\"] If feat_list is None, all features inside clus1 and clus2 will be re-assigned If feat_list is specified, it should only contain columns in clus1 and/or clus2, and only the specified columns will be re-assigned. (Default value = None) Returns ------- fin_clus1, fin_clus2, max_var fin_clus1: final cluster 1's list of feature columns e.g. [\"col1\", \"col2\"] fin_clus2: final cluster 2's list of feature columns e.g. [\"col3\", \"col4\"] max_var: the maximum variance achieved e.g. 0.9 \"\"\" if feat_list is None : feat_list = clus1 + clus2 init_var = self . _calc_tot_var ( df , clus1 , clus2 )[ 0 ] fin_clus1 , fin_clus2 = clus1 [:], clus2 [:] check_var , max_var = ( init_var ,) * 2 while True : for feat in feat_list : new_clus1 , new_clus2 = fin_clus1 [:], fin_clus2 [:] if feat in new_clus1 : new_clus1 . remove ( feat ) new_clus2 . append ( feat ) elif feat in new_clus2 : new_clus1 . append ( feat ) new_clus2 . remove ( feat ) else : continue new_var = self . _calc_tot_var ( df , new_clus1 , new_clus2 )[ 0 ] if new_var > check_var : check_var = new_var fin_clus1 , fin_clus2 = new_clus1 [:], new_clus2 [:] if max_var == check_var : break else : max_var = check_var return fin_clus1 , fin_clus2 , max_var def _reassign_rs ( self , df , clus1 , clus2 , n_rs = 0 ): \"\"\" This function should be called after _reassign, and it performs random shuffling of n_rs times to run the re-assignment Parameters ---------- df Spark dataframe clus1 List of feature columns in first cluster e.g. [\"col1\", \"col2\"] clus2 List of feature columns in second cluster e.g. [\"col3\", \"col4\"] n_rs Number of random shuffles e.g. 2 If n_rs is 0, random shuffling of the features after re-assignment will not be conducted. If n_rs is >0, random shuffling of n_rs times will be conducted to perform re-assignment. (Default value = 0) Returns ------- fin_rs_clus1, fin_rs_clus2, max_rs_var fin_rs_clus1: final cluster 1's list of feature columns after random shuffling e.g. [\"col1\", \"col2\"] fin_rs_clus2: final cluster 2's list of feature columns after random shuffling e.g. [\"col3\", \"col4\"] max_rs_var: the maximum variance achieved after random shuffling e.g. 0.9 \"\"\" feat_list = clus1 + clus2 fin_rs_clus1 , fin_rs_clus2 , max_rs_var = self . _reassign ( df , clus1 , clus2 ) for _ in range ( n_rs ): random . shuffle ( feat_list ) rs_clus1 , rs_clus2 , rs_var = self . _reassign ( df , clus1 , clus2 , feat_list ) if rs_var > max_rs_var : max_rs_var = rs_var fin_rs_clus1 , fin_rs_clus2 = rs_clus1 , rs_clus2 return fin_rs_clus1 , fin_rs_clus2 , max_rs_var def _varclusspark ( self , spark ): \"\"\" This function is the main function which performs variable clustering. By default, it begins with all variables in a single cluster. It then repeats the following steps: 1. A cluster is chosen for splitting. This cluster has the largest eigenvalue associated with the 2nd PC 2. Find the first 2 PCs of the chosen cluster and split into 2 clusters, then perform an orthoblique rotation (raw quartimax rotation on the eigenvectors) 3. Variables are iteratively re-assigned to clusters to maximize the variance (a) Nearest Component Sorting (NCS) Phase: In each iteration, the cluster components are computed. Each variable is assigned to the rotated component with which it has the highest squared correlation (b) Search Phase: Assign each variable into different cluster to see if this increase the variance explained. If variable is re-assigned here, the components of the clusters involved are recomputed before next variable is tested The procedure stops when the 2nd eigenvalue of each cluster is smaller than maxeigval2 parameter. Parameters ---------- spark Spark Session Returns ------- \"\"\" \"\"\"\"\"\" ClusInfo = collections . namedtuple ( \"ClusInfo\" , [ \"clus\" , \"eigval1\" , \"eigval2\" , \"eigvecs\" , \"varprop\" ] ) c_eigvals , c_eigvecs , c_corrs , c_varprops = self . correig ( self . df . select ( self . feat_list ) ) self . corrs = c_corrs clus0 = ClusInfo ( clus = self . feat_list , eigval1 = c_eigvals [ 0 ], eigval2 = c_eigvals [ 1 ], eigvecs = c_eigvecs , varprop = c_varprops [ 0 ], ) self . clusters = collections . OrderedDict ([( 0 , clus0 )]) while True : if self . maxclus is not None and len ( self . clusters ) >= self . maxclus : break idx = max ( self . clusters , key = lambda x : self . clusters . get ( x ) . eigval2 ) if self . clusters [ idx ] . eigval2 > self . maxeigval2 : split_clus = self . clusters [ idx ] . clus c_eigvals , c_eigvecs , split_corrs , _ = self . correig ( self . df . select ( split_clus ) ) else : break if c_eigvals [ 1 ] > self . maxeigval2 : clus1 , clus2 = [], [] rotator = Rotator ( method = \"quartimax\" ) r_eigvecs = rotator . fit_transform ( pd . DataFrame ( c_eigvecs )) num_rows , num_cols = r_eigvecs . shape r_eigvecs_dm = DenseMatrix ( num_rows , num_cols , r_eigvecs . ravel ( order = \"F\" ) ) r_eigvecs_rm = RowMatrix ( spark . sparkContext . parallelize ( r_eigvecs . T )) dm = DenseMatrix ( num_rows , num_rows , split_corrs . values . ravel ()) comb_sigmas = r_eigvecs_rm . multiply ( dm ) . multiply ( r_eigvecs_dm ) comb_sigmas = np . sqrt ( np . diag ( comb_sigmas . rows . map ( lambda row : np . array ( row )) . collect ()) ) comb_sigma1 = comb_sigmas [ 0 ] comb_sigma2 = comb_sigmas [ 1 ] for feat in split_clus : comb_cov1 = np . dot ( r_eigvecs [:, 0 ], split_corrs [ feat ] . values . T ) comb_cov2 = np . dot ( r_eigvecs [:, 1 ], split_corrs [ feat ] . values . T ) corr_pc1 = comb_cov1 / comb_sigma1 corr_pc2 = comb_cov2 / comb_sigma2 if abs ( corr_pc1 ) > abs ( corr_pc2 ): clus1 . append ( feat ) else : clus2 . append ( feat ) fin_clus1 , fin_clus2 , _ = self . _reassign_rs ( self . df , clus1 , clus2 , self . n_rs ) c1_eigvals , c1_eigvecs , _ , c1_varprops = self . correig ( self . df . select ( fin_clus1 ) ) c2_eigvals , c2_eigvecs , _ , c2_varprops = self . correig ( self . df . select ( fin_clus2 ) ) self . clusters [ idx ] = ClusInfo ( clus = fin_clus1 , eigval1 = c1_eigvals [ 0 ], eigval2 = c1_eigvals [ 1 ], eigvecs = c1_eigvecs , varprop = c1_varprops [ 0 ], ) self . clusters [ len ( self . clusters )] = ClusInfo ( clus = fin_clus2 , eigval1 = c2_eigvals [ 0 ], eigval2 = c2_eigvals [ 1 ], eigvecs = c2_eigvecs , varprop = c2_varprops [ 0 ], ) else : break return self def _rsquarespark ( self ): \"\"\" After variable clustering is done, this function calculates the square correlation of each feature with (1) its own cluster (2) the \"nearest cluster\" (3) RS-ratio using own cluster and \"nearest clutser\" RS_Own: Squared correlation between variable and its own cluster RS_NC: The largest squared correlation among correlations between variable and all other clusters RS_Ratio: (1-RS_Own)/(1-RS_NC) Returns ------- rs_table Pandas dataframe [Cluster, Variable, RS_Own, RS_NC, RS_Ratio] Cluster: integer-type column starting from 0 to maximum number of clusters Variable: string-type column. Each row represents a feature name RS_Own: float-type column indicating the squared correlation between Variable and its Cluster RS_NC: float-type column indicating the squared correlation between Variable and its \"nearest cluster\" RS_Ratio: float-type column (1-RS_Own)/(1-RS_NC) \"\"\" cols = [ \"Cluster\" , \"Variable\" , \"RS_Own\" , \"RS_NC\" , \"RS_Ratio\" ] rs_table = pd . DataFrame ( columns = cols ) sigmas = [] for _ , clusinfo in self . clusters . items (): c_eigvec = clusinfo . eigvecs [:, 0 ] c_sigma = math . sqrt ( np . dot ( np . dot ( c_eigvec , self . corr_df . loc [ clusinfo . clus , clusinfo . clus ] . values ), c_eigvec . T , ) ) sigmas . append ( c_sigma ) n_row = 0 for i , clus_own in self . clusters . items (): for feat in clus_own . clus : row = [ i , feat ] cov_own = np . dot ( clus_own . eigvecs [:, 0 ], self . corr_df . loc [ feat , clus_own . clus ] . values . T , ) if len ( clus_own . clus ) == 1 and feat == clus_own . clus [ 0 ]: rs_own = 1 else : rs_own = ( cov_own / sigmas [ i ]) ** 2 rs_others = [] for j , clus_other in self . clusters . items (): if j == i : continue cov_other = np . dot ( clus_other . eigvecs [:, 0 ], self . corr_df . loc [ feat , clus_other . clus ] . values . T , ) rs = ( cov_other / sigmas [ j ]) ** 2 rs_others . append ( rs ) rs_nc = max ( rs_others ) if len ( rs_others ) > 0 else 0 row += [ rs_own , rs_nc , ( 1 - rs_own ) / ( 1 - rs_nc )] rs_table . loc [ n_row ] = row n_row += 1 return rs_table","title":"association_eval_varclus"},{"location":"api/data_analyzer/association_eval_varclus.html#_1","text":"Classes class VarClusHiSpark ( df, feat_list=None, maxeigval2=1, maxclus=None, n_rs=0) This class is a scalable version of VarClusHi library to perform variable clustering on PySpark dataframes with necessary optimizations, and sampling is not required. Variable Clustering groups attributes that are as correlated as possible among themselves within a cluster and as uncorrelated as possible with attribute in other clusters. By default, it begins with all variables in a single cluster. It then repeats the following steps: 1. A cluster is chosen for splitting. This cluster has the largest eigenvalue associated with the 2nd PC 2. Find the first 2 PCs of the chosen cluster and split into 2 clusters, then perform an orthoblique rotation (raw quartimax rotation on the eigenvectors) 3. Variables are iteratively re-assigned to clusters to maximize the variance: (a) Nearest Component Sorting (NCS) Phase: In each iteration, the cluster components are computed. Each variable is assigned to the rotated component with which it has the highest squared correlation (b) Search Phase: Assign each variable into different cluster to see if this increase the variance explained. If variable is re-assigned here, the components of the clusters involved are recomputed before next variable is tested The procedure stops when the 2nd eigenvalue of each cluster is smaller than maxeigval2 parameter.","title":""},{"location":"api/data_analyzer/association_evaluator.html","text":"association_evaluator This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target variable. Association between attributes is measured by: - correlation_matrix - variable_clustering Association between an attribute and binary target is measured by: - IV_calculation - IG_calculation Expand source code # coding=utf-8 \"\"\" This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target variable. Association between attributes is measured by: - correlation_matrix - variable_clustering Association between an attribute and binary target is measured by: - IV_calculation - IG_calculation \"\"\" import math import warnings import pandas as pd import pyspark from pyspark.ml.feature import VectorAssembler from pyspark.ml.stat import Correlation from pyspark.sql import Window from pyspark.sql import functions as F from anovos.data_analyzer.stats_generator import uniqueCount_computation from anovos.data_analyzer.association_eval_varclus import VarClusHiSpark from anovos.data_ingest.data_sampling import data_sample from anovos.data_transformer.transformers import ( attribute_binning , cat_to_num_unsupervised , imputation_MMM , monotonic_binning , ) from anovos.shared.utils import attributeType_segregation def correlation_matrix ( spark , idf , list_of_cols = \"all\" , drop_cols = [], use_sampling = False , sample_size = 1000000 , print_impact = False , ): \"\"\" This function calculates correlation coefficient statistical, which measures the strength of the relationship between the relative movements of two attributes. Pearson\u2019s correlation coefficient is a standard approach of measuring correlation between two variables. This function supports numerical columns only. If Dataframe contains categorical columns also then those columns must be first converted to numerical columns. Anovos has multiple functions to help convert categorical columns into numerical columns. Functions cat_to_num_supervised and cat_to_num_unsupervised can be used for this. Some data cleaning treatment can also be done on categorical columns before converting them to numerical columns. Few functions to help in columns treatment are outlier_categories, measure_of_cardinality, IDness_detection etc. This correlation_matrix function returns a correlation matrix dataframe of schema \u2013 attribute, <attribute_names>. Correlation between attribute X and Y can be found at intersection of a) row with value X in \u2018attribute\u2019 column and b) column\u2018Y\u2019 (or row with value Y in \u2018attribute\u2019 column and column \u2018X\u2019). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) use_sampling True, False This argument is to tell function whether to compute correlation matrix on full dataframe or only on small sample of dataframe, sample size is decided by another argument called sample_size.(Default value = False) sample_size int If use_sampling is True then sample size is decided by this argument.(Default value = 1000000) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute,*attribute_names] \"\"\" num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if use_sampling : if idf . count () > sample_size : warnings . warn ( \"Using sampling. Only \" + str ( sample_size ) + \" random sampled rows are considered.\" ) idf = data_sample ( idf , fraction = float ( sample_size ) / idf . count (), method_type = \"random\" ) assembler = VectorAssembler ( inputCols = list_of_cols , outputCol = \"features\" , handleInvalid = \"skip\" ) idf_vector = assembler . transform ( idf ) . select ( \"features\" ) matrix = Correlation . corr ( idf_vector , \"features\" , \"pearson\" ) result = matrix . collect ()[ 0 ][ \"pearson(features)\" ] . values odf_pd = pd . DataFrame ( result . reshape ( - 1 , len ( list_of_cols )), columns = list_of_cols , index = list_of_cols ) odf_pd [ \"attribute\" ] = odf_pd . index list_of_cols . sort () odf = ( spark . createDataFrame ( odf_pd ) . select ([ \"attribute\" ] + list_of_cols ) . orderBy ( \"attribute\" ) ) if print_impact : odf . show ( odf . count ()) return odf def variable_clustering ( spark , idf , list_of_cols = \"all\" , drop_cols = [], stats_mode = {}, persist = True , print_impact = False , ): \"\"\" This function performs Variable Clustering with necessary pre-processing techniques, including low-cardinality columns removal, categorical-to-numerical transformation and null values imputation. It works as a wrapper of VarClusHiSpark class which groups correlated attributes within a cluster and assign uncorrelated attributes into other clusters. For more details on the algorithm, please check anovos.data_analyzer.association_eval_varclus It returns a Spark Dataframe with schema \u2013 Cluster, Attribute, RS_Ratio. Attributes similar to each other are grouped together with the same cluster id. The attribute with the lowest (1 \u2014 RS_Ratio) can be chosen as a representative of the cluster while discarding the other attributes from that cluster. This can also help in achieving the dimension reduction, if required. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. This is used for MMM imputation as Variable Clustering doesn\u2019t work with missing values. (Default value = {}) persist Boolean argument - True or False. This argument is used to determine whether to persist on pre-processing (low-cardinality columns removal, categorical-to-numerical transformation and null values imputation) results of input dataset. persist=True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [Cluster, Attribute, RS_Ratio] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if persist : idf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] idf = idf . select ( list_of_cols ) cat_cols = attributeType_segregation ( idf )[ 1 ] for i in idf . dtypes : if i [ 1 ] . startswith ( \"decimal\" ): idf = idf . withColumn ( i [ 0 ], F . col ( i [ 0 ]) . cast ( \"double\" )) idf_encoded = cat_to_num_unsupervised ( spark , idf , list_of_cols = cat_cols , method_type = \"label_encoding\" ) num_cols = attributeType_segregation ( idf_encoded )[ 0 ] idf_encoded = idf_encoded . select ( num_cols ) idf_imputed = imputation_MMM ( spark , idf_encoded , stats_mode = stats_mode ) if persist : idf_imputed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () idf . unpersist () vc = VarClusHiSpark ( idf_imputed , maxeigval2 = 1 , maxclus = None ) vc . _varclusspark ( spark ) odf_pd = vc . _rsquarespark () odf = spark . createDataFrame ( odf_pd ) . select ( \"Cluster\" , F . col ( \"Variable\" ) . alias ( \"Attribute\" ), F . round ( F . col ( \"RS_Ratio\" ), 4 ) . alias ( \"RS_Ratio\" ), ) if print_impact : odf . show ( odf . count ()) if persist : idf_imputed . unpersist () return odf def IV_calculation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , encoding_configs = { \"bin_method\" : \"equal_frequency\" , \"bin_size\" : 10 , \"monotonicity_check\" : 0 , }, print_impact = False , ): \"\"\" Information Value (IV) is simple and powerful technique to conduct attribute relevance analysis. It measures how well an attribute is able to distinguish between a binary target variable i.e. label 0 from label 1, and hence helps in ranking attributes on the basis of their importance. In the heart of IV methodology are groups (bins) of observations. For categorical attributes, usually each category is a bin while numerical attributes need to be split into categories. IV = \u2211 (% of non-events - % of events) * WOE <br>where: <br>WOE = In(% of non-events \u2797 % of events) <br>% of event = % label 1 in a bin <br>% of non-event = % label 0 in a bin General rule of thumb while creating the bins are that a) each bin should have at least 5% of the observations, b) the WOE should be monotonic, i.e. either growing or decreasing with the bins, and c) missing values should be binned separately. An article from listendata.com can be referred for good understanding of IV & WOE concepts. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, iv] \"\"\" if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if idf . where ( F . col ( label_col ) == event_label ) . count () == 0 : raise TypeError ( \"Invalid input for Event Label Value\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if ( len ( num_cols ) > 0 ) & bool ( encoding_configs ): bin_size = encoding_configs [ \"bin_size\" ] bin_method = encoding_configs [ \"bin_method\" ] monotonicity_check = encoding_configs [ \"monotonicity_check\" ] if monotonicity_check == 1 : idf_encoded = monotonic_binning ( spark , idf , num_cols , [], label_col , event_label , bin_method , bin_size ) else : idf_encoded = attribute_binning ( spark , idf , num_cols , label_col , bin_method , bin_size ) else : idf_encoded = idf list_df = [] idf_encoded = idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) for col in list_of_cols : df_agg = ( idf_encoded . select ( col , label_col ) . groupby ( col ) . agg ( F . count ( F . when ( F . col ( label_col ) != event_label , F . col ( label_col )) ) . alias ( \"label_0\" ), F . count ( F . when ( F . col ( label_col ) == event_label , F . col ( label_col )) ) . alias ( \"label_1\" ), ) . withColumn ( \"label_0_total\" , F . sum ( F . col ( \"label_0\" )) . over ( Window . partitionBy ()) ) . withColumn ( \"label_1_total\" , F . sum ( F . col ( \"label_1\" )) . over ( Window . partitionBy ()) ) ) out_df = ( df_agg . withColumn ( \"event_pcr\" , F . col ( \"label_1\" ) / F . col ( \"label_1_total\" )) . withColumn ( \"nonevent_pcr\" , F . col ( \"label_0\" ) / F . col ( \"label_0_total\" )) . withColumn ( \"diff_event\" , F . col ( \"nonevent_pcr\" ) - F . col ( \"event_pcr\" )) . withColumn ( \"const\" , F . lit ( 0.5 )) . withColumn ( \"woe\" , F . when ( ( F . col ( \"nonevent_pcr\" ) != 0 ) & ( F . col ( \"event_pcr\" ) != 0 ), F . log ( F . col ( \"nonevent_pcr\" ) / F . col ( \"event_pcr\" )), ) . otherwise ( F . log ( (( F . col ( \"label_0\" ) + F . col ( \"const\" )) / F . col ( \"label_0_total\" )) / (( F . col ( \"label_1\" ) + F . col ( \"const\" )) / F . col ( \"label_1_total\" )) ) ), ) . withColumn ( \"iv_single\" , F . col ( \"woe\" ) * F . col ( \"diff_event\" )) . withColumn ( \"iv\" , F . sum ( F . col ( \"iv_single\" )) . over ( Window . partitionBy ())) . withColumn ( \"attribute\" , F . lit ( str ( col ))) . select ( \"attribute\" , \"iv\" ) . distinct () ) list_df . append ( out_df ) def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) odf = unionAll ( list_df ) if print_impact : odf . show ( odf . count ()) idf_encoded . unpersist () return odf def IG_calculation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , encoding_configs = { \"bin_method\" : \"equal_frequency\" , \"bin_size\" : 10 , \"monotonicity_check\" : 0 , }, print_impact = False , ): \"\"\" Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event*log\u2061(%event)-(1-%event)*log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i*log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i )*log\u2061(1-%\u3016event\u3017_i) Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, id] \"\"\" if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if idf . where ( F . col ( label_col ) == event_label ) . count () == 0 : raise TypeError ( \"Invalid input for Event Label Value\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if ( len ( num_cols ) > 0 ) & bool ( encoding_configs ): bin_size = encoding_configs [ \"bin_size\" ] bin_method = encoding_configs [ \"bin_method\" ] monotonicity_check = encoding_configs [ \"monotonicity_check\" ] if monotonicity_check == 1 : idf_encoded = monotonic_binning ( spark , idf , num_cols , [], label_col , event_label , bin_method , bin_size ) else : idf_encoded = attribute_binning ( spark , idf , num_cols , label_col , bin_method , bin_size ) else : idf_encoded = idf output = [] total_event = idf . where ( F . col ( label_col ) == event_label ) . count () / idf . count () total_entropy = - ( total_event * math . log2 ( total_event ) + (( 1 - total_event ) * math . log2 (( 1 - total_event ))) ) idf_encoded = idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) for col in list_of_cols : idf_entropy = ( ( idf_encoded . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . agg ( F . sum ( F . col ( label_col )) . alias ( \"event_count\" ), F . count ( F . col ( label_col )) . alias ( \"total_count\" ), ) . withColumn ( \"event_pct\" , F . col ( \"event_count\" ) / F . col ( \"total_count\" )) . withColumn ( \"segment_pct\" , F . col ( \"total_count\" ) / F . sum ( \"total_count\" ) . over ( Window . partitionBy ()), ) . withColumn ( \"entropy\" , - F . col ( \"segment_pct\" ) * ( ( F . col ( \"event_pct\" ) * F . log2 ( F . col ( \"event_pct\" ))) + (( 1 - F . col ( \"event_pct\" )) * F . log2 (( 1 - F . col ( \"event_pct\" )))) ), ) ) . groupBy () . agg ( F . sum ( F . col ( \"entropy\" )) . alias ( \"entropy_sum\" )) . withColumn ( \"attribute\" , F . lit ( str ( col ))) . withColumn ( \"entropy_total\" , F . lit ( float ( total_entropy ))) . withColumn ( \"ig\" , F . col ( \"entropy_total\" ) - F . col ( \"entropy_sum\" )) . select ( \"attribute\" , \"ig\" ) ) output . append ( idf_entropy ) def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) odf = unionAll ( output ) if print_impact : odf . show ( odf . count ()) idf_encoded . unpersist () return odf Functions def IG_calculation ( spark, idf, list_of_cols='all', drop_cols=[], label_col='label', event_label=1, encoding_configs={'bin_method': 'equal_frequency', 'bin_size': 10, 'monotonicity_check': 0}, print_impact=False) Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event log\u2061(%event)-(1-%event) log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i ) log\u2061(1-%\u3016event\u3017_i) Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, id] Expand source code def IG_calculation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , encoding_configs = { \"bin_method\" : \"equal_frequency\" , \"bin_size\" : 10 , \"monotonicity_check\" : 0 , }, print_impact = False , ): \"\"\" Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event*log\u2061(%event)-(1-%event)*log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i*log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i )*log\u2061(1-%\u3016event\u3017_i) Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, id] \"\"\" if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if idf . where ( F . col ( label_col ) == event_label ) . count () == 0 : raise TypeError ( \"Invalid input for Event Label Value\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if ( len ( num_cols ) > 0 ) & bool ( encoding_configs ): bin_size = encoding_configs [ \"bin_size\" ] bin_method = encoding_configs [ \"bin_method\" ] monotonicity_check = encoding_configs [ \"monotonicity_check\" ] if monotonicity_check == 1 : idf_encoded = monotonic_binning ( spark , idf , num_cols , [], label_col , event_label , bin_method , bin_size ) else : idf_encoded = attribute_binning ( spark , idf , num_cols , label_col , bin_method , bin_size ) else : idf_encoded = idf output = [] total_event = idf . where ( F . col ( label_col ) == event_label ) . count () / idf . count () total_entropy = - ( total_event * math . log2 ( total_event ) + (( 1 - total_event ) * math . log2 (( 1 - total_event ))) ) idf_encoded = idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) for col in list_of_cols : idf_entropy = ( ( idf_encoded . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . agg ( F . sum ( F . col ( label_col )) . alias ( \"event_count\" ), F . count ( F . col ( label_col )) . alias ( \"total_count\" ), ) . withColumn ( \"event_pct\" , F . col ( \"event_count\" ) / F . col ( \"total_count\" )) . withColumn ( \"segment_pct\" , F . col ( \"total_count\" ) / F . sum ( \"total_count\" ) . over ( Window . partitionBy ()), ) . withColumn ( \"entropy\" , - F . col ( \"segment_pct\" ) * ( ( F . col ( \"event_pct\" ) * F . log2 ( F . col ( \"event_pct\" ))) + (( 1 - F . col ( \"event_pct\" )) * F . log2 (( 1 - F . col ( \"event_pct\" )))) ), ) ) . groupBy () . agg ( F . sum ( F . col ( \"entropy\" )) . alias ( \"entropy_sum\" )) . withColumn ( \"attribute\" , F . lit ( str ( col ))) . withColumn ( \"entropy_total\" , F . lit ( float ( total_entropy ))) . withColumn ( \"ig\" , F . col ( \"entropy_total\" ) - F . col ( \"entropy_sum\" )) . select ( \"attribute\" , \"ig\" ) ) output . append ( idf_entropy ) def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) odf = unionAll ( output ) if print_impact : odf . show ( odf . count ()) idf_encoded . unpersist () return odf def IV_calculation ( spark, idf, list_of_cols='all', drop_cols=[], label_col='label', event_label=1, encoding_configs={'bin_method': 'equal_frequency', 'bin_size': 10, 'monotonicity_check': 0}, print_impact=False) Information Value (IV) is simple and powerful technique to conduct attribute relevance analysis. It measures how well an attribute is able to distinguish between a binary target variable i.e. label 0 from label 1, and hence helps in ranking attributes on the basis of their importance. In the heart of IV methodology are groups (bins) of observations. For categorical attributes, usually each category is a bin while numerical attributes need to be split into categories. IV = \u2211 (% of non-events - % of events) * WOE where: WOE = In(% of non-events \u2797 % of events) % of event = % label 1 in a bin % of non-event = % label 0 in a bin General rule of thumb while creating the bins are that a) each bin should have at least 5% of the observations, b) the WOE should be monotonic, i.e. either growing or decreasing with the bins, and c) missing values should be binned separately. An article from listendata.com can be referred for good understanding of IV & WOE concepts. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, iv] Expand source code def IV_calculation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , encoding_configs = { \"bin_method\" : \"equal_frequency\" , \"bin_size\" : 10 , \"monotonicity_check\" : 0 , }, print_impact = False , ): \"\"\" Information Value (IV) is simple and powerful technique to conduct attribute relevance analysis. It measures how well an attribute is able to distinguish between a binary target variable i.e. label 0 from label 1, and hence helps in ranking attributes on the basis of their importance. In the heart of IV methodology are groups (bins) of observations. For categorical attributes, usually each category is a bin while numerical attributes need to be split into categories. IV = \u2211 (% of non-events - % of events) * WOE <br>where: <br>WOE = In(% of non-events \u2797 % of events) <br>% of event = % label 1 in a bin <br>% of non-event = % label 0 in a bin General rule of thumb while creating the bins are that a) each bin should have at least 5% of the observations, b) the WOE should be monotonic, i.e. either growing or decreasing with the bins, and c) missing values should be binned separately. An article from listendata.com can be referred for good understanding of IV & WOE concepts. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, iv] \"\"\" if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if idf . where ( F . col ( label_col ) == event_label ) . count () == 0 : raise TypeError ( \"Invalid input for Event Label Value\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if ( len ( num_cols ) > 0 ) & bool ( encoding_configs ): bin_size = encoding_configs [ \"bin_size\" ] bin_method = encoding_configs [ \"bin_method\" ] monotonicity_check = encoding_configs [ \"monotonicity_check\" ] if monotonicity_check == 1 : idf_encoded = monotonic_binning ( spark , idf , num_cols , [], label_col , event_label , bin_method , bin_size ) else : idf_encoded = attribute_binning ( spark , idf , num_cols , label_col , bin_method , bin_size ) else : idf_encoded = idf list_df = [] idf_encoded = idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) for col in list_of_cols : df_agg = ( idf_encoded . select ( col , label_col ) . groupby ( col ) . agg ( F . count ( F . when ( F . col ( label_col ) != event_label , F . col ( label_col )) ) . alias ( \"label_0\" ), F . count ( F . when ( F . col ( label_col ) == event_label , F . col ( label_col )) ) . alias ( \"label_1\" ), ) . withColumn ( \"label_0_total\" , F . sum ( F . col ( \"label_0\" )) . over ( Window . partitionBy ()) ) . withColumn ( \"label_1_total\" , F . sum ( F . col ( \"label_1\" )) . over ( Window . partitionBy ()) ) ) out_df = ( df_agg . withColumn ( \"event_pcr\" , F . col ( \"label_1\" ) / F . col ( \"label_1_total\" )) . withColumn ( \"nonevent_pcr\" , F . col ( \"label_0\" ) / F . col ( \"label_0_total\" )) . withColumn ( \"diff_event\" , F . col ( \"nonevent_pcr\" ) - F . col ( \"event_pcr\" )) . withColumn ( \"const\" , F . lit ( 0.5 )) . withColumn ( \"woe\" , F . when ( ( F . col ( \"nonevent_pcr\" ) != 0 ) & ( F . col ( \"event_pcr\" ) != 0 ), F . log ( F . col ( \"nonevent_pcr\" ) / F . col ( \"event_pcr\" )), ) . otherwise ( F . log ( (( F . col ( \"label_0\" ) + F . col ( \"const\" )) / F . col ( \"label_0_total\" )) / (( F . col ( \"label_1\" ) + F . col ( \"const\" )) / F . col ( \"label_1_total\" )) ) ), ) . withColumn ( \"iv_single\" , F . col ( \"woe\" ) * F . col ( \"diff_event\" )) . withColumn ( \"iv\" , F . sum ( F . col ( \"iv_single\" )) . over ( Window . partitionBy ())) . withColumn ( \"attribute\" , F . lit ( str ( col ))) . select ( \"attribute\" , \"iv\" ) . distinct () ) list_df . append ( out_df ) def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) odf = unionAll ( list_df ) if print_impact : odf . show ( odf . count ()) idf_encoded . unpersist () return odf def correlation_matrix ( spark, idf, list_of_cols='all', drop_cols=[], use_sampling=False, sample_size=1000000, print_impact=False) This function calculates correlation coefficient statistical, which measures the strength of the relationship between the relative movements of two attributes. Pearson\u2019s correlation coefficient is a standard approach of measuring correlation between two variables. This function supports numerical columns only. If Dataframe contains categorical columns also then those columns must be first converted to numerical columns. Anovos has multiple functions to help convert categorical columns into numerical columns. Functions cat_to_num_supervised and cat_to_num_unsupervised can be used for this. Some data cleaning treatment can also be done on categorical columns before converting them to numerical columns. Few functions to help in columns treatment are outlier_categories, measure_of_cardinality, IDness_detection etc. This correlation_matrix function returns a correlation matrix dataframe of schema \u2013 attribute, . Correlation between attribute X and Y can be found at intersection of a) row with value X in \u2018attribute\u2019 column and b) column\u2018Y\u2019 (or row with value Y in \u2018attribute\u2019 column and column \u2018X\u2019). Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) use_sampling True, False This argument is to tell function whether to compute correlation matrix on full dataframe or only on small sample of dataframe, sample size is decided by another argument called sample_size.(Default value = False) sample_size int If use_sampling is True then sample size is decided by this argument.(Default value = 1000000) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute,*attribute_names] Expand source code def correlation_matrix ( spark , idf , list_of_cols = \"all\" , drop_cols = [], use_sampling = False , sample_size = 1000000 , print_impact = False , ): \"\"\" This function calculates correlation coefficient statistical, which measures the strength of the relationship between the relative movements of two attributes. Pearson\u2019s correlation coefficient is a standard approach of measuring correlation between two variables. This function supports numerical columns only. If Dataframe contains categorical columns also then those columns must be first converted to numerical columns. Anovos has multiple functions to help convert categorical columns into numerical columns. Functions cat_to_num_supervised and cat_to_num_unsupervised can be used for this. Some data cleaning treatment can also be done on categorical columns before converting them to numerical columns. Few functions to help in columns treatment are outlier_categories, measure_of_cardinality, IDness_detection etc. This correlation_matrix function returns a correlation matrix dataframe of schema \u2013 attribute, <attribute_names>. Correlation between attribute X and Y can be found at intersection of a) row with value X in \u2018attribute\u2019 column and b) column\u2018Y\u2019 (or row with value Y in \u2018attribute\u2019 column and column \u2018X\u2019). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) use_sampling True, False This argument is to tell function whether to compute correlation matrix on full dataframe or only on small sample of dataframe, sample size is decided by another argument called sample_size.(Default value = False) sample_size int If use_sampling is True then sample size is decided by this argument.(Default value = 1000000) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute,*attribute_names] \"\"\" num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if use_sampling : if idf . count () > sample_size : warnings . warn ( \"Using sampling. Only \" + str ( sample_size ) + \" random sampled rows are considered.\" ) idf = data_sample ( idf , fraction = float ( sample_size ) / idf . count (), method_type = \"random\" ) assembler = VectorAssembler ( inputCols = list_of_cols , outputCol = \"features\" , handleInvalid = \"skip\" ) idf_vector = assembler . transform ( idf ) . select ( \"features\" ) matrix = Correlation . corr ( idf_vector , \"features\" , \"pearson\" ) result = matrix . collect ()[ 0 ][ \"pearson(features)\" ] . values odf_pd = pd . DataFrame ( result . reshape ( - 1 , len ( list_of_cols )), columns = list_of_cols , index = list_of_cols ) odf_pd [ \"attribute\" ] = odf_pd . index list_of_cols . sort () odf = ( spark . createDataFrame ( odf_pd ) . select ([ \"attribute\" ] + list_of_cols ) . orderBy ( \"attribute\" ) ) if print_impact : odf . show ( odf . count ()) return odf def variable_clustering ( spark, idf, list_of_cols='all', drop_cols=[], stats_mode={}, persist=True, print_impact=False) This function performs Variable Clustering with necessary pre-processing techniques, including low-cardinality columns removal, categorical-to-numerical transformation and null values imputation. It works as a wrapper of VarClusHiSpark class which groups correlated attributes within a cluster and assign uncorrelated attributes into other clusters. For more details on the algorithm, please check anovos.data_analyzer.association_eval_varclus It returns a Spark Dataframe with schema \u2013 Cluster, Attribute, RS_Ratio. Attributes similar to each other are grouped together with the same cluster id. The attribute with the lowest (1 \u2014 RS_Ratio) can be chosen as a representative of the cluster while discarding the other attributes from that cluster. This can also help in achieving the dimension reduction, if required. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. This is used for MMM imputation as Variable Clustering doesn\u2019t work with missing values. (Default value = {}) persist Boolean argument - True or False. This argument is used to determine whether to persist on pre-processing (low-cardinality columns removal, categorical-to-numerical transformation and null values imputation) results of input dataset. persist=True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [Cluster, Attribute, RS_Ratio] Expand source code def variable_clustering ( spark , idf , list_of_cols = \"all\" , drop_cols = [], stats_mode = {}, persist = True , print_impact = False , ): \"\"\" This function performs Variable Clustering with necessary pre-processing techniques, including low-cardinality columns removal, categorical-to-numerical transformation and null values imputation. It works as a wrapper of VarClusHiSpark class which groups correlated attributes within a cluster and assign uncorrelated attributes into other clusters. For more details on the algorithm, please check anovos.data_analyzer.association_eval_varclus It returns a Spark Dataframe with schema \u2013 Cluster, Attribute, RS_Ratio. Attributes similar to each other are grouped together with the same cluster id. The attribute with the lowest (1 \u2014 RS_Ratio) can be chosen as a representative of the cluster while discarding the other attributes from that cluster. This can also help in achieving the dimension reduction, if required. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. This is used for MMM imputation as Variable Clustering doesn\u2019t work with missing values. (Default value = {}) persist Boolean argument - True or False. This argument is used to determine whether to persist on pre-processing (low-cardinality columns removal, categorical-to-numerical transformation and null values imputation) results of input dataset. persist=True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [Cluster, Attribute, RS_Ratio] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if persist : idf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] idf = idf . select ( list_of_cols ) cat_cols = attributeType_segregation ( idf )[ 1 ] for i in idf . dtypes : if i [ 1 ] . startswith ( \"decimal\" ): idf = idf . withColumn ( i [ 0 ], F . col ( i [ 0 ]) . cast ( \"double\" )) idf_encoded = cat_to_num_unsupervised ( spark , idf , list_of_cols = cat_cols , method_type = \"label_encoding\" ) num_cols = attributeType_segregation ( idf_encoded )[ 0 ] idf_encoded = idf_encoded . select ( num_cols ) idf_imputed = imputation_MMM ( spark , idf_encoded , stats_mode = stats_mode ) if persist : idf_imputed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () idf . unpersist () vc = VarClusHiSpark ( idf_imputed , maxeigval2 = 1 , maxclus = None ) vc . _varclusspark ( spark ) odf_pd = vc . _rsquarespark () odf = spark . createDataFrame ( odf_pd ) . select ( \"Cluster\" , F . col ( \"Variable\" ) . alias ( \"Attribute\" ), F . round ( F . col ( \"RS_Ratio\" ), 4 ) . alias ( \"RS_Ratio\" ), ) if print_impact : odf . show ( odf . count ()) if persist : idf_imputed . unpersist () return odf","title":"<code>association_evaluator</code>"},{"location":"api/data_analyzer/association_evaluator.html#association_evaluator","text":"This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target variable. Association between attributes is measured by: - correlation_matrix - variable_clustering Association between an attribute and binary target is measured by: - IV_calculation - IG_calculation Expand source code # coding=utf-8 \"\"\" This submodule focuses on understanding the interaction between different attributes and/or the relationship between an attribute & the binary target variable. Association between attributes is measured by: - correlation_matrix - variable_clustering Association between an attribute and binary target is measured by: - IV_calculation - IG_calculation \"\"\" import math import warnings import pandas as pd import pyspark from pyspark.ml.feature import VectorAssembler from pyspark.ml.stat import Correlation from pyspark.sql import Window from pyspark.sql import functions as F from anovos.data_analyzer.stats_generator import uniqueCount_computation from anovos.data_analyzer.association_eval_varclus import VarClusHiSpark from anovos.data_ingest.data_sampling import data_sample from anovos.data_transformer.transformers import ( attribute_binning , cat_to_num_unsupervised , imputation_MMM , monotonic_binning , ) from anovos.shared.utils import attributeType_segregation def correlation_matrix ( spark , idf , list_of_cols = \"all\" , drop_cols = [], use_sampling = False , sample_size = 1000000 , print_impact = False , ): \"\"\" This function calculates correlation coefficient statistical, which measures the strength of the relationship between the relative movements of two attributes. Pearson\u2019s correlation coefficient is a standard approach of measuring correlation between two variables. This function supports numerical columns only. If Dataframe contains categorical columns also then those columns must be first converted to numerical columns. Anovos has multiple functions to help convert categorical columns into numerical columns. Functions cat_to_num_supervised and cat_to_num_unsupervised can be used for this. Some data cleaning treatment can also be done on categorical columns before converting them to numerical columns. Few functions to help in columns treatment are outlier_categories, measure_of_cardinality, IDness_detection etc. This correlation_matrix function returns a correlation matrix dataframe of schema \u2013 attribute, <attribute_names>. Correlation between attribute X and Y can be found at intersection of a) row with value X in \u2018attribute\u2019 column and b) column\u2018Y\u2019 (or row with value Y in \u2018attribute\u2019 column and column \u2018X\u2019). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) use_sampling True, False This argument is to tell function whether to compute correlation matrix on full dataframe or only on small sample of dataframe, sample size is decided by another argument called sample_size.(Default value = False) sample_size int If use_sampling is True then sample size is decided by this argument.(Default value = 1000000) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute,*attribute_names] \"\"\" num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if use_sampling : if idf . count () > sample_size : warnings . warn ( \"Using sampling. Only \" + str ( sample_size ) + \" random sampled rows are considered.\" ) idf = data_sample ( idf , fraction = float ( sample_size ) / idf . count (), method_type = \"random\" ) assembler = VectorAssembler ( inputCols = list_of_cols , outputCol = \"features\" , handleInvalid = \"skip\" ) idf_vector = assembler . transform ( idf ) . select ( \"features\" ) matrix = Correlation . corr ( idf_vector , \"features\" , \"pearson\" ) result = matrix . collect ()[ 0 ][ \"pearson(features)\" ] . values odf_pd = pd . DataFrame ( result . reshape ( - 1 , len ( list_of_cols )), columns = list_of_cols , index = list_of_cols ) odf_pd [ \"attribute\" ] = odf_pd . index list_of_cols . sort () odf = ( spark . createDataFrame ( odf_pd ) . select ([ \"attribute\" ] + list_of_cols ) . orderBy ( \"attribute\" ) ) if print_impact : odf . show ( odf . count ()) return odf def variable_clustering ( spark , idf , list_of_cols = \"all\" , drop_cols = [], stats_mode = {}, persist = True , print_impact = False , ): \"\"\" This function performs Variable Clustering with necessary pre-processing techniques, including low-cardinality columns removal, categorical-to-numerical transformation and null values imputation. It works as a wrapper of VarClusHiSpark class which groups correlated attributes within a cluster and assign uncorrelated attributes into other clusters. For more details on the algorithm, please check anovos.data_analyzer.association_eval_varclus It returns a Spark Dataframe with schema \u2013 Cluster, Attribute, RS_Ratio. Attributes similar to each other are grouped together with the same cluster id. The attribute with the lowest (1 \u2014 RS_Ratio) can be chosen as a representative of the cluster while discarding the other attributes from that cluster. This can also help in achieving the dimension reduction, if required. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. This is used for MMM imputation as Variable Clustering doesn\u2019t work with missing values. (Default value = {}) persist Boolean argument - True or False. This argument is used to determine whether to persist on pre-processing (low-cardinality columns removal, categorical-to-numerical transformation and null values imputation) results of input dataset. persist=True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [Cluster, Attribute, RS_Ratio] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if persist : idf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] idf = idf . select ( list_of_cols ) cat_cols = attributeType_segregation ( idf )[ 1 ] for i in idf . dtypes : if i [ 1 ] . startswith ( \"decimal\" ): idf = idf . withColumn ( i [ 0 ], F . col ( i [ 0 ]) . cast ( \"double\" )) idf_encoded = cat_to_num_unsupervised ( spark , idf , list_of_cols = cat_cols , method_type = \"label_encoding\" ) num_cols = attributeType_segregation ( idf_encoded )[ 0 ] idf_encoded = idf_encoded . select ( num_cols ) idf_imputed = imputation_MMM ( spark , idf_encoded , stats_mode = stats_mode ) if persist : idf_imputed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () idf . unpersist () vc = VarClusHiSpark ( idf_imputed , maxeigval2 = 1 , maxclus = None ) vc . _varclusspark ( spark ) odf_pd = vc . _rsquarespark () odf = spark . createDataFrame ( odf_pd ) . select ( \"Cluster\" , F . col ( \"Variable\" ) . alias ( \"Attribute\" ), F . round ( F . col ( \"RS_Ratio\" ), 4 ) . alias ( \"RS_Ratio\" ), ) if print_impact : odf . show ( odf . count ()) if persist : idf_imputed . unpersist () return odf def IV_calculation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , encoding_configs = { \"bin_method\" : \"equal_frequency\" , \"bin_size\" : 10 , \"monotonicity_check\" : 0 , }, print_impact = False , ): \"\"\" Information Value (IV) is simple and powerful technique to conduct attribute relevance analysis. It measures how well an attribute is able to distinguish between a binary target variable i.e. label 0 from label 1, and hence helps in ranking attributes on the basis of their importance. In the heart of IV methodology are groups (bins) of observations. For categorical attributes, usually each category is a bin while numerical attributes need to be split into categories. IV = \u2211 (% of non-events - % of events) * WOE <br>where: <br>WOE = In(% of non-events \u2797 % of events) <br>% of event = % label 1 in a bin <br>% of non-event = % label 0 in a bin General rule of thumb while creating the bins are that a) each bin should have at least 5% of the observations, b) the WOE should be monotonic, i.e. either growing or decreasing with the bins, and c) missing values should be binned separately. An article from listendata.com can be referred for good understanding of IV & WOE concepts. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, iv] \"\"\" if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if idf . where ( F . col ( label_col ) == event_label ) . count () == 0 : raise TypeError ( \"Invalid input for Event Label Value\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if ( len ( num_cols ) > 0 ) & bool ( encoding_configs ): bin_size = encoding_configs [ \"bin_size\" ] bin_method = encoding_configs [ \"bin_method\" ] monotonicity_check = encoding_configs [ \"monotonicity_check\" ] if monotonicity_check == 1 : idf_encoded = monotonic_binning ( spark , idf , num_cols , [], label_col , event_label , bin_method , bin_size ) else : idf_encoded = attribute_binning ( spark , idf , num_cols , label_col , bin_method , bin_size ) else : idf_encoded = idf list_df = [] idf_encoded = idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) for col in list_of_cols : df_agg = ( idf_encoded . select ( col , label_col ) . groupby ( col ) . agg ( F . count ( F . when ( F . col ( label_col ) != event_label , F . col ( label_col )) ) . alias ( \"label_0\" ), F . count ( F . when ( F . col ( label_col ) == event_label , F . col ( label_col )) ) . alias ( \"label_1\" ), ) . withColumn ( \"label_0_total\" , F . sum ( F . col ( \"label_0\" )) . over ( Window . partitionBy ()) ) . withColumn ( \"label_1_total\" , F . sum ( F . col ( \"label_1\" )) . over ( Window . partitionBy ()) ) ) out_df = ( df_agg . withColumn ( \"event_pcr\" , F . col ( \"label_1\" ) / F . col ( \"label_1_total\" )) . withColumn ( \"nonevent_pcr\" , F . col ( \"label_0\" ) / F . col ( \"label_0_total\" )) . withColumn ( \"diff_event\" , F . col ( \"nonevent_pcr\" ) - F . col ( \"event_pcr\" )) . withColumn ( \"const\" , F . lit ( 0.5 )) . withColumn ( \"woe\" , F . when ( ( F . col ( \"nonevent_pcr\" ) != 0 ) & ( F . col ( \"event_pcr\" ) != 0 ), F . log ( F . col ( \"nonevent_pcr\" ) / F . col ( \"event_pcr\" )), ) . otherwise ( F . log ( (( F . col ( \"label_0\" ) + F . col ( \"const\" )) / F . col ( \"label_0_total\" )) / (( F . col ( \"label_1\" ) + F . col ( \"const\" )) / F . col ( \"label_1_total\" )) ) ), ) . withColumn ( \"iv_single\" , F . col ( \"woe\" ) * F . col ( \"diff_event\" )) . withColumn ( \"iv\" , F . sum ( F . col ( \"iv_single\" )) . over ( Window . partitionBy ())) . withColumn ( \"attribute\" , F . lit ( str ( col ))) . select ( \"attribute\" , \"iv\" ) . distinct () ) list_df . append ( out_df ) def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) odf = unionAll ( list_df ) if print_impact : odf . show ( odf . count ()) idf_encoded . unpersist () return odf def IG_calculation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , encoding_configs = { \"bin_method\" : \"equal_frequency\" , \"bin_size\" : 10 , \"monotonicity_check\" : 0 , }, print_impact = False , ): \"\"\" Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event*log\u2061(%event)-(1-%event)*log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i*log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i )*log\u2061(1-%\u3016event\u3017_i) Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) encoding_configs Takes input in dictionary format. {} i.e. empty dict means no encoding is required. In case numerical columns are present and encoding is required, following keys shall be provided - \"bin_size\" (Default value = 10) i.e. no. of bins for converting the numerical columns to categorical, \"bin_method\" i.e. method of binning - \"equal_frequency\" or \"equal_range\" (Default value = \"equal_frequency\") and \"monotonicity_check\" 1 for monotonic binning else 0. monotonicity_check of 1 will dynamically calculate the bin_size ensuring monotonic nature but can be expensive operation (Default value = 0). print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, id] \"\"\" if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if idf . where ( F . col ( label_col ) == event_label ) . count () == 0 : raise TypeError ( \"Invalid input for Event Label Value\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if ( len ( num_cols ) > 0 ) & bool ( encoding_configs ): bin_size = encoding_configs [ \"bin_size\" ] bin_method = encoding_configs [ \"bin_method\" ] monotonicity_check = encoding_configs [ \"monotonicity_check\" ] if monotonicity_check == 1 : idf_encoded = monotonic_binning ( spark , idf , num_cols , [], label_col , event_label , bin_method , bin_size ) else : idf_encoded = attribute_binning ( spark , idf , num_cols , label_col , bin_method , bin_size ) else : idf_encoded = idf output = [] total_event = idf . where ( F . col ( label_col ) == event_label ) . count () / idf . count () total_entropy = - ( total_event * math . log2 ( total_event ) + (( 1 - total_event ) * math . log2 (( 1 - total_event ))) ) idf_encoded = idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) for col in list_of_cols : idf_entropy = ( ( idf_encoded . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . agg ( F . sum ( F . col ( label_col )) . alias ( \"event_count\" ), F . count ( F . col ( label_col )) . alias ( \"total_count\" ), ) . withColumn ( \"event_pct\" , F . col ( \"event_count\" ) / F . col ( \"total_count\" )) . withColumn ( \"segment_pct\" , F . col ( \"total_count\" ) / F . sum ( \"total_count\" ) . over ( Window . partitionBy ()), ) . withColumn ( \"entropy\" , - F . col ( \"segment_pct\" ) * ( ( F . col ( \"event_pct\" ) * F . log2 ( F . col ( \"event_pct\" ))) + (( 1 - F . col ( \"event_pct\" )) * F . log2 (( 1 - F . col ( \"event_pct\" )))) ), ) ) . groupBy () . agg ( F . sum ( F . col ( \"entropy\" )) . alias ( \"entropy_sum\" )) . withColumn ( \"attribute\" , F . lit ( str ( col ))) . withColumn ( \"entropy_total\" , F . lit ( float ( total_entropy ))) . withColumn ( \"ig\" , F . col ( \"entropy_total\" ) - F . col ( \"entropy_sum\" )) . select ( \"attribute\" , \"ig\" ) ) output . append ( idf_entropy ) def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) odf = unionAll ( output ) if print_impact : odf . show ( odf . count ()) idf_encoded . unpersist () return odf","title":"association_evaluator"},{"location":"api/data_analyzer/association_evaluator.html#functions","text":"def IG_calculation ( spark, idf, list_of_cols='all', drop_cols=[], label_col='label', event_label=1, encoding_configs={'bin_method': 'equal_frequency', 'bin_size': 10, 'monotonicity_check': 0}, print_impact=False) Information Gain (IG) is another powerful technique for feature selection analysis. Information gain is calculated by comparing the entropy of the dataset before and after a transformation (introduction of attribute in this particular case). Similar to IV calculation, each category is a bin for categorical attributes, while numerical attributes need to be split into categories. IG = Total Entropy \u2013 Entropy Total Entropy= -%event log\u2061(%event)-(1-%event) log\u2061(1-%event) Entropy = \u2211(-%\u3016event\u3017_i log\u2061(%\u3016event\u3017_i )-(1-%\u3016event\u3017_i ) log\u2061(1-%\u3016event\u3017_i)","title":"Functions"},{"location":"api/data_analyzer/geospatial_analyzer.html","text":"geospatial_analyzer This module help to analyze & summarize the geospatial related data fields which are identified through the auto-detection module. Additionally, it generates the intermediate output which are fed in to the reporting section. As a part of generation of final output, there are various functions created under this sub-module. All of them are listed below. descriptive_stats_gen lat_long_col_stats_gen geohash_col_stats_gen stats_gen_lat_long_geo geo_cluster_analysis geo_cluster_generator generate_loc_charts_processor generate_loc_charts_controller geospatial_autodetection Respective functions have sections containing the detailed definition of the parameters used for computing. Expand source code # coding=utf-8 \"\"\"This module help to analyze & summarize the geospatial related data fields which are identified through the auto-detection module. Additionally, it generates the intermediate output which are fed in to the reporting section. As a part of generation of final output, there are various functions created under this sub-module. All of them are listed below. - descriptive_stats_gen - lat_long_col_stats_gen - geohash_col_stats_gen - stats_gen_lat_long_geo - geo_cluster_analysis - geo_cluster_generator - generate_loc_charts_processor - generate_loc_charts_controller - geospatial_autodetection Respective functions have sections containing the detailed definition of the parameters used for computing. \"\"\" from anovos.shared.utils import ends_with , output_to_local , path_ak8s_modify from anovos.data_ingest import data_sampling from anovos.data_ingest.geo_auto_detection import ll_gh_cols , geo_to_latlong import pandas as pd import numpy as np from sklearn.cluster import MiniBatchKMeans from sklearn.metrics import silhouette_score from itertools import product from pathlib import Path from pyspark.sql import functions as F from sklearn.cluster import DBSCAN import subprocess import plotly.express as px import plotly.graph_objects as go import warnings warnings . filterwarnings ( \"ignore\" ) global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" blank_chart = go . Figure () blank_chart . layout . plot_bgcolor = global_plot_bg_color blank_chart . layout . paper_bgcolor = global_paper_bg_color blank_chart . update_xaxes ( visible = False ) blank_chart . update_yaxes ( visible = False ) mapbox_list = [ \"open-street-map\" , \"white-bg\" , \"carto-positron\" , \"carto-darkmatter\" , \"stamen- terrain\" , \"stamen-toner\" , \"stamen-watercolor\" , ] def descriptive_stats_gen ( df , lat_col , long_col , geohash_col , id_col , master_path , max_val ): \"\"\" This function is the base function to produce descriptive stats for geospatial fields, and save relevant outputs in csv format inside master_path. If lat_col and long_col are valid, two tables will be generated - An overall summary table: This table has two columns: \"stats\" and \"count\", and 5 rows. These 5 rows summarizes the count of distinct {lat, long} pair count, latitude and longitude and shows the most common {lat,long} pair with occurrence respectively. - A top lat-long pairs table: This table shows the top lat-long pairs based on occurrence, and max_val parameter determines the number of records. If geohash_col is valid, two tables will be generated - An overall summary table: This table has two columns: \"stats\" and \"count\", and 3 rows. These 3 rows displays the total number of distinct geohashes, precision level observed for geohashes and the most common geohash respectively. - A top geohash distribution table: This table shows the top geohash distributions based on occurrence, and max_val parameter determines the number of records. Parameters ---------- df DataFrame to be analyzed lat_col Latitude column long_col Longitude column geohash_col Geohash column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns ------- DataFrame[CSV] \"\"\" if ( lat_col is not None ) & ( long_col is not None ): dist_lat_long , dist_lat , dist_long = ( df . select ( lat_col , long_col ) . distinct () . count (), df . select ( lat_col ) . distinct () . count (), df . select ( long_col ) . distinct () . count (), ) top_lat_long = ( df . withColumn ( \"lat_long_pair\" , F . concat ( F . lit ( \"[\" ), F . col ( lat_col ), F . lit ( \",\" ), F . col ( long_col ), F . lit ( \"]\" ) ), ) . groupBy ( \"lat_long_pair\" ) . agg ( F . countDistinct ( id_col ) . alias ( \"count_id\" ), F . count ( id_col ) . alias ( \"count_records\" ), ) . orderBy ( \"count_id\" , ascending = False ) . limit ( max_val ) ) most_lat_long = top_lat_long . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] most_lat_long_cnt = top_lat_long . rdd . flatMap ( lambda x : x ) . collect ()[ 1 ] top_lat_long = top_lat_long . toPandas () d1 = dist_lat_long , dist_lat , dist_long , most_lat_long , most_lat_long_cnt d1_desc = ( \"Distinct {Lat, Long} Pair\" , \"Distinct Latitude\" , \"Distinct Longitude\" , \"Most Common {Lat, Long} Pair\" , \"Most Common {Lat, Long} Pair Occurence\" , ) gen_stats = ( pd . DataFrame ( d1 , d1_desc ) . reset_index () . rename ( columns = { \"index\" : \"Stats\" , 0 : \"Count\" }) ) l = [ \"Overall_Summary\" , \"Top_\" + str ( max_val ) + \"_Lat_Long\" ] for idx , i in enumerate ([ gen_stats , top_lat_long ]): i . to_csv ( ends_with ( master_path ) + l [ idx ] + \"_1_\" + lat_col + \"_\" + long_col + \".csv\" , index = False , ) if geohash_col is not None : dist_geohash = df . select ( geohash_col ) . distinct () . count () precision_geohash = ( df . select ( F . max ( F . length ( F . col ( geohash_col )))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) max_occuring_geohash = ( df . groupBy ( geohash_col ) . agg ( F . count ( id_col ) . alias ( \"count_records\" )) . orderBy ( \"count_records\" , ascending = False ) . limit ( 1 ) ) geohash_val = max_occuring_geohash . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] geohash_cnt = max_occuring_geohash . rdd . flatMap ( lambda x : x ) . collect ()[ 1 ] l = [ \"Overall_Summary\" , \"Top_\" + str ( max_val ) + \"_Geohash_Distribution\" ] geohash_area_width_height_1_12 = [ \"5,009.4km x 4,992.6km\" , \"1,252.3km x 624.1km\" , \"156.5km x 156km\" , \"39.1km x 19.5km\" , \"4.9km x 4.9km\" , \"1.2km x 609.4m\" , \"152.9m x 152.4m\" , \"38.2m x 19m\" , \"4.8m x 4.8m\" , \"1.2m x 59.5cm\" , \"14.9cm x 14.9cm\" , \"3.7cm x 1.9cm\" , ] pd . DataFrame ( [ [ \"Total number of Distinct Geohashes\" , str ( dist_geohash )], [ \"The Precision level observed for the Geohashes\" , str ( precision_geohash ) + \" [Reference Area Width x Height : \" + str ( geohash_area_width_height_1_12 [ precision_geohash - 1 ]) + \"] \" , ], [ \"The Most Common Geohash\" , str ( geohash_val ) + \" , \" + str ( geohash_cnt ), ], ], columns = [ \"Stats\" , \"Count\" ], ) . to_csv ( ends_with ( master_path ) + l [ 0 ] + \"_2_\" + geohash_col + \".csv\" , index = False ) df . withColumn ( \"geohash_\" + str ( precision_geohash ), F . substring ( F . col ( geohash_col ), 1 , precision_geohash ), ) . groupBy ( \"geohash_\" + str ( precision_geohash )) . agg ( F . countDistinct ( id_col ) . alias ( \"count_id\" ), F . count ( id_col ) . alias ( \"count_records\" ), ) . orderBy ( \"count_id\" , ascending = False ) . limit ( max_val ) . toPandas () . to_csv ( ends_with ( master_path ) + l [ 1 ] + \"_2_\" + geohash_col + \".csv\" , index = False ) def lat_long_col_stats_gen ( df , lat_col , long_col , id_col , master_path , max_val ): \"\"\" This function helps to produce descriptive stats for the latitude and longitude columns. If there's more than 1 latitude-longitude pair, an iteration through all pairs will be conducted. Each pair will have its own descriptive statistics tables generated by \"descriptive_stats_gen\" function. Parameters ---------- df DataFrame to be analyzed lat_col Latitude column long_col Longitude column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns ------- \"\"\" if len ( lat_col ) == 1 & len ( long_col ) == 1 : descriptive_stats_gen ( df , lat_col [ 0 ], long_col [ 0 ], None , id_col , master_path , max_val ) else : for i in range ( 0 , len ( lat_col )): descriptive_stats_gen ( df , lat_col [ i ], long_col [ i ], None , id_col , master_path , max_val ) def geohash_col_stats_gen ( df , geohash_col , id_col , master_path , max_val ): \"\"\" This function helps to produce descriptive stats for the geohash columns. If there's more than 1 geohash column, an iteratio through all geohash columns will be conducted. Each geohash column will have its own descriptive statistics tables generated by \"descriptive_stats_gen\" function. Parameters ---------- df Analysis DataFrame geohash_col Geohash column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns ------- \"\"\" if len ( geohash_col ) == 1 : descriptive_stats_gen ( df , None , None , geohash_col [ 0 ], id_col , master_path , max_val ) else : for i in range ( 0 , len ( geohash_col )): descriptive_stats_gen ( df , None , None , geohash_col [ i ], id_col , master_path , max_val ) def stats_gen_lat_long_geo ( df , lat_col , long_col , geohash_col , id_col , master_path , max_val ): \"\"\" This function is the main function used when generating geospatial-analysis tab for Anovos full report. It helps to produce descriptive statistics files for the geospatial fields by calling \"lat_long_col_stats_gen\" and \"geohash_col_stats_gen\" respectively, and the files will be used for generating Anovos full report's Geospatial Analyzer tab. If lat_col and long_col are valid, \"lat_long_col_stats_gen\" function will be called and intermediate files (overall summary and tables showing top lat-long pairs) will be stored inside master_path. If geohash_col is valid, \"geohash_col_stats_gen\" function will be called and intermediate files (overall summary and tables showing top geohash distribution) will be stored inside master_path. Parameters ---------- df Analysis DataFrame lat_col Latitude column long_col Longitude column geohash_col Geohash column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns ------- \"\"\" if lat_col : len_lat = len ( lat_col ) ll_stats = lat_long_col_stats_gen ( df , lat_col , long_col , id_col , master_path , max_val ) else : len_lat = 0 if geohash_col : len_geohash_col = len ( geohash_col ) geohash_stats = geohash_col_stats_gen ( df , geohash_col , id_col , master_path , max_val ) else : len_geohash_col = 0 if ( len_lat + len_geohash_col ) == 1 : if len_lat == 0 : return geohash_stats else : return ll_stats elif ( len_lat + len_geohash_col ) > 1 : if ( len_lat > 1 ) and ( len_geohash_col == 0 ): return ll_stats elif ( len_lat == 0 ) and ( len_geohash_col > 1 ): return geohash_stats elif ( len_lat >= 1 ) and ( len_geohash_col >= 1 ): return ll_stats , geohash_stats def geo_cluster_analysis ( df , lat_col , long_col , max_cluster , eps , min_samples , master_path , col_name , global_map_box_val , ): \"\"\" This function is the base function to generate cluster analysis statistics for the geospatial fields, and save 8 plots in JSON format inside master_path. K-Means and DBSCAN are the two clustering algorihtm used and the 8 plots are divided into 4 sections as below: - Cluster Identification: The first plot displays the cluster-identification process using K-Means algorithm. It is an elbow curve plot showing the distortion vs. number of clusters, and identifies the optimal number of clusters with a vertical line at K. The second plot displays the cluster-identification process using DBSCAN algorithm. It shows the distribution of silouhette scores across different parameters in a heatmap, and a darker color represents smaller scores. - Cluster Distribution The first plot shows distribution of clusters generated by K-Means algorithm in a pie-chart, and the distance is calculated using Euclidean distance. The second plot shows distribution of clusters generated by DBSCAN algorithm in a pie-chart, and the distance is calculated using Haversine distance. - Visualization The first plow is a Mapbox scatter plot of cluster-wise geospatial datapoints using K-Means algorithm. Color-coded datapoints are shown in a map which allows zoom-in, zoom-out, and latitude, longitude and cluster information are displayed for each label. The second plow is a Mapbox scatter plot of cluster-wise geospatial datapoints using DBSCAN algorithm. Color-coded datapoints are shown in a map which allows zoom-in, zoom-out, and latitude, longitude and cluster information are displayed for each label. Displaying these two plots together allows users to have an intuitive impact of results generated by different clustering techniques. - Outlier Points Unlike other sections, this section only contains results generated by DBSCAN algorithm. The first plot is a scatter plot of outlier points captured using DBSCAN algorithm with Euclidean distance calculation. The x-axis is longitude and y-axis is latitude, and outlier points will be marked as \"X\". The second plot is a scatter plot of outlier points captured using DBSCAN algorithm with Haversine distance calculation. The x-axis is longitude and y-axis is latitude, and outlier points will be marked as \"X\". Parameters ---------- df Analysis DataFrame lat_col Latitude column long_col Longitude column max_cluster Maximum number of iterations to decide on the optimum cluster eps Epsilon value range (Min EPS, Max EPS, Interval) used for DBSCAN clustering min_samples Minimum Sample Size range (Min Sample Size, Max Sample Size, Interval) used for DBSCAN clustering master_path Path containing all the output from analyzed data col_name Analysis column global_map_box_val Geospatial Chart Theme Index Returns ------- \"\"\" df_ = df [[ lat_col , long_col ]] max_k = int ( max_cluster ) ## iterations distortions = [] for i in range ( 2 , max_k + 1 ): if len ( df_ ) >= i : model = MiniBatchKMeans ( n_clusters = i , init = \"k-means++\" , max_iter = 300 , n_init = 10 , random_state = 0 ) model . fit ( df_ ) distortions . append ( model . inertia_ ) ## best k: the lowest derivative k = [ i * 100 for i in np . diff ( distortions , 2 )] . index ( min ([ i * 100 for i in np . diff ( distortions , 2 )]) ) ## plot f1 = go . Figure () f1 . add_trace ( go . Scatter ( x = list ( range ( 1 , len ( distortions ) + 1 )), y = distortions , mode = \"lines+markers\" , name = \"lines+markers\" , line = dict ( color = global_theme [ 2 ], width = 2 , dash = \"dash\" ), marker = dict ( size = 10 ), ) ) f1 . update_yaxes ( title = \"Distortion\" , showgrid = True , gridwidth = 1 , gridcolor = px . colors . sequential . gray [ 10 ], ) f1 . update_xaxes ( title = \"Values of K\" ) f1 . add_vline ( x = k , line_width = 3 , line_dash = \"dash\" , line_color = global_theme [ 4 ]) f1 . update_layout ( title_text = \"Elbow Curve Showing the Optimal Number of Clusters [K : \" + str ( k ) + \"] <br><sup>Algorithm Used : KMeans</sup>\" ) f1 . layout . plot_bgcolor = global_plot_bg_color f1 . layout . paper_bgcolor = global_paper_bg_color f1 . write_json ( ends_with ( master_path ) + \"cluster_plot_1_elbow_\" + col_name ) model = MiniBatchKMeans ( n_clusters = k , init = \"k-means++\" , max_iter = 300 , n_init = 10 , random_state = 0 ) df_ [ \"cluster\" ] = model . fit_predict ( df_ ) df_ . to_csv ( ends_with ( master_path ) + \"cluster_output_kmeans_\" + col_name + \".csv\" , index = False , ) # Use `hole` to create a donut-like pie chart cluster_dtls = df_ . groupby ([ \"cluster\" ]) . size () . reset_index ( name = \"counts\" ) f2 = go . Figure ( go . Pie ( labels = list ( cluster_dtls . cluster . values ), values = list ( cluster_dtls . counts . values ), hole = 0.3 , marker_colors = global_theme , text = list ( cluster_dtls . cluster . values ), ) ) f2 . update_layout ( title_text = \"Distribution of Clusters\" + \"<br><sup>Algorithm Used : K-Means (Distance : Euclidean) </sup>\" , legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ), ) f2 . write_json ( ends_with ( master_path ) + \"cluster_plot_2_kmeans_\" + col_name ) f3 = px . scatter_mapbox ( df_ , lat = lat_col , lon = long_col , color = \"cluster\" , color_continuous_scale = global_theme , mapbox_style = mapbox_list [ global_map_box_val ], ) f3 . update_geos ( fitbounds = \"locations\" ) f3 . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ]) f3 . update_layout ( title_text = \"Cluster Wise Geospatial Datapoints \" + \"<br><sup>Algorithm Used : K-Means</sup>\" ) f3 . update_layout ( coloraxis_showscale = False , autosize = False , width = 1200 , height = 900 ) f3 . write_json ( ends_with ( master_path ) + \"cluster_plot_3_kmeans_\" + col_name ) # Reading in 2D Feature Space df_ = df [[ lat_col , long_col ]] # DBSCAN model with parameters eps = eps . split ( \",\" ) min_samples = min_samples . split ( \",\" ) for i in range ( 3 ): eps [ i ] = float ( eps [ i ]) min_samples [ i ] = float ( min_samples [ i ]) DBSCAN_params = list ( product ( np . arange ( eps [ 0 ], eps [ 1 ], eps [ 2 ]), np . arange ( min_samples [ 0 ], min_samples [ 1 ], min_samples [ 2 ]), ) ) no_of_clusters = [] sil_score = [] for p in DBSCAN_params : try : DBS_clustering = DBSCAN ( eps = p [ 0 ], min_samples = p [ 1 ], metric = \"haversine\" ) . fit ( df_ ) sil_score . append ( silhouette_score ( df_ , DBS_clustering . labels_ )) except : sil_score . append ( 0 ) tmp = pd . DataFrame . from_records ( DBSCAN_params , columns = [ \"Eps\" , \"Min_samples\" ]) tmp [ \"Sil_score\" ] = sil_score eps_ , min_samples_ = list ( tmp . sort_values ( \"Sil_score\" , ascending = False ) . values [ 0 ])[ 0 : 2 ] DBS_clustering = DBSCAN ( eps = eps_ , min_samples = min_samples_ , metric = \"haversine\" ) . fit ( df_ ) DBSCAN_clustered = df_ . copy () DBSCAN_clustered . loc [:, \"Cluster\" ] = DBS_clustering . labels_ DBSCAN_clustered . to_csv ( ends_with ( master_path ) + \"cluster_output_dbscan_\" + col_name + \".csv\" , index = False , ) pivot_1 = pd . pivot_table ( tmp , values = \"Sil_score\" , index = \"Min_samples\" , columns = \"Eps\" ) f1_ = px . imshow ( pivot_1 . values , text_auto = \".3f\" , color_continuous_scale = global_theme , aspect = \"auto\" , y = list ( pivot_1 . index ), x = list ( pivot_1 . columns ), ) f1_ . update_xaxes ( title = \"Eps\" ) f1_ . update_yaxes ( title = \"Min_samples\" ) f1_ . update_traces ( text = np . around ( pivot_1 . values , decimals = 3 ), texttemplate = \"% {text} \" ) f1_ . update_layout ( title_text = \"Distribution of Silhouette Scores Across Different Parameters \" + \"<br><sup>Algorithm Used : DBSCAN</sup>\" ) f1_ . layout . plot_bgcolor = global_plot_bg_color f1_ . layout . paper_bgcolor = global_paper_bg_color f1_ . write_json ( ends_with ( master_path ) + \"cluster_plot_1_silhoutte_\" + col_name ) DBSCAN_clustered . loc [ DBSCAN_clustered [ \"Cluster\" ] == - 1 , \"Cluster\" ] = 999 cluster_dtls_ = ( DBSCAN_clustered . groupby ([ \"Cluster\" ]) . size () . reset_index ( name = \"counts\" ) ) f2_ = go . Figure ( go . Pie ( labels = list ( cluster_dtls_ . Cluster . values ), values = list ( cluster_dtls_ . counts . values ), hole = 0.3 , marker_colors = global_theme , text = list ( cluster_dtls_ . Cluster . values ), ) ) f2_ . update_layout ( title_text = \"Distribution of Clusters\" + \"<br><sup>Algorithm Used : DBSCAN (Distance : Haversine) </sup>\" , legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ), ) f2_ . write_json ( ends_with ( master_path ) + \"cluster_plot_2_dbscan_\" + col_name ) f3_ = px . scatter_mapbox ( DBSCAN_clustered , lat = lat_col , lon = long_col , color = \"Cluster\" , color_continuous_scale = global_theme , mapbox_style = mapbox_list [ global_map_box_val ], ) f3_ . update_geos ( fitbounds = \"locations\" ) f3_ . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ]) f3_ . update_layout ( title_text = \"Cluster Wise Geospatial Datapoints \" + \"<br><sup>Algorithm Used : DBSCAN</sup>\" ) f3_ . update_layout ( autosize = False , width = 1200 , height = 900 ) f3_ . update_coloraxes ( showscale = False ) f3_ . write_json ( ends_with ( master_path ) + \"cluster_plot_3_dbscan_\" + col_name ) try : DBSCAN_clustered_ = df_ . copy () df_outlier = DBSCAN ( eps = eps_ , min_samples = min_samples_ ) . fit ( DBSCAN_clustered_ ) DBSCAN_clustered_ . loc [:, \"Cluster\" ] = df_outlier . labels_ DBSCAN_clustered_ = DBSCAN_clustered_ [ DBSCAN_clustered_ . Cluster . values == - 1 ] DBSCAN_clustered_ [ \"outlier\" ] = 1 f4 = go . Figure ( go . Scatter ( mode = \"markers\" , x = DBSCAN_clustered_ [ long_col ], y = DBSCAN_clustered_ [ lat_col ], marker_symbol = \"x-thin\" , marker_line_color = \"black\" , marker_color = \"black\" , marker_line_width = 2 , marker_size = 20 , ) ) f4 . layout . plot_bgcolor = global_plot_bg_color f4 . layout . paper_bgcolor = global_paper_bg_color f4 . update_xaxes ( title_text = \"longitude\" ) f4 . update_yaxes ( title_text = \"latitude\" ) f4 . update_layout ( autosize = False , width = 1200 , height = 900 ) f4 . update_layout ( title_text = \"Outlier Points Captured By Cluster Analysis\" + \"<br><sup>Algorithm Used : DBSCAN (Distance : Euclidean)</sup>\" ) f4 . write_json ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_1_\" + col_name ) except : f4 = blank_chart f4 . update_layout ( title_text = \"No Outliers Were Found Using DBSCAN (Distance : Euclidean)\" ) f4 . write_json ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_1_\" + col_name ) try : df_outlier_ = DBSCAN_clustered [ DBSCAN_clustered . Cluster . values == 999 ] f4_ = go . Figure ( go . Scatter ( mode = \"markers\" , x = df_outlier_ [ long_col ], y = df_outlier_ [ lat_col ], marker_symbol = \"x-thin\" , marker_line_color = \"black\" , marker_color = \"black\" , marker_line_width = 2 , marker_size = 20 , ) ) f4_ . layout . plot_bgcolor = global_plot_bg_color f4_ . layout . paper_bgcolor = global_paper_bg_color f4_ . update_xaxes ( title_text = \"longitude\" ) f4_ . update_yaxes ( title_text = \"latitude\" ) f4_ . update_layout ( autosize = False , width = 1200 , height = 900 ) f4_ . update_layout ( title_text = \"Outlier Points Captured By Cluster Analysis\" + \"<br><sup>Algorithm Used : DBSCAN (Distance : Haversine)</sup>\" ) f4_ . write_json ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_2_\" + col_name ) except : f4_ = blank_chart f4_ . update_layout ( title_text = \"No Outliers Were Found Using DBSCAN (Distance : Haversine)\" ) f4_ . write_json ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_2_\" + col_name ) def geo_cluster_generator ( df , lat_col_list , long_col_list , geo_col_list , max_cluster , eps , min_samples , master_path , global_map_box_val , max_records , ): \"\"\" This function helps to trigger cluster analysis stats for the identified geospatial fields by calling \"geo_cluster_analysis\" function. If lat-long pairs are available, cluster analysis of each pair will be conducted and intermediate files will be saved inside master_path. If geohash columns are available, cluster analysis of each geohash column will be conducted and intermediate files will be saved into master_path. Parameters ---------- df Analysis DataFrame lat_col_list Latitude columns identified in the data long_col_list Longitude columns identified in the data geo_col_list Geohash columns identified in the data max_cluster Maximum number of iterations to decide on the optimum cluster eps Epsilon value range (Min EPS, Max EPS, Interval) used for DBSCAN clustering min_samples Minimum Sample Size range (Min Sample Size, Max Sample Size, Interval) used for DBSCAN clustering master_path Path containing all the output from analyzed data global_map_box_val Geospatial Chart Theme Index max_records Maximum geospatial points analyzed Returns ------- \"\"\" if isinstance ( df , pd . DataFrame ): pass else : cnt_records = df . count () frac_sample = float ( max_records ) / float ( cnt_records ) if frac_sample > 1 : frac_sample_ = 1.0 else : frac_sample_ = float ( frac_sample ) df = df . select ( * [ lat_col_list + long_col_list + geo_col_list ]) . dropna () if frac_sample_ == 1.0 : df = df . toPandas () else : df = data_sampling . data_sample ( df , strata_cols = \"all\" , fraction = frac_sample_ ) . toPandas () try : lat_col = lat_col_list long_col = long_col_list except : lat_col = [] try : geohash_col = geo_col_list except : geohash_col = [] if len ( lat_col ) >= 1 : for idx , i in enumerate ( lat_col ): col_name = lat_col [ idx ] + \"_\" + long_col [ idx ] geo_cluster_analysis ( df , lat_col [ idx ], long_col [ idx ], max_cluster , eps , min_samples , master_path , col_name , global_map_box_val , ) if len ( geohash_col ) >= 1 : for idx , i in enumerate ( geohash_col ): col_name = geohash_col [ idx ] df_ = df df_ [ \"latitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_name ], 0 ), axis = 1 ) df_ [ \"longitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_name ], 1 ), axis = 1 ) geo_cluster_analysis ( df_ , \"latitude\" , \"longitude\" , max_cluster , eps , min_samples , master_path , col_name , global_map_box_val , ) def generate_loc_charts_processor ( df , lat_col , long_col , geohash_col , max_val , id_col , global_map_box_val , master_path ): \"\"\" This function helps to generate the output of location charts for the geospatial fields, and save Mapbox scatter plots in JSON format inside master_path. If lat-long pairs are available, Mapbox scatter plot of each pair will be generated to visualize the locations of each datapoint. If geohash columns are available, every geohash column will go through geohash-to-lat-long transformation, and Mapbox scatter plot of the transformed lat-long pairs will be generated. Parameters ---------- df Analysis DataFrame lat_col Latitude columns identified in the data long_col Longitude columns identified in the data geohash_col Geohash columns identified in the data max_val Maximum geospatial points analyzed id_col ID column global_map_box_val Geospatial Chart Theme Index master_path Path containing all the output from analyzed data Returns ------- \"\"\" if lat_col : cols_to_select = lat_col + long_col + [ id_col ] elif geohash_col : cols_to_select = geohash_col + [ id_col ] df = df . select ( cols_to_select ) . dropna () if lat_col : if len ( lat_col ) == 1 : df_ = ( df . groupBy ( lat_col [ 0 ], long_col [ 0 ]) . agg ( F . countDistinct ( id_col ) . alias ( \"count\" )) . orderBy ( \"count\" , ascending = False ) . limit ( max_val ) . toPandas () ) base_map = px . scatter_mapbox ( df_ , lat = lat_col [ 0 ], lon = long_col [ 0 ], mapbox_style = mapbox_list [ global_map_box_val ], size = \"count\" , color_discrete_sequence = global_theme , ) base_map . update_geos ( fitbounds = \"locations\" ) base_map . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ], autosize = False , width = 1200 , height = 900 , ) base_map . write_json ( ends_with ( master_path ) + \"loc_charts_ll_\" + lat_col [ 0 ] + \"_\" + long_col [ 0 ] ) elif len ( lat_col ) > 1 : # l = [] for i in range ( 0 , len ( lat_col )): df_ = ( df . groupBy ( lat_col [ i ], long_col [ i ]) . agg ( F . countDistinct ( id_col ) . alias ( \"count\" )) . orderBy ( \"count\" , ascending = False ) . limit ( max_val ) . toPandas () ) base_map = px . scatter_mapbox ( df_ , lat = lat_col [ i ], lon = long_col [ i ], mapbox_style = mapbox_list [ global_map_box_val ], size = \"count\" , color_discrete_sequence = global_theme , ) base_map . update_geos ( fitbounds = \"locations\" ) base_map . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ], autosize = False , width = 1200 , height = 900 , ) base_map . write_json ( ends_with ( master_path ) + \"loc_charts_ll_\" + lat_col [ i ] + \"_\" + long_col [ i ] ) if geohash_col : if len ( geohash_col ) == 1 : col_ = geohash_col [ 0 ] df_ = ( df . groupBy ( col_ ) . agg ( F . countDistinct ( id_col ) . alias ( \"count\" )) . orderBy ( \"count\" , ascending = False ) . limit ( max_val ) . toPandas () ) df_ [ \"latitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_ ], 0 ), axis = 1 ) df_ [ \"longitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_ ], 1 ), axis = 1 ) base_map = px . scatter_mapbox ( df_ , lat = \"latitude\" , lon = \"longitude\" , mapbox_style = mapbox_list [ global_map_box_val ], size = \"count\" , color_discrete_sequence = global_theme , ) base_map . update_geos ( fitbounds = \"locations\" ) base_map . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ], autosize = False , width = 1200 , height = 900 , ) base_map . write_json ( ends_with ( master_path ) + \"loc_charts_gh_\" + col_ ) elif len ( geohash_col ) > 1 : # l = [] for i in range ( 0 , len ( geohash_col )): col_ = geohash_col [ i ] df_ = ( df . groupBy ( col_ ) . agg ( F . countDistinct ( id_col ) . alias ( \"count\" )) . orderBy ( \"count\" , ascending = False ) . limit ( max_val ) . toPandas () ) df_ [ \"latitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_ ], 0 ), axis = 1 ) df_ [ \"longitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_ ], 1 ), axis = 1 ) base_map = px . scatter_mapbox ( df_ , lat = \"latitude\" , lon = \"longitude\" , mapbox_style = mapbox_list [ global_map_box_val ], size = \"count\" , color_discrete_sequence = global_theme , ) base_map . update_geos ( fitbounds = \"locations\" ) base_map . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ], autosize = False , width = 1200 , height = 900 , ) base_map . write_json ( ends_with ( master_path ) + \"loc_charts_gh_\" + col_ ) def generate_loc_charts_controller ( df , id_col , lat_col , long_col , geohash_col , max_val , global_map_box_val , master_path ): \"\"\" This function helps to trigger the output generation of location charts for the geospatial fields. If lat-long pairs are available, \"generate_loc_charts_processor\" will be called (with geohash_cols set to None) and Mapbox scatter plot will be generated for each pair. If geohash columns are available, \"generate_loc_charts_processor\" will be called (with lat_col, long_col both set to None) and Mapbox scatter plot will be generated for each geohash column. Parameters ---------- df Analysis DataFrame id_col ID column lat_col Latitude columns identified in the data long_col Longitude columns identified in the data geohash_col Geohash columns identified in the data max_val Maximum geospatial points analyzed global_map_box_val Geospatial Chart Theme Index master_path Path containing all the output from analyzed data Returns ------- \"\"\" if lat_col : len_lat = len ( lat_col ) ll_plot = generate_loc_charts_processor ( df , lat_col = lat_col , long_col = long_col , geohash_col = None , max_val = max_val , id_col = id_col , global_map_box_val = global_map_box_val , master_path = master_path , ) else : len_lat = 0 if geohash_col : len_geohash_col = len ( geohash_col ) geohash_plot = generate_loc_charts_processor ( df , lat_col = None , long_col = None , geohash_col = geohash_col , max_val = max_val , id_col = id_col , global_map_box_val = global_map_box_val , master_path = master_path , ) else : len_geohash_col = 0 if ( len_lat + len_geohash_col ) == 1 : if len_lat == 0 : return geohash_plot else : return ll_plot elif ( len_lat + len_geohash_col ) > 1 : if ( len_lat > 1 ) and ( len_geohash_col == 0 ): return ll_plot elif ( len_lat == 0 ) and ( len_geohash_col > 1 ): return geohash_plot elif ( len_lat >= 1 ) and ( len_geohash_col >= 1 ): return ll_plot , geohash_plot def geospatial_autodetection ( df , id_col , master_path , max_records , top_geo_records , max_cluster , eps , min_samples , global_map_box_val , run_type , auth_key , ): \"\"\" This function helps to trigger the output of intermediate data which is further used for producing the geospatial-analysis tab in Anovos full report. Descriptive statistics, cluster analysis and visualization of geospatial fields will be triggered in sequence for each lat-long pair and geohash column respectively. Descriptive anallysis is conducted by calling \"stats_gen_lat_long_geo\" function, cluster analysis is conducted by calling \"geo_cluster_generator\" fucntion and visualization of geospatial fields is generated by calling \"generate_loc_charts_controller\" function. Parameters ---------- df Analysis DataFrame id_col ID column master_path Path containing all the output from analyzed data max_records Maximum geospatial points analyzed top_geo_records Top geospatial records displayed max_cluster Maximum number of iterations to decide on the optimum cluster eps Epsilon value range (Min EPS, Max EPS, Interval) used for DBSCAN clustering min_samples Minimum Sample Size range (Min Sample Size, Max Sample Size, Interval) used for DBSCAN clustering global_map_box_val Geospatial Chart Theme Index run_type Option to choose between run type \"Local\" or \"EMR\" or \"Azure\" or \"ak8s\" basis the user flexibility. Default option is set as \"Local\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns ------- \"\"\" if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = output_to_local ( master_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) lat_cols , long_cols , gh_cols = ll_gh_cols ( df , max_records ) try : len_lat_col = len ( lat_cols ) except : len_lat_col = 0 try : len_geohash_col = len ( gh_cols ) except : len_geohash_col = 0 if ( len_lat_col > 0 ) or ( len_geohash_col > 0 ): df . persist () stats_gen_lat_long_geo ( df , lat_cols , long_cols , gh_cols , id_col , local_path , top_geo_records ) geo_cluster_generator ( df , lat_cols , long_cols , gh_cols , max_cluster , eps , min_samples , local_path , global_map_box_val , max_records , ) generate_loc_charts_controller ( df , id_col , lat_cols , long_cols , gh_cols , max_records , global_map_box_val , local_path , ) return lat_cols , long_cols , gh_cols elif len_lat_col + len_geohash_col == 0 : return [], [], [] if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( master_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( master_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) Functions def descriptive_stats_gen ( df, lat_col, long_col, geohash_col, id_col, master_path, max_val) This function is the base function to produce descriptive stats for geospatial fields, and save relevant outputs in csv format inside master_path. If lat_col and long_col are valid, two tables will be generated - An overall summary table: This table has two columns: \"stats\" and \"count\", and 5 rows. These 5 rows summarizes the count of distinct {lat, long} pair count, latitude and longitude and shows the most common {lat,long} pair with occurrence respectively. - A top lat-long pairs table: This table shows the top lat-long pairs based on occurrence, and max_val parameter determines the number of records. If geohash_col is valid, two tables will be generated - An overall summary table: This table has two columns: \"stats\" and \"count\", and 3 rows. These 3 rows displays the total number of distinct geohashes, precision level observed for geohashes and the most common geohash respectively. - A top geohash distribution table: This table shows the top geohash distributions based on occurrence, and max_val parameter determines the number of records. Parameters df DataFrame to be analyzed lat_col Latitude column long_col Longitude column geohash_col Geohash column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns DataFrame[CSV] Expand source code def descriptive_stats_gen ( df , lat_col , long_col , geohash_col , id_col , master_path , max_val ): \"\"\" This function is the base function to produce descriptive stats for geospatial fields, and save relevant outputs in csv format inside master_path. If lat_col and long_col are valid, two tables will be generated - An overall summary table: This table has two columns: \"stats\" and \"count\", and 5 rows. These 5 rows summarizes the count of distinct {lat, long} pair count, latitude and longitude and shows the most common {lat,long} pair with occurrence respectively. - A top lat-long pairs table: This table shows the top lat-long pairs based on occurrence, and max_val parameter determines the number of records. If geohash_col is valid, two tables will be generated - An overall summary table: This table has two columns: \"stats\" and \"count\", and 3 rows. These 3 rows displays the total number of distinct geohashes, precision level observed for geohashes and the most common geohash respectively. - A top geohash distribution table: This table shows the top geohash distributions based on occurrence, and max_val parameter determines the number of records. Parameters ---------- df DataFrame to be analyzed lat_col Latitude column long_col Longitude column geohash_col Geohash column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns ------- DataFrame[CSV] \"\"\" if ( lat_col is not None ) & ( long_col is not None ): dist_lat_long , dist_lat , dist_long = ( df . select ( lat_col , long_col ) . distinct () . count (), df . select ( lat_col ) . distinct () . count (), df . select ( long_col ) . distinct () . count (), ) top_lat_long = ( df . withColumn ( \"lat_long_pair\" , F . concat ( F . lit ( \"[\" ), F . col ( lat_col ), F . lit ( \",\" ), F . col ( long_col ), F . lit ( \"]\" ) ), ) . groupBy ( \"lat_long_pair\" ) . agg ( F . countDistinct ( id_col ) . alias ( \"count_id\" ), F . count ( id_col ) . alias ( \"count_records\" ), ) . orderBy ( \"count_id\" , ascending = False ) . limit ( max_val ) ) most_lat_long = top_lat_long . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] most_lat_long_cnt = top_lat_long . rdd . flatMap ( lambda x : x ) . collect ()[ 1 ] top_lat_long = top_lat_long . toPandas () d1 = dist_lat_long , dist_lat , dist_long , most_lat_long , most_lat_long_cnt d1_desc = ( \"Distinct {Lat, Long} Pair\" , \"Distinct Latitude\" , \"Distinct Longitude\" , \"Most Common {Lat, Long} Pair\" , \"Most Common {Lat, Long} Pair Occurence\" , ) gen_stats = ( pd . DataFrame ( d1 , d1_desc ) . reset_index () . rename ( columns = { \"index\" : \"Stats\" , 0 : \"Count\" }) ) l = [ \"Overall_Summary\" , \"Top_\" + str ( max_val ) + \"_Lat_Long\" ] for idx , i in enumerate ([ gen_stats , top_lat_long ]): i . to_csv ( ends_with ( master_path ) + l [ idx ] + \"_1_\" + lat_col + \"_\" + long_col + \".csv\" , index = False , ) if geohash_col is not None : dist_geohash = df . select ( geohash_col ) . distinct () . count () precision_geohash = ( df . select ( F . max ( F . length ( F . col ( geohash_col )))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) max_occuring_geohash = ( df . groupBy ( geohash_col ) . agg ( F . count ( id_col ) . alias ( \"count_records\" )) . orderBy ( \"count_records\" , ascending = False ) . limit ( 1 ) ) geohash_val = max_occuring_geohash . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] geohash_cnt = max_occuring_geohash . rdd . flatMap ( lambda x : x ) . collect ()[ 1 ] l = [ \"Overall_Summary\" , \"Top_\" + str ( max_val ) + \"_Geohash_Distribution\" ] geohash_area_width_height_1_12 = [ \"5,009.4km x 4,992.6km\" , \"1,252.3km x 624.1km\" , \"156.5km x 156km\" , \"39.1km x 19.5km\" , \"4.9km x 4.9km\" , \"1.2km x 609.4m\" , \"152.9m x 152.4m\" , \"38.2m x 19m\" , \"4.8m x 4.8m\" , \"1.2m x 59.5cm\" , \"14.9cm x 14.9cm\" , \"3.7cm x 1.9cm\" , ] pd . DataFrame ( [ [ \"Total number of Distinct Geohashes\" , str ( dist_geohash )], [ \"The Precision level observed for the Geohashes\" , str ( precision_geohash ) + \" [Reference Area Width x Height : \" + str ( geohash_area_width_height_1_12 [ precision_geohash - 1 ]) + \"] \" , ], [ \"The Most Common Geohash\" , str ( geohash_val ) + \" , \" + str ( geohash_cnt ), ], ], columns = [ \"Stats\" , \"Count\" ], ) . to_csv ( ends_with ( master_path ) + l [ 0 ] + \"_2_\" + geohash_col + \".csv\" , index = False ) df . withColumn ( \"geohash_\" + str ( precision_geohash ), F . substring ( F . col ( geohash_col ), 1 , precision_geohash ), ) . groupBy ( \"geohash_\" + str ( precision_geohash )) . agg ( F . countDistinct ( id_col ) . alias ( \"count_id\" ), F . count ( id_col ) . alias ( \"count_records\" ), ) . orderBy ( \"count_id\" , ascending = False ) . limit ( max_val ) . toPandas () . to_csv ( ends_with ( master_path ) + l [ 1 ] + \"_2_\" + geohash_col + \".csv\" , index = False ) def generate_loc_charts_controller ( df, id_col, lat_col, long_col, geohash_col, max_val, global_map_box_val, master_path) This function helps to trigger the output generation of location charts for the geospatial fields. If lat-long pairs are available, \"generate_loc_charts_processor\" will be called (with geohash_cols set to None) and Mapbox scatter plot will be generated for each pair. If geohash columns are available, \"generate_loc_charts_processor\" will be called (with lat_col, long_col both set to None) and Mapbox scatter plot will be generated for each geohash column. Parameters df Analysis DataFrame id_col ID column lat_col Latitude columns identified in the data long_col Longitude columns identified in the data geohash_col Geohash columns identified in the data max_val Maximum geospatial points analyzed global_map_box_val Geospatial Chart Theme Index master_path Path containing all the output from analyzed data Returns Expand source code def generate_loc_charts_controller ( df , id_col , lat_col , long_col , geohash_col , max_val , global_map_box_val , master_path ): \"\"\" This function helps to trigger the output generation of location charts for the geospatial fields. If lat-long pairs are available, \"generate_loc_charts_processor\" will be called (with geohash_cols set to None) and Mapbox scatter plot will be generated for each pair. If geohash columns are available, \"generate_loc_charts_processor\" will be called (with lat_col, long_col both set to None) and Mapbox scatter plot will be generated for each geohash column. Parameters ---------- df Analysis DataFrame id_col ID column lat_col Latitude columns identified in the data long_col Longitude columns identified in the data geohash_col Geohash columns identified in the data max_val Maximum geospatial points analyzed global_map_box_val Geospatial Chart Theme Index master_path Path containing all the output from analyzed data Returns ------- \"\"\" if lat_col : len_lat = len ( lat_col ) ll_plot = generate_loc_charts_processor ( df , lat_col = lat_col , long_col = long_col , geohash_col = None , max_val = max_val , id_col = id_col , global_map_box_val = global_map_box_val , master_path = master_path , ) else : len_lat = 0 if geohash_col : len_geohash_col = len ( geohash_col ) geohash_plot = generate_loc_charts_processor ( df , lat_col = None , long_col = None , geohash_col = geohash_col , max_val = max_val , id_col = id_col , global_map_box_val = global_map_box_val , master_path = master_path , ) else : len_geohash_col = 0 if ( len_lat + len_geohash_col ) == 1 : if len_lat == 0 : return geohash_plot else : return ll_plot elif ( len_lat + len_geohash_col ) > 1 : if ( len_lat > 1 ) and ( len_geohash_col == 0 ): return ll_plot elif ( len_lat == 0 ) and ( len_geohash_col > 1 ): return geohash_plot elif ( len_lat >= 1 ) and ( len_geohash_col >= 1 ): return ll_plot , geohash_plot def generate_loc_charts_processor ( df, lat_col, long_col, geohash_col, max_val, id_col, global_map_box_val, master_path) This function helps to generate the output of location charts for the geospatial fields, and save Mapbox scatter plots in JSON format inside master_path. If lat-long pairs are available, Mapbox scatter plot of each pair will be generated to visualize the locations of each datapoint. If geohash columns are available, every geohash column will go through geohash-to-lat-long transformation, and Mapbox scatter plot of the transformed lat-long pairs will be generated. Parameters df Analysis DataFrame lat_col Latitude columns identified in the data long_col Longitude columns identified in the data geohash_col Geohash columns identified in the data max_val Maximum geospatial points analyzed id_col ID column global_map_box_val Geospatial Chart Theme Index master_path Path containing all the output from analyzed data Returns Expand source code def generate_loc_charts_processor ( df , lat_col , long_col , geohash_col , max_val , id_col , global_map_box_val , master_path ): \"\"\" This function helps to generate the output of location charts for the geospatial fields, and save Mapbox scatter plots in JSON format inside master_path. If lat-long pairs are available, Mapbox scatter plot of each pair will be generated to visualize the locations of each datapoint. If geohash columns are available, every geohash column will go through geohash-to-lat-long transformation, and Mapbox scatter plot of the transformed lat-long pairs will be generated. Parameters ---------- df Analysis DataFrame lat_col Latitude columns identified in the data long_col Longitude columns identified in the data geohash_col Geohash columns identified in the data max_val Maximum geospatial points analyzed id_col ID column global_map_box_val Geospatial Chart Theme Index master_path Path containing all the output from analyzed data Returns ------- \"\"\" if lat_col : cols_to_select = lat_col + long_col + [ id_col ] elif geohash_col : cols_to_select = geohash_col + [ id_col ] df = df . select ( cols_to_select ) . dropna () if lat_col : if len ( lat_col ) == 1 : df_ = ( df . groupBy ( lat_col [ 0 ], long_col [ 0 ]) . agg ( F . countDistinct ( id_col ) . alias ( \"count\" )) . orderBy ( \"count\" , ascending = False ) . limit ( max_val ) . toPandas () ) base_map = px . scatter_mapbox ( df_ , lat = lat_col [ 0 ], lon = long_col [ 0 ], mapbox_style = mapbox_list [ global_map_box_val ], size = \"count\" , color_discrete_sequence = global_theme , ) base_map . update_geos ( fitbounds = \"locations\" ) base_map . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ], autosize = False , width = 1200 , height = 900 , ) base_map . write_json ( ends_with ( master_path ) + \"loc_charts_ll_\" + lat_col [ 0 ] + \"_\" + long_col [ 0 ] ) elif len ( lat_col ) > 1 : # l = [] for i in range ( 0 , len ( lat_col )): df_ = ( df . groupBy ( lat_col [ i ], long_col [ i ]) . agg ( F . countDistinct ( id_col ) . alias ( \"count\" )) . orderBy ( \"count\" , ascending = False ) . limit ( max_val ) . toPandas () ) base_map = px . scatter_mapbox ( df_ , lat = lat_col [ i ], lon = long_col [ i ], mapbox_style = mapbox_list [ global_map_box_val ], size = \"count\" , color_discrete_sequence = global_theme , ) base_map . update_geos ( fitbounds = \"locations\" ) base_map . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ], autosize = False , width = 1200 , height = 900 , ) base_map . write_json ( ends_with ( master_path ) + \"loc_charts_ll_\" + lat_col [ i ] + \"_\" + long_col [ i ] ) if geohash_col : if len ( geohash_col ) == 1 : col_ = geohash_col [ 0 ] df_ = ( df . groupBy ( col_ ) . agg ( F . countDistinct ( id_col ) . alias ( \"count\" )) . orderBy ( \"count\" , ascending = False ) . limit ( max_val ) . toPandas () ) df_ [ \"latitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_ ], 0 ), axis = 1 ) df_ [ \"longitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_ ], 1 ), axis = 1 ) base_map = px . scatter_mapbox ( df_ , lat = \"latitude\" , lon = \"longitude\" , mapbox_style = mapbox_list [ global_map_box_val ], size = \"count\" , color_discrete_sequence = global_theme , ) base_map . update_geos ( fitbounds = \"locations\" ) base_map . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ], autosize = False , width = 1200 , height = 900 , ) base_map . write_json ( ends_with ( master_path ) + \"loc_charts_gh_\" + col_ ) elif len ( geohash_col ) > 1 : # l = [] for i in range ( 0 , len ( geohash_col )): col_ = geohash_col [ i ] df_ = ( df . groupBy ( col_ ) . agg ( F . countDistinct ( id_col ) . alias ( \"count\" )) . orderBy ( \"count\" , ascending = False ) . limit ( max_val ) . toPandas () ) df_ [ \"latitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_ ], 0 ), axis = 1 ) df_ [ \"longitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_ ], 1 ), axis = 1 ) base_map = px . scatter_mapbox ( df_ , lat = \"latitude\" , lon = \"longitude\" , mapbox_style = mapbox_list [ global_map_box_val ], size = \"count\" , color_discrete_sequence = global_theme , ) base_map . update_geos ( fitbounds = \"locations\" ) base_map . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ], autosize = False , width = 1200 , height = 900 , ) base_map . write_json ( ends_with ( master_path ) + \"loc_charts_gh_\" + col_ ) def geo_cluster_analysis ( df, lat_col, long_col, max_cluster, eps, min_samples, master_path, col_name, global_map_box_val) This function is the base function to generate cluster analysis statistics for the geospatial fields, and save 8 plots in JSON format inside master_path. K-Means and DBSCAN are the two clustering algorihtm used and the 8 plots are divided into 4 sections as below: - Cluster Identification: The first plot displays the cluster-identification process using K-Means algorithm. It is an elbow curve plot showing the distortion vs. number of clusters, and identifies the optimal number of clusters with a vertical line at K. The second plot displays the cluster-identification process using DBSCAN algorithm. It shows the distribution of silouhette scores across different parameters in a heatmap, and a darker color represents smaller scores. Cluster Distribution The first plot shows distribution of clusters generated by K-Means algorithm in a pie-chart, and the distance is calculated using Euclidean distance. The second plot shows distribution of clusters generated by DBSCAN algorithm in a pie-chart, and the distance is calculated using Haversine distance. Visualization The first plow is a Mapbox scatter plot of cluster-wise geospatial datapoints using K-Means algorithm. Color-coded datapoints are shown in a map which allows zoom-in, zoom-out, and latitude, longitude and cluster information are displayed for each label. The second plow is a Mapbox scatter plot of cluster-wise geospatial datapoints using DBSCAN algorithm. Color-coded datapoints are shown in a map which allows zoom-in, zoom-out, and latitude, longitude and cluster information are displayed for each label. Displaying these two plots together allows users to have an intuitive impact of results generated by different clustering techniques. Outlier Points Unlike other sections, this section only contains results generated by DBSCAN algorithm. The first plot is a scatter plot of outlier points captured using DBSCAN algorithm with Euclidean distance calculation. The x-axis is longitude and y-axis is latitude, and outlier points will be marked as \"X\". The second plot is a scatter plot of outlier points captured using DBSCAN algorithm with Haversine distance calculation. The x-axis is longitude and y-axis is latitude, and outlier points will be marked as \"X\". Parameters df Analysis DataFrame lat_col Latitude column long_col Longitude column max_cluster Maximum number of iterations to decide on the optimum cluster eps Epsilon value range (Min EPS, Max EPS, Interval) used for DBSCAN clustering min_samples Minimum Sample Size range (Min Sample Size, Max Sample Size, Interval) used for DBSCAN clustering master_path Path containing all the output from analyzed data col_name Analysis column global_map_box_val Geospatial Chart Theme Index Returns Expand source code def geo_cluster_analysis ( df , lat_col , long_col , max_cluster , eps , min_samples , master_path , col_name , global_map_box_val , ): \"\"\" This function is the base function to generate cluster analysis statistics for the geospatial fields, and save 8 plots in JSON format inside master_path. K-Means and DBSCAN are the two clustering algorihtm used and the 8 plots are divided into 4 sections as below: - Cluster Identification: The first plot displays the cluster-identification process using K-Means algorithm. It is an elbow curve plot showing the distortion vs. number of clusters, and identifies the optimal number of clusters with a vertical line at K. The second plot displays the cluster-identification process using DBSCAN algorithm. It shows the distribution of silouhette scores across different parameters in a heatmap, and a darker color represents smaller scores. - Cluster Distribution The first plot shows distribution of clusters generated by K-Means algorithm in a pie-chart, and the distance is calculated using Euclidean distance. The second plot shows distribution of clusters generated by DBSCAN algorithm in a pie-chart, and the distance is calculated using Haversine distance. - Visualization The first plow is a Mapbox scatter plot of cluster-wise geospatial datapoints using K-Means algorithm. Color-coded datapoints are shown in a map which allows zoom-in, zoom-out, and latitude, longitude and cluster information are displayed for each label. The second plow is a Mapbox scatter plot of cluster-wise geospatial datapoints using DBSCAN algorithm. Color-coded datapoints are shown in a map which allows zoom-in, zoom-out, and latitude, longitude and cluster information are displayed for each label. Displaying these two plots together allows users to have an intuitive impact of results generated by different clustering techniques. - Outlier Points Unlike other sections, this section only contains results generated by DBSCAN algorithm. The first plot is a scatter plot of outlier points captured using DBSCAN algorithm with Euclidean distance calculation. The x-axis is longitude and y-axis is latitude, and outlier points will be marked as \"X\". The second plot is a scatter plot of outlier points captured using DBSCAN algorithm with Haversine distance calculation. The x-axis is longitude and y-axis is latitude, and outlier points will be marked as \"X\". Parameters ---------- df Analysis DataFrame lat_col Latitude column long_col Longitude column max_cluster Maximum number of iterations to decide on the optimum cluster eps Epsilon value range (Min EPS, Max EPS, Interval) used for DBSCAN clustering min_samples Minimum Sample Size range (Min Sample Size, Max Sample Size, Interval) used for DBSCAN clustering master_path Path containing all the output from analyzed data col_name Analysis column global_map_box_val Geospatial Chart Theme Index Returns ------- \"\"\" df_ = df [[ lat_col , long_col ]] max_k = int ( max_cluster ) ## iterations distortions = [] for i in range ( 2 , max_k + 1 ): if len ( df_ ) >= i : model = MiniBatchKMeans ( n_clusters = i , init = \"k-means++\" , max_iter = 300 , n_init = 10 , random_state = 0 ) model . fit ( df_ ) distortions . append ( model . inertia_ ) ## best k: the lowest derivative k = [ i * 100 for i in np . diff ( distortions , 2 )] . index ( min ([ i * 100 for i in np . diff ( distortions , 2 )]) ) ## plot f1 = go . Figure () f1 . add_trace ( go . Scatter ( x = list ( range ( 1 , len ( distortions ) + 1 )), y = distortions , mode = \"lines+markers\" , name = \"lines+markers\" , line = dict ( color = global_theme [ 2 ], width = 2 , dash = \"dash\" ), marker = dict ( size = 10 ), ) ) f1 . update_yaxes ( title = \"Distortion\" , showgrid = True , gridwidth = 1 , gridcolor = px . colors . sequential . gray [ 10 ], ) f1 . update_xaxes ( title = \"Values of K\" ) f1 . add_vline ( x = k , line_width = 3 , line_dash = \"dash\" , line_color = global_theme [ 4 ]) f1 . update_layout ( title_text = \"Elbow Curve Showing the Optimal Number of Clusters [K : \" + str ( k ) + \"] <br><sup>Algorithm Used : KMeans</sup>\" ) f1 . layout . plot_bgcolor = global_plot_bg_color f1 . layout . paper_bgcolor = global_paper_bg_color f1 . write_json ( ends_with ( master_path ) + \"cluster_plot_1_elbow_\" + col_name ) model = MiniBatchKMeans ( n_clusters = k , init = \"k-means++\" , max_iter = 300 , n_init = 10 , random_state = 0 ) df_ [ \"cluster\" ] = model . fit_predict ( df_ ) df_ . to_csv ( ends_with ( master_path ) + \"cluster_output_kmeans_\" + col_name + \".csv\" , index = False , ) # Use `hole` to create a donut-like pie chart cluster_dtls = df_ . groupby ([ \"cluster\" ]) . size () . reset_index ( name = \"counts\" ) f2 = go . Figure ( go . Pie ( labels = list ( cluster_dtls . cluster . values ), values = list ( cluster_dtls . counts . values ), hole = 0.3 , marker_colors = global_theme , text = list ( cluster_dtls . cluster . values ), ) ) f2 . update_layout ( title_text = \"Distribution of Clusters\" + \"<br><sup>Algorithm Used : K-Means (Distance : Euclidean) </sup>\" , legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ), ) f2 . write_json ( ends_with ( master_path ) + \"cluster_plot_2_kmeans_\" + col_name ) f3 = px . scatter_mapbox ( df_ , lat = lat_col , lon = long_col , color = \"cluster\" , color_continuous_scale = global_theme , mapbox_style = mapbox_list [ global_map_box_val ], ) f3 . update_geos ( fitbounds = \"locations\" ) f3 . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ]) f3 . update_layout ( title_text = \"Cluster Wise Geospatial Datapoints \" + \"<br><sup>Algorithm Used : K-Means</sup>\" ) f3 . update_layout ( coloraxis_showscale = False , autosize = False , width = 1200 , height = 900 ) f3 . write_json ( ends_with ( master_path ) + \"cluster_plot_3_kmeans_\" + col_name ) # Reading in 2D Feature Space df_ = df [[ lat_col , long_col ]] # DBSCAN model with parameters eps = eps . split ( \",\" ) min_samples = min_samples . split ( \",\" ) for i in range ( 3 ): eps [ i ] = float ( eps [ i ]) min_samples [ i ] = float ( min_samples [ i ]) DBSCAN_params = list ( product ( np . arange ( eps [ 0 ], eps [ 1 ], eps [ 2 ]), np . arange ( min_samples [ 0 ], min_samples [ 1 ], min_samples [ 2 ]), ) ) no_of_clusters = [] sil_score = [] for p in DBSCAN_params : try : DBS_clustering = DBSCAN ( eps = p [ 0 ], min_samples = p [ 1 ], metric = \"haversine\" ) . fit ( df_ ) sil_score . append ( silhouette_score ( df_ , DBS_clustering . labels_ )) except : sil_score . append ( 0 ) tmp = pd . DataFrame . from_records ( DBSCAN_params , columns = [ \"Eps\" , \"Min_samples\" ]) tmp [ \"Sil_score\" ] = sil_score eps_ , min_samples_ = list ( tmp . sort_values ( \"Sil_score\" , ascending = False ) . values [ 0 ])[ 0 : 2 ] DBS_clustering = DBSCAN ( eps = eps_ , min_samples = min_samples_ , metric = \"haversine\" ) . fit ( df_ ) DBSCAN_clustered = df_ . copy () DBSCAN_clustered . loc [:, \"Cluster\" ] = DBS_clustering . labels_ DBSCAN_clustered . to_csv ( ends_with ( master_path ) + \"cluster_output_dbscan_\" + col_name + \".csv\" , index = False , ) pivot_1 = pd . pivot_table ( tmp , values = \"Sil_score\" , index = \"Min_samples\" , columns = \"Eps\" ) f1_ = px . imshow ( pivot_1 . values , text_auto = \".3f\" , color_continuous_scale = global_theme , aspect = \"auto\" , y = list ( pivot_1 . index ), x = list ( pivot_1 . columns ), ) f1_ . update_xaxes ( title = \"Eps\" ) f1_ . update_yaxes ( title = \"Min_samples\" ) f1_ . update_traces ( text = np . around ( pivot_1 . values , decimals = 3 ), texttemplate = \"% {text} \" ) f1_ . update_layout ( title_text = \"Distribution of Silhouette Scores Across Different Parameters \" + \"<br><sup>Algorithm Used : DBSCAN</sup>\" ) f1_ . layout . plot_bgcolor = global_plot_bg_color f1_ . layout . paper_bgcolor = global_paper_bg_color f1_ . write_json ( ends_with ( master_path ) + \"cluster_plot_1_silhoutte_\" + col_name ) DBSCAN_clustered . loc [ DBSCAN_clustered [ \"Cluster\" ] == - 1 , \"Cluster\" ] = 999 cluster_dtls_ = ( DBSCAN_clustered . groupby ([ \"Cluster\" ]) . size () . reset_index ( name = \"counts\" ) ) f2_ = go . Figure ( go . Pie ( labels = list ( cluster_dtls_ . Cluster . values ), values = list ( cluster_dtls_ . counts . values ), hole = 0.3 , marker_colors = global_theme , text = list ( cluster_dtls_ . Cluster . values ), ) ) f2_ . update_layout ( title_text = \"Distribution of Clusters\" + \"<br><sup>Algorithm Used : DBSCAN (Distance : Haversine) </sup>\" , legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ), ) f2_ . write_json ( ends_with ( master_path ) + \"cluster_plot_2_dbscan_\" + col_name ) f3_ = px . scatter_mapbox ( DBSCAN_clustered , lat = lat_col , lon = long_col , color = \"Cluster\" , color_continuous_scale = global_theme , mapbox_style = mapbox_list [ global_map_box_val ], ) f3_ . update_geos ( fitbounds = \"locations\" ) f3_ . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ]) f3_ . update_layout ( title_text = \"Cluster Wise Geospatial Datapoints \" + \"<br><sup>Algorithm Used : DBSCAN</sup>\" ) f3_ . update_layout ( autosize = False , width = 1200 , height = 900 ) f3_ . update_coloraxes ( showscale = False ) f3_ . write_json ( ends_with ( master_path ) + \"cluster_plot_3_dbscan_\" + col_name ) try : DBSCAN_clustered_ = df_ . copy () df_outlier = DBSCAN ( eps = eps_ , min_samples = min_samples_ ) . fit ( DBSCAN_clustered_ ) DBSCAN_clustered_ . loc [:, \"Cluster\" ] = df_outlier . labels_ DBSCAN_clustered_ = DBSCAN_clustered_ [ DBSCAN_clustered_ . Cluster . values == - 1 ] DBSCAN_clustered_ [ \"outlier\" ] = 1 f4 = go . Figure ( go . Scatter ( mode = \"markers\" , x = DBSCAN_clustered_ [ long_col ], y = DBSCAN_clustered_ [ lat_col ], marker_symbol = \"x-thin\" , marker_line_color = \"black\" , marker_color = \"black\" , marker_line_width = 2 , marker_size = 20 , ) ) f4 . layout . plot_bgcolor = global_plot_bg_color f4 . layout . paper_bgcolor = global_paper_bg_color f4 . update_xaxes ( title_text = \"longitude\" ) f4 . update_yaxes ( title_text = \"latitude\" ) f4 . update_layout ( autosize = False , width = 1200 , height = 900 ) f4 . update_layout ( title_text = \"Outlier Points Captured By Cluster Analysis\" + \"<br><sup>Algorithm Used : DBSCAN (Distance : Euclidean)</sup>\" ) f4 . write_json ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_1_\" + col_name ) except : f4 = blank_chart f4 . update_layout ( title_text = \"No Outliers Were Found Using DBSCAN (Distance : Euclidean)\" ) f4 . write_json ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_1_\" + col_name ) try : df_outlier_ = DBSCAN_clustered [ DBSCAN_clustered . Cluster . values == 999 ] f4_ = go . Figure ( go . Scatter ( mode = \"markers\" , x = df_outlier_ [ long_col ], y = df_outlier_ [ lat_col ], marker_symbol = \"x-thin\" , marker_line_color = \"black\" , marker_color = \"black\" , marker_line_width = 2 , marker_size = 20 , ) ) f4_ . layout . plot_bgcolor = global_plot_bg_color f4_ . layout . paper_bgcolor = global_paper_bg_color f4_ . update_xaxes ( title_text = \"longitude\" ) f4_ . update_yaxes ( title_text = \"latitude\" ) f4_ . update_layout ( autosize = False , width = 1200 , height = 900 ) f4_ . update_layout ( title_text = \"Outlier Points Captured By Cluster Analysis\" + \"<br><sup>Algorithm Used : DBSCAN (Distance : Haversine)</sup>\" ) f4_ . write_json ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_2_\" + col_name ) except : f4_ = blank_chart f4_ . update_layout ( title_text = \"No Outliers Were Found Using DBSCAN (Distance : Haversine)\" ) f4_ . write_json ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_2_\" + col_name ) def geo_cluster_generator ( df, lat_col_list, long_col_list, geo_col_list, max_cluster, eps, min_samples, master_path, global_map_box_val, max_records) This function helps to trigger cluster analysis stats for the identified geospatial fields by calling \"geo_cluster_analysis\" function. If lat-long pairs are available, cluster analysis of each pair will be conducted and intermediate files will be saved inside master_path. If geohash columns are available, cluster analysis of each geohash column will be conducted and intermediate files will be saved into master_path. Parameters df Analysis DataFrame lat_col_list Latitude columns identified in the data long_col_list Longitude columns identified in the data geo_col_list Geohash columns identified in the data max_cluster Maximum number of iterations to decide on the optimum cluster eps Epsilon value range (Min EPS, Max EPS, Interval) used for DBSCAN clustering min_samples Minimum Sample Size range (Min Sample Size, Max Sample Size, Interval) used for DBSCAN clustering master_path Path containing all the output from analyzed data global_map_box_val Geospatial Chart Theme Index max_records Maximum geospatial points analyzed Returns Expand source code def geo_cluster_generator ( df , lat_col_list , long_col_list , geo_col_list , max_cluster , eps , min_samples , master_path , global_map_box_val , max_records , ): \"\"\" This function helps to trigger cluster analysis stats for the identified geospatial fields by calling \"geo_cluster_analysis\" function. If lat-long pairs are available, cluster analysis of each pair will be conducted and intermediate files will be saved inside master_path. If geohash columns are available, cluster analysis of each geohash column will be conducted and intermediate files will be saved into master_path. Parameters ---------- df Analysis DataFrame lat_col_list Latitude columns identified in the data long_col_list Longitude columns identified in the data geo_col_list Geohash columns identified in the data max_cluster Maximum number of iterations to decide on the optimum cluster eps Epsilon value range (Min EPS, Max EPS, Interval) used for DBSCAN clustering min_samples Minimum Sample Size range (Min Sample Size, Max Sample Size, Interval) used for DBSCAN clustering master_path Path containing all the output from analyzed data global_map_box_val Geospatial Chart Theme Index max_records Maximum geospatial points analyzed Returns ------- \"\"\" if isinstance ( df , pd . DataFrame ): pass else : cnt_records = df . count () frac_sample = float ( max_records ) / float ( cnt_records ) if frac_sample > 1 : frac_sample_ = 1.0 else : frac_sample_ = float ( frac_sample ) df = df . select ( * [ lat_col_list + long_col_list + geo_col_list ]) . dropna () if frac_sample_ == 1.0 : df = df . toPandas () else : df = data_sampling . data_sample ( df , strata_cols = \"all\" , fraction = frac_sample_ ) . toPandas () try : lat_col = lat_col_list long_col = long_col_list except : lat_col = [] try : geohash_col = geo_col_list except : geohash_col = [] if len ( lat_col ) >= 1 : for idx , i in enumerate ( lat_col ): col_name = lat_col [ idx ] + \"_\" + long_col [ idx ] geo_cluster_analysis ( df , lat_col [ idx ], long_col [ idx ], max_cluster , eps , min_samples , master_path , col_name , global_map_box_val , ) if len ( geohash_col ) >= 1 : for idx , i in enumerate ( geohash_col ): col_name = geohash_col [ idx ] df_ = df df_ [ \"latitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_name ], 0 ), axis = 1 ) df_ [ \"longitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_name ], 1 ), axis = 1 ) geo_cluster_analysis ( df_ , \"latitude\" , \"longitude\" , max_cluster , eps , min_samples , master_path , col_name , global_map_box_val , ) def geohash_col_stats_gen ( df, geohash_col, id_col, master_path, max_val) This function helps to produce descriptive stats for the geohash columns. If there's more than 1 geohash column, an iteratio through all geohash columns will be conducted. Each geohash column will have its own descriptive statistics tables generated by \"descriptive_stats_gen\" function. Parameters df Analysis DataFrame geohash_col Geohash column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns Expand source code def geohash_col_stats_gen ( df , geohash_col , id_col , master_path , max_val ): \"\"\" This function helps to produce descriptive stats for the geohash columns. If there's more than 1 geohash column, an iteratio through all geohash columns will be conducted. Each geohash column will have its own descriptive statistics tables generated by \"descriptive_stats_gen\" function. Parameters ---------- df Analysis DataFrame geohash_col Geohash column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns ------- \"\"\" if len ( geohash_col ) == 1 : descriptive_stats_gen ( df , None , None , geohash_col [ 0 ], id_col , master_path , max_val ) else : for i in range ( 0 , len ( geohash_col )): descriptive_stats_gen ( df , None , None , geohash_col [ i ], id_col , master_path , max_val ) def geospatial_autodetection ( df, id_col, master_path, max_records, top_geo_records, max_cluster, eps, min_samples, global_map_box_val, run_type, auth_key) This function helps to trigger the output of intermediate data which is further used for producing the geospatial-analysis tab in Anovos full report. Descriptive statistics, cluster analysis and visualization of geospatial fields will be triggered in sequence for each lat-long pair and geohash column respectively. Descriptive anallysis is conducted by calling \"stats_gen_lat_long_geo\" function, cluster analysis is conducted by calling \"geo_cluster_generator\" fucntion and visualization of geospatial fields is generated by calling \"generate_loc_charts_controller\" function. Parameters df Analysis DataFrame id_col ID column master_path Path containing all the output from analyzed data max_records Maximum geospatial points analyzed top_geo_records Top geospatial records displayed max_cluster Maximum number of iterations to decide on the optimum cluster eps Epsilon value range (Min EPS, Max EPS, Interval) used for DBSCAN clustering min_samples Minimum Sample Size range (Min Sample Size, Max Sample Size, Interval) used for DBSCAN clustering global_map_box_val Geospatial Chart Theme Index run_type Option to choose between run type \"Local\" or \"EMR\" or \"Azure\" or \"ak8s\" basis the user flexibility. Default option is set as \"Local\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns Expand source code def geospatial_autodetection ( df , id_col , master_path , max_records , top_geo_records , max_cluster , eps , min_samples , global_map_box_val , run_type , auth_key , ): \"\"\" This function helps to trigger the output of intermediate data which is further used for producing the geospatial-analysis tab in Anovos full report. Descriptive statistics, cluster analysis and visualization of geospatial fields will be triggered in sequence for each lat-long pair and geohash column respectively. Descriptive anallysis is conducted by calling \"stats_gen_lat_long_geo\" function, cluster analysis is conducted by calling \"geo_cluster_generator\" fucntion and visualization of geospatial fields is generated by calling \"generate_loc_charts_controller\" function. Parameters ---------- df Analysis DataFrame id_col ID column master_path Path containing all the output from analyzed data max_records Maximum geospatial points analyzed top_geo_records Top geospatial records displayed max_cluster Maximum number of iterations to decide on the optimum cluster eps Epsilon value range (Min EPS, Max EPS, Interval) used for DBSCAN clustering min_samples Minimum Sample Size range (Min Sample Size, Max Sample Size, Interval) used for DBSCAN clustering global_map_box_val Geospatial Chart Theme Index run_type Option to choose between run type \"Local\" or \"EMR\" or \"Azure\" or \"ak8s\" basis the user flexibility. Default option is set as \"Local\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns ------- \"\"\" if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = output_to_local ( master_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) lat_cols , long_cols , gh_cols = ll_gh_cols ( df , max_records ) try : len_lat_col = len ( lat_cols ) except : len_lat_col = 0 try : len_geohash_col = len ( gh_cols ) except : len_geohash_col = 0 if ( len_lat_col > 0 ) or ( len_geohash_col > 0 ): df . persist () stats_gen_lat_long_geo ( df , lat_cols , long_cols , gh_cols , id_col , local_path , top_geo_records ) geo_cluster_generator ( df , lat_cols , long_cols , gh_cols , max_cluster , eps , min_samples , local_path , global_map_box_val , max_records , ) generate_loc_charts_controller ( df , id_col , lat_cols , long_cols , gh_cols , max_records , global_map_box_val , local_path , ) return lat_cols , long_cols , gh_cols elif len_lat_col + len_geohash_col == 0 : return [], [], [] if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( master_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( master_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) def lat_long_col_stats_gen ( df, lat_col, long_col, id_col, master_path, max_val) This function helps to produce descriptive stats for the latitude and longitude columns. If there's more than 1 latitude-longitude pair, an iteration through all pairs will be conducted. Each pair will have its own descriptive statistics tables generated by \"descriptive_stats_gen\" function. Parameters df DataFrame to be analyzed lat_col Latitude column long_col Longitude column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns Expand source code def lat_long_col_stats_gen ( df , lat_col , long_col , id_col , master_path , max_val ): \"\"\" This function helps to produce descriptive stats for the latitude and longitude columns. If there's more than 1 latitude-longitude pair, an iteration through all pairs will be conducted. Each pair will have its own descriptive statistics tables generated by \"descriptive_stats_gen\" function. Parameters ---------- df DataFrame to be analyzed lat_col Latitude column long_col Longitude column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns ------- \"\"\" if len ( lat_col ) == 1 & len ( long_col ) == 1 : descriptive_stats_gen ( df , lat_col [ 0 ], long_col [ 0 ], None , id_col , master_path , max_val ) else : for i in range ( 0 , len ( lat_col )): descriptive_stats_gen ( df , lat_col [ i ], long_col [ i ], None , id_col , master_path , max_val ) def stats_gen_lat_long_geo ( df, lat_col, long_col, geohash_col, id_col, master_path, max_val) This function is the main function used when generating geospatial-analysis tab for Anovos full report. It helps to produce descriptive statistics files for the geospatial fields by calling \"lat_long_col_stats_gen\" and \"geohash_col_stats_gen\" respectively, and the files will be used for generating Anovos full report's Geospatial Analyzer tab. If lat_col and long_col are valid, \"lat_long_col_stats_gen\" function will be called and intermediate files (overall summary and tables showing top lat-long pairs) will be stored inside master_path. If geohash_col is valid, \"geohash_col_stats_gen\" function will be called and intermediate files (overall summary and tables showing top geohash distribution) will be stored inside master_path. Parameters df Analysis DataFrame lat_col Latitude column long_col Longitude column geohash_col Geohash column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns Expand source code def stats_gen_lat_long_geo ( df , lat_col , long_col , geohash_col , id_col , master_path , max_val ): \"\"\" This function is the main function used when generating geospatial-analysis tab for Anovos full report. It helps to produce descriptive statistics files for the geospatial fields by calling \"lat_long_col_stats_gen\" and \"geohash_col_stats_gen\" respectively, and the files will be used for generating Anovos full report's Geospatial Analyzer tab. If lat_col and long_col are valid, \"lat_long_col_stats_gen\" function will be called and intermediate files (overall summary and tables showing top lat-long pairs) will be stored inside master_path. If geohash_col is valid, \"geohash_col_stats_gen\" function will be called and intermediate files (overall summary and tables showing top geohash distribution) will be stored inside master_path. Parameters ---------- df Analysis DataFrame lat_col Latitude column long_col Longitude column geohash_col Geohash column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns ------- \"\"\" if lat_col : len_lat = len ( lat_col ) ll_stats = lat_long_col_stats_gen ( df , lat_col , long_col , id_col , master_path , max_val ) else : len_lat = 0 if geohash_col : len_geohash_col = len ( geohash_col ) geohash_stats = geohash_col_stats_gen ( df , geohash_col , id_col , master_path , max_val ) else : len_geohash_col = 0 if ( len_lat + len_geohash_col ) == 1 : if len_lat == 0 : return geohash_stats else : return ll_stats elif ( len_lat + len_geohash_col ) > 1 : if ( len_lat > 1 ) and ( len_geohash_col == 0 ): return ll_stats elif ( len_lat == 0 ) and ( len_geohash_col > 1 ): return geohash_stats elif ( len_lat >= 1 ) and ( len_geohash_col >= 1 ): return ll_stats , geohash_stats","title":"<code>geospatial_analyzer</code>"},{"location":"api/data_analyzer/geospatial_analyzer.html#geospatial_analyzer","text":"This module help to analyze & summarize the geospatial related data fields which are identified through the auto-detection module. Additionally, it generates the intermediate output which are fed in to the reporting section. As a part of generation of final output, there are various functions created under this sub-module. All of them are listed below. descriptive_stats_gen lat_long_col_stats_gen geohash_col_stats_gen stats_gen_lat_long_geo geo_cluster_analysis geo_cluster_generator generate_loc_charts_processor generate_loc_charts_controller geospatial_autodetection Respective functions have sections containing the detailed definition of the parameters used for computing. Expand source code # coding=utf-8 \"\"\"This module help to analyze & summarize the geospatial related data fields which are identified through the auto-detection module. Additionally, it generates the intermediate output which are fed in to the reporting section. As a part of generation of final output, there are various functions created under this sub-module. All of them are listed below. - descriptive_stats_gen - lat_long_col_stats_gen - geohash_col_stats_gen - stats_gen_lat_long_geo - geo_cluster_analysis - geo_cluster_generator - generate_loc_charts_processor - generate_loc_charts_controller - geospatial_autodetection Respective functions have sections containing the detailed definition of the parameters used for computing. \"\"\" from anovos.shared.utils import ends_with , output_to_local , path_ak8s_modify from anovos.data_ingest import data_sampling from anovos.data_ingest.geo_auto_detection import ll_gh_cols , geo_to_latlong import pandas as pd import numpy as np from sklearn.cluster import MiniBatchKMeans from sklearn.metrics import silhouette_score from itertools import product from pathlib import Path from pyspark.sql import functions as F from sklearn.cluster import DBSCAN import subprocess import plotly.express as px import plotly.graph_objects as go import warnings warnings . filterwarnings ( \"ignore\" ) global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" blank_chart = go . Figure () blank_chart . layout . plot_bgcolor = global_plot_bg_color blank_chart . layout . paper_bgcolor = global_paper_bg_color blank_chart . update_xaxes ( visible = False ) blank_chart . update_yaxes ( visible = False ) mapbox_list = [ \"open-street-map\" , \"white-bg\" , \"carto-positron\" , \"carto-darkmatter\" , \"stamen- terrain\" , \"stamen-toner\" , \"stamen-watercolor\" , ] def descriptive_stats_gen ( df , lat_col , long_col , geohash_col , id_col , master_path , max_val ): \"\"\" This function is the base function to produce descriptive stats for geospatial fields, and save relevant outputs in csv format inside master_path. If lat_col and long_col are valid, two tables will be generated - An overall summary table: This table has two columns: \"stats\" and \"count\", and 5 rows. These 5 rows summarizes the count of distinct {lat, long} pair count, latitude and longitude and shows the most common {lat,long} pair with occurrence respectively. - A top lat-long pairs table: This table shows the top lat-long pairs based on occurrence, and max_val parameter determines the number of records. If geohash_col is valid, two tables will be generated - An overall summary table: This table has two columns: \"stats\" and \"count\", and 3 rows. These 3 rows displays the total number of distinct geohashes, precision level observed for geohashes and the most common geohash respectively. - A top geohash distribution table: This table shows the top geohash distributions based on occurrence, and max_val parameter determines the number of records. Parameters ---------- df DataFrame to be analyzed lat_col Latitude column long_col Longitude column geohash_col Geohash column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns ------- DataFrame[CSV] \"\"\" if ( lat_col is not None ) & ( long_col is not None ): dist_lat_long , dist_lat , dist_long = ( df . select ( lat_col , long_col ) . distinct () . count (), df . select ( lat_col ) . distinct () . count (), df . select ( long_col ) . distinct () . count (), ) top_lat_long = ( df . withColumn ( \"lat_long_pair\" , F . concat ( F . lit ( \"[\" ), F . col ( lat_col ), F . lit ( \",\" ), F . col ( long_col ), F . lit ( \"]\" ) ), ) . groupBy ( \"lat_long_pair\" ) . agg ( F . countDistinct ( id_col ) . alias ( \"count_id\" ), F . count ( id_col ) . alias ( \"count_records\" ), ) . orderBy ( \"count_id\" , ascending = False ) . limit ( max_val ) ) most_lat_long = top_lat_long . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] most_lat_long_cnt = top_lat_long . rdd . flatMap ( lambda x : x ) . collect ()[ 1 ] top_lat_long = top_lat_long . toPandas () d1 = dist_lat_long , dist_lat , dist_long , most_lat_long , most_lat_long_cnt d1_desc = ( \"Distinct {Lat, Long} Pair\" , \"Distinct Latitude\" , \"Distinct Longitude\" , \"Most Common {Lat, Long} Pair\" , \"Most Common {Lat, Long} Pair Occurence\" , ) gen_stats = ( pd . DataFrame ( d1 , d1_desc ) . reset_index () . rename ( columns = { \"index\" : \"Stats\" , 0 : \"Count\" }) ) l = [ \"Overall_Summary\" , \"Top_\" + str ( max_val ) + \"_Lat_Long\" ] for idx , i in enumerate ([ gen_stats , top_lat_long ]): i . to_csv ( ends_with ( master_path ) + l [ idx ] + \"_1_\" + lat_col + \"_\" + long_col + \".csv\" , index = False , ) if geohash_col is not None : dist_geohash = df . select ( geohash_col ) . distinct () . count () precision_geohash = ( df . select ( F . max ( F . length ( F . col ( geohash_col )))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) max_occuring_geohash = ( df . groupBy ( geohash_col ) . agg ( F . count ( id_col ) . alias ( \"count_records\" )) . orderBy ( \"count_records\" , ascending = False ) . limit ( 1 ) ) geohash_val = max_occuring_geohash . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] geohash_cnt = max_occuring_geohash . rdd . flatMap ( lambda x : x ) . collect ()[ 1 ] l = [ \"Overall_Summary\" , \"Top_\" + str ( max_val ) + \"_Geohash_Distribution\" ] geohash_area_width_height_1_12 = [ \"5,009.4km x 4,992.6km\" , \"1,252.3km x 624.1km\" , \"156.5km x 156km\" , \"39.1km x 19.5km\" , \"4.9km x 4.9km\" , \"1.2km x 609.4m\" , \"152.9m x 152.4m\" , \"38.2m x 19m\" , \"4.8m x 4.8m\" , \"1.2m x 59.5cm\" , \"14.9cm x 14.9cm\" , \"3.7cm x 1.9cm\" , ] pd . DataFrame ( [ [ \"Total number of Distinct Geohashes\" , str ( dist_geohash )], [ \"The Precision level observed for the Geohashes\" , str ( precision_geohash ) + \" [Reference Area Width x Height : \" + str ( geohash_area_width_height_1_12 [ precision_geohash - 1 ]) + \"] \" , ], [ \"The Most Common Geohash\" , str ( geohash_val ) + \" , \" + str ( geohash_cnt ), ], ], columns = [ \"Stats\" , \"Count\" ], ) . to_csv ( ends_with ( master_path ) + l [ 0 ] + \"_2_\" + geohash_col + \".csv\" , index = False ) df . withColumn ( \"geohash_\" + str ( precision_geohash ), F . substring ( F . col ( geohash_col ), 1 , precision_geohash ), ) . groupBy ( \"geohash_\" + str ( precision_geohash )) . agg ( F . countDistinct ( id_col ) . alias ( \"count_id\" ), F . count ( id_col ) . alias ( \"count_records\" ), ) . orderBy ( \"count_id\" , ascending = False ) . limit ( max_val ) . toPandas () . to_csv ( ends_with ( master_path ) + l [ 1 ] + \"_2_\" + geohash_col + \".csv\" , index = False ) def lat_long_col_stats_gen ( df , lat_col , long_col , id_col , master_path , max_val ): \"\"\" This function helps to produce descriptive stats for the latitude and longitude columns. If there's more than 1 latitude-longitude pair, an iteration through all pairs will be conducted. Each pair will have its own descriptive statistics tables generated by \"descriptive_stats_gen\" function. Parameters ---------- df DataFrame to be analyzed lat_col Latitude column long_col Longitude column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns ------- \"\"\" if len ( lat_col ) == 1 & len ( long_col ) == 1 : descriptive_stats_gen ( df , lat_col [ 0 ], long_col [ 0 ], None , id_col , master_path , max_val ) else : for i in range ( 0 , len ( lat_col )): descriptive_stats_gen ( df , lat_col [ i ], long_col [ i ], None , id_col , master_path , max_val ) def geohash_col_stats_gen ( df , geohash_col , id_col , master_path , max_val ): \"\"\" This function helps to produce descriptive stats for the geohash columns. If there's more than 1 geohash column, an iteratio through all geohash columns will be conducted. Each geohash column will have its own descriptive statistics tables generated by \"descriptive_stats_gen\" function. Parameters ---------- df Analysis DataFrame geohash_col Geohash column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns ------- \"\"\" if len ( geohash_col ) == 1 : descriptive_stats_gen ( df , None , None , geohash_col [ 0 ], id_col , master_path , max_val ) else : for i in range ( 0 , len ( geohash_col )): descriptive_stats_gen ( df , None , None , geohash_col [ i ], id_col , master_path , max_val ) def stats_gen_lat_long_geo ( df , lat_col , long_col , geohash_col , id_col , master_path , max_val ): \"\"\" This function is the main function used when generating geospatial-analysis tab for Anovos full report. It helps to produce descriptive statistics files for the geospatial fields by calling \"lat_long_col_stats_gen\" and \"geohash_col_stats_gen\" respectively, and the files will be used for generating Anovos full report's Geospatial Analyzer tab. If lat_col and long_col are valid, \"lat_long_col_stats_gen\" function will be called and intermediate files (overall summary and tables showing top lat-long pairs) will be stored inside master_path. If geohash_col is valid, \"geohash_col_stats_gen\" function will be called and intermediate files (overall summary and tables showing top geohash distribution) will be stored inside master_path. Parameters ---------- df Analysis DataFrame lat_col Latitude column long_col Longitude column geohash_col Geohash column id_col ID column master_path Path containing all the output from analyzed data max_val Top geospatial records displayed Returns ------- \"\"\" if lat_col : len_lat = len ( lat_col ) ll_stats = lat_long_col_stats_gen ( df , lat_col , long_col , id_col , master_path , max_val ) else : len_lat = 0 if geohash_col : len_geohash_col = len ( geohash_col ) geohash_stats = geohash_col_stats_gen ( df , geohash_col , id_col , master_path , max_val ) else : len_geohash_col = 0 if ( len_lat + len_geohash_col ) == 1 : if len_lat == 0 : return geohash_stats else : return ll_stats elif ( len_lat + len_geohash_col ) > 1 : if ( len_lat > 1 ) and ( len_geohash_col == 0 ): return ll_stats elif ( len_lat == 0 ) and ( len_geohash_col > 1 ): return geohash_stats elif ( len_lat >= 1 ) and ( len_geohash_col >= 1 ): return ll_stats , geohash_stats def geo_cluster_analysis ( df , lat_col , long_col , max_cluster , eps , min_samples , master_path , col_name , global_map_box_val , ): \"\"\" This function is the base function to generate cluster analysis statistics for the geospatial fields, and save 8 plots in JSON format inside master_path. K-Means and DBSCAN are the two clustering algorihtm used and the 8 plots are divided into 4 sections as below: - Cluster Identification: The first plot displays the cluster-identification process using K-Means algorithm. It is an elbow curve plot showing the distortion vs. number of clusters, and identifies the optimal number of clusters with a vertical line at K. The second plot displays the cluster-identification process using DBSCAN algorithm. It shows the distribution of silouhette scores across different parameters in a heatmap, and a darker color represents smaller scores. - Cluster Distribution The first plot shows distribution of clusters generated by K-Means algorithm in a pie-chart, and the distance is calculated using Euclidean distance. The second plot shows distribution of clusters generated by DBSCAN algorithm in a pie-chart, and the distance is calculated using Haversine distance. - Visualization The first plow is a Mapbox scatter plot of cluster-wise geospatial datapoints using K-Means algorithm. Color-coded datapoints are shown in a map which allows zoom-in, zoom-out, and latitude, longitude and cluster information are displayed for each label. The second plow is a Mapbox scatter plot of cluster-wise geospatial datapoints using DBSCAN algorithm. Color-coded datapoints are shown in a map which allows zoom-in, zoom-out, and latitude, longitude and cluster information are displayed for each label. Displaying these two plots together allows users to have an intuitive impact of results generated by different clustering techniques. - Outlier Points Unlike other sections, this section only contains results generated by DBSCAN algorithm. The first plot is a scatter plot of outlier points captured using DBSCAN algorithm with Euclidean distance calculation. The x-axis is longitude and y-axis is latitude, and outlier points will be marked as \"X\". The second plot is a scatter plot of outlier points captured using DBSCAN algorithm with Haversine distance calculation. The x-axis is longitude and y-axis is latitude, and outlier points will be marked as \"X\". Parameters ---------- df Analysis DataFrame lat_col Latitude column long_col Longitude column max_cluster Maximum number of iterations to decide on the optimum cluster eps Epsilon value range (Min EPS, Max EPS, Interval) used for DBSCAN clustering min_samples Minimum Sample Size range (Min Sample Size, Max Sample Size, Interval) used for DBSCAN clustering master_path Path containing all the output from analyzed data col_name Analysis column global_map_box_val Geospatial Chart Theme Index Returns ------- \"\"\" df_ = df [[ lat_col , long_col ]] max_k = int ( max_cluster ) ## iterations distortions = [] for i in range ( 2 , max_k + 1 ): if len ( df_ ) >= i : model = MiniBatchKMeans ( n_clusters = i , init = \"k-means++\" , max_iter = 300 , n_init = 10 , random_state = 0 ) model . fit ( df_ ) distortions . append ( model . inertia_ ) ## best k: the lowest derivative k = [ i * 100 for i in np . diff ( distortions , 2 )] . index ( min ([ i * 100 for i in np . diff ( distortions , 2 )]) ) ## plot f1 = go . Figure () f1 . add_trace ( go . Scatter ( x = list ( range ( 1 , len ( distortions ) + 1 )), y = distortions , mode = \"lines+markers\" , name = \"lines+markers\" , line = dict ( color = global_theme [ 2 ], width = 2 , dash = \"dash\" ), marker = dict ( size = 10 ), ) ) f1 . update_yaxes ( title = \"Distortion\" , showgrid = True , gridwidth = 1 , gridcolor = px . colors . sequential . gray [ 10 ], ) f1 . update_xaxes ( title = \"Values of K\" ) f1 . add_vline ( x = k , line_width = 3 , line_dash = \"dash\" , line_color = global_theme [ 4 ]) f1 . update_layout ( title_text = \"Elbow Curve Showing the Optimal Number of Clusters [K : \" + str ( k ) + \"] <br><sup>Algorithm Used : KMeans</sup>\" ) f1 . layout . plot_bgcolor = global_plot_bg_color f1 . layout . paper_bgcolor = global_paper_bg_color f1 . write_json ( ends_with ( master_path ) + \"cluster_plot_1_elbow_\" + col_name ) model = MiniBatchKMeans ( n_clusters = k , init = \"k-means++\" , max_iter = 300 , n_init = 10 , random_state = 0 ) df_ [ \"cluster\" ] = model . fit_predict ( df_ ) df_ . to_csv ( ends_with ( master_path ) + \"cluster_output_kmeans_\" + col_name + \".csv\" , index = False , ) # Use `hole` to create a donut-like pie chart cluster_dtls = df_ . groupby ([ \"cluster\" ]) . size () . reset_index ( name = \"counts\" ) f2 = go . Figure ( go . Pie ( labels = list ( cluster_dtls . cluster . values ), values = list ( cluster_dtls . counts . values ), hole = 0.3 , marker_colors = global_theme , text = list ( cluster_dtls . cluster . values ), ) ) f2 . update_layout ( title_text = \"Distribution of Clusters\" + \"<br><sup>Algorithm Used : K-Means (Distance : Euclidean) </sup>\" , legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ), ) f2 . write_json ( ends_with ( master_path ) + \"cluster_plot_2_kmeans_\" + col_name ) f3 = px . scatter_mapbox ( df_ , lat = lat_col , lon = long_col , color = \"cluster\" , color_continuous_scale = global_theme , mapbox_style = mapbox_list [ global_map_box_val ], ) f3 . update_geos ( fitbounds = \"locations\" ) f3 . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ]) f3 . update_layout ( title_text = \"Cluster Wise Geospatial Datapoints \" + \"<br><sup>Algorithm Used : K-Means</sup>\" ) f3 . update_layout ( coloraxis_showscale = False , autosize = False , width = 1200 , height = 900 ) f3 . write_json ( ends_with ( master_path ) + \"cluster_plot_3_kmeans_\" + col_name ) # Reading in 2D Feature Space df_ = df [[ lat_col , long_col ]] # DBSCAN model with parameters eps = eps . split ( \",\" ) min_samples = min_samples . split ( \",\" ) for i in range ( 3 ): eps [ i ] = float ( eps [ i ]) min_samples [ i ] = float ( min_samples [ i ]) DBSCAN_params = list ( product ( np . arange ( eps [ 0 ], eps [ 1 ], eps [ 2 ]), np . arange ( min_samples [ 0 ], min_samples [ 1 ], min_samples [ 2 ]), ) ) no_of_clusters = [] sil_score = [] for p in DBSCAN_params : try : DBS_clustering = DBSCAN ( eps = p [ 0 ], min_samples = p [ 1 ], metric = \"haversine\" ) . fit ( df_ ) sil_score . append ( silhouette_score ( df_ , DBS_clustering . labels_ )) except : sil_score . append ( 0 ) tmp = pd . DataFrame . from_records ( DBSCAN_params , columns = [ \"Eps\" , \"Min_samples\" ]) tmp [ \"Sil_score\" ] = sil_score eps_ , min_samples_ = list ( tmp . sort_values ( \"Sil_score\" , ascending = False ) . values [ 0 ])[ 0 : 2 ] DBS_clustering = DBSCAN ( eps = eps_ , min_samples = min_samples_ , metric = \"haversine\" ) . fit ( df_ ) DBSCAN_clustered = df_ . copy () DBSCAN_clustered . loc [:, \"Cluster\" ] = DBS_clustering . labels_ DBSCAN_clustered . to_csv ( ends_with ( master_path ) + \"cluster_output_dbscan_\" + col_name + \".csv\" , index = False , ) pivot_1 = pd . pivot_table ( tmp , values = \"Sil_score\" , index = \"Min_samples\" , columns = \"Eps\" ) f1_ = px . imshow ( pivot_1 . values , text_auto = \".3f\" , color_continuous_scale = global_theme , aspect = \"auto\" , y = list ( pivot_1 . index ), x = list ( pivot_1 . columns ), ) f1_ . update_xaxes ( title = \"Eps\" ) f1_ . update_yaxes ( title = \"Min_samples\" ) f1_ . update_traces ( text = np . around ( pivot_1 . values , decimals = 3 ), texttemplate = \"% {text} \" ) f1_ . update_layout ( title_text = \"Distribution of Silhouette Scores Across Different Parameters \" + \"<br><sup>Algorithm Used : DBSCAN</sup>\" ) f1_ . layout . plot_bgcolor = global_plot_bg_color f1_ . layout . paper_bgcolor = global_paper_bg_color f1_ . write_json ( ends_with ( master_path ) + \"cluster_plot_1_silhoutte_\" + col_name ) DBSCAN_clustered . loc [ DBSCAN_clustered [ \"Cluster\" ] == - 1 , \"Cluster\" ] = 999 cluster_dtls_ = ( DBSCAN_clustered . groupby ([ \"Cluster\" ]) . size () . reset_index ( name = \"counts\" ) ) f2_ = go . Figure ( go . Pie ( labels = list ( cluster_dtls_ . Cluster . values ), values = list ( cluster_dtls_ . counts . values ), hole = 0.3 , marker_colors = global_theme , text = list ( cluster_dtls_ . Cluster . values ), ) ) f2_ . update_layout ( title_text = \"Distribution of Clusters\" + \"<br><sup>Algorithm Used : DBSCAN (Distance : Haversine) </sup>\" , legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ), ) f2_ . write_json ( ends_with ( master_path ) + \"cluster_plot_2_dbscan_\" + col_name ) f3_ = px . scatter_mapbox ( DBSCAN_clustered , lat = lat_col , lon = long_col , color = \"Cluster\" , color_continuous_scale = global_theme , mapbox_style = mapbox_list [ global_map_box_val ], ) f3_ . update_geos ( fitbounds = \"locations\" ) f3_ . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ]) f3_ . update_layout ( title_text = \"Cluster Wise Geospatial Datapoints \" + \"<br><sup>Algorithm Used : DBSCAN</sup>\" ) f3_ . update_layout ( autosize = False , width = 1200 , height = 900 ) f3_ . update_coloraxes ( showscale = False ) f3_ . write_json ( ends_with ( master_path ) + \"cluster_plot_3_dbscan_\" + col_name ) try : DBSCAN_clustered_ = df_ . copy () df_outlier = DBSCAN ( eps = eps_ , min_samples = min_samples_ ) . fit ( DBSCAN_clustered_ ) DBSCAN_clustered_ . loc [:, \"Cluster\" ] = df_outlier . labels_ DBSCAN_clustered_ = DBSCAN_clustered_ [ DBSCAN_clustered_ . Cluster . values == - 1 ] DBSCAN_clustered_ [ \"outlier\" ] = 1 f4 = go . Figure ( go . Scatter ( mode = \"markers\" , x = DBSCAN_clustered_ [ long_col ], y = DBSCAN_clustered_ [ lat_col ], marker_symbol = \"x-thin\" , marker_line_color = \"black\" , marker_color = \"black\" , marker_line_width = 2 , marker_size = 20 , ) ) f4 . layout . plot_bgcolor = global_plot_bg_color f4 . layout . paper_bgcolor = global_paper_bg_color f4 . update_xaxes ( title_text = \"longitude\" ) f4 . update_yaxes ( title_text = \"latitude\" ) f4 . update_layout ( autosize = False , width = 1200 , height = 900 ) f4 . update_layout ( title_text = \"Outlier Points Captured By Cluster Analysis\" + \"<br><sup>Algorithm Used : DBSCAN (Distance : Euclidean)</sup>\" ) f4 . write_json ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_1_\" + col_name ) except : f4 = blank_chart f4 . update_layout ( title_text = \"No Outliers Were Found Using DBSCAN (Distance : Euclidean)\" ) f4 . write_json ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_1_\" + col_name ) try : df_outlier_ = DBSCAN_clustered [ DBSCAN_clustered . Cluster . values == 999 ] f4_ = go . Figure ( go . Scatter ( mode = \"markers\" , x = df_outlier_ [ long_col ], y = df_outlier_ [ lat_col ], marker_symbol = \"x-thin\" , marker_line_color = \"black\" , marker_color = \"black\" , marker_line_width = 2 , marker_size = 20 , ) ) f4_ . layout . plot_bgcolor = global_plot_bg_color f4_ . layout . paper_bgcolor = global_paper_bg_color f4_ . update_xaxes ( title_text = \"longitude\" ) f4_ . update_yaxes ( title_text = \"latitude\" ) f4_ . update_layout ( autosize = False , width = 1200 , height = 900 ) f4_ . update_layout ( title_text = \"Outlier Points Captured By Cluster Analysis\" + \"<br><sup>Algorithm Used : DBSCAN (Distance : Haversine)</sup>\" ) f4_ . write_json ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_2_\" + col_name ) except : f4_ = blank_chart f4_ . update_layout ( title_text = \"No Outliers Were Found Using DBSCAN (Distance : Haversine)\" ) f4_ . write_json ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_2_\" + col_name ) def geo_cluster_generator ( df , lat_col_list , long_col_list , geo_col_list , max_cluster , eps , min_samples , master_path , global_map_box_val , max_records , ): \"\"\" This function helps to trigger cluster analysis stats for the identified geospatial fields by calling \"geo_cluster_analysis\" function. If lat-long pairs are available, cluster analysis of each pair will be conducted and intermediate files will be saved inside master_path. If geohash columns are available, cluster analysis of each geohash column will be conducted and intermediate files will be saved into master_path. Parameters ---------- df Analysis DataFrame lat_col_list Latitude columns identified in the data long_col_list Longitude columns identified in the data geo_col_list Geohash columns identified in the data max_cluster Maximum number of iterations to decide on the optimum cluster eps Epsilon value range (Min EPS, Max EPS, Interval) used for DBSCAN clustering min_samples Minimum Sample Size range (Min Sample Size, Max Sample Size, Interval) used for DBSCAN clustering master_path Path containing all the output from analyzed data global_map_box_val Geospatial Chart Theme Index max_records Maximum geospatial points analyzed Returns ------- \"\"\" if isinstance ( df , pd . DataFrame ): pass else : cnt_records = df . count () frac_sample = float ( max_records ) / float ( cnt_records ) if frac_sample > 1 : frac_sample_ = 1.0 else : frac_sample_ = float ( frac_sample ) df = df . select ( * [ lat_col_list + long_col_list + geo_col_list ]) . dropna () if frac_sample_ == 1.0 : df = df . toPandas () else : df = data_sampling . data_sample ( df , strata_cols = \"all\" , fraction = frac_sample_ ) . toPandas () try : lat_col = lat_col_list long_col = long_col_list except : lat_col = [] try : geohash_col = geo_col_list except : geohash_col = [] if len ( lat_col ) >= 1 : for idx , i in enumerate ( lat_col ): col_name = lat_col [ idx ] + \"_\" + long_col [ idx ] geo_cluster_analysis ( df , lat_col [ idx ], long_col [ idx ], max_cluster , eps , min_samples , master_path , col_name , global_map_box_val , ) if len ( geohash_col ) >= 1 : for idx , i in enumerate ( geohash_col ): col_name = geohash_col [ idx ] df_ = df df_ [ \"latitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_name ], 0 ), axis = 1 ) df_ [ \"longitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_name ], 1 ), axis = 1 ) geo_cluster_analysis ( df_ , \"latitude\" , \"longitude\" , max_cluster , eps , min_samples , master_path , col_name , global_map_box_val , ) def generate_loc_charts_processor ( df , lat_col , long_col , geohash_col , max_val , id_col , global_map_box_val , master_path ): \"\"\" This function helps to generate the output of location charts for the geospatial fields, and save Mapbox scatter plots in JSON format inside master_path. If lat-long pairs are available, Mapbox scatter plot of each pair will be generated to visualize the locations of each datapoint. If geohash columns are available, every geohash column will go through geohash-to-lat-long transformation, and Mapbox scatter plot of the transformed lat-long pairs will be generated. Parameters ---------- df Analysis DataFrame lat_col Latitude columns identified in the data long_col Longitude columns identified in the data geohash_col Geohash columns identified in the data max_val Maximum geospatial points analyzed id_col ID column global_map_box_val Geospatial Chart Theme Index master_path Path containing all the output from analyzed data Returns ------- \"\"\" if lat_col : cols_to_select = lat_col + long_col + [ id_col ] elif geohash_col : cols_to_select = geohash_col + [ id_col ] df = df . select ( cols_to_select ) . dropna () if lat_col : if len ( lat_col ) == 1 : df_ = ( df . groupBy ( lat_col [ 0 ], long_col [ 0 ]) . agg ( F . countDistinct ( id_col ) . alias ( \"count\" )) . orderBy ( \"count\" , ascending = False ) . limit ( max_val ) . toPandas () ) base_map = px . scatter_mapbox ( df_ , lat = lat_col [ 0 ], lon = long_col [ 0 ], mapbox_style = mapbox_list [ global_map_box_val ], size = \"count\" , color_discrete_sequence = global_theme , ) base_map . update_geos ( fitbounds = \"locations\" ) base_map . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ], autosize = False , width = 1200 , height = 900 , ) base_map . write_json ( ends_with ( master_path ) + \"loc_charts_ll_\" + lat_col [ 0 ] + \"_\" + long_col [ 0 ] ) elif len ( lat_col ) > 1 : # l = [] for i in range ( 0 , len ( lat_col )): df_ = ( df . groupBy ( lat_col [ i ], long_col [ i ]) . agg ( F . countDistinct ( id_col ) . alias ( \"count\" )) . orderBy ( \"count\" , ascending = False ) . limit ( max_val ) . toPandas () ) base_map = px . scatter_mapbox ( df_ , lat = lat_col [ i ], lon = long_col [ i ], mapbox_style = mapbox_list [ global_map_box_val ], size = \"count\" , color_discrete_sequence = global_theme , ) base_map . update_geos ( fitbounds = \"locations\" ) base_map . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ], autosize = False , width = 1200 , height = 900 , ) base_map . write_json ( ends_with ( master_path ) + \"loc_charts_ll_\" + lat_col [ i ] + \"_\" + long_col [ i ] ) if geohash_col : if len ( geohash_col ) == 1 : col_ = geohash_col [ 0 ] df_ = ( df . groupBy ( col_ ) . agg ( F . countDistinct ( id_col ) . alias ( \"count\" )) . orderBy ( \"count\" , ascending = False ) . limit ( max_val ) . toPandas () ) df_ [ \"latitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_ ], 0 ), axis = 1 ) df_ [ \"longitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_ ], 1 ), axis = 1 ) base_map = px . scatter_mapbox ( df_ , lat = \"latitude\" , lon = \"longitude\" , mapbox_style = mapbox_list [ global_map_box_val ], size = \"count\" , color_discrete_sequence = global_theme , ) base_map . update_geos ( fitbounds = \"locations\" ) base_map . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ], autosize = False , width = 1200 , height = 900 , ) base_map . write_json ( ends_with ( master_path ) + \"loc_charts_gh_\" + col_ ) elif len ( geohash_col ) > 1 : # l = [] for i in range ( 0 , len ( geohash_col )): col_ = geohash_col [ i ] df_ = ( df . groupBy ( col_ ) . agg ( F . countDistinct ( id_col ) . alias ( \"count\" )) . orderBy ( \"count\" , ascending = False ) . limit ( max_val ) . toPandas () ) df_ [ \"latitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_ ], 0 ), axis = 1 ) df_ [ \"longitude\" ] = df_ . apply ( lambda x : geo_to_latlong ( x [ col_ ], 1 ), axis = 1 ) base_map = px . scatter_mapbox ( df_ , lat = \"latitude\" , lon = \"longitude\" , mapbox_style = mapbox_list [ global_map_box_val ], size = \"count\" , color_discrete_sequence = global_theme , ) base_map . update_geos ( fitbounds = \"locations\" ) base_map . update_layout ( mapbox_style = mapbox_list [ global_map_box_val ], autosize = False , width = 1200 , height = 900 , ) base_map . write_json ( ends_with ( master_path ) + \"loc_charts_gh_\" + col_ ) def generate_loc_charts_controller ( df , id_col , lat_col , long_col , geohash_col , max_val , global_map_box_val , master_path ): \"\"\" This function helps to trigger the output generation of location charts for the geospatial fields. If lat-long pairs are available, \"generate_loc_charts_processor\" will be called (with geohash_cols set to None) and Mapbox scatter plot will be generated for each pair. If geohash columns are available, \"generate_loc_charts_processor\" will be called (with lat_col, long_col both set to None) and Mapbox scatter plot will be generated for each geohash column. Parameters ---------- df Analysis DataFrame id_col ID column lat_col Latitude columns identified in the data long_col Longitude columns identified in the data geohash_col Geohash columns identified in the data max_val Maximum geospatial points analyzed global_map_box_val Geospatial Chart Theme Index master_path Path containing all the output from analyzed data Returns ------- \"\"\" if lat_col : len_lat = len ( lat_col ) ll_plot = generate_loc_charts_processor ( df , lat_col = lat_col , long_col = long_col , geohash_col = None , max_val = max_val , id_col = id_col , global_map_box_val = global_map_box_val , master_path = master_path , ) else : len_lat = 0 if geohash_col : len_geohash_col = len ( geohash_col ) geohash_plot = generate_loc_charts_processor ( df , lat_col = None , long_col = None , geohash_col = geohash_col , max_val = max_val , id_col = id_col , global_map_box_val = global_map_box_val , master_path = master_path , ) else : len_geohash_col = 0 if ( len_lat + len_geohash_col ) == 1 : if len_lat == 0 : return geohash_plot else : return ll_plot elif ( len_lat + len_geohash_col ) > 1 : if ( len_lat > 1 ) and ( len_geohash_col == 0 ): return ll_plot elif ( len_lat == 0 ) and ( len_geohash_col > 1 ): return geohash_plot elif ( len_lat >= 1 ) and ( len_geohash_col >= 1 ): return ll_plot , geohash_plot def geospatial_autodetection ( df , id_col , master_path , max_records , top_geo_records , max_cluster , eps , min_samples , global_map_box_val , run_type , auth_key , ): \"\"\" This function helps to trigger the output of intermediate data which is further used for producing the geospatial-analysis tab in Anovos full report. Descriptive statistics, cluster analysis and visualization of geospatial fields will be triggered in sequence for each lat-long pair and geohash column respectively. Descriptive anallysis is conducted by calling \"stats_gen_lat_long_geo\" function, cluster analysis is conducted by calling \"geo_cluster_generator\" fucntion and visualization of geospatial fields is generated by calling \"generate_loc_charts_controller\" function. Parameters ---------- df Analysis DataFrame id_col ID column master_path Path containing all the output from analyzed data max_records Maximum geospatial points analyzed top_geo_records Top geospatial records displayed max_cluster Maximum number of iterations to decide on the optimum cluster eps Epsilon value range (Min EPS, Max EPS, Interval) used for DBSCAN clustering min_samples Minimum Sample Size range (Min Sample Size, Max Sample Size, Interval) used for DBSCAN clustering global_map_box_val Geospatial Chart Theme Index run_type Option to choose between run type \"Local\" or \"EMR\" or \"Azure\" or \"ak8s\" basis the user flexibility. Default option is set as \"Local\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns ------- \"\"\" if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = output_to_local ( master_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) lat_cols , long_cols , gh_cols = ll_gh_cols ( df , max_records ) try : len_lat_col = len ( lat_cols ) except : len_lat_col = 0 try : len_geohash_col = len ( gh_cols ) except : len_geohash_col = 0 if ( len_lat_col > 0 ) or ( len_geohash_col > 0 ): df . persist () stats_gen_lat_long_geo ( df , lat_cols , long_cols , gh_cols , id_col , local_path , top_geo_records ) geo_cluster_generator ( df , lat_cols , long_cols , gh_cols , max_cluster , eps , min_samples , local_path , global_map_box_val , max_records , ) generate_loc_charts_controller ( df , id_col , lat_cols , long_cols , gh_cols , max_records , global_map_box_val , local_path , ) return lat_cols , long_cols , gh_cols elif len_lat_col + len_geohash_col == 0 : return [], [], [] if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( master_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( master_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ])","title":"geospatial_analyzer"},{"location":"api/data_analyzer/geospatial_analyzer.html#functions","text":"def descriptive_stats_gen ( df, lat_col, long_col, geohash_col, id_col, master_path, max_val) This function is the base function to produce descriptive stats for geospatial fields, and save relevant outputs in csv format inside master_path. If lat_col and long_col are valid, two tables will be generated - An overall summary table: This table has two columns: \"stats\" and \"count\", and 5 rows. These 5 rows summarizes the count of distinct {lat, long} pair count, latitude and longitude and shows the most common {lat,long} pair with occurrence respectively. - A top lat-long pairs table: This table shows the top lat-long pairs based on occurrence, and max_val parameter determines the number of records. If geohash_col is valid, two tables will be generated - An overall summary table: This table has two columns: \"stats\" and \"count\", and 3 rows. These 3 rows displays the total number of distinct geohashes, precision level observed for geohashes and the most common geohash respectively. - A top geohash distribution table: This table shows the top geohash distributions based on occurrence, and max_val parameter determines the number of records.","title":"Functions"},{"location":"api/data_analyzer/quality_checker.html","text":"quality_checker This submodule focuses on assessing the data quality at both row-level and column-level and also provides an appropriate treatment option to fix quality issues. At the row level, the following checks are done: duplicate_detection nullRows_detection At the column level, the following checks are done: nullColumns_detection outlier_detection IDness_detection biasedness_detection invalidEntries_detection Expand source code # coding=utf-8 \"\"\" This submodule focuses on assessing the data quality at both row-level and column-level and also provides an appropriate treatment option to fix quality issues. At the row level, the following checks are done: - duplicate_detection - nullRows_detection At the column level, the following checks are done: - nullColumns_detection - outlier_detection - IDness_detection - biasedness_detection - invalidEntries_detection \"\"\" import copy import functools import pandas as pd import re import warnings from pyspark.sql import functions as F from pyspark.sql import types as T from anovos.data_analyzer.stats_generator import ( measures_of_cardinality , missingCount_computation , mode_computation , uniqueCount_computation , ) from anovos.data_ingest.data_ingest import read_dataset from anovos.data_transformer.transformers import ( auto_imputation , imputation_matrixFactorization , imputation_MMM , imputation_sklearn , ) from anovos.shared.utils import ( attributeType_segregation , get_dtype , transpose_dataframe , ) def duplicate_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = True , print_impact = False ): \"\"\" As the name implies, this function detects duplication in the input dataset. This means, for a pair of duplicate rows, the values in each column coincide. Duplication check is confined to the list of columns passed in the arguments. As part of treatment, duplicated rows are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after deduplication (if treated else the original dataset). The 2nd dataframe is of schema \u2013 metric, value and contains the total number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, duplicate rows are removed from the input dataframe. (Default value = True) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- if print_impact is True: odf : DataFrame de-duplicated dataframe if treated, else original input dataframe. odf_print : DataFrame schema [metric, value] and contains metrics - number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. if print_impact is False: odf : DataFrame de-duplicated dataframe if treated, else original input dataframe. \"\"\" if not treatment and not print_impact : warnings . warn ( \"The original idf will be the only output. Set print_impact=True to perform detection without treatment\" ) return idf if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) odf_tmp = idf . groupby ( list_of_cols ) . count () . drop ( \"count\" ) odf = odf_tmp if treatment else idf if print_impact : idf_count = idf . count () odf_tmp_count = odf_tmp . count () odf_print = spark . createDataFrame ( [ [ \"rows_count\" , float ( idf_count )], [ \"unique_rows_count\" , float ( odf_tmp_count )], [ \"duplicate_rows\" , float ( idf_count - odf_tmp_count )], [ \"duplicate_pct\" , round (( idf_count - odf_tmp_count ) / idf_count , 4 )], ], schema = [ \"metric\" , \"value\" ], ) print ( \"No. of Rows: \" + str ( idf_count )) print ( \"No. of UNIQUE Rows: \" + str ( odf_tmp_count )) print ( \"No. of Duplicate Rows: \" + str ( idf_count - odf_tmp_count )) print ( \"Percentage of Duplicate Rows: \" + str ( round (( idf_count - odf_tmp_count ) / idf_count , 4 )) ) if print_impact : return odf , odf_print else : return odf def nullRows_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , print_impact = False , ): \"\"\" This function inspects the row quality and computes the number of columns that are missing for a row. This metric is further aggregated to check how many columns are missing for how many rows (or % rows). Intuition is if too many columns are missing for a row, removing it from the modeling may give better results than relying on its imputed values. Therefore as part of the treatment, rows with missing columns above the specified threshold are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after filtering rows with a high number of missing columns (if treated else the original dataframe). The 2nd dataframe is of schema \u2013 null_cols_count, row_count, row_pct, flagged/treated. | null_cols_count | row_count | row_pct | flagged | |-----------------|-----------|---------|---------| | 5 | 11 | 3.0E-4 | 0 | | 7 | 1306 | 0.0401 | 1 | Interpretation: 1306 rows (4.01% of total rows) have 7 missing columns and flagged for are removal because null_cols_count is above the threshold. If treatment is True, then flagged column is renamed as treated to show rows which has been removed. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, rows with high no. of null columns (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines % of columns allowed to be Null per row and takes value between 0 to 1. If % of null columns is above the threshold for a row, it is removed from the dataframe. There is no row removal if the threshold is 1.0. And if the threshold is 0, all rows with null value are removed. (Default value = 0.8) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame Dataframe after row removal if treated, else original input dataframe. odf_print : DataFrame schema [null_cols_count, row_count, row_pct, flagged/treated]. null_cols_count is defined as no. of missing columns in a row. row_count is no. of rows with null_cols_count missing columns. row_pct is row_count divided by number of rows. flagged/treated is 1 if null_cols_count is more than (threshold X Number of Columns), else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) treatment_threshold = float ( treatment_threshold ) if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) def null_count ( * cols ): return cols . count ( None ) f_null_count = F . udf ( null_count , T . LongType ()) odf_tmp = idf . withColumn ( \"null_cols_count\" , f_null_count ( * list_of_cols )) . withColumn ( \"flagged\" , F . when ( F . col ( \"null_cols_count\" ) > ( len ( list_of_cols ) * treatment_threshold ), 1 ) . otherwise ( 0 ), ) if treatment_threshold == 1 : odf_tmp = odf_tmp . withColumn ( \"flagged\" , F . when ( F . col ( \"null_cols_count\" ) == len ( list_of_cols ), 1 ) . otherwise ( 0 ), ) odf_print = ( odf_tmp . groupBy ( \"null_cols_count\" , \"flagged\" ) . agg ( F . count ( F . lit ( 1 )) . alias ( \"row_count\" )) . withColumn ( \"row_pct\" , F . round ( F . col ( \"row_count\" ) / float ( idf . count ()), 4 )) . select ( \"null_cols_count\" , \"row_count\" , \"row_pct\" , \"flagged\" ) . orderBy ( \"null_cols_count\" ) ) if treatment : odf = odf_tmp . where ( F . col ( \"flagged\" ) == 0 ) . drop ( * [ \"null_cols_count\" , \"flagged\" ]) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( odf . count ()) return odf , odf_print def nullColumns_detection ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], treatment = False , treatment_method = \"row_removal\" , treatment_configs = {}, stats_missing = {}, stats_unique = {}, stats_mode = {}, print_impact = False , ): \"\"\" This function inspects the column quality and computes the number of rows that are missing for a column. This function also leverages statistics computed as part of the State Generator module. Statistics are not computed twice if already available. As part of treatments, it currently supports the following methods \u2013 Mean Median Mode (MMM), row_removal, column_removal, KNN, regression, Matrix Factorization (MF), auto imputation (auto). - MMM replaces null value with the measure of central tendency (mode for categorical features and mean/median for numerical features). - row_removal removes all rows with any missing value (output of this treatment is same as nullRows_detection with treatment_threshold of 0). - column_removal remove a column if %rows with a missing value is above treatment_threshold. - KNN/regression create an imputation model for every to-be-imputed column based on the rest of columns in the list_of_cols columns. KNN leverages sklearn.impute.KNNImputer and regression sklearn.impute.IterativeImputer. Since sklearn algorithms are not scalable, we create imputation model on sample dataset and apply that model on the whole dataset in distributed manner using pyspark pandas udf. - Matrix Factorization leverages pyspark.ml.recommendation.ALS algorithm. - auto imputation compares all imputation methods and select the best imputation method based on the least RMSE. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after imputation (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, missing_count, missing_pct. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to inspect e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, missing values are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"row_removal\", \"column_removal\", \"KNN\", \"regression\", \"MF\", \"auto\". (Default value = \"row_removal\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1 (remove column if % of rows with missing value is above this threshold) For row_removal, this argument can be skipped. For MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For KNN, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"KNN\" For regression, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"regression\" For MF, arguments corresponding to imputation_matrixFactorization function (transformer module) are provided, where each key is an argument from imputation_matrixFactorization function. For auto, arguments corresponding to auto_imputation function (transformer module) are provided, where each key is an argument from auto_imputation function. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics or the impact of imputation (if applicable).(Default value = False) Returns ------- odf : DataFrame Imputed dataframe if treated, else original input dataframe. odf_print : DataFrame schema [attribute, missing_count, missing_pct]. missing_count is number of rows with null values for an attribute, and missing_pct is missing_count divided by number of rows. \"\"\" if stats_missing == {}: odf_print = missingCount_computation ( spark , idf ) else : odf_print = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) missing_cols = ( odf_print . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Null Detection - No column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"missing_count\" , T . StringType (), True ), T . StructField ( \"missing_pct\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if treatment_method not in ( \"MMM\" , \"row_removal\" , \"column_removal\" , \"KNN\" , \"regression\" , \"MF\" , \"auto\" , ): raise TypeError ( \"Invalid input for method_type\" ) treatment_threshold = treatment_configs . pop ( \"treatment_threshold\" , None ) if treatment_threshold : treatment_threshold = float ( treatment_threshold ) else : if treatment_method == \"column_removal\" : raise TypeError ( \"Invalid input for column removal threshold\" ) odf_print = odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) if treatment : if treatment_threshold : threshold_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"missing_pct\" ) > treatment_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if treatment_method == \"column_removal\" : odf = idf . drop ( * threshold_cols ) if print_impact : odf_print . show ( len ( list_of_cols )) print ( \"Removed Columns: \" , threshold_cols ) if treatment_method == \"row_removal\" : remove_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] odf = idf . dropna ( subset = list_of_cols ) if print_impact : odf_print . show ( len ( list_of_cols )) print ( \"Before Count: \" + str ( idf . count ())) print ( \"After Count: \" + str ( odf . count ())) if treatment_method == \"MMM\" : if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] odf = imputation_MMM ( spark , idf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , stats_mode = stats_mode , print_impact = print_impact ) if treatment_method in ( \"KNN\" , \"regression\" , \"MF\" , \"auto\" ): if treatment_threshold : list_of_cols = threshold_cols list_of_cols = [ e for e in list_of_cols if e in num_cols ] func_mapping = { \"KNN\" : imputation_sklearn , \"regression\" : imputation_sklearn , \"MF\" : imputation_matrixFactorization , \"auto\" : auto_imputation , } func = func_mapping [ treatment_method ] odf = func ( spark , idf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , print_impact = print_impact ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print def outlier_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], detection_side = \"upper\" , detection_configs = { \"pctile_lower\" : 0.05 , \"pctile_upper\" : 0.95 , \"stdev_lower\" : 3.0 , \"stdev_upper\" : 3.0 , \"IQR_lower\" : 1.5 , \"IQR_upper\" : 1.5 , \"min_validation\" : 2 , }, treatment = True , treatment_method = \"value_replacement\" , pre_existing_model = False , model_path = \"NA\" , sample_size = 1000000 , output_mode = \"replace\" , print_impact = False , ): \"\"\" In Machine Learning, outlier detection identifies values that deviate drastically from the rest of the attribute values. An outlier may be caused simply by chance, measurement error, or inherent heavy-tailed distribution. This function identifies extreme values in both directions (or any direction provided by the user via detection_side argument). By default, outlier is identified by 3 different methodologies and tagged an outlier only if it is validated by at least 2 methods. Users can customize the methodologies they would like to apply and the minimum number of methodologies to be validated under detection_configs argument. - Percentile Method: In this methodology, a value higher than a certain (default 95th) percentile value is considered as an outlier. Similarly, a value lower than a certain (default 5th) percentile value is considered as an outlier. - Standard Deviation Method: In this methodology, if a value is a certain number of standard deviations (default 3) away from the mean, it is identified as an outlier. - Interquartile Range (IQR) Method: if a value is a certain number of IQRs (default 1.5) below Q1 or above Q3, it is identified as an outlier. Q1 is in first quantile/25th percentile, Q3 is in third quantile/75th percentile, and IQR is the difference between third quantile & first quantile. As part of treatments available, outlier values can be replaced by null so that it can be imputed by a reliable imputation methodology (null_replacement). It can also be replaced by maximum or minimum permissible by above methodologies (value_replacement). Lastly, rows can be removed if it is identified with any outlier (row_removal). This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating outlier (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, lower_outliers, upper_outliers. If outliers are checked only for upper end, then lower_outliers column will be shown all zero. Similarly if checked only for lower end, then upper_outliers will be zero for all attributes. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_side \"upper\", \"lower\", \"both\". \"lower\" detects outliers in the lower spectrum of the column range, whereas \"upper\" detects in the upper spectrum. \"both\" detects in both upper and lower end of the spectrum. (Default value = \"upper\") detection_configs Takes input in dictionary format with keys representing upper & lower parameter for three outlier detection methodologies. a) Percentile Method: lower and upper percentile threshold can be set via \"pctile_lower\" & \"pctile_upper\" (default 0.05 & 0.95) Any value above \"pctile_upper\" is considered as an outlier. Similarly, a value lower than \"pctile_lower\" is considered as an outlier. b) Standard Deviation Method: In this methodology, if a value which is below (mean - \"stdev_lower\" * standard deviation) or above (mean + \"stdev_upper\" * standard deviation), then it is identified as an outlier (default 3.0 & 3.0). c) Interquartile Range (IQR) Method: A value which is below (Q1 \u2013 \"IQR_lower\" * IQR) or above (Q3 + \"IQR_lower\" * IQR) is identified as outliers, where Q1 is first quartile/25th percentile, Q3 is third quartile/75th percentile and IQR is difference between third quartile & first quartile (default 1.5 & 1.5). If an attribute value is less (more) than its derived lower (upper) bound value, it is considered as outlier by a methodology. A attribute value is considered as outlier if it is declared as outlier by at least 'min_validation' methodologies (default 2). If 'min_validation' is not specified, the total number of methodologies will be used. In addition, it cannot be larger than the total number of methodologies applied. If detection_side is \"upper\", then \"pctile_lower\", \"stdev_lower\" and \"IQR_lower\" will be ignored and vice versa. Examples (detection_side = \"lower\") - If detection_configs={\"pctile_lower\": 0.05, \"stdev_lower\": 3.0, \"min_validation\": 1}, Percentile and Standard Deviation methods will be applied and a value is considered as outlier if at least 1 methodology categorizes it as an outlier. - If detection_configs={\"pctile_lower\": 0.05, \"stdev_lower\": 3.0}, since \"min_validation\" is not specified, 2 will be used because there are 2 methodologies specified. A value is considered as outlier if at both 2 methodologies categorize it as an outlier. treatment Boolean argument - True or False. If True, outliers are treated as per treatment_method argument. If treatment is False, print_impact should be True to perform detection without treatment. (Default value = True) treatment_method \"null_replacement\", \"row_removal\", \"value_replacement\". In \"null_replacement\", outlier values are replaced by null so that it can be imputed by a reliable imputation methodology. In \"value_replacement\", outlier values are replaced by maximum or minimum permissible value by above methodologies. Lastly in \"row_removal\", rows are removed if it is found with any outlier. (Default value = \"value_replacement\") pre_existing_model Boolean argument \u2013 True or False. True if the model with upper/lower permissible values for each attribute exists already to be used, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. sample_size The maximum number of rows used to calculate the thresholds of outlier detection. Relevant computation includes percentiles, means, standard deviations and quantiles calculation. The computed thresholds will be applied over all rows in the original idf to detect outliers. If the number of rows of idf is smaller than sample_size, the original idf will be used. (Default value = 1000000) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") print_impact True, False This argument is to calculate and print out the impact of treatment (if applicable). If treatment is False, print_impact should be True to perform detection without treatment. (Default value = False). Returns ------- if print_impact is True: odf : DataFrame Dataframe with outliers treated if treatment is True, else original input dataframe. odf_print : DataFrame schema [attribute, lower_outliers, upper_outliers, excluded_due_to_skewness]. lower_outliers is no. of outliers found in the lower spectrum of the attribute range, upper_outliers is outlier count in the upper spectrum, and excluded_due_to_skewness is 0 or 1 indicating whether an attribute is excluded from detection due to skewness. if print_impact is False: odf : DataFrame Dataframe with outliers treated if treatment is True, else original input dataframe. \"\"\" column_order = idf . columns num_cols = attributeType_segregation ( idf )[ 0 ] if not treatment and not print_impact : if ( not pre_existing_model and model_path == \"NA\" ) | pre_existing_model : warnings . warn ( \"The original idf will be the only output. Set print_impact=True to perform detection without treatment\" ) return idf if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"lower_outliers\" , T . StringType (), True ), T . StructField ( \"upper_outliers\" , T . StringType (), True ), ] ) empty_odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) if not list_of_cols : warnings . warn ( \"No Outlier Check - No numerical column to analyze\" ) if print_impact : empty_odf_print . show () return idf , empty_odf_print else : return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if detection_side not in ( \"upper\" , \"lower\" , \"both\" ): raise TypeError ( \"Invalid input for detection_side\" ) if treatment_method not in ( \"null_replacement\" , \"row_removal\" , \"value_replacement\" ): raise TypeError ( \"Invalid input for treatment_method\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) for arg in [ \"pctile_lower\" , \"pctile_upper\" ]: if arg in detection_configs : if ( detection_configs [ arg ] < 0 ) | ( detection_configs [ arg ] > 1 ): raise TypeError ( \"Invalid input for \" + arg ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/outlier_numcols\" ) model_dict_list = ( df_model . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . rdd . map ( lambda row : { row [ 0 ]: row [ 1 ]}) . collect () ) model_dict = {} for d in model_dict_list : model_dict . update ( d ) params = [] present_cols , skewed_cols = [], [] for i in list_of_cols : param = model_dict . get ( i ) if param : if \"skewed_attribute\" in param : skewed_cols . append ( i ) else : param = [ float ( p ) if p else p for p in param ] params . append ( param ) present_cols . append ( i ) diff_cols = list ( set ( list_of_cols ) - set ( present_cols ) - set ( skewed_cols )) if diff_cols : warnings . warn ( \"Columns not found in model_path: \" + \",\" . join ( diff_cols )) if skewed_cols : warnings . warn ( \"Columns excluded from outlier detection due to highly skewed distribution: \" + \",\" . join ( skewed_cols ) ) list_of_cols = present_cols if not list_of_cols : warnings . warn ( \"No Outlier Check - No numerical column to analyze\" ) if print_impact : empty_odf_print . show () return idf , empty_odf_print else : return idf else : check_dict = { \"pctile\" : { \"lower\" : 0 , \"upper\" : 0 }, \"stdev\" : { \"lower\" : 0 , \"upper\" : 0 }, \"IQR\" : { \"lower\" : 0 , \"upper\" : 0 }, } side_mapping = { \"lower\" : [ \"lower\" ], \"upper\" : [ \"upper\" ], \"both\" : [ \"lower\" , \"upper\" ], } for methodology in [ \"pctile\" , \"stdev\" , \"IQR\" ]: for side in side_mapping [ detection_side ]: if methodology + \"_\" + side in detection_configs : check_dict [ methodology ][ side ] = 1 methodologies = [] for key , val in list ( check_dict . items ()): val_list = list ( val . values ()) if detection_side == \"both\" : if val_list in ([ 1 , 0 ], [ 0 , 1 ]): raise TypeError ( \"Invalid input for detection_configs. If detection_side is 'both', the methodologies used on both sides should be the same\" ) if val_list [ 0 ]: methodologies . append ( key ) else : if val [ detection_side ]: methodologies . append ( key ) num_methodologies = len ( methodologies ) if \"min_validation\" in detection_configs : if detection_configs [ \"min_validation\" ] > num_methodologies : raise TypeError ( \"Invalid input for min_validation of detection_configs. It cannot be larger than the total number of methodologies on any side that detection will be applied over.\" ) else : # if min_validation is not present, num of specified methodologies will be used detection_configs [ \"min_validation\" ] = num_methodologies empty_params = [[ None , None ]] * len ( list_of_cols ) idf_count = idf . count () if idf_count > sample_size : idf_sample = idf . sample ( sample_size / idf_count , False , 11 ) . select ( list_of_cols ) else : idf_sample = idf . select ( list_of_cols ) for i in list_of_cols : if get_dtype ( idf_sample , i ) . startswith ( \"decimal\" ): idf_sample = idf_sample . withColumn ( i , F . col ( i ) . cast ( T . DoubleType ())) pctiles = [ detection_configs . get ( \"pctile_lower\" , 0.05 ), detection_configs . get ( \"pctile_upper\" , 0.95 ), ] pctile_params = idf_sample . approxQuantile ( list_of_cols , pctiles , 0.01 ) skewed_cols = [] for i , p in zip ( list_of_cols , pctile_params ): if p [ 0 ] == p [ 1 ]: skewed_cols . append ( i ) if skewed_cols : warnings . warn ( \"Columns excluded from outlier detection due to highly skewed distribution: \" + \",\" . join ( skewed_cols ) ) for i in skewed_cols : idx = list_of_cols . index ( i ) list_of_cols . pop ( idx ) pctile_params . pop ( idx ) if \"pctile\" not in methodologies : pctile_params = copy . deepcopy ( empty_params ) if \"stdev\" in methodologies : exprs = [ f ( F . col ( c )) for f in [ F . mean , F . stddev ] for c in list_of_cols ] stats = idf_sample . select ( exprs ) . rdd . flatMap ( lambda x : x ) . collect () mean , stdev = stats [: len ( list_of_cols )], stats [ len ( list_of_cols ) :] stdev_lower = pd . Series ( mean ) - detection_configs . get ( \"stdev_lower\" , 0.0 ) * pd . Series ( stdev ) stdev_upper = pd . Series ( mean ) + detection_configs . get ( \"stdev_upper\" , 0.0 ) * pd . Series ( stdev ) stdev_params = list ( zip ( stdev_lower , stdev_upper )) else : stdev_params = copy . deepcopy ( empty_params ) if \"IQR\" in methodologies : quantiles = idf_sample . approxQuantile ( list_of_cols , [ 0.25 , 0.75 ], 0.01 ) IQR_params = [ [ e [ 0 ] - detection_configs . get ( \"IQR_lower\" , 0.0 ) * ( e [ 1 ] - e [ 0 ]), e [ 1 ] + detection_configs . get ( \"IQR_upper\" , 0.0 ) * ( e [ 1 ] - e [ 0 ]), ] for e in quantiles ] else : IQR_params = copy . deepcopy ( empty_params ) n = detection_configs [ \"min_validation\" ] params = [] for x , y , z in list ( zip ( pctile_params , stdev_params , IQR_params )): lower = sorted ( [ i for i in [ x [ 0 ], y [ 0 ], z [ 0 ]] if i is not None ], reverse = True )[ n - 1 ] upper = sorted ([ i for i in [ x [ 1 ], y [ 1 ], z [ 1 ]] if i is not None ])[ n - 1 ] if detection_side == \"lower\" : param = [ lower , None ] elif detection_side == \"upper\" : param = [ None , upper ] else : param = [ lower , upper ] params . append ( param ) # Saving model File if required if model_path != \"NA\" : if detection_side == \"lower\" : skewed_param = [ \"skewed_attribute\" , None ] elif detection_side == \"upper\" : skewed_param = [ None , \"skewed_attribute\" ] else : skewed_param = [ \"skewed_attribute\" , \"skewed_attribute\" ] schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"parameters\" , T . ArrayType ( T . StringType ()), True ), ] ) df_model = spark . createDataFrame ( zip ( list_of_cols + skewed_cols , params + [ skewed_param ] * len ( skewed_cols ), ), schema = schema , ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/outlier_numcols\" , mode = \"overwrite\" ) if not treatment and not print_impact : return idf def composite_outlier_pandas ( col_param ): def inner ( v ): v = v . astype ( float , errors = \"raise\" ) if detection_side in ( \"lower\" , \"both\" ): lower_v = (( v - col_param [ 0 ]) < 0 ) . replace ( True , - 1 ) . replace ( False , 0 ) if detection_side in ( \"upper\" , \"both\" ): upper_v = (( v - col_param [ 1 ]) > 0 ) . replace ( True , 1 ) . replace ( False , 0 ) if detection_side == \"upper\" : return upper_v elif detection_side == \"lower\" : return lower_v else : return lower_v + upper_v return inner odf = idf list_odf = [] for index , i in enumerate ( list_of_cols ): f_composite_outlier = F . pandas_udf ( composite_outlier_pandas ( params [ index ]), returnType = T . IntegerType () ) odf = odf . withColumn ( i + \"_outliered\" , f_composite_outlier ( i )) if print_impact : odf_agg_col = ( odf . select ( i + \"_outliered\" ) . groupby () . pivot ( i + \"_outliered\" ) . count () ) odf_print_col = ( odf_agg_col . withColumn ( \"lower_outliers\" , F . col ( \"-1\" ) if \"-1\" in odf_agg_col . columns else F . lit ( 0 ), ) . withColumn ( \"upper_outliers\" , F . col ( \"1\" ) if \"1\" in odf_agg_col . columns else F . lit ( 0 ), ) . withColumn ( \"excluded_due_to_skewness\" , F . lit ( 0 )) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"lower_outliers\" , \"upper_outliers\" , \"excluded_due_to_skewness\" , ) . fillna ( 0 ) ) list_odf . append ( odf_print_col ) if treatment & ( treatment_method in ( \"value_replacement\" , \"null_replacement\" )): replace_vals = { \"value_replacement\" : [ params [ index ][ 0 ], params [ index ][ 1 ]], \"null_replacement\" : [ None , None ], } odf = odf . withColumn ( i + \"_outliered\" , F . when ( F . col ( i + \"_outliered\" ) == 1 , replace_vals [ treatment_method ][ 1 ] ) . otherwise ( F . when ( F . col ( i + \"_outliered\" ) == - 1 , replace_vals [ treatment_method ][ 0 ], ) . otherwise ( F . col ( i )) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_outliered\" , i ) if print_impact : def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) odf_print = unionAll ( list_odf ) if skewed_cols : skewed_cols_print = [( i , 0 , 0 , 1 ) for i in skewed_cols ] skewed_cols_odf_print = spark . createDataFrame ( skewed_cols_print , schema = odf_print . columns ) odf_print = unionAll ([ odf_print , skewed_cols_odf_print ]) odf_print . show ( len ( list_of_cols ) + len ( skewed_cols ), False ) if treatment & ( treatment_method == \"row_removal\" ): conditions = [ ( F . col ( i + \"_outliered\" ) == 0 ) | ( F . col ( i + \"_outliered\" ) . isNull ()) for i in list_of_cols ] conditions_combined = functools . reduce ( lambda a , b : a & b , conditions ) odf = odf . where ( conditions_combined ) . drop ( * [ i + \"_outliered\" for i in list_of_cols ] ) if treatment : if output_mode == \"replace\" : odf = odf . select ( column_order ) else : odf = idf if print_impact : return odf , odf_print else : return odf def IDness_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , stats_unique = {}, print_impact = False , ): \"\"\" IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for discrete features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, unique_values, IDness. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high IDness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of IDness (calculated as no. of unique values divided by no. of non-null values) for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if its unique values count is more than 80% of total rows (after excluding null values). stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print : DataFrame schema [attribute, unique_values, IDness, flagged/treated]. unique_values is no. of distinct values in a column, IDness is unique_values divided by no. of non-null values. A column is flagged 1 if IDness is above the threshold, else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No IDness Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), T . StructField ( \"IDness\" , T . StringType (), True ), T . StructField ( \"flagged\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print treatment_threshold = float ( treatment_threshold ) if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if stats_unique == {}: odf_print = measures_of_cardinality ( spark , idf , list_of_cols ) else : odf_print = read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols ) ) odf_print = odf_print . withColumn ( \"flagged\" , F . when ( F . col ( \"IDness\" ) >= treatment_threshold , 1 ) . otherwise ( 0 ) ) if treatment : remove_cols = ( odf_print . where ( F . col ( \"flagged\" ) == 1 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) odf = idf . drop ( * remove_cols ) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) if treatment : print ( \"Removed Columns: \" , remove_cols ) return odf , odf_print def biasedness_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , stats_mode = {}, print_impact = False , ): \"\"\" This function flags column if they are biased or skewed towards one specific value and leverages mode_pct computation from Measures of Central Tendency i.e. number of rows with mode value (most frequently seen value) divided by number of non-null values. It varies between 0 to 100% where biasedness of 100% means there is only a single value (other than null). The function flags a column if its biasedness is above a certain threshold. Such columns can be deleted from the modelling analysis, if required. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high biased columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, mode, mode_pct. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high biasedness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of biasedness (frequency of most-frequently seen value)for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if the number of rows with most-frequently seen value is more than 80% of total rows (after excluding null values). stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print : DataFrame schema [attribute, mode, mode_rows, mode_pct, flagged/treated]. mode is the most frequently seen value, mode_rows is number of rows with mode value, and mode_pct is number of rows with mode value divided by non-null values. A column is flagged 1 if mode_pct is above the threshold else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No biasedness Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . StringType (), True ), T . StructField ( \"mode_pct\" , T . StringType (), True ), T . StructField ( \"flagged\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if stats_mode == {}: odf_print = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"count\" ), \"summary\" ) . withColumnRenamed ( \"key\" , \"attribute\" ) . join ( mode_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" ) . withColumn ( \"mode_pct\" , F . round ( F . col ( \"mode_rows\" ) / F . col ( \"count\" ) . cast ( T . DoubleType ()), 4 ), ) . select ( \"attribute\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) ) else : odf_print = ( read_dataset ( spark , ** stats_mode ) . select ( \"attribute\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) odf_print = odf_print . withColumn ( \"flagged\" , F . when ( ( F . col ( \"mode_pct\" ) >= treatment_threshold ) | ( F . col ( \"mode_pct\" ) . isNull ()), 1 ) . otherwise ( 0 ), ) if treatment : remove_cols = ( odf_print . where ( ( F . col ( \"mode_pct\" ) >= treatment_threshold ) | ( F . col ( \"mode_pct\" ) . isNull ()) ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) odf = idf . drop ( * remove_cols ) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) if treatment : print ( \"Removed Columns: \" , remove_cols ) return odf , odf_print def invalidEntries_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], detection_type = \"auto\" , invalid_entries = [], valid_entries = [], partial_match = False , treatment = False , treatment_method = \"null_replacement\" , treatment_configs = {}, stats_missing = {}, stats_unique = {}, stats_mode = {}, output_mode = \"replace\" , print_impact = False , ): \"\"\" This function checks for certain suspicious patterns in attributes\u2019 values. Patterns that are considered for this quality check: - Missing Values: The function checks for all column values which directly or indirectly indicate the missing value in an attribute such as 'nan', 'null', 'na', 'inf', 'n/a', 'not defined' etc. The function also check for special characters. - Repetitive Characters: Certain attributes\u2019 values with repetitive characters may be default value or system error, rather than being a legit value etc xx, zzzzz, 99999 etc. Such values are flagged for the user to take an appropriate action. There may be certain false positive which are legit values. - Consecutive Characters: Similar to repetitive characters, consecutive characters (at least 3 characters long) such as abc, 1234 etc may not be legit values, and hence flagged. There may be certain false positive which are legit values. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating the invalid values (or the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, invalid_entries, invalid_count, invalid_pct. All potential invalid values (separated by delimiter pipe \u201c|\u201d) are shown under invalid_entries column. Total number of rows impacted by these entries for each attribute is shown under invalid_count. invalid_pct is invalid_count divided by number of rows. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_type \"auto\",\"manual\",\"both\" (Default value = \"auto\") invalid_entries List of values or regex patterns to be classified as invalid. Valid only for \"auto\" or \"both\" detection type. (Default value = []) valid_entries List of values or regex patterns such that a value will be classified as invalid if it does not match any value or regex pattern in it. Valid only for \"auto\" or \"both\" detection type. (Default value = []) partial_match Boolean argument \u2013 True or False. If True, values with substring same as invalid_entries is declared invalid. (Default value = False) treatment Boolean argument \u2013 True or False. If True, outliers are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"null_replacement\", \"column_removal\" (more methods to be added soon). MMM (Mean Median Mode) replaces invalid value by the measure of central tendency (mode for categorical features and mean or median for numerical features). null_replacement removes all values with any invalid values as null. column_removal remove a column if % of rows with invalid value is above a threshold (defined by key \"treatment_threshold\" under treatment_configs argument). (Default value = \"null_replacement\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1. For value replacement, by MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For null_replacement, this argument can be skipped. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_invalid\" e.g. column X is appended as X_invalid. (Default value = \"replace\") print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame Dataframe after treatment if applicable, else original input dataframe. odf_print : DataFrame schema [attribute, invalid_entries, invalid_count, invalid_pct]. invalid_entries are all potential invalid values (separated by delimiter pipe \u201c|\u201d), invalid_count is no. of rows which are impacted by invalid entries, and invalid_pct is invalid_count divided by no of rows. \"\"\" if list_of_cols == \"all\" : list_of_cols = [] for i in idf . dtypes : if i [ 1 ] in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . append ( i [ 0 ]) if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Invalid Entries Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"invalid_entries\" , T . StringType (), True ), T . StructField ( \"invalid_count\" , T . StringType (), True ), T . StructField ( \"invalid_pct\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if treatment_method not in ( \"MMM\" , \"null_replacement\" , \"column_removal\" ): raise TypeError ( \"Invalid input for method_type\" ) treatment_threshold = treatment_configs . pop ( \"treatment_threshold\" , None ) if treatment_threshold : treatment_threshold = float ( treatment_threshold ) else : if treatment_method == \"column_removal\" : raise TypeError ( \"Invalid input for column removal threshold\" ) null_vocab = [ \"\" , \" \" , \"nan\" , \"null\" , \"na\" , \"inf\" , \"n/a\" , \"not defined\" , \"none\" , \"undefined\" , \"blank\" , \"unknown\" , ] special_chars_vocab = [ \"&\" , \"$\" , \";\" , \":\" , \".\" , \",\" , \"*\" , \"#\" , \"@\" , \"_\" , \"?\" , \"%\" , \"!\" , \"^\" , \"(\" , \")\" , \"-\" , \"/\" , \"'\" , ] def detect ( * v ): output = [] for idx , e in enumerate ( v ): if e is None : output . append ( None ) continue if detection_type in ( \"auto\" , \"both\" ): e = str ( e ) . lower () . strip () # Null & Special Chars Search if e in ( null_vocab + special_chars_vocab ): output . append ( 1 ) continue # Consecutive Identical Chars Search regex = \" \\\\ b([a-zA-Z0-9]) \\\\ 1 \\\\ 1+ \\\\ b\" p = re . compile ( regex ) if re . search ( p , e ): output . append ( 1 ) continue # Ordered Chars Search l = len ( e ) check = 0 if l >= 3 : for i in range ( 1 , l ): if ord ( e [ i ]) - ord ( e [ i - 1 ]) != 1 : check = 1 break if check == 0 : output . append ( 1 ) continue check = 0 if detection_type in ( \"manual\" , \"both\" ): e = str ( e ) . lower () . strip () for regex in invalid_entries : p = re . compile ( regex ) if partial_match : if re . search ( p , e ): check = 1 output . append ( 1 ) break else : if p . fullmatch ( e ): check = 1 output . append ( 1 ) break match_valid_entries = [] for regex in valid_entries : p = re . compile ( regex ) if partial_match : if re . search ( p , e ): match_valid_entries . append ( 1 ) else : match_valid_entries . append ( 0 ) else : if p . fullmatch ( e ): match_valid_entries . append ( 1 ) else : match_valid_entries . append ( 0 ) if ( len ( match_valid_entries ) > 0 ) & ( sum ( match_valid_entries ) == 0 ): check = 1 output . append ( 1 ) if check == 0 : output . append ( 0 ) return output f_detect = F . udf ( detect , T . ArrayType ( T . LongType ())) odf = idf . withColumn ( \"invalid\" , f_detect ( * list_of_cols )) odf . persist () output_print = [] for index , i in enumerate ( list_of_cols ): tmp = odf . withColumn ( i + \"_invalid\" , F . col ( \"invalid\" )[ index ]) invalid = ( tmp . where ( F . col ( i + \"_invalid\" ) == 1 ) . select ( i ) . distinct () . rdd . flatMap ( lambda x : x ) . collect () ) invalid = [ str ( x ) for x in invalid ] invalid_count = tmp . where ( F . col ( i + \"_invalid\" ) == 1 ) . count () output_print . append ( [ i , \"|\" . join ( invalid ), invalid_count , round ( invalid_count / idf . count (), 4 )] ) odf_print = spark . createDataFrame ( output_print , schema = [ \"attribute\" , \"invalid_entries\" , \"invalid_count\" , \"invalid_pct\" ], ) if treatment : if treatment_threshold : threshold_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"invalid_pct\" ) > treatment_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if treatment_method in ( \"null_replacement\" , \"MMM\" ): for index , i in enumerate ( list_of_cols ): if treatment_threshold : if i not in threshold_cols : odf = odf . drop ( i + \"_invalid\" ) continue odf = odf . withColumn ( i + \"_invalid\" , F . when ( F . col ( \"invalid\" )[ index ] == 1 , None ) . otherwise ( F . col ( i )), ) if output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_invalid\" , i ) else : if ( odf_print . where ( F . col ( \"attribute\" ) == i ) . select ( \"invalid_pct\" ) . collect ()[ 0 ][ 0 ] == 0.0 ): odf = odf . drop ( i + \"_invalid\" ) odf = odf . drop ( \"invalid\" ) if treatment_method == \"column_removal\" : odf = idf . drop ( * threshold_cols ) if print_impact : print ( \"Removed Columns: \" , threshold_cols ) if treatment_method == \"MMM\" : if stats_unique == {} or output_mode == \"append\" : remove_cols = ( uniqueCount_computation ( spark , odf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] if output_mode == \"append\" : if len ( list_of_cols ) > 0 : list_of_cols = [ e + \"_invalid\" for e in list_of_cols ] odf = imputation_MMM ( spark , odf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , stats_mode = stats_mode , print_impact = print_impact ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print Functions def IDness_detection ( spark, idf, list_of_cols='all', drop_cols=[], treatment=False, treatment_threshold=0.8, stats_unique={}, print_impact=False) IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for discrete features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, unique_values, IDness. Parameters spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high IDness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of IDness (calculated as no. of unique values divided by no. of non-null values) for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if its unique values count is more than 80% of total rows (after excluding null values). stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns odf :\u2002 DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print :\u2002 DataFrame schema [attribute, unique_values, IDness, flagged/treated]. unique_values is no. of distinct values in a column, IDness is unique_values divided by no. of non-null values. A column is flagged 1 if IDness is above the threshold, else 0. Expand source code def IDness_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , stats_unique = {}, print_impact = False , ): \"\"\" IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for discrete features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, unique_values, IDness. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high IDness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of IDness (calculated as no. of unique values divided by no. of non-null values) for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if its unique values count is more than 80% of total rows (after excluding null values). stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print : DataFrame schema [attribute, unique_values, IDness, flagged/treated]. unique_values is no. of distinct values in a column, IDness is unique_values divided by no. of non-null values. A column is flagged 1 if IDness is above the threshold, else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No IDness Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), T . StructField ( \"IDness\" , T . StringType (), True ), T . StructField ( \"flagged\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print treatment_threshold = float ( treatment_threshold ) if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if stats_unique == {}: odf_print = measures_of_cardinality ( spark , idf , list_of_cols ) else : odf_print = read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols ) ) odf_print = odf_print . withColumn ( \"flagged\" , F . when ( F . col ( \"IDness\" ) >= treatment_threshold , 1 ) . otherwise ( 0 ) ) if treatment : remove_cols = ( odf_print . where ( F . col ( \"flagged\" ) == 1 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) odf = idf . drop ( * remove_cols ) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) if treatment : print ( \"Removed Columns: \" , remove_cols ) return odf , odf_print def biasedness_detection ( spark, idf, list_of_cols='all', drop_cols=[], treatment=False, treatment_threshold=0.8, stats_mode={}, print_impact=False) This function flags column if they are biased or skewed towards one specific value and leverages mode_pct computation from Measures of Central Tendency i.e. number of rows with mode value (most frequently seen value) divided by number of non-null values. It varies between 0 to 100% where biasedness of 100% means there is only a single value (other than null). The function flags a column if its biasedness is above a certain threshold. Such columns can be deleted from the modelling analysis, if required. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high biased columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, mode, mode_pct. Parameters spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high biasedness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of biasedness (frequency of most-frequently seen value)for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if the number of rows with most-frequently seen value is more than 80% of total rows (after excluding null values). stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns odf :\u2002 DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print :\u2002 DataFrame schema [attribute, mode, mode_rows, mode_pct, flagged/treated]. mode is the most frequently seen value, mode_rows is number of rows with mode value, and mode_pct is number of rows with mode value divided by non-null values. A column is flagged 1 if mode_pct is above the threshold else 0. Expand source code def biasedness_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , stats_mode = {}, print_impact = False , ): \"\"\" This function flags column if they are biased or skewed towards one specific value and leverages mode_pct computation from Measures of Central Tendency i.e. number of rows with mode value (most frequently seen value) divided by number of non-null values. It varies between 0 to 100% where biasedness of 100% means there is only a single value (other than null). The function flags a column if its biasedness is above a certain threshold. Such columns can be deleted from the modelling analysis, if required. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high biased columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, mode, mode_pct. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high biasedness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of biasedness (frequency of most-frequently seen value)for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if the number of rows with most-frequently seen value is more than 80% of total rows (after excluding null values). stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print : DataFrame schema [attribute, mode, mode_rows, mode_pct, flagged/treated]. mode is the most frequently seen value, mode_rows is number of rows with mode value, and mode_pct is number of rows with mode value divided by non-null values. A column is flagged 1 if mode_pct is above the threshold else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No biasedness Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . StringType (), True ), T . StructField ( \"mode_pct\" , T . StringType (), True ), T . StructField ( \"flagged\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if stats_mode == {}: odf_print = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"count\" ), \"summary\" ) . withColumnRenamed ( \"key\" , \"attribute\" ) . join ( mode_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" ) . withColumn ( \"mode_pct\" , F . round ( F . col ( \"mode_rows\" ) / F . col ( \"count\" ) . cast ( T . DoubleType ()), 4 ), ) . select ( \"attribute\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) ) else : odf_print = ( read_dataset ( spark , ** stats_mode ) . select ( \"attribute\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) odf_print = odf_print . withColumn ( \"flagged\" , F . when ( ( F . col ( \"mode_pct\" ) >= treatment_threshold ) | ( F . col ( \"mode_pct\" ) . isNull ()), 1 ) . otherwise ( 0 ), ) if treatment : remove_cols = ( odf_print . where ( ( F . col ( \"mode_pct\" ) >= treatment_threshold ) | ( F . col ( \"mode_pct\" ) . isNull ()) ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) odf = idf . drop ( * remove_cols ) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) if treatment : print ( \"Removed Columns: \" , remove_cols ) return odf , odf_print def duplicate_detection ( spark, idf, list_of_cols='all', drop_cols=[], treatment=True, print_impact=False) As the name implies, this function detects duplication in the input dataset. This means, for a pair of duplicate rows, the values in each column coincide. Duplication check is confined to the list of columns passed in the arguments. As part of treatment, duplicated rows are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after deduplication (if treated else the original dataset). The 2nd dataframe is of schema \u2013 metric, value and contains the total number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, duplicate rows are removed from the input dataframe. (Default value = True) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns if print_impact is True: odf : DataFrame de-duplicated dataframe if treated, else original input dataframe. odf_print : DataFrame schema [metric, value] and contains metrics - number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. if print_impact is False: odf : DataFrame de-duplicated dataframe if treated, else original input dataframe. Expand source code def duplicate_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = True , print_impact = False ): \"\"\" As the name implies, this function detects duplication in the input dataset. This means, for a pair of duplicate rows, the values in each column coincide. Duplication check is confined to the list of columns passed in the arguments. As part of treatment, duplicated rows are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after deduplication (if treated else the original dataset). The 2nd dataframe is of schema \u2013 metric, value and contains the total number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, duplicate rows are removed from the input dataframe. (Default value = True) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- if print_impact is True: odf : DataFrame de-duplicated dataframe if treated, else original input dataframe. odf_print : DataFrame schema [metric, value] and contains metrics - number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. if print_impact is False: odf : DataFrame de-duplicated dataframe if treated, else original input dataframe. \"\"\" if not treatment and not print_impact : warnings . warn ( \"The original idf will be the only output. Set print_impact=True to perform detection without treatment\" ) return idf if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) odf_tmp = idf . groupby ( list_of_cols ) . count () . drop ( \"count\" ) odf = odf_tmp if treatment else idf if print_impact : idf_count = idf . count () odf_tmp_count = odf_tmp . count () odf_print = spark . createDataFrame ( [ [ \"rows_count\" , float ( idf_count )], [ \"unique_rows_count\" , float ( odf_tmp_count )], [ \"duplicate_rows\" , float ( idf_count - odf_tmp_count )], [ \"duplicate_pct\" , round (( idf_count - odf_tmp_count ) / idf_count , 4 )], ], schema = [ \"metric\" , \"value\" ], ) print ( \"No. of Rows: \" + str ( idf_count )) print ( \"No. of UNIQUE Rows: \" + str ( odf_tmp_count )) print ( \"No. of Duplicate Rows: \" + str ( idf_count - odf_tmp_count )) print ( \"Percentage of Duplicate Rows: \" + str ( round (( idf_count - odf_tmp_count ) / idf_count , 4 )) ) if print_impact : return odf , odf_print else : return odf def invalidEntries_detection ( spark, idf, list_of_cols='all', drop_cols=[], detection_type='auto', invalid_entries=[], valid_entries=[], partial_match=False, treatment=False, treatment_method='null_replacement', treatment_configs={}, stats_missing={}, stats_unique={}, stats_mode={}, output_mode='replace', print_impact=False) This function checks for certain suspicious patterns in attributes\u2019 values. Patterns that are considered for this quality check: Missing Values: The function checks for all column values which directly or indirectly indicate the missing value in an attribute such as 'nan', 'null', 'na', 'inf', 'n/a', 'not defined' etc. The function also check for special characters. Repetitive Characters: Certain attributes\u2019 values with repetitive characters may be default value or system error, rather than being a legit value etc xx, zzzzz, 99999 etc. Such values are flagged for the user to take an appropriate action. There may be certain false positive which are legit values. Consecutive Characters: Similar to repetitive characters, consecutive characters (at least 3 characters long) such as abc, 1234 etc may not be legit values, and hence flagged. There may be certain false positive which are legit values. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating the invalid values (or the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, invalid_entries, invalid_count, invalid_pct. All potential invalid values (separated by delimiter pipe \u201c|\u201d) are shown under invalid_entries column. Total number of rows impacted by these entries for each attribute is shown under invalid_count. invalid_pct is invalid_count divided by number of rows. Parameters spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_type \"auto\",\"manual\",\"both\" (Default value = \"auto\") invalid_entries List of values or regex patterns to be classified as invalid. Valid only for \"auto\" or \"both\" detection type. (Default value = []) valid_entries List of values or regex patterns such that a value will be classified as invalid if it does not match any value or regex pattern in it. Valid only for \"auto\" or \"both\" detection type. (Default value = []) partial_match Boolean argument \u2013 True or False. If True, values with substring same as invalid_entries is declared invalid. (Default value = False) treatment Boolean argument \u2013 True or False. If True, outliers are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"null_replacement\", \"column_removal\" (more methods to be added soon). MMM (Mean Median Mode) replaces invalid value by the measure of central tendency (mode for categorical features and mean or median for numerical features). null_replacement removes all values with any invalid values as null. column_removal remove a column if % of rows with invalid value is above a threshold (defined by key \"treatment_threshold\" under treatment_configs argument). (Default value = \"null_replacement\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1. For value replacement, by MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For null_replacement, this argument can be skipped. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_invalid\" e.g. column X is appended as X_invalid. (Default value = \"replace\") print_impact True, False This argument is to print out the statistics.(Default value = False) Returns odf :\u2002 DataFrame Dataframe after treatment if applicable, else original input dataframe. odf_print :\u2002 DataFrame schema [attribute, invalid_entries, invalid_count, invalid_pct]. invalid_entries are all potential invalid values (separated by delimiter pipe \u201c|\u201d), invalid_count is no. of rows which are impacted by invalid entries, and invalid_pct is invalid_count divided by no of rows. Expand source code def invalidEntries_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], detection_type = \"auto\" , invalid_entries = [], valid_entries = [], partial_match = False , treatment = False , treatment_method = \"null_replacement\" , treatment_configs = {}, stats_missing = {}, stats_unique = {}, stats_mode = {}, output_mode = \"replace\" , print_impact = False , ): \"\"\" This function checks for certain suspicious patterns in attributes\u2019 values. Patterns that are considered for this quality check: - Missing Values: The function checks for all column values which directly or indirectly indicate the missing value in an attribute such as 'nan', 'null', 'na', 'inf', 'n/a', 'not defined' etc. The function also check for special characters. - Repetitive Characters: Certain attributes\u2019 values with repetitive characters may be default value or system error, rather than being a legit value etc xx, zzzzz, 99999 etc. Such values are flagged for the user to take an appropriate action. There may be certain false positive which are legit values. - Consecutive Characters: Similar to repetitive characters, consecutive characters (at least 3 characters long) such as abc, 1234 etc may not be legit values, and hence flagged. There may be certain false positive which are legit values. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating the invalid values (or the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, invalid_entries, invalid_count, invalid_pct. All potential invalid values (separated by delimiter pipe \u201c|\u201d) are shown under invalid_entries column. Total number of rows impacted by these entries for each attribute is shown under invalid_count. invalid_pct is invalid_count divided by number of rows. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_type \"auto\",\"manual\",\"both\" (Default value = \"auto\") invalid_entries List of values or regex patterns to be classified as invalid. Valid only for \"auto\" or \"both\" detection type. (Default value = []) valid_entries List of values or regex patterns such that a value will be classified as invalid if it does not match any value or regex pattern in it. Valid only for \"auto\" or \"both\" detection type. (Default value = []) partial_match Boolean argument \u2013 True or False. If True, values with substring same as invalid_entries is declared invalid. (Default value = False) treatment Boolean argument \u2013 True or False. If True, outliers are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"null_replacement\", \"column_removal\" (more methods to be added soon). MMM (Mean Median Mode) replaces invalid value by the measure of central tendency (mode for categorical features and mean or median for numerical features). null_replacement removes all values with any invalid values as null. column_removal remove a column if % of rows with invalid value is above a threshold (defined by key \"treatment_threshold\" under treatment_configs argument). (Default value = \"null_replacement\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1. For value replacement, by MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For null_replacement, this argument can be skipped. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_invalid\" e.g. column X is appended as X_invalid. (Default value = \"replace\") print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame Dataframe after treatment if applicable, else original input dataframe. odf_print : DataFrame schema [attribute, invalid_entries, invalid_count, invalid_pct]. invalid_entries are all potential invalid values (separated by delimiter pipe \u201c|\u201d), invalid_count is no. of rows which are impacted by invalid entries, and invalid_pct is invalid_count divided by no of rows. \"\"\" if list_of_cols == \"all\" : list_of_cols = [] for i in idf . dtypes : if i [ 1 ] in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . append ( i [ 0 ]) if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Invalid Entries Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"invalid_entries\" , T . StringType (), True ), T . StructField ( \"invalid_count\" , T . StringType (), True ), T . StructField ( \"invalid_pct\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if treatment_method not in ( \"MMM\" , \"null_replacement\" , \"column_removal\" ): raise TypeError ( \"Invalid input for method_type\" ) treatment_threshold = treatment_configs . pop ( \"treatment_threshold\" , None ) if treatment_threshold : treatment_threshold = float ( treatment_threshold ) else : if treatment_method == \"column_removal\" : raise TypeError ( \"Invalid input for column removal threshold\" ) null_vocab = [ \"\" , \" \" , \"nan\" , \"null\" , \"na\" , \"inf\" , \"n/a\" , \"not defined\" , \"none\" , \"undefined\" , \"blank\" , \"unknown\" , ] special_chars_vocab = [ \"&\" , \"$\" , \";\" , \":\" , \".\" , \",\" , \"*\" , \"#\" , \"@\" , \"_\" , \"?\" , \"%\" , \"!\" , \"^\" , \"(\" , \")\" , \"-\" , \"/\" , \"'\" , ] def detect ( * v ): output = [] for idx , e in enumerate ( v ): if e is None : output . append ( None ) continue if detection_type in ( \"auto\" , \"both\" ): e = str ( e ) . lower () . strip () # Null & Special Chars Search if e in ( null_vocab + special_chars_vocab ): output . append ( 1 ) continue # Consecutive Identical Chars Search regex = \" \\\\ b([a-zA-Z0-9]) \\\\ 1 \\\\ 1+ \\\\ b\" p = re . compile ( regex ) if re . search ( p , e ): output . append ( 1 ) continue # Ordered Chars Search l = len ( e ) check = 0 if l >= 3 : for i in range ( 1 , l ): if ord ( e [ i ]) - ord ( e [ i - 1 ]) != 1 : check = 1 break if check == 0 : output . append ( 1 ) continue check = 0 if detection_type in ( \"manual\" , \"both\" ): e = str ( e ) . lower () . strip () for regex in invalid_entries : p = re . compile ( regex ) if partial_match : if re . search ( p , e ): check = 1 output . append ( 1 ) break else : if p . fullmatch ( e ): check = 1 output . append ( 1 ) break match_valid_entries = [] for regex in valid_entries : p = re . compile ( regex ) if partial_match : if re . search ( p , e ): match_valid_entries . append ( 1 ) else : match_valid_entries . append ( 0 ) else : if p . fullmatch ( e ): match_valid_entries . append ( 1 ) else : match_valid_entries . append ( 0 ) if ( len ( match_valid_entries ) > 0 ) & ( sum ( match_valid_entries ) == 0 ): check = 1 output . append ( 1 ) if check == 0 : output . append ( 0 ) return output f_detect = F . udf ( detect , T . ArrayType ( T . LongType ())) odf = idf . withColumn ( \"invalid\" , f_detect ( * list_of_cols )) odf . persist () output_print = [] for index , i in enumerate ( list_of_cols ): tmp = odf . withColumn ( i + \"_invalid\" , F . col ( \"invalid\" )[ index ]) invalid = ( tmp . where ( F . col ( i + \"_invalid\" ) == 1 ) . select ( i ) . distinct () . rdd . flatMap ( lambda x : x ) . collect () ) invalid = [ str ( x ) for x in invalid ] invalid_count = tmp . where ( F . col ( i + \"_invalid\" ) == 1 ) . count () output_print . append ( [ i , \"|\" . join ( invalid ), invalid_count , round ( invalid_count / idf . count (), 4 )] ) odf_print = spark . createDataFrame ( output_print , schema = [ \"attribute\" , \"invalid_entries\" , \"invalid_count\" , \"invalid_pct\" ], ) if treatment : if treatment_threshold : threshold_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"invalid_pct\" ) > treatment_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if treatment_method in ( \"null_replacement\" , \"MMM\" ): for index , i in enumerate ( list_of_cols ): if treatment_threshold : if i not in threshold_cols : odf = odf . drop ( i + \"_invalid\" ) continue odf = odf . withColumn ( i + \"_invalid\" , F . when ( F . col ( \"invalid\" )[ index ] == 1 , None ) . otherwise ( F . col ( i )), ) if output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_invalid\" , i ) else : if ( odf_print . where ( F . col ( \"attribute\" ) == i ) . select ( \"invalid_pct\" ) . collect ()[ 0 ][ 0 ] == 0.0 ): odf = odf . drop ( i + \"_invalid\" ) odf = odf . drop ( \"invalid\" ) if treatment_method == \"column_removal\" : odf = idf . drop ( * threshold_cols ) if print_impact : print ( \"Removed Columns: \" , threshold_cols ) if treatment_method == \"MMM\" : if stats_unique == {} or output_mode == \"append\" : remove_cols = ( uniqueCount_computation ( spark , odf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] if output_mode == \"append\" : if len ( list_of_cols ) > 0 : list_of_cols = [ e + \"_invalid\" for e in list_of_cols ] odf = imputation_MMM ( spark , odf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , stats_mode = stats_mode , print_impact = print_impact ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print def nullColumns_detection ( spark, idf, list_of_cols='missing', drop_cols=[], treatment=False, treatment_method='row_removal', treatment_configs={}, stats_missing={}, stats_unique={}, stats_mode={}, print_impact=False) This function inspects the column quality and computes the number of rows that are missing for a column. This function also leverages statistics computed as part of the State Generator module. Statistics are not computed twice if already available. As part of treatments, it currently supports the following methods \u2013 Mean Median Mode (MMM), row_removal, column_removal, KNN, regression, Matrix Factorization (MF), auto imputation (auto). - MMM replaces null value with the measure of central tendency (mode for categorical features and mean/median for numerical features). - row_removal removes all rows with any missing value (output of this treatment is same as nullRows_detection with treatment_threshold of 0). - column_removal remove a column if %rows with a missing value is above treatment_threshold. - KNN/regression create an imputation model for every to-be-imputed column based on the rest of columns in the list_of_cols columns. KNN leverages sklearn.impute.KNNImputer and regression sklearn.impute.IterativeImputer. Since sklearn algorithms are not scalable, we create imputation model on sample dataset and apply that model on the whole dataset in distributed manner using pyspark pandas udf. - Matrix Factorization leverages pyspark.ml.recommendation.ALS algorithm. - auto imputation compares all imputation methods and select the best imputation method based on the least RMSE. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after imputation (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, missing_count, missing_pct. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to inspect e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, missing values are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"row_removal\", \"column_removal\", \"KNN\", \"regression\", \"MF\", \"auto\". (Default value = \"row_removal\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1 (remove column if % of rows with missing value is above this threshold) For row_removal, this argument can be skipped. For MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For KNN, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"KNN\" For regression, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"regression\" For MF, arguments corresponding to imputation_matrixFactorization function (transformer module) are provided, where each key is an argument from imputation_matrixFactorization function. For auto, arguments corresponding to auto_imputation function (transformer module) are provided, where each key is an argument from auto_imputation function. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics or the impact of imputation (if applicable).(Default value = False) Returns odf :\u2002 DataFrame Imputed dataframe if treated, else original input dataframe. odf_print :\u2002 DataFrame schema [attribute, missing_count, missing_pct]. missing_count is number of rows with null values for an attribute, and missing_pct is missing_count divided by number of rows. Expand source code def nullColumns_detection ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], treatment = False , treatment_method = \"row_removal\" , treatment_configs = {}, stats_missing = {}, stats_unique = {}, stats_mode = {}, print_impact = False , ): \"\"\" This function inspects the column quality and computes the number of rows that are missing for a column. This function also leverages statistics computed as part of the State Generator module. Statistics are not computed twice if already available. As part of treatments, it currently supports the following methods \u2013 Mean Median Mode (MMM), row_removal, column_removal, KNN, regression, Matrix Factorization (MF), auto imputation (auto). - MMM replaces null value with the measure of central tendency (mode for categorical features and mean/median for numerical features). - row_removal removes all rows with any missing value (output of this treatment is same as nullRows_detection with treatment_threshold of 0). - column_removal remove a column if %rows with a missing value is above treatment_threshold. - KNN/regression create an imputation model for every to-be-imputed column based on the rest of columns in the list_of_cols columns. KNN leverages sklearn.impute.KNNImputer and regression sklearn.impute.IterativeImputer. Since sklearn algorithms are not scalable, we create imputation model on sample dataset and apply that model on the whole dataset in distributed manner using pyspark pandas udf. - Matrix Factorization leverages pyspark.ml.recommendation.ALS algorithm. - auto imputation compares all imputation methods and select the best imputation method based on the least RMSE. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after imputation (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, missing_count, missing_pct. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to inspect e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, missing values are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"row_removal\", \"column_removal\", \"KNN\", \"regression\", \"MF\", \"auto\". (Default value = \"row_removal\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1 (remove column if % of rows with missing value is above this threshold) For row_removal, this argument can be skipped. For MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For KNN, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"KNN\" For regression, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"regression\" For MF, arguments corresponding to imputation_matrixFactorization function (transformer module) are provided, where each key is an argument from imputation_matrixFactorization function. For auto, arguments corresponding to auto_imputation function (transformer module) are provided, where each key is an argument from auto_imputation function. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics or the impact of imputation (if applicable).(Default value = False) Returns ------- odf : DataFrame Imputed dataframe if treated, else original input dataframe. odf_print : DataFrame schema [attribute, missing_count, missing_pct]. missing_count is number of rows with null values for an attribute, and missing_pct is missing_count divided by number of rows. \"\"\" if stats_missing == {}: odf_print = missingCount_computation ( spark , idf ) else : odf_print = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) missing_cols = ( odf_print . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Null Detection - No column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"missing_count\" , T . StringType (), True ), T . StructField ( \"missing_pct\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if treatment_method not in ( \"MMM\" , \"row_removal\" , \"column_removal\" , \"KNN\" , \"regression\" , \"MF\" , \"auto\" , ): raise TypeError ( \"Invalid input for method_type\" ) treatment_threshold = treatment_configs . pop ( \"treatment_threshold\" , None ) if treatment_threshold : treatment_threshold = float ( treatment_threshold ) else : if treatment_method == \"column_removal\" : raise TypeError ( \"Invalid input for column removal threshold\" ) odf_print = odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) if treatment : if treatment_threshold : threshold_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"missing_pct\" ) > treatment_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if treatment_method == \"column_removal\" : odf = idf . drop ( * threshold_cols ) if print_impact : odf_print . show ( len ( list_of_cols )) print ( \"Removed Columns: \" , threshold_cols ) if treatment_method == \"row_removal\" : remove_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] odf = idf . dropna ( subset = list_of_cols ) if print_impact : odf_print . show ( len ( list_of_cols )) print ( \"Before Count: \" + str ( idf . count ())) print ( \"After Count: \" + str ( odf . count ())) if treatment_method == \"MMM\" : if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] odf = imputation_MMM ( spark , idf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , stats_mode = stats_mode , print_impact = print_impact ) if treatment_method in ( \"KNN\" , \"regression\" , \"MF\" , \"auto\" ): if treatment_threshold : list_of_cols = threshold_cols list_of_cols = [ e for e in list_of_cols if e in num_cols ] func_mapping = { \"KNN\" : imputation_sklearn , \"regression\" : imputation_sklearn , \"MF\" : imputation_matrixFactorization , \"auto\" : auto_imputation , } func = func_mapping [ treatment_method ] odf = func ( spark , idf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , print_impact = print_impact ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print def nullRows_detection ( spark, idf, list_of_cols='all', drop_cols=[], treatment=False, treatment_threshold=0.8, print_impact=False) This function inspects the row quality and computes the number of columns that are missing for a row. This metric is further aggregated to check how many columns are missing for how many rows (or % rows). Intuition is if too many columns are missing for a row, removing it from the modeling may give better results than relying on its imputed values. Therefore as part of the treatment, rows with missing columns above the specified threshold are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after filtering rows with a high number of missing columns (if treated else the original dataframe). The 2nd dataframe is of schema \u2013 null_cols_count, row_count, row_pct, flagged/treated. null_cols_count row_count row_pct flagged 5 11 3.0E-4 0 7 1306 0.0401 1 Interpretation: 1306 rows (4.01% of total rows) have 7 missing columns and flagged for are removal because null_cols_count is above the threshold. If treatment is True, then flagged column is renamed as treated to show rows which has been removed. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, rows with high no. of null columns (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines % of columns allowed to be Null per row and takes value between 0 to 1. If % of null columns is above the threshold for a row, it is removed from the dataframe. There is no row removal if the threshold is 1.0. And if the threshold is 0, all rows with null value are removed. (Default value = 0.8) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns odf :\u2002 DataFrame Dataframe after row removal if treated, else original input dataframe. odf_print :\u2002 DataFrame schema [null_cols_count, row_count, row_pct, flagged/treated]. null_cols_count is defined as no. of missing columns in a row. row_count is no. of rows with null_cols_count missing columns. row_pct is row_count divided by number of rows. flagged/treated is 1 if null_cols_count is more than (threshold X Number of Columns), else 0. Expand source code def nullRows_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , print_impact = False , ): \"\"\" This function inspects the row quality and computes the number of columns that are missing for a row. This metric is further aggregated to check how many columns are missing for how many rows (or % rows). Intuition is if too many columns are missing for a row, removing it from the modeling may give better results than relying on its imputed values. Therefore as part of the treatment, rows with missing columns above the specified threshold are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after filtering rows with a high number of missing columns (if treated else the original dataframe). The 2nd dataframe is of schema \u2013 null_cols_count, row_count, row_pct, flagged/treated. | null_cols_count | row_count | row_pct | flagged | |-----------------|-----------|---------|---------| | 5 | 11 | 3.0E-4 | 0 | | 7 | 1306 | 0.0401 | 1 | Interpretation: 1306 rows (4.01% of total rows) have 7 missing columns and flagged for are removal because null_cols_count is above the threshold. If treatment is True, then flagged column is renamed as treated to show rows which has been removed. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, rows with high no. of null columns (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines % of columns allowed to be Null per row and takes value between 0 to 1. If % of null columns is above the threshold for a row, it is removed from the dataframe. There is no row removal if the threshold is 1.0. And if the threshold is 0, all rows with null value are removed. (Default value = 0.8) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame Dataframe after row removal if treated, else original input dataframe. odf_print : DataFrame schema [null_cols_count, row_count, row_pct, flagged/treated]. null_cols_count is defined as no. of missing columns in a row. row_count is no. of rows with null_cols_count missing columns. row_pct is row_count divided by number of rows. flagged/treated is 1 if null_cols_count is more than (threshold X Number of Columns), else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) treatment_threshold = float ( treatment_threshold ) if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) def null_count ( * cols ): return cols . count ( None ) f_null_count = F . udf ( null_count , T . LongType ()) odf_tmp = idf . withColumn ( \"null_cols_count\" , f_null_count ( * list_of_cols )) . withColumn ( \"flagged\" , F . when ( F . col ( \"null_cols_count\" ) > ( len ( list_of_cols ) * treatment_threshold ), 1 ) . otherwise ( 0 ), ) if treatment_threshold == 1 : odf_tmp = odf_tmp . withColumn ( \"flagged\" , F . when ( F . col ( \"null_cols_count\" ) == len ( list_of_cols ), 1 ) . otherwise ( 0 ), ) odf_print = ( odf_tmp . groupBy ( \"null_cols_count\" , \"flagged\" ) . agg ( F . count ( F . lit ( 1 )) . alias ( \"row_count\" )) . withColumn ( \"row_pct\" , F . round ( F . col ( \"row_count\" ) / float ( idf . count ()), 4 )) . select ( \"null_cols_count\" , \"row_count\" , \"row_pct\" , \"flagged\" ) . orderBy ( \"null_cols_count\" ) ) if treatment : odf = odf_tmp . where ( F . col ( \"flagged\" ) == 0 ) . drop ( * [ \"null_cols_count\" , \"flagged\" ]) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( odf . count ()) return odf , odf_print def outlier_detection ( spark, idf, list_of_cols='all', drop_cols=[], detection_side='upper', detection_configs={'pctile_lower': 0.05, 'pctile_upper': 0.95, 'stdev_lower': 3.0, 'stdev_upper': 3.0, 'IQR_lower': 1.5, 'IQR_upper': 1.5, 'min_validation': 2}, treatment=True, treatment_method='value_replacement', pre_existing_model=False, model_path='NA', sample_size=1000000, output_mode='replace', print_impact=False) In Machine Learning, outlier detection identifies values that deviate drastically from the rest of the attribute values. An outlier may be caused simply by chance, measurement error, or inherent heavy-tailed distribution. This function identifies extreme values in both directions (or any direction provided by the user via detection_side argument). By default, outlier is identified by 3 different methodologies and tagged an outlier only if it is validated by at least 2 methods. Users can customize the methodologies they would like to apply and the minimum number of methodologies to be validated under detection_configs argument. Percentile Method: In this methodology, a value higher than a certain (default 95th) percentile value is considered as an outlier. Similarly, a value lower than a certain (default 5th) percentile value is considered as an outlier. Standard Deviation Method: In this methodology, if a value is a certain number of standard deviations (default 3) away from the mean, it is identified as an outlier. Interquartile Range (IQR) Method: if a value is a certain number of IQRs (default 1.5) below Q1 or above Q3, it is identified as an outlier. Q1 is in first quantile/25th percentile, Q3 is in third quantile/75th percentile, and IQR is the difference between third quantile & first quantile. As part of treatments available, outlier values can be replaced by null so that it can be imputed by a reliable imputation methodology (null_replacement). It can also be replaced by maximum or minimum permissible by above methodologies (value_replacement). Lastly, rows can be removed if it is identified with any outlier (row_removal). This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating outlier (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, lower_outliers, upper_outliers. If outliers are checked only for upper end, then lower_outliers column will be shown all zero. Similarly if checked only for lower end, then upper_outliers will be zero for all attributes. Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_side \"upper\", \"lower\", \"both\". \"lower\" detects outliers in the lower spectrum of the column range, whereas \"upper\" detects in the upper spectrum. \"both\" detects in both upper and lower end of the spectrum. (Default value = \"upper\") detection_configs Takes input in dictionary format with keys representing upper & lower parameter for three outlier detection methodologies. a) Percentile Method: lower and upper percentile threshold can be set via \"pctile_lower\" & \"pctile_upper\" (default 0.05 & 0.95) Any value above \"pctile_upper\" is considered as an outlier. Similarly, a value lower than \"pctile_lower\" is considered as an outlier. b) Standard Deviation Method: In this methodology, if a value which is below (mean - \"stdev_lower\" * standard deviation) or above (mean + \"stdev_upper\" * standard deviation), then it is identified as an outlier (default 3.0 & 3.0). c) Interquartile Range (IQR) Method: A value which is below (Q1 \u2013 \"IQR_lower\" * IQR) or above (Q3 + \"IQR_lower\" * IQR) is identified as outliers, where Q1 is first quartile/25th percentile, Q3 is third quartile/75th percentile and IQR is difference between third quartile & first quartile (default 1.5 & 1.5). If an attribute value is less (more) than its derived lower (upper) bound value, it is considered as outlier by a methodology. A attribute value is considered as outlier if it is declared as outlier by at least 'min_validation' methodologies (default 2). If 'min_validation' is not specified, the total number of methodologies will be used. In addition, it cannot be larger than the total number of methodologies applied. If detection_side is \"upper\", then \"pctile_lower\", \"stdev_lower\" and \"IQR_lower\" will be ignored and vice versa. Examples (detection_side = \"lower\") - If detection_configs={\"pctile_lower\": 0.05, \"stdev_lower\": 3.0, \"min_validation\": 1}, Percentile and Standard Deviation methods will be applied and a value is considered as outlier if at least 1 methodology categorizes it as an outlier. - If detection_configs={\"pctile_lower\": 0.05, \"stdev_lower\": 3.0}, since \"min_validation\" is not specified, 2 will be used because there are 2 methodologies specified. A value is considered as outlier if at both 2 methodologies categorize it as an outlier. treatment Boolean argument - True or False. If True, outliers are treated as per treatment_method argument. If treatment is False, print_impact should be True to perform detection without treatment. (Default value = True) treatment_method \"null_replacement\", \"row_removal\", \"value_replacement\". In \"null_replacement\", outlier values are replaced by null so that it can be imputed by a reliable imputation methodology. In \"value_replacement\", outlier values are replaced by maximum or minimum permissible value by above methodologies. Lastly in \"row_removal\", rows are removed if it is found with any outlier. (Default value = \"value_replacement\") pre_existing_model Boolean argument \u2013 True or False. True if the model with upper/lower permissible values for each attribute exists already to be used, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. sample_size The maximum number of rows used to calculate the thresholds of outlier detection. Relevant computation includes percentiles, means, standard deviations and quantiles calculation. The computed thresholds will be applied over all rows in the original idf to detect outliers. If the number of rows of idf is smaller than sample_size, the original idf will be used. (Default value = 1000000) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") print_impact True, False This argument is to calculate and print out the impact of treatment (if applicable). If treatment is False, print_impact should be True to perform detection without treatment. (Default value = False). Returns if print_impact is True: odf : DataFrame Dataframe with outliers treated if treatment is True, else original input dataframe. odf_print : DataFrame schema [attribute, lower_outliers, upper_outliers, excluded_due_to_skewness]. lower_outliers is no. of outliers found in the lower spectrum of the attribute range, upper_outliers is outlier count in the upper spectrum, and excluded_due_to_skewness is 0 or 1 indicating whether an attribute is excluded from detection due to skewness. if print_impact is False: odf : DataFrame Dataframe with outliers treated if treatment is True, else original input dataframe. Expand source code def outlier_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], detection_side = \"upper\" , detection_configs = { \"pctile_lower\" : 0.05 , \"pctile_upper\" : 0.95 , \"stdev_lower\" : 3.0 , \"stdev_upper\" : 3.0 , \"IQR_lower\" : 1.5 , \"IQR_upper\" : 1.5 , \"min_validation\" : 2 , }, treatment = True , treatment_method = \"value_replacement\" , pre_existing_model = False , model_path = \"NA\" , sample_size = 1000000 , output_mode = \"replace\" , print_impact = False , ): \"\"\" In Machine Learning, outlier detection identifies values that deviate drastically from the rest of the attribute values. An outlier may be caused simply by chance, measurement error, or inherent heavy-tailed distribution. This function identifies extreme values in both directions (or any direction provided by the user via detection_side argument). By default, outlier is identified by 3 different methodologies and tagged an outlier only if it is validated by at least 2 methods. Users can customize the methodologies they would like to apply and the minimum number of methodologies to be validated under detection_configs argument. - Percentile Method: In this methodology, a value higher than a certain (default 95th) percentile value is considered as an outlier. Similarly, a value lower than a certain (default 5th) percentile value is considered as an outlier. - Standard Deviation Method: In this methodology, if a value is a certain number of standard deviations (default 3) away from the mean, it is identified as an outlier. - Interquartile Range (IQR) Method: if a value is a certain number of IQRs (default 1.5) below Q1 or above Q3, it is identified as an outlier. Q1 is in first quantile/25th percentile, Q3 is in third quantile/75th percentile, and IQR is the difference between third quantile & first quantile. As part of treatments available, outlier values can be replaced by null so that it can be imputed by a reliable imputation methodology (null_replacement). It can also be replaced by maximum or minimum permissible by above methodologies (value_replacement). Lastly, rows can be removed if it is identified with any outlier (row_removal). This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating outlier (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, lower_outliers, upper_outliers. If outliers are checked only for upper end, then lower_outliers column will be shown all zero. Similarly if checked only for lower end, then upper_outliers will be zero for all attributes. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_side \"upper\", \"lower\", \"both\". \"lower\" detects outliers in the lower spectrum of the column range, whereas \"upper\" detects in the upper spectrum. \"both\" detects in both upper and lower end of the spectrum. (Default value = \"upper\") detection_configs Takes input in dictionary format with keys representing upper & lower parameter for three outlier detection methodologies. a) Percentile Method: lower and upper percentile threshold can be set via \"pctile_lower\" & \"pctile_upper\" (default 0.05 & 0.95) Any value above \"pctile_upper\" is considered as an outlier. Similarly, a value lower than \"pctile_lower\" is considered as an outlier. b) Standard Deviation Method: In this methodology, if a value which is below (mean - \"stdev_lower\" * standard deviation) or above (mean + \"stdev_upper\" * standard deviation), then it is identified as an outlier (default 3.0 & 3.0). c) Interquartile Range (IQR) Method: A value which is below (Q1 \u2013 \"IQR_lower\" * IQR) or above (Q3 + \"IQR_lower\" * IQR) is identified as outliers, where Q1 is first quartile/25th percentile, Q3 is third quartile/75th percentile and IQR is difference between third quartile & first quartile (default 1.5 & 1.5). If an attribute value is less (more) than its derived lower (upper) bound value, it is considered as outlier by a methodology. A attribute value is considered as outlier if it is declared as outlier by at least 'min_validation' methodologies (default 2). If 'min_validation' is not specified, the total number of methodologies will be used. In addition, it cannot be larger than the total number of methodologies applied. If detection_side is \"upper\", then \"pctile_lower\", \"stdev_lower\" and \"IQR_lower\" will be ignored and vice versa. Examples (detection_side = \"lower\") - If detection_configs={\"pctile_lower\": 0.05, \"stdev_lower\": 3.0, \"min_validation\": 1}, Percentile and Standard Deviation methods will be applied and a value is considered as outlier if at least 1 methodology categorizes it as an outlier. - If detection_configs={\"pctile_lower\": 0.05, \"stdev_lower\": 3.0}, since \"min_validation\" is not specified, 2 will be used because there are 2 methodologies specified. A value is considered as outlier if at both 2 methodologies categorize it as an outlier. treatment Boolean argument - True or False. If True, outliers are treated as per treatment_method argument. If treatment is False, print_impact should be True to perform detection without treatment. (Default value = True) treatment_method \"null_replacement\", \"row_removal\", \"value_replacement\". In \"null_replacement\", outlier values are replaced by null so that it can be imputed by a reliable imputation methodology. In \"value_replacement\", outlier values are replaced by maximum or minimum permissible value by above methodologies. Lastly in \"row_removal\", rows are removed if it is found with any outlier. (Default value = \"value_replacement\") pre_existing_model Boolean argument \u2013 True or False. True if the model with upper/lower permissible values for each attribute exists already to be used, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. sample_size The maximum number of rows used to calculate the thresholds of outlier detection. Relevant computation includes percentiles, means, standard deviations and quantiles calculation. The computed thresholds will be applied over all rows in the original idf to detect outliers. If the number of rows of idf is smaller than sample_size, the original idf will be used. (Default value = 1000000) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") print_impact True, False This argument is to calculate and print out the impact of treatment (if applicable). If treatment is False, print_impact should be True to perform detection without treatment. (Default value = False). Returns ------- if print_impact is True: odf : DataFrame Dataframe with outliers treated if treatment is True, else original input dataframe. odf_print : DataFrame schema [attribute, lower_outliers, upper_outliers, excluded_due_to_skewness]. lower_outliers is no. of outliers found in the lower spectrum of the attribute range, upper_outliers is outlier count in the upper spectrum, and excluded_due_to_skewness is 0 or 1 indicating whether an attribute is excluded from detection due to skewness. if print_impact is False: odf : DataFrame Dataframe with outliers treated if treatment is True, else original input dataframe. \"\"\" column_order = idf . columns num_cols = attributeType_segregation ( idf )[ 0 ] if not treatment and not print_impact : if ( not pre_existing_model and model_path == \"NA\" ) | pre_existing_model : warnings . warn ( \"The original idf will be the only output. Set print_impact=True to perform detection without treatment\" ) return idf if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"lower_outliers\" , T . StringType (), True ), T . StructField ( \"upper_outliers\" , T . StringType (), True ), ] ) empty_odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) if not list_of_cols : warnings . warn ( \"No Outlier Check - No numerical column to analyze\" ) if print_impact : empty_odf_print . show () return idf , empty_odf_print else : return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if detection_side not in ( \"upper\" , \"lower\" , \"both\" ): raise TypeError ( \"Invalid input for detection_side\" ) if treatment_method not in ( \"null_replacement\" , \"row_removal\" , \"value_replacement\" ): raise TypeError ( \"Invalid input for treatment_method\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) for arg in [ \"pctile_lower\" , \"pctile_upper\" ]: if arg in detection_configs : if ( detection_configs [ arg ] < 0 ) | ( detection_configs [ arg ] > 1 ): raise TypeError ( \"Invalid input for \" + arg ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/outlier_numcols\" ) model_dict_list = ( df_model . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . rdd . map ( lambda row : { row [ 0 ]: row [ 1 ]}) . collect () ) model_dict = {} for d in model_dict_list : model_dict . update ( d ) params = [] present_cols , skewed_cols = [], [] for i in list_of_cols : param = model_dict . get ( i ) if param : if \"skewed_attribute\" in param : skewed_cols . append ( i ) else : param = [ float ( p ) if p else p for p in param ] params . append ( param ) present_cols . append ( i ) diff_cols = list ( set ( list_of_cols ) - set ( present_cols ) - set ( skewed_cols )) if diff_cols : warnings . warn ( \"Columns not found in model_path: \" + \",\" . join ( diff_cols )) if skewed_cols : warnings . warn ( \"Columns excluded from outlier detection due to highly skewed distribution: \" + \",\" . join ( skewed_cols ) ) list_of_cols = present_cols if not list_of_cols : warnings . warn ( \"No Outlier Check - No numerical column to analyze\" ) if print_impact : empty_odf_print . show () return idf , empty_odf_print else : return idf else : check_dict = { \"pctile\" : { \"lower\" : 0 , \"upper\" : 0 }, \"stdev\" : { \"lower\" : 0 , \"upper\" : 0 }, \"IQR\" : { \"lower\" : 0 , \"upper\" : 0 }, } side_mapping = { \"lower\" : [ \"lower\" ], \"upper\" : [ \"upper\" ], \"both\" : [ \"lower\" , \"upper\" ], } for methodology in [ \"pctile\" , \"stdev\" , \"IQR\" ]: for side in side_mapping [ detection_side ]: if methodology + \"_\" + side in detection_configs : check_dict [ methodology ][ side ] = 1 methodologies = [] for key , val in list ( check_dict . items ()): val_list = list ( val . values ()) if detection_side == \"both\" : if val_list in ([ 1 , 0 ], [ 0 , 1 ]): raise TypeError ( \"Invalid input for detection_configs. If detection_side is 'both', the methodologies used on both sides should be the same\" ) if val_list [ 0 ]: methodologies . append ( key ) else : if val [ detection_side ]: methodologies . append ( key ) num_methodologies = len ( methodologies ) if \"min_validation\" in detection_configs : if detection_configs [ \"min_validation\" ] > num_methodologies : raise TypeError ( \"Invalid input for min_validation of detection_configs. It cannot be larger than the total number of methodologies on any side that detection will be applied over.\" ) else : # if min_validation is not present, num of specified methodologies will be used detection_configs [ \"min_validation\" ] = num_methodologies empty_params = [[ None , None ]] * len ( list_of_cols ) idf_count = idf . count () if idf_count > sample_size : idf_sample = idf . sample ( sample_size / idf_count , False , 11 ) . select ( list_of_cols ) else : idf_sample = idf . select ( list_of_cols ) for i in list_of_cols : if get_dtype ( idf_sample , i ) . startswith ( \"decimal\" ): idf_sample = idf_sample . withColumn ( i , F . col ( i ) . cast ( T . DoubleType ())) pctiles = [ detection_configs . get ( \"pctile_lower\" , 0.05 ), detection_configs . get ( \"pctile_upper\" , 0.95 ), ] pctile_params = idf_sample . approxQuantile ( list_of_cols , pctiles , 0.01 ) skewed_cols = [] for i , p in zip ( list_of_cols , pctile_params ): if p [ 0 ] == p [ 1 ]: skewed_cols . append ( i ) if skewed_cols : warnings . warn ( \"Columns excluded from outlier detection due to highly skewed distribution: \" + \",\" . join ( skewed_cols ) ) for i in skewed_cols : idx = list_of_cols . index ( i ) list_of_cols . pop ( idx ) pctile_params . pop ( idx ) if \"pctile\" not in methodologies : pctile_params = copy . deepcopy ( empty_params ) if \"stdev\" in methodologies : exprs = [ f ( F . col ( c )) for f in [ F . mean , F . stddev ] for c in list_of_cols ] stats = idf_sample . select ( exprs ) . rdd . flatMap ( lambda x : x ) . collect () mean , stdev = stats [: len ( list_of_cols )], stats [ len ( list_of_cols ) :] stdev_lower = pd . Series ( mean ) - detection_configs . get ( \"stdev_lower\" , 0.0 ) * pd . Series ( stdev ) stdev_upper = pd . Series ( mean ) + detection_configs . get ( \"stdev_upper\" , 0.0 ) * pd . Series ( stdev ) stdev_params = list ( zip ( stdev_lower , stdev_upper )) else : stdev_params = copy . deepcopy ( empty_params ) if \"IQR\" in methodologies : quantiles = idf_sample . approxQuantile ( list_of_cols , [ 0.25 , 0.75 ], 0.01 ) IQR_params = [ [ e [ 0 ] - detection_configs . get ( \"IQR_lower\" , 0.0 ) * ( e [ 1 ] - e [ 0 ]), e [ 1 ] + detection_configs . get ( \"IQR_upper\" , 0.0 ) * ( e [ 1 ] - e [ 0 ]), ] for e in quantiles ] else : IQR_params = copy . deepcopy ( empty_params ) n = detection_configs [ \"min_validation\" ] params = [] for x , y , z in list ( zip ( pctile_params , stdev_params , IQR_params )): lower = sorted ( [ i for i in [ x [ 0 ], y [ 0 ], z [ 0 ]] if i is not None ], reverse = True )[ n - 1 ] upper = sorted ([ i for i in [ x [ 1 ], y [ 1 ], z [ 1 ]] if i is not None ])[ n - 1 ] if detection_side == \"lower\" : param = [ lower , None ] elif detection_side == \"upper\" : param = [ None , upper ] else : param = [ lower , upper ] params . append ( param ) # Saving model File if required if model_path != \"NA\" : if detection_side == \"lower\" : skewed_param = [ \"skewed_attribute\" , None ] elif detection_side == \"upper\" : skewed_param = [ None , \"skewed_attribute\" ] else : skewed_param = [ \"skewed_attribute\" , \"skewed_attribute\" ] schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"parameters\" , T . ArrayType ( T . StringType ()), True ), ] ) df_model = spark . createDataFrame ( zip ( list_of_cols + skewed_cols , params + [ skewed_param ] * len ( skewed_cols ), ), schema = schema , ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/outlier_numcols\" , mode = \"overwrite\" ) if not treatment and not print_impact : return idf def composite_outlier_pandas ( col_param ): def inner ( v ): v = v . astype ( float , errors = \"raise\" ) if detection_side in ( \"lower\" , \"both\" ): lower_v = (( v - col_param [ 0 ]) < 0 ) . replace ( True , - 1 ) . replace ( False , 0 ) if detection_side in ( \"upper\" , \"both\" ): upper_v = (( v - col_param [ 1 ]) > 0 ) . replace ( True , 1 ) . replace ( False , 0 ) if detection_side == \"upper\" : return upper_v elif detection_side == \"lower\" : return lower_v else : return lower_v + upper_v return inner odf = idf list_odf = [] for index , i in enumerate ( list_of_cols ): f_composite_outlier = F . pandas_udf ( composite_outlier_pandas ( params [ index ]), returnType = T . IntegerType () ) odf = odf . withColumn ( i + \"_outliered\" , f_composite_outlier ( i )) if print_impact : odf_agg_col = ( odf . select ( i + \"_outliered\" ) . groupby () . pivot ( i + \"_outliered\" ) . count () ) odf_print_col = ( odf_agg_col . withColumn ( \"lower_outliers\" , F . col ( \"-1\" ) if \"-1\" in odf_agg_col . columns else F . lit ( 0 ), ) . withColumn ( \"upper_outliers\" , F . col ( \"1\" ) if \"1\" in odf_agg_col . columns else F . lit ( 0 ), ) . withColumn ( \"excluded_due_to_skewness\" , F . lit ( 0 )) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"lower_outliers\" , \"upper_outliers\" , \"excluded_due_to_skewness\" , ) . fillna ( 0 ) ) list_odf . append ( odf_print_col ) if treatment & ( treatment_method in ( \"value_replacement\" , \"null_replacement\" )): replace_vals = { \"value_replacement\" : [ params [ index ][ 0 ], params [ index ][ 1 ]], \"null_replacement\" : [ None , None ], } odf = odf . withColumn ( i + \"_outliered\" , F . when ( F . col ( i + \"_outliered\" ) == 1 , replace_vals [ treatment_method ][ 1 ] ) . otherwise ( F . when ( F . col ( i + \"_outliered\" ) == - 1 , replace_vals [ treatment_method ][ 0 ], ) . otherwise ( F . col ( i )) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_outliered\" , i ) if print_impact : def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) odf_print = unionAll ( list_odf ) if skewed_cols : skewed_cols_print = [( i , 0 , 0 , 1 ) for i in skewed_cols ] skewed_cols_odf_print = spark . createDataFrame ( skewed_cols_print , schema = odf_print . columns ) odf_print = unionAll ([ odf_print , skewed_cols_odf_print ]) odf_print . show ( len ( list_of_cols ) + len ( skewed_cols ), False ) if treatment & ( treatment_method == \"row_removal\" ): conditions = [ ( F . col ( i + \"_outliered\" ) == 0 ) | ( F . col ( i + \"_outliered\" ) . isNull ()) for i in list_of_cols ] conditions_combined = functools . reduce ( lambda a , b : a & b , conditions ) odf = odf . where ( conditions_combined ) . drop ( * [ i + \"_outliered\" for i in list_of_cols ] ) if treatment : if output_mode == \"replace\" : odf = odf . select ( column_order ) else : odf = idf if print_impact : return odf , odf_print else : return odf","title":"<code>quality_checker</code>"},{"location":"api/data_analyzer/quality_checker.html#quality_checker","text":"This submodule focuses on assessing the data quality at both row-level and column-level and also provides an appropriate treatment option to fix quality issues. At the row level, the following checks are done: duplicate_detection nullRows_detection At the column level, the following checks are done: nullColumns_detection outlier_detection IDness_detection biasedness_detection invalidEntries_detection Expand source code # coding=utf-8 \"\"\" This submodule focuses on assessing the data quality at both row-level and column-level and also provides an appropriate treatment option to fix quality issues. At the row level, the following checks are done: - duplicate_detection - nullRows_detection At the column level, the following checks are done: - nullColumns_detection - outlier_detection - IDness_detection - biasedness_detection - invalidEntries_detection \"\"\" import copy import functools import pandas as pd import re import warnings from pyspark.sql import functions as F from pyspark.sql import types as T from anovos.data_analyzer.stats_generator import ( measures_of_cardinality , missingCount_computation , mode_computation , uniqueCount_computation , ) from anovos.data_ingest.data_ingest import read_dataset from anovos.data_transformer.transformers import ( auto_imputation , imputation_matrixFactorization , imputation_MMM , imputation_sklearn , ) from anovos.shared.utils import ( attributeType_segregation , get_dtype , transpose_dataframe , ) def duplicate_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = True , print_impact = False ): \"\"\" As the name implies, this function detects duplication in the input dataset. This means, for a pair of duplicate rows, the values in each column coincide. Duplication check is confined to the list of columns passed in the arguments. As part of treatment, duplicated rows are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after deduplication (if treated else the original dataset). The 2nd dataframe is of schema \u2013 metric, value and contains the total number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, duplicate rows are removed from the input dataframe. (Default value = True) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- if print_impact is True: odf : DataFrame de-duplicated dataframe if treated, else original input dataframe. odf_print : DataFrame schema [metric, value] and contains metrics - number of rows, number of unique rows, number of duplicate rows and percentage of duplicate rows in total. if print_impact is False: odf : DataFrame de-duplicated dataframe if treated, else original input dataframe. \"\"\" if not treatment and not print_impact : warnings . warn ( \"The original idf will be the only output. Set print_impact=True to perform detection without treatment\" ) return idf if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) odf_tmp = idf . groupby ( list_of_cols ) . count () . drop ( \"count\" ) odf = odf_tmp if treatment else idf if print_impact : idf_count = idf . count () odf_tmp_count = odf_tmp . count () odf_print = spark . createDataFrame ( [ [ \"rows_count\" , float ( idf_count )], [ \"unique_rows_count\" , float ( odf_tmp_count )], [ \"duplicate_rows\" , float ( idf_count - odf_tmp_count )], [ \"duplicate_pct\" , round (( idf_count - odf_tmp_count ) / idf_count , 4 )], ], schema = [ \"metric\" , \"value\" ], ) print ( \"No. of Rows: \" + str ( idf_count )) print ( \"No. of UNIQUE Rows: \" + str ( odf_tmp_count )) print ( \"No. of Duplicate Rows: \" + str ( idf_count - odf_tmp_count )) print ( \"Percentage of Duplicate Rows: \" + str ( round (( idf_count - odf_tmp_count ) / idf_count , 4 )) ) if print_impact : return odf , odf_print else : return odf def nullRows_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , print_impact = False , ): \"\"\" This function inspects the row quality and computes the number of columns that are missing for a row. This metric is further aggregated to check how many columns are missing for how many rows (or % rows). Intuition is if too many columns are missing for a row, removing it from the modeling may give better results than relying on its imputed values. Therefore as part of the treatment, rows with missing columns above the specified threshold are removed. This function returns two dataframes in tuple format; the 1st dataframe is the input dataset after filtering rows with a high number of missing columns (if treated else the original dataframe). The 2nd dataframe is of schema \u2013 null_cols_count, row_count, row_pct, flagged/treated. | null_cols_count | row_count | row_pct | flagged | |-----------------|-----------|---------|---------| | 5 | 11 | 3.0E-4 | 0 | | 7 | 1306 | 0.0401 | 1 | Interpretation: 1306 rows (4.01% of total rows) have 7 missing columns and flagged for are removal because null_cols_count is above the threshold. If treatment is True, then flagged column is renamed as treated to show rows which has been removed. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, rows with high no. of null columns (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines % of columns allowed to be Null per row and takes value between 0 to 1. If % of null columns is above the threshold for a row, it is removed from the dataframe. There is no row removal if the threshold is 1.0. And if the threshold is 0, all rows with null value are removed. (Default value = 0.8) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame Dataframe after row removal if treated, else original input dataframe. odf_print : DataFrame schema [null_cols_count, row_count, row_pct, flagged/treated]. null_cols_count is defined as no. of missing columns in a row. row_count is no. of rows with null_cols_count missing columns. row_pct is row_count divided by number of rows. flagged/treated is 1 if null_cols_count is more than (threshold X Number of Columns), else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) treatment_threshold = float ( treatment_threshold ) if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) def null_count ( * cols ): return cols . count ( None ) f_null_count = F . udf ( null_count , T . LongType ()) odf_tmp = idf . withColumn ( \"null_cols_count\" , f_null_count ( * list_of_cols )) . withColumn ( \"flagged\" , F . when ( F . col ( \"null_cols_count\" ) > ( len ( list_of_cols ) * treatment_threshold ), 1 ) . otherwise ( 0 ), ) if treatment_threshold == 1 : odf_tmp = odf_tmp . withColumn ( \"flagged\" , F . when ( F . col ( \"null_cols_count\" ) == len ( list_of_cols ), 1 ) . otherwise ( 0 ), ) odf_print = ( odf_tmp . groupBy ( \"null_cols_count\" , \"flagged\" ) . agg ( F . count ( F . lit ( 1 )) . alias ( \"row_count\" )) . withColumn ( \"row_pct\" , F . round ( F . col ( \"row_count\" ) / float ( idf . count ()), 4 )) . select ( \"null_cols_count\" , \"row_count\" , \"row_pct\" , \"flagged\" ) . orderBy ( \"null_cols_count\" ) ) if treatment : odf = odf_tmp . where ( F . col ( \"flagged\" ) == 0 ) . drop ( * [ \"null_cols_count\" , \"flagged\" ]) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( odf . count ()) return odf , odf_print def nullColumns_detection ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], treatment = False , treatment_method = \"row_removal\" , treatment_configs = {}, stats_missing = {}, stats_unique = {}, stats_mode = {}, print_impact = False , ): \"\"\" This function inspects the column quality and computes the number of rows that are missing for a column. This function also leverages statistics computed as part of the State Generator module. Statistics are not computed twice if already available. As part of treatments, it currently supports the following methods \u2013 Mean Median Mode (MMM), row_removal, column_removal, KNN, regression, Matrix Factorization (MF), auto imputation (auto). - MMM replaces null value with the measure of central tendency (mode for categorical features and mean/median for numerical features). - row_removal removes all rows with any missing value (output of this treatment is same as nullRows_detection with treatment_threshold of 0). - column_removal remove a column if %rows with a missing value is above treatment_threshold. - KNN/regression create an imputation model for every to-be-imputed column based on the rest of columns in the list_of_cols columns. KNN leverages sklearn.impute.KNNImputer and regression sklearn.impute.IterativeImputer. Since sklearn algorithms are not scalable, we create imputation model on sample dataset and apply that model on the whole dataset in distributed manner using pyspark pandas udf. - Matrix Factorization leverages pyspark.ml.recommendation.ALS algorithm. - auto imputation compares all imputation methods and select the best imputation method based on the least RMSE. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after imputation (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, missing_count, missing_pct. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to inspect e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, missing values are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"row_removal\", \"column_removal\", \"KNN\", \"regression\", \"MF\", \"auto\". (Default value = \"row_removal\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1 (remove column if % of rows with missing value is above this threshold) For row_removal, this argument can be skipped. For MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For KNN, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"KNN\" For regression, arguments corresponding to imputation_sklearn function (transformer module) are provided, where each key is an argument from imputation_sklearn function. method_type should be \"regression\" For MF, arguments corresponding to imputation_matrixFactorization function (transformer module) are provided, where each key is an argument from imputation_matrixFactorization function. For auto, arguments corresponding to auto_imputation function (transformer module) are provided, where each key is an argument from auto_imputation function. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics or the impact of imputation (if applicable).(Default value = False) Returns ------- odf : DataFrame Imputed dataframe if treated, else original input dataframe. odf_print : DataFrame schema [attribute, missing_count, missing_pct]. missing_count is number of rows with null values for an attribute, and missing_pct is missing_count divided by number of rows. \"\"\" if stats_missing == {}: odf_print = missingCount_computation ( spark , idf ) else : odf_print = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) missing_cols = ( odf_print . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Null Detection - No column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"missing_count\" , T . StringType (), True ), T . StructField ( \"missing_pct\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if treatment_method not in ( \"MMM\" , \"row_removal\" , \"column_removal\" , \"KNN\" , \"regression\" , \"MF\" , \"auto\" , ): raise TypeError ( \"Invalid input for method_type\" ) treatment_threshold = treatment_configs . pop ( \"treatment_threshold\" , None ) if treatment_threshold : treatment_threshold = float ( treatment_threshold ) else : if treatment_method == \"column_removal\" : raise TypeError ( \"Invalid input for column removal threshold\" ) odf_print = odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) if treatment : if treatment_threshold : threshold_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"missing_pct\" ) > treatment_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if treatment_method == \"column_removal\" : odf = idf . drop ( * threshold_cols ) if print_impact : odf_print . show ( len ( list_of_cols )) print ( \"Removed Columns: \" , threshold_cols ) if treatment_method == \"row_removal\" : remove_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] odf = idf . dropna ( subset = list_of_cols ) if print_impact : odf_print . show ( len ( list_of_cols )) print ( \"Before Count: \" + str ( idf . count ())) print ( \"After Count: \" + str ( odf . count ())) if treatment_method == \"MMM\" : if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] odf = imputation_MMM ( spark , idf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , stats_mode = stats_mode , print_impact = print_impact ) if treatment_method in ( \"KNN\" , \"regression\" , \"MF\" , \"auto\" ): if treatment_threshold : list_of_cols = threshold_cols list_of_cols = [ e for e in list_of_cols if e in num_cols ] func_mapping = { \"KNN\" : imputation_sklearn , \"regression\" : imputation_sklearn , \"MF\" : imputation_matrixFactorization , \"auto\" : auto_imputation , } func = func_mapping [ treatment_method ] odf = func ( spark , idf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , print_impact = print_impact ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print def outlier_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], detection_side = \"upper\" , detection_configs = { \"pctile_lower\" : 0.05 , \"pctile_upper\" : 0.95 , \"stdev_lower\" : 3.0 , \"stdev_upper\" : 3.0 , \"IQR_lower\" : 1.5 , \"IQR_upper\" : 1.5 , \"min_validation\" : 2 , }, treatment = True , treatment_method = \"value_replacement\" , pre_existing_model = False , model_path = \"NA\" , sample_size = 1000000 , output_mode = \"replace\" , print_impact = False , ): \"\"\" In Machine Learning, outlier detection identifies values that deviate drastically from the rest of the attribute values. An outlier may be caused simply by chance, measurement error, or inherent heavy-tailed distribution. This function identifies extreme values in both directions (or any direction provided by the user via detection_side argument). By default, outlier is identified by 3 different methodologies and tagged an outlier only if it is validated by at least 2 methods. Users can customize the methodologies they would like to apply and the minimum number of methodologies to be validated under detection_configs argument. - Percentile Method: In this methodology, a value higher than a certain (default 95th) percentile value is considered as an outlier. Similarly, a value lower than a certain (default 5th) percentile value is considered as an outlier. - Standard Deviation Method: In this methodology, if a value is a certain number of standard deviations (default 3) away from the mean, it is identified as an outlier. - Interquartile Range (IQR) Method: if a value is a certain number of IQRs (default 1.5) below Q1 or above Q3, it is identified as an outlier. Q1 is in first quantile/25th percentile, Q3 is in third quantile/75th percentile, and IQR is the difference between third quantile & first quantile. As part of treatments available, outlier values can be replaced by null so that it can be imputed by a reliable imputation methodology (null_replacement). It can also be replaced by maximum or minimum permissible by above methodologies (value_replacement). Lastly, rows can be removed if it is identified with any outlier (row_removal). This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating outlier (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, lower_outliers, upper_outliers. If outliers are checked only for upper end, then lower_outliers column will be shown all zero. Similarly if checked only for lower end, then upper_outliers will be zero for all attributes. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_side \"upper\", \"lower\", \"both\". \"lower\" detects outliers in the lower spectrum of the column range, whereas \"upper\" detects in the upper spectrum. \"both\" detects in both upper and lower end of the spectrum. (Default value = \"upper\") detection_configs Takes input in dictionary format with keys representing upper & lower parameter for three outlier detection methodologies. a) Percentile Method: lower and upper percentile threshold can be set via \"pctile_lower\" & \"pctile_upper\" (default 0.05 & 0.95) Any value above \"pctile_upper\" is considered as an outlier. Similarly, a value lower than \"pctile_lower\" is considered as an outlier. b) Standard Deviation Method: In this methodology, if a value which is below (mean - \"stdev_lower\" * standard deviation) or above (mean + \"stdev_upper\" * standard deviation), then it is identified as an outlier (default 3.0 & 3.0). c) Interquartile Range (IQR) Method: A value which is below (Q1 \u2013 \"IQR_lower\" * IQR) or above (Q3 + \"IQR_lower\" * IQR) is identified as outliers, where Q1 is first quartile/25th percentile, Q3 is third quartile/75th percentile and IQR is difference between third quartile & first quartile (default 1.5 & 1.5). If an attribute value is less (more) than its derived lower (upper) bound value, it is considered as outlier by a methodology. A attribute value is considered as outlier if it is declared as outlier by at least 'min_validation' methodologies (default 2). If 'min_validation' is not specified, the total number of methodologies will be used. In addition, it cannot be larger than the total number of methodologies applied. If detection_side is \"upper\", then \"pctile_lower\", \"stdev_lower\" and \"IQR_lower\" will be ignored and vice versa. Examples (detection_side = \"lower\") - If detection_configs={\"pctile_lower\": 0.05, \"stdev_lower\": 3.0, \"min_validation\": 1}, Percentile and Standard Deviation methods will be applied and a value is considered as outlier if at least 1 methodology categorizes it as an outlier. - If detection_configs={\"pctile_lower\": 0.05, \"stdev_lower\": 3.0}, since \"min_validation\" is not specified, 2 will be used because there are 2 methodologies specified. A value is considered as outlier if at both 2 methodologies categorize it as an outlier. treatment Boolean argument - True or False. If True, outliers are treated as per treatment_method argument. If treatment is False, print_impact should be True to perform detection without treatment. (Default value = True) treatment_method \"null_replacement\", \"row_removal\", \"value_replacement\". In \"null_replacement\", outlier values are replaced by null so that it can be imputed by a reliable imputation methodology. In \"value_replacement\", outlier values are replaced by maximum or minimum permissible value by above methodologies. Lastly in \"row_removal\", rows are removed if it is found with any outlier. (Default value = \"value_replacement\") pre_existing_model Boolean argument \u2013 True or False. True if the model with upper/lower permissible values for each attribute exists already to be used, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. sample_size The maximum number of rows used to calculate the thresholds of outlier detection. Relevant computation includes percentiles, means, standard deviations and quantiles calculation. The computed thresholds will be applied over all rows in the original idf to detect outliers. If the number of rows of idf is smaller than sample_size, the original idf will be used. (Default value = 1000000) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") print_impact True, False This argument is to calculate and print out the impact of treatment (if applicable). If treatment is False, print_impact should be True to perform detection without treatment. (Default value = False). Returns ------- if print_impact is True: odf : DataFrame Dataframe with outliers treated if treatment is True, else original input dataframe. odf_print : DataFrame schema [attribute, lower_outliers, upper_outliers, excluded_due_to_skewness]. lower_outliers is no. of outliers found in the lower spectrum of the attribute range, upper_outliers is outlier count in the upper spectrum, and excluded_due_to_skewness is 0 or 1 indicating whether an attribute is excluded from detection due to skewness. if print_impact is False: odf : DataFrame Dataframe with outliers treated if treatment is True, else original input dataframe. \"\"\" column_order = idf . columns num_cols = attributeType_segregation ( idf )[ 0 ] if not treatment and not print_impact : if ( not pre_existing_model and model_path == \"NA\" ) | pre_existing_model : warnings . warn ( \"The original idf will be the only output. Set print_impact=True to perform detection without treatment\" ) return idf if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"lower_outliers\" , T . StringType (), True ), T . StructField ( \"upper_outliers\" , T . StringType (), True ), ] ) empty_odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) if not list_of_cols : warnings . warn ( \"No Outlier Check - No numerical column to analyze\" ) if print_impact : empty_odf_print . show () return idf , empty_odf_print else : return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if detection_side not in ( \"upper\" , \"lower\" , \"both\" ): raise TypeError ( \"Invalid input for detection_side\" ) if treatment_method not in ( \"null_replacement\" , \"row_removal\" , \"value_replacement\" ): raise TypeError ( \"Invalid input for treatment_method\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) for arg in [ \"pctile_lower\" , \"pctile_upper\" ]: if arg in detection_configs : if ( detection_configs [ arg ] < 0 ) | ( detection_configs [ arg ] > 1 ): raise TypeError ( \"Invalid input for \" + arg ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/outlier_numcols\" ) model_dict_list = ( df_model . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . rdd . map ( lambda row : { row [ 0 ]: row [ 1 ]}) . collect () ) model_dict = {} for d in model_dict_list : model_dict . update ( d ) params = [] present_cols , skewed_cols = [], [] for i in list_of_cols : param = model_dict . get ( i ) if param : if \"skewed_attribute\" in param : skewed_cols . append ( i ) else : param = [ float ( p ) if p else p for p in param ] params . append ( param ) present_cols . append ( i ) diff_cols = list ( set ( list_of_cols ) - set ( present_cols ) - set ( skewed_cols )) if diff_cols : warnings . warn ( \"Columns not found in model_path: \" + \",\" . join ( diff_cols )) if skewed_cols : warnings . warn ( \"Columns excluded from outlier detection due to highly skewed distribution: \" + \",\" . join ( skewed_cols ) ) list_of_cols = present_cols if not list_of_cols : warnings . warn ( \"No Outlier Check - No numerical column to analyze\" ) if print_impact : empty_odf_print . show () return idf , empty_odf_print else : return idf else : check_dict = { \"pctile\" : { \"lower\" : 0 , \"upper\" : 0 }, \"stdev\" : { \"lower\" : 0 , \"upper\" : 0 }, \"IQR\" : { \"lower\" : 0 , \"upper\" : 0 }, } side_mapping = { \"lower\" : [ \"lower\" ], \"upper\" : [ \"upper\" ], \"both\" : [ \"lower\" , \"upper\" ], } for methodology in [ \"pctile\" , \"stdev\" , \"IQR\" ]: for side in side_mapping [ detection_side ]: if methodology + \"_\" + side in detection_configs : check_dict [ methodology ][ side ] = 1 methodologies = [] for key , val in list ( check_dict . items ()): val_list = list ( val . values ()) if detection_side == \"both\" : if val_list in ([ 1 , 0 ], [ 0 , 1 ]): raise TypeError ( \"Invalid input for detection_configs. If detection_side is 'both', the methodologies used on both sides should be the same\" ) if val_list [ 0 ]: methodologies . append ( key ) else : if val [ detection_side ]: methodologies . append ( key ) num_methodologies = len ( methodologies ) if \"min_validation\" in detection_configs : if detection_configs [ \"min_validation\" ] > num_methodologies : raise TypeError ( \"Invalid input for min_validation of detection_configs. It cannot be larger than the total number of methodologies on any side that detection will be applied over.\" ) else : # if min_validation is not present, num of specified methodologies will be used detection_configs [ \"min_validation\" ] = num_methodologies empty_params = [[ None , None ]] * len ( list_of_cols ) idf_count = idf . count () if idf_count > sample_size : idf_sample = idf . sample ( sample_size / idf_count , False , 11 ) . select ( list_of_cols ) else : idf_sample = idf . select ( list_of_cols ) for i in list_of_cols : if get_dtype ( idf_sample , i ) . startswith ( \"decimal\" ): idf_sample = idf_sample . withColumn ( i , F . col ( i ) . cast ( T . DoubleType ())) pctiles = [ detection_configs . get ( \"pctile_lower\" , 0.05 ), detection_configs . get ( \"pctile_upper\" , 0.95 ), ] pctile_params = idf_sample . approxQuantile ( list_of_cols , pctiles , 0.01 ) skewed_cols = [] for i , p in zip ( list_of_cols , pctile_params ): if p [ 0 ] == p [ 1 ]: skewed_cols . append ( i ) if skewed_cols : warnings . warn ( \"Columns excluded from outlier detection due to highly skewed distribution: \" + \",\" . join ( skewed_cols ) ) for i in skewed_cols : idx = list_of_cols . index ( i ) list_of_cols . pop ( idx ) pctile_params . pop ( idx ) if \"pctile\" not in methodologies : pctile_params = copy . deepcopy ( empty_params ) if \"stdev\" in methodologies : exprs = [ f ( F . col ( c )) for f in [ F . mean , F . stddev ] for c in list_of_cols ] stats = idf_sample . select ( exprs ) . rdd . flatMap ( lambda x : x ) . collect () mean , stdev = stats [: len ( list_of_cols )], stats [ len ( list_of_cols ) :] stdev_lower = pd . Series ( mean ) - detection_configs . get ( \"stdev_lower\" , 0.0 ) * pd . Series ( stdev ) stdev_upper = pd . Series ( mean ) + detection_configs . get ( \"stdev_upper\" , 0.0 ) * pd . Series ( stdev ) stdev_params = list ( zip ( stdev_lower , stdev_upper )) else : stdev_params = copy . deepcopy ( empty_params ) if \"IQR\" in methodologies : quantiles = idf_sample . approxQuantile ( list_of_cols , [ 0.25 , 0.75 ], 0.01 ) IQR_params = [ [ e [ 0 ] - detection_configs . get ( \"IQR_lower\" , 0.0 ) * ( e [ 1 ] - e [ 0 ]), e [ 1 ] + detection_configs . get ( \"IQR_upper\" , 0.0 ) * ( e [ 1 ] - e [ 0 ]), ] for e in quantiles ] else : IQR_params = copy . deepcopy ( empty_params ) n = detection_configs [ \"min_validation\" ] params = [] for x , y , z in list ( zip ( pctile_params , stdev_params , IQR_params )): lower = sorted ( [ i for i in [ x [ 0 ], y [ 0 ], z [ 0 ]] if i is not None ], reverse = True )[ n - 1 ] upper = sorted ([ i for i in [ x [ 1 ], y [ 1 ], z [ 1 ]] if i is not None ])[ n - 1 ] if detection_side == \"lower\" : param = [ lower , None ] elif detection_side == \"upper\" : param = [ None , upper ] else : param = [ lower , upper ] params . append ( param ) # Saving model File if required if model_path != \"NA\" : if detection_side == \"lower\" : skewed_param = [ \"skewed_attribute\" , None ] elif detection_side == \"upper\" : skewed_param = [ None , \"skewed_attribute\" ] else : skewed_param = [ \"skewed_attribute\" , \"skewed_attribute\" ] schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"parameters\" , T . ArrayType ( T . StringType ()), True ), ] ) df_model = spark . createDataFrame ( zip ( list_of_cols + skewed_cols , params + [ skewed_param ] * len ( skewed_cols ), ), schema = schema , ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/outlier_numcols\" , mode = \"overwrite\" ) if not treatment and not print_impact : return idf def composite_outlier_pandas ( col_param ): def inner ( v ): v = v . astype ( float , errors = \"raise\" ) if detection_side in ( \"lower\" , \"both\" ): lower_v = (( v - col_param [ 0 ]) < 0 ) . replace ( True , - 1 ) . replace ( False , 0 ) if detection_side in ( \"upper\" , \"both\" ): upper_v = (( v - col_param [ 1 ]) > 0 ) . replace ( True , 1 ) . replace ( False , 0 ) if detection_side == \"upper\" : return upper_v elif detection_side == \"lower\" : return lower_v else : return lower_v + upper_v return inner odf = idf list_odf = [] for index , i in enumerate ( list_of_cols ): f_composite_outlier = F . pandas_udf ( composite_outlier_pandas ( params [ index ]), returnType = T . IntegerType () ) odf = odf . withColumn ( i + \"_outliered\" , f_composite_outlier ( i )) if print_impact : odf_agg_col = ( odf . select ( i + \"_outliered\" ) . groupby () . pivot ( i + \"_outliered\" ) . count () ) odf_print_col = ( odf_agg_col . withColumn ( \"lower_outliers\" , F . col ( \"-1\" ) if \"-1\" in odf_agg_col . columns else F . lit ( 0 ), ) . withColumn ( \"upper_outliers\" , F . col ( \"1\" ) if \"1\" in odf_agg_col . columns else F . lit ( 0 ), ) . withColumn ( \"excluded_due_to_skewness\" , F . lit ( 0 )) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"lower_outliers\" , \"upper_outliers\" , \"excluded_due_to_skewness\" , ) . fillna ( 0 ) ) list_odf . append ( odf_print_col ) if treatment & ( treatment_method in ( \"value_replacement\" , \"null_replacement\" )): replace_vals = { \"value_replacement\" : [ params [ index ][ 0 ], params [ index ][ 1 ]], \"null_replacement\" : [ None , None ], } odf = odf . withColumn ( i + \"_outliered\" , F . when ( F . col ( i + \"_outliered\" ) == 1 , replace_vals [ treatment_method ][ 1 ] ) . otherwise ( F . when ( F . col ( i + \"_outliered\" ) == - 1 , replace_vals [ treatment_method ][ 0 ], ) . otherwise ( F . col ( i )) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_outliered\" , i ) if print_impact : def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) odf_print = unionAll ( list_odf ) if skewed_cols : skewed_cols_print = [( i , 0 , 0 , 1 ) for i in skewed_cols ] skewed_cols_odf_print = spark . createDataFrame ( skewed_cols_print , schema = odf_print . columns ) odf_print = unionAll ([ odf_print , skewed_cols_odf_print ]) odf_print . show ( len ( list_of_cols ) + len ( skewed_cols ), False ) if treatment & ( treatment_method == \"row_removal\" ): conditions = [ ( F . col ( i + \"_outliered\" ) == 0 ) | ( F . col ( i + \"_outliered\" ) . isNull ()) for i in list_of_cols ] conditions_combined = functools . reduce ( lambda a , b : a & b , conditions ) odf = odf . where ( conditions_combined ) . drop ( * [ i + \"_outliered\" for i in list_of_cols ] ) if treatment : if output_mode == \"replace\" : odf = odf . select ( column_order ) else : odf = idf if print_impact : return odf , odf_print else : return odf def IDness_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , stats_unique = {}, print_impact = False , ): \"\"\" IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for discrete features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, unique_values, IDness. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high IDness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of IDness (calculated as no. of unique values divided by no. of non-null values) for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if its unique values count is more than 80% of total rows (after excluding null values). stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print : DataFrame schema [attribute, unique_values, IDness, flagged/treated]. unique_values is no. of distinct values in a column, IDness is unique_values divided by no. of non-null values. A column is flagged 1 if IDness is above the threshold, else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No IDness Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), T . StructField ( \"IDness\" , T . StringType (), True ), T . StructField ( \"flagged\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print treatment_threshold = float ( treatment_threshold ) if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if stats_unique == {}: odf_print = measures_of_cardinality ( spark , idf , list_of_cols ) else : odf_print = read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols ) ) odf_print = odf_print . withColumn ( \"flagged\" , F . when ( F . col ( \"IDness\" ) >= treatment_threshold , 1 ) . otherwise ( 0 ) ) if treatment : remove_cols = ( odf_print . where ( F . col ( \"flagged\" ) == 1 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) odf = idf . drop ( * remove_cols ) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) if treatment : print ( \"Removed Columns: \" , remove_cols ) return odf , odf_print def biasedness_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], treatment = False , treatment_threshold = 0.8 , stats_mode = {}, print_impact = False , ): \"\"\" This function flags column if they are biased or skewed towards one specific value and leverages mode_pct computation from Measures of Central Tendency i.e. number of rows with mode value (most frequently seen value) divided by number of non-null values. It varies between 0 to 100% where biasedness of 100% means there is only a single value (other than null). The function flags a column if its biasedness is above a certain threshold. Such columns can be deleted from the modelling analysis, if required. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high biased columns (the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, mode, mode_pct. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) treatment Boolean argument \u2013 True or False. If True, columns with high biasedness (defined by treatment_threshold argument) are removed from the input dataframe. (Default value = False) treatment_threshold Defines acceptable level of biasedness (frequency of most-frequently seen value)for a column and takes value between 0 to 1. Default threshold of 0.8 can be interpreted as remove column if the number of rows with most-frequently seen value is more than 80% of total rows (after excluding null values). stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False This argument is to print out the statistics and the impact of treatment (if applicable).(Default value = False) Returns ------- odf : DataFrame Dataframe after column removal if treated, else original input dataframe. odf_print : DataFrame schema [attribute, mode, mode_rows, mode_pct, flagged/treated]. mode is the most frequently seen value, mode_rows is number of rows with mode value, and mode_pct is number of rows with mode value divided by non-null values. A column is flagged 1 if mode_pct is above the threshold else 0. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) for i in idf . select ( list_of_cols ) . dtypes : if i [ 1 ] not in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . remove ( i [ 0 ]) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No biasedness Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . StringType (), True ), T . StructField ( \"mode_pct\" , T . StringType (), True ), T . StructField ( \"flagged\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if ( treatment_threshold < 0 ) | ( treatment_threshold > 1 ): raise TypeError ( \"Invalid input for Treatment Threshold Value\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if stats_mode == {}: odf_print = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"count\" ), \"summary\" ) . withColumnRenamed ( \"key\" , \"attribute\" ) . join ( mode_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" ) . withColumn ( \"mode_pct\" , F . round ( F . col ( \"mode_rows\" ) / F . col ( \"count\" ) . cast ( T . DoubleType ()), 4 ), ) . select ( \"attribute\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) ) else : odf_print = ( read_dataset ( spark , ** stats_mode ) . select ( \"attribute\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) odf_print = odf_print . withColumn ( \"flagged\" , F . when ( ( F . col ( \"mode_pct\" ) >= treatment_threshold ) | ( F . col ( \"mode_pct\" ) . isNull ()), 1 ) . otherwise ( 0 ), ) if treatment : remove_cols = ( odf_print . where ( ( F . col ( \"mode_pct\" ) >= treatment_threshold ) | ( F . col ( \"mode_pct\" ) . isNull ()) ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) odf = idf . drop ( * remove_cols ) odf_print = odf_print . withColumnRenamed ( \"flagged\" , \"treated\" ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) if treatment : print ( \"Removed Columns: \" , remove_cols ) return odf , odf_print def invalidEntries_detection ( spark , idf , list_of_cols = \"all\" , drop_cols = [], detection_type = \"auto\" , invalid_entries = [], valid_entries = [], partial_match = False , treatment = False , treatment_method = \"null_replacement\" , treatment_configs = {}, stats_missing = {}, stats_unique = {}, stats_mode = {}, output_mode = \"replace\" , print_impact = False , ): \"\"\" This function checks for certain suspicious patterns in attributes\u2019 values. Patterns that are considered for this quality check: - Missing Values: The function checks for all column values which directly or indirectly indicate the missing value in an attribute such as 'nan', 'null', 'na', 'inf', 'n/a', 'not defined' etc. The function also check for special characters. - Repetitive Characters: Certain attributes\u2019 values with repetitive characters may be default value or system error, rather than being a legit value etc xx, zzzzz, 99999 etc. Such values are flagged for the user to take an appropriate action. There may be certain false positive which are legit values. - Consecutive Characters: Similar to repetitive characters, consecutive characters (at least 3 characters long) such as abc, 1234 etc may not be legit values, and hence flagged. There may be certain false positive which are legit values. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after treating the invalid values (or the original dataset if no treatment) and 2nd dataframe is of schema \u2013 attribute, invalid_entries, invalid_count, invalid_pct. All potential invalid values (separated by delimiter pipe \u201c|\u201d) are shown under invalid_entries column. Total number of rows impacted by these entries for each attribute is shown under invalid_count. invalid_pct is invalid_count divided by number of rows. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) detection_type \"auto\",\"manual\",\"both\" (Default value = \"auto\") invalid_entries List of values or regex patterns to be classified as invalid. Valid only for \"auto\" or \"both\" detection type. (Default value = []) valid_entries List of values or regex patterns such that a value will be classified as invalid if it does not match any value or regex pattern in it. Valid only for \"auto\" or \"both\" detection type. (Default value = []) partial_match Boolean argument \u2013 True or False. If True, values with substring same as invalid_entries is declared invalid. (Default value = False) treatment Boolean argument \u2013 True or False. If True, outliers are treated as per treatment_method argument. (Default value = False) treatment_method \"MMM\", \"null_replacement\", \"column_removal\" (more methods to be added soon). MMM (Mean Median Mode) replaces invalid value by the measure of central tendency (mode for categorical features and mean or median for numerical features). null_replacement removes all values with any invalid values as null. column_removal remove a column if % of rows with invalid value is above a threshold (defined by key \"treatment_threshold\" under treatment_configs argument). (Default value = \"null_replacement\") treatment_configs Takes input in dictionary format. For column_removal treatment, key \u2018treatment_threshold\u2019 is provided with a value between 0 to 1. For value replacement, by MMM, arguments corresponding to imputation_MMM function (transformer module) are provided, where each key is an argument from imputation_MMM function. For null_replacement, this argument can be skipped. (Default value = {}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with treated column. \u201cappend\u201d option append treated column to the input dataset with a postfix \"_invalid\" e.g. column X is appended as X_invalid. (Default value = \"replace\") print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- odf : DataFrame Dataframe after treatment if applicable, else original input dataframe. odf_print : DataFrame schema [attribute, invalid_entries, invalid_count, invalid_pct]. invalid_entries are all potential invalid values (separated by delimiter pipe \u201c|\u201d), invalid_count is no. of rows which are impacted by invalid entries, and invalid_pct is invalid_count divided by no of rows. \"\"\" if list_of_cols == \"all\" : list_of_cols = [] for i in idf . dtypes : if i [ 1 ] in ( \"string\" , \"int\" , \"bigint\" , \"long\" ): list_of_cols . append ( i [ 0 ]) if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Invalid Entries Check - No discrete column(s) to analyze\" ) odf = idf schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"invalid_entries\" , T . StringType (), True ), T . StructField ( \"invalid_count\" , T . StringType (), True ), T . StructField ( \"invalid_pct\" , T . StringType (), True ), ] ) odf_print = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf , odf_print if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if str ( treatment ) . lower () == \"true\" : treatment = True elif str ( treatment ) . lower () == \"false\" : treatment = False else : raise TypeError ( \"Non-Boolean input for treatment\" ) if treatment_method not in ( \"MMM\" , \"null_replacement\" , \"column_removal\" ): raise TypeError ( \"Invalid input for method_type\" ) treatment_threshold = treatment_configs . pop ( \"treatment_threshold\" , None ) if treatment_threshold : treatment_threshold = float ( treatment_threshold ) else : if treatment_method == \"column_removal\" : raise TypeError ( \"Invalid input for column removal threshold\" ) null_vocab = [ \"\" , \" \" , \"nan\" , \"null\" , \"na\" , \"inf\" , \"n/a\" , \"not defined\" , \"none\" , \"undefined\" , \"blank\" , \"unknown\" , ] special_chars_vocab = [ \"&\" , \"$\" , \";\" , \":\" , \".\" , \",\" , \"*\" , \"#\" , \"@\" , \"_\" , \"?\" , \"%\" , \"!\" , \"^\" , \"(\" , \")\" , \"-\" , \"/\" , \"'\" , ] def detect ( * v ): output = [] for idx , e in enumerate ( v ): if e is None : output . append ( None ) continue if detection_type in ( \"auto\" , \"both\" ): e = str ( e ) . lower () . strip () # Null & Special Chars Search if e in ( null_vocab + special_chars_vocab ): output . append ( 1 ) continue # Consecutive Identical Chars Search regex = \" \\\\ b([a-zA-Z0-9]) \\\\ 1 \\\\ 1+ \\\\ b\" p = re . compile ( regex ) if re . search ( p , e ): output . append ( 1 ) continue # Ordered Chars Search l = len ( e ) check = 0 if l >= 3 : for i in range ( 1 , l ): if ord ( e [ i ]) - ord ( e [ i - 1 ]) != 1 : check = 1 break if check == 0 : output . append ( 1 ) continue check = 0 if detection_type in ( \"manual\" , \"both\" ): e = str ( e ) . lower () . strip () for regex in invalid_entries : p = re . compile ( regex ) if partial_match : if re . search ( p , e ): check = 1 output . append ( 1 ) break else : if p . fullmatch ( e ): check = 1 output . append ( 1 ) break match_valid_entries = [] for regex in valid_entries : p = re . compile ( regex ) if partial_match : if re . search ( p , e ): match_valid_entries . append ( 1 ) else : match_valid_entries . append ( 0 ) else : if p . fullmatch ( e ): match_valid_entries . append ( 1 ) else : match_valid_entries . append ( 0 ) if ( len ( match_valid_entries ) > 0 ) & ( sum ( match_valid_entries ) == 0 ): check = 1 output . append ( 1 ) if check == 0 : output . append ( 0 ) return output f_detect = F . udf ( detect , T . ArrayType ( T . LongType ())) odf = idf . withColumn ( \"invalid\" , f_detect ( * list_of_cols )) odf . persist () output_print = [] for index , i in enumerate ( list_of_cols ): tmp = odf . withColumn ( i + \"_invalid\" , F . col ( \"invalid\" )[ index ]) invalid = ( tmp . where ( F . col ( i + \"_invalid\" ) == 1 ) . select ( i ) . distinct () . rdd . flatMap ( lambda x : x ) . collect () ) invalid = [ str ( x ) for x in invalid ] invalid_count = tmp . where ( F . col ( i + \"_invalid\" ) == 1 ) . count () output_print . append ( [ i , \"|\" . join ( invalid ), invalid_count , round ( invalid_count / idf . count (), 4 )] ) odf_print = spark . createDataFrame ( output_print , schema = [ \"attribute\" , \"invalid_entries\" , \"invalid_count\" , \"invalid_pct\" ], ) if treatment : if treatment_threshold : threshold_cols = ( odf_print . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) . where ( F . col ( \"invalid_pct\" ) > treatment_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if treatment_method in ( \"null_replacement\" , \"MMM\" ): for index , i in enumerate ( list_of_cols ): if treatment_threshold : if i not in threshold_cols : odf = odf . drop ( i + \"_invalid\" ) continue odf = odf . withColumn ( i + \"_invalid\" , F . when ( F . col ( \"invalid\" )[ index ] == 1 , None ) . otherwise ( F . col ( i )), ) if output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_invalid\" , i ) else : if ( odf_print . where ( F . col ( \"attribute\" ) == i ) . select ( \"invalid_pct\" ) . collect ()[ 0 ][ 0 ] == 0.0 ): odf = odf . drop ( i + \"_invalid\" ) odf = odf . drop ( \"invalid\" ) if treatment_method == \"column_removal\" : odf = idf . drop ( * threshold_cols ) if print_impact : print ( \"Removed Columns: \" , threshold_cols ) if treatment_method == \"MMM\" : if stats_unique == {} or output_mode == \"append\" : remove_cols = ( uniqueCount_computation ( spark , odf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = [ e for e in list_of_cols if e not in remove_cols ] if treatment_threshold : list_of_cols = [ e for e in threshold_cols if e not in remove_cols ] if output_mode == \"append\" : if len ( list_of_cols ) > 0 : list_of_cols = [ e + \"_invalid\" for e in list_of_cols ] odf = imputation_MMM ( spark , odf , list_of_cols , ** treatment_configs , stats_missing = stats_missing , stats_mode = stats_mode , print_impact = print_impact ) else : odf = idf if print_impact : odf_print . show ( len ( list_of_cols )) return odf , odf_print","title":"quality_checker"},{"location":"api/data_analyzer/quality_checker.html#functions","text":"def IDness_detection ( spark, idf, list_of_cols='all', drop_cols=[], treatment=False, treatment_threshold=0.8, stats_unique={}, print_impact=False) IDness of an attribute is defined as the ratio of number of unique values seen in an attribute by number of non-null rows. It varies between 0 to 100% where IDness of 100% means there are as many unique values as number of rows (primary key in the input dataset). IDness is computed only for discrete features. This function leverages the statistics from Measures of Cardinality function and flag the columns if IDness is above a certain threshold. Such columns can be deleted from the modelling analysis if directed for a treatment. This function returns two dataframes in tuple format \u2013 1st dataframe is input dataset after removing high IDness columns (if treated else the original dataset) and 2nd dataframe is of schema \u2013 attribute, unique_values, IDness.","title":"Functions"},{"location":"api/data_analyzer/stats_generator.html","text":"stats_generator This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are split into different metric types, and each function below corresponds to one metric type: global_summary measures_of_counts measures_of_centralTendency measures_of_cardinality measures_of_dispersion measures_of_percentiles measures_of_shape Above primary functions are supported by below functions, which can be used independently as well: missingCount_computation nonzeroCount_computation mode_computation uniqueCount_computation Expand source code # coding=utf-8 \"\"\" This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are split into different metric types, and each function below corresponds to one metric type: - global_summary - measures_of_counts - measures_of_centralTendency - measures_of_cardinality - measures_of_dispersion - measures_of_percentiles - measures_of_shape Above primary functions are supported by below functions, which can be used independently as well: - missingCount_computation - nonzeroCount_computation - mode_computation - uniqueCount_computation \"\"\" import warnings from pyspark.mllib.linalg import Vectors from pyspark.mllib.stat import Statistics from pyspark.sql import functions as F from pyspark.sql import types as T import pyspark from anovos.shared.utils import attributeType_segregation , transpose_dataframe def global_summary ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The global summary function computes the universal statistics/metrics and returns a Spark DataFrame with schema \u2013 metric, value. The metrics computed in this function - No. of rows, No. of columns, No. of categorical columns along with column names, No. of numerical columns along with the column names, No. of non-numerical non-categorical columns such as date type, array type etc. along with column names. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [metric, value] \"\"\" if list_of_cols == \"all\" : list_of_cols = idf . columns if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) row_count = idf . count () col_count = len ( list_of_cols ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) numcol_count = len ( num_cols ) catcol_count = len ( cat_cols ) othercol_count = len ( other_cols ) if print_impact : print ( \"No. of Rows: %s \" % \" {0:,} \" . format ( row_count )) print ( \"No. of Columns: %s \" % \" {0:,} \" . format ( col_count )) print ( \"Numerical Columns: %s \" % \" {0:,} \" . format ( numcol_count )) if numcol_count > 0 : print ( num_cols ) print ( \"Categorical Columns: %s \" % \" {0:,} \" . format ( catcol_count )) if catcol_count > 0 : print ( cat_cols ) if othercol_count > 0 : print ( \"Other Columns: %s \" % \" {0:,} \" . format ( othercol_count )) print ( other_cols ) odf = spark . createDataFrame ( [ [ \"rows_count\" , str ( row_count )], [ \"columns_count\" , str ( col_count )], [ \"numcols_count\" , str ( numcol_count )], [ \"numcols_name\" , \", \" . join ( num_cols )], [ \"catcols_count\" , str ( catcol_count )], [ \"catcols_name\" , \", \" . join ( cat_cols )], [ \"othercols_count\" , str ( othercol_count )], [ \"othercols_name\" , \", \" . join ( other_cols )], ], schema = [ \"metric\" , \"value\" ], ) return odf def missingCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, missing_count, missing_pct] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) idf_stats = idf . select ( list_of_cols ) . summary ( \"count\" ) odf = ( transpose_dataframe ( idf_stats , \"summary\" ) . withColumn ( \"missing_count\" , F . lit ( idf . count ()) - F . col ( \"count\" ) . cast ( T . LongType ()) ) . withColumn ( \"missing_pct\" , F . round ( F . col ( \"missing_count\" ) / F . lit ( idf . count ()), 4 ) ) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), \"missing_count\" , \"missing_pct\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def nonzeroCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, nonzero_count, nonzero_pct] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Non-Zero Count Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"nonzero_count\" , T . StringType (), True ), T . StructField ( \"nonzero_pct\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf tmp = idf . select ( list_of_cols ) . fillna ( 0 ) . rdd . map ( lambda row : Vectors . dense ( row )) nonzero_count = Statistics . colStats ( tmp ) . numNonzeros () odf = spark . createDataFrame ( zip ( list_of_cols , [ int ( i ) for i in nonzero_count ]), schema = ( \"attribute\" , \"nonzero_count\" ), ) . withColumn ( \"nonzero_pct\" , F . round ( F . col ( \"nonzero_count\" ) / F . lit ( idf . count ()), 4 )) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_counts ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Counts function computes different count metrics for each column. It returns a Spark DataFrame with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. - Fill Count/Rate is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary functionality of Spark SQL. - Missing Count/Rate is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. - Non Zero Count/Rate is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a supporting function nonzeroCount_computation. Under the hood, it leverage Multivariate Statistical Summary of Spark MLlib. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"count\" ), \"summary\" ) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), F . col ( \"count\" ) . cast ( T . LongType ()) . alias ( \"fill_count\" ), ) . withColumn ( \"fill_pct\" , F . round ( F . col ( \"fill_count\" ) / F . lit ( idf . count ()), 4 )) . withColumn ( \"missing_count\" , F . lit ( idf . count ()) - F . col ( \"fill_count\" ) . cast ( T . LongType ()) ) . withColumn ( \"missing_pct\" , F . round ( 1 - F . col ( \"fill_pct\" ), 4 )) . join ( nonzeroCount_computation ( spark , idf , num_cols ), \"attribute\" , \"full_outer\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def mode_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, mode, mode_rows] In case there is tie between multiple values, one value is randomly picked as mode. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Mode Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf list_df = [] for col in list_of_cols : out_df = ( idf . select ( col ) . dropna () . groupby ( col ) . count () . orderBy ( \"count\" , ascending = False ) . limit ( 1 ) . select ( F . lit ( col ) . alias ( \"attribute\" ), F . col ( col ) . alias ( \"mode\" ), F . col ( \"count\" ) . alias ( \"mode_rows\" ), ) ) list_df . append ( out_df ) def unionAll ( dfs ): first , * _ = dfs schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . LongType (), True ), ] ) return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), schema ) odf = unionAll ( list_df ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_centralTendency ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark DataFrame with schema \u2013 attribute, mean, median, mode, mode_rows, mode_pct. - Mean is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Mean is calculated only for numerical columns. - Median is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Median is calculated only for numerical columns. - Mode is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns). - Mode Rows is the numer of rows seen with Mode value. Mode Rows is calculated only for discrete columns (categorical + Integer/Long columns). - Mode Pct is defined as Mode Rows divided by non-null values seen in a column. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, mean, median, mode, mode_rows, mode_pct] \"\"\" num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) df_mode_compute = mode_computation ( spark , idf , list_of_cols ) summary_lst = [] for col in list_of_cols : summary_col = ( idf . select ( col ) . summary ( \"mean\" , \"50%\" , \"count\" ) . rdd . map ( lambda x : x [ 1 ]) . collect () ) summary_col = [ str ( i ) for i in summary_col if type ( i ) != \"str\" ] summary_col . insert ( 0 , col ) summary_lst . append ( summary_col ) summary_df = spark . createDataFrame ( summary_lst , schema = ( \"key\" , \"mean\" , \"50%\" , \"count\" ), ) odf = ( summary_df . withColumn ( \"mean\" , F . when ( F . col ( \"key\" ) . isin ( num_cols ), F . round ( F . col ( \"mean\" ) . cast ( T . DoubleType ()), 4 ), ) . otherwise ( None ), ) . withColumn ( \"median\" , F . when ( F . col ( \"key\" ) . isin ( num_cols ), F . round ( F . col ( \"50%\" ) . cast ( T . DoubleType ()), 4 ), ) . otherwise ( None ), ) . withColumnRenamed ( \"key\" , \"attribute\" ) . join ( df_mode_compute , \"attribute\" , \"full_outer\" ) . withColumn ( \"mode_pct\" , F . round ( F . col ( \"mode_rows\" ) / F . col ( \"count\" ) . cast ( T . DoubleType ()), 4 ), ) . select ( \"attribute\" , \"mean\" , \"median\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def uniqueCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], compute_approx_unique_count = False , rsd = None , print_impact = False , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) compute_approx_unique_count boolean, optional This flag tells the function whether to compute approximate unique count or exact unique count (Default value = False) rsd float, optional This is used when compute_approx_unique_count is True. This is the maximum relative standard deviation allowed (default = 0.05). For rsd < 0.01, it is more efficient to use :func:`countDistinct` print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, unique_values] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if rsd != None and rsd < 0 : raise ValueError ( \"rsd value can not be less than 0 (default value is 0.05)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Unique Count Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf if compute_approx_unique_count : uniquevalue_count = idf . agg ( * ( F . approx_count_distinct ( F . col ( i ), rsd ) . alias ( i ) for i in list_of_cols ) ) else : uniquevalue_count = idf . agg ( * ( F . countDistinct ( F . col ( i )) . alias ( i ) for i in list_of_cols ) ) odf = spark . createDataFrame ( zip ( list_of_cols , uniquevalue_count . rdd . map ( list ) . collect ()[ 0 ]), schema = ( \"attribute\" , \"unique_values\" ), ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_cardinality ( spark , idf , list_of_cols = \"all\" , drop_cols = [], use_approx_unique_count = True , rsd = None , print_impact = False , ): \"\"\" The Measures of Cardinality function provides statistics that are related to unique values seen in an attribute. These statistics are calculated only for discrete columns (categorical + Integer/Long columns). It returns a Spark Dataframe with schema \u2013 attribute, unique_values, IDness. - Unique Value is defined as a distinct value count of a column. It relies on a supporting function uniqueCount_computation for its computation and leverages the countDistinct/approx_count_distinct functionality of Spark SQL. - IDness is calculated as Unique Values divided by non-null values seen in a column. Non-null values count is used instead of total count because too many null values can give misleading results even if the column have all unique values (except null). It uses supporting functions - uniqueCount_computation and missingCount_computation. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) use_approx_unique_count boolean, optional This flag tells the function whether to use approximate unique count to compute the IDness or use exact unique count (Default value = True) rsd float, optional This is used when use_approx_unique_count is True. This is the maximum relative standard deviation allowed (default = 0.05). For rsd < 0.01, it is more efficient to set use_approx_unique_count as False print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, unique_values, IDness] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if rsd != None and rsd < 0 : raise ValueError ( \"rsd value can not be less than 0 (default value is 0.05)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Cardinality Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), T . StructField ( \"IDness\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf odf = ( uniqueCount_computation ( spark , idf , list_of_cols , compute_approx_unique_count = use_approx_unique_count , rsd = rsd , ) . join ( missingCount_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" , ) . withColumn ( \"IDness\" , F . round ( F . col ( \"unique_values\" ) / ( F . lit ( idf . count ()) - F . col ( \"missing_count\" )), 4 , ), ) . select ( \"attribute\" , \"unique_values\" , \"IDness\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_dispersion ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Dispersion function provides statistics that describe the spread of a numerical attribute. Alternatively, these statistics are also known as measures of spread. It returns a Spark DataFrame with schema \u2013 attribute, stddev, variance, cov, IQR, range. - Standard Deviation (stddev) measures how concentrated an attribute is around the mean or average. It leverages \u2018stddev\u2019 statistic from summary functionality of Spark SQL. - Variance is the squared value of Standard Deviation. - Coefficient of Variance (cov) is computed as ratio of Standard Deviation & Mean. It leverages \u2018stddev\u2019 and \u2018mean\u2019 statistic from the summary functionality of Spark SQL. - Interquartile Range (IQR): It describes the difference between the third quartile (75th percentile) and the first quartile (25th percentile), telling us about the range where middle half values are seen. It leverage \u201825%\u2019 and \u201875%\u2019 statistics from the summary functionality of Spark SQL. - Range is simply the difference between the maximum value and the minimum value. It leverage \u2018min\u2019 and \u2018max\u2019 statistics from the summary functionality of Spark Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, stddev, variance, cov, IQR, range] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Dispersion Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"stddev\" , T . StringType (), True ), T . StructField ( \"variance\" , T . StringType (), True ), T . StructField ( \"cov\" , T . StringType (), True ), T . StructField ( \"IQR\" , T . StringType (), True ), T . StructField ( \"range\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"stddev\" , \"min\" , \"max\" , \"mean\" , \"25%\" , \"75%\" ), \"summary\" , ) . withColumn ( \"stddev\" , F . round ( F . col ( \"stddev\" ) . cast ( T . DoubleType ()), 4 )) . withColumn ( \"variance\" , F . round ( F . col ( \"stddev\" ) * F . col ( \"stddev\" ), 4 )) . withColumn ( \"range\" , F . round ( F . col ( \"max\" ) - F . col ( \"min\" ), 4 )) . withColumn ( \"cov\" , F . round ( F . col ( \"stddev\" ) / F . col ( \"mean\" ), 4 )) . withColumn ( \"IQR\" , F . round ( F . col ( \"75%\" ) - F . col ( \"25%\" ), 4 )) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), \"stddev\" , \"variance\" , \"cov\" , \"IQR\" , \"range\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_percentiles ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Percentiles function provides statistics at different percentiles. Nth percentile can be interpreted as N% of rows having values lesser than or equal to Nth percentile value. It is prominently used for quick detection of skewness or outlier. Alternatively, these statistics are also known as measures of position. These statistics are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max. It leverage \u2018N%\u2019 statistics from summary functionality of Spark SQL where N is 0 for min and 100 for max. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Percentiles Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"min\" , T . StringType (), True ), T . StructField ( \"1%\" , T . StringType (), True ), T . StructField ( \"5%\" , T . StringType (), True ), T . StructField ( \"10%\" , T . StringType (), True ), T . StructField ( \"25%\" , T . StringType (), True ), T . StructField ( \"50%\" , T . StringType (), True ), T . StructField ( \"75%\" , T . StringType (), True ), T . StructField ( \"90%\" , T . StringType (), True ), T . StructField ( \"95%\" , T . StringType (), True ), T . StructField ( \"99%\" , T . StringType (), True ), T . StructField ( \"max\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf stats = [ \"min\" , \"1%\" , \"5%\" , \"10%\" , \"25%\" , \"50%\" , \"75%\" , \"90%\" , \"95%\" , \"99%\" , \"max\" ] odf = transpose_dataframe ( idf . select ( list_of_cols ) . summary ( * stats ), \"summary\" ) . withColumnRenamed ( \"key\" , \"attribute\" ) for i in odf . columns : if i != \"attribute\" : odf = odf . withColumn ( i , F . round ( F . col ( i ) . cast ( \"Double\" ), 4 )) odf = odf . select ([ \"attribute\" ] + stats ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_shape ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Shapes function provides statistics related to the shape of an attribute's distribution. Alternatively, these statistics are also known as measures of the moment and are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, skewness, kurtosis. - Skewness describes how much-skewed values are relative to a perfect bell curve observed in normal distribution and the direction of skew. If the majority of the values are at the left and the right tail is longer, we say that the distribution is skewed right or positively skewed; if the peak is toward the right and the left tail is longer, we say that the distribution is skewed left or negatively skewed. It leverage skewness functionality of Spark SQL. - (Excess) Kurtosis describes how tall and sharp the central peak is relative to a perfect bell curve observed in the normal distribution. The reference standard is a normal distribution, which has a kurtosis of 3. In token of this, often, the excess kurtosis is presented: excess kurtosis is simply kurtosis\u22123. Higher (positive) values indicate a higher, sharper peak; lower (negative) values indicate a less distinct peak. It leverages kurtosis functionality of Spark SQL. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, skewness, kurtosis] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Skewness/Kurtosis Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"skewness\" , T . StringType (), True ), T . StructField ( \"kurtosis\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf exprs = [ f ( F . col ( c )) for f in [ F . skewness , F . kurtosis ] for c in list_of_cols ] list_result = idf . groupby () . agg ( * exprs ) . rdd . flatMap ( lambda x : x ) . collect () shapes = [] for i in range ( int ( len ( list_result ) / 2 )): shapes . append ( [ list_of_cols [ i ], list_result [ i ], list_result [ i + int ( len ( list_result ) / 2 )], ] ) odf = ( spark . createDataFrame ( shapes , schema = ( \"attribute\" , \"skewness\" , \"kurtosis\" )) . withColumn ( \"skewness\" , F . round ( F . col ( \"skewness\" ), 4 )) . withColumn ( \"kurtosis\" , F . round ( F . col ( \"kurtosis\" ), 4 )) ) if print_impact : odf . show ( len ( list_of_cols )) return odf Functions def global_summary ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The global summary function computes the universal statistics/metrics and returns a Spark DataFrame with schema \u2013 metric, value. The metrics computed in this function - No. of rows, No. of columns, No. of categorical columns along with column names, No. of numerical columns along with the column names, No. of non-numerical non-categorical columns such as date type, array type etc. along with column names. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [metric, value] Expand source code def global_summary ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The global summary function computes the universal statistics/metrics and returns a Spark DataFrame with schema \u2013 metric, value. The metrics computed in this function - No. of rows, No. of columns, No. of categorical columns along with column names, No. of numerical columns along with the column names, No. of non-numerical non-categorical columns such as date type, array type etc. along with column names. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [metric, value] \"\"\" if list_of_cols == \"all\" : list_of_cols = idf . columns if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) row_count = idf . count () col_count = len ( list_of_cols ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) numcol_count = len ( num_cols ) catcol_count = len ( cat_cols ) othercol_count = len ( other_cols ) if print_impact : print ( \"No. of Rows: %s \" % \" {0:,} \" . format ( row_count )) print ( \"No. of Columns: %s \" % \" {0:,} \" . format ( col_count )) print ( \"Numerical Columns: %s \" % \" {0:,} \" . format ( numcol_count )) if numcol_count > 0 : print ( num_cols ) print ( \"Categorical Columns: %s \" % \" {0:,} \" . format ( catcol_count )) if catcol_count > 0 : print ( cat_cols ) if othercol_count > 0 : print ( \"Other Columns: %s \" % \" {0:,} \" . format ( othercol_count )) print ( other_cols ) odf = spark . createDataFrame ( [ [ \"rows_count\" , str ( row_count )], [ \"columns_count\" , str ( col_count )], [ \"numcols_count\" , str ( numcol_count )], [ \"numcols_name\" , \", \" . join ( num_cols )], [ \"catcols_count\" , str ( catcol_count )], [ \"catcols_name\" , \", \" . join ( cat_cols )], [ \"othercols_count\" , str ( othercol_count )], [ \"othercols_name\" , \", \" . join ( other_cols )], ], schema = [ \"metric\" , \"value\" ], ) return odf def measures_of_cardinality ( spark, idf, list_of_cols='all', drop_cols=[], use_approx_unique_count=True, rsd=None, print_impact=False) The Measures of Cardinality function provides statistics that are related to unique values seen in an attribute. These statistics are calculated only for discrete columns (categorical + Integer/Long columns). It returns a Spark Dataframe with schema \u2013 attribute, unique_values, IDness. Unique Value is defined as a distinct value count of a column. It relies on a supporting function uniqueCount_computation for its computation and leverages the countDistinct/approx_count_distinct functionality of Spark SQL. IDness is calculated as Unique Values divided by non-null values seen in a column. Non-null values count is used instead of total count because too many null values can give misleading results even if the column have all unique values (except null). It uses supporting functions - uniqueCount_computation and missingCount_computation. Parameters spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) use_approx_unique_count boolean, optional This flag tells the function whether to use approximate unique count to compute the IDness or use exact unique count (Default value = True) rsd float, optional This is used when use_approx_unique_count is True. This is the maximum relative standard deviation allowed (default = 0.05). For rsd < 0.01, it is more efficient to set use_approx_unique_count as False print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, unique_values, IDness] Expand source code def measures_of_cardinality ( spark , idf , list_of_cols = \"all\" , drop_cols = [], use_approx_unique_count = True , rsd = None , print_impact = False , ): \"\"\" The Measures of Cardinality function provides statistics that are related to unique values seen in an attribute. These statistics are calculated only for discrete columns (categorical + Integer/Long columns). It returns a Spark Dataframe with schema \u2013 attribute, unique_values, IDness. - Unique Value is defined as a distinct value count of a column. It relies on a supporting function uniqueCount_computation for its computation and leverages the countDistinct/approx_count_distinct functionality of Spark SQL. - IDness is calculated as Unique Values divided by non-null values seen in a column. Non-null values count is used instead of total count because too many null values can give misleading results even if the column have all unique values (except null). It uses supporting functions - uniqueCount_computation and missingCount_computation. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) use_approx_unique_count boolean, optional This flag tells the function whether to use approximate unique count to compute the IDness or use exact unique count (Default value = True) rsd float, optional This is used when use_approx_unique_count is True. This is the maximum relative standard deviation allowed (default = 0.05). For rsd < 0.01, it is more efficient to set use_approx_unique_count as False print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, unique_values, IDness] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if rsd != None and rsd < 0 : raise ValueError ( \"rsd value can not be less than 0 (default value is 0.05)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Cardinality Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), T . StructField ( \"IDness\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf odf = ( uniqueCount_computation ( spark , idf , list_of_cols , compute_approx_unique_count = use_approx_unique_count , rsd = rsd , ) . join ( missingCount_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" , ) . withColumn ( \"IDness\" , F . round ( F . col ( \"unique_values\" ) / ( F . lit ( idf . count ()) - F . col ( \"missing_count\" )), 4 , ), ) . select ( \"attribute\" , \"unique_values\" , \"IDness\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_centralTendency ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark DataFrame with schema \u2013 attribute, mean, median, mode, mode_rows, mode_pct. Mean is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Mean is calculated only for numerical columns. Median is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Median is calculated only for numerical columns. Mode is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns). Mode Rows is the numer of rows seen with Mode value. Mode Rows is calculated only for discrete columns (categorical + Integer/Long columns). Mode Pct is defined as Mode Rows divided by non-null values seen in a column. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns). Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, mean, median, mode, mode_rows, mode_pct] Expand source code def measures_of_centralTendency ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark DataFrame with schema \u2013 attribute, mean, median, mode, mode_rows, mode_pct. - Mean is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Mean is calculated only for numerical columns. - Median is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Median is calculated only for numerical columns. - Mode is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns). - Mode Rows is the numer of rows seen with Mode value. Mode Rows is calculated only for discrete columns (categorical + Integer/Long columns). - Mode Pct is defined as Mode Rows divided by non-null values seen in a column. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, mean, median, mode, mode_rows, mode_pct] \"\"\" num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) df_mode_compute = mode_computation ( spark , idf , list_of_cols ) summary_lst = [] for col in list_of_cols : summary_col = ( idf . select ( col ) . summary ( \"mean\" , \"50%\" , \"count\" ) . rdd . map ( lambda x : x [ 1 ]) . collect () ) summary_col = [ str ( i ) for i in summary_col if type ( i ) != \"str\" ] summary_col . insert ( 0 , col ) summary_lst . append ( summary_col ) summary_df = spark . createDataFrame ( summary_lst , schema = ( \"key\" , \"mean\" , \"50%\" , \"count\" ), ) odf = ( summary_df . withColumn ( \"mean\" , F . when ( F . col ( \"key\" ) . isin ( num_cols ), F . round ( F . col ( \"mean\" ) . cast ( T . DoubleType ()), 4 ), ) . otherwise ( None ), ) . withColumn ( \"median\" , F . when ( F . col ( \"key\" ) . isin ( num_cols ), F . round ( F . col ( \"50%\" ) . cast ( T . DoubleType ()), 4 ), ) . otherwise ( None ), ) . withColumnRenamed ( \"key\" , \"attribute\" ) . join ( df_mode_compute , \"attribute\" , \"full_outer\" ) . withColumn ( \"mode_pct\" , F . round ( F . col ( \"mode_rows\" ) / F . col ( \"count\" ) . cast ( T . DoubleType ()), 4 ), ) . select ( \"attribute\" , \"mean\" , \"median\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_counts ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The Measures of Counts function computes different count metrics for each column. It returns a Spark DataFrame with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. Fill Count/Rate is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary functionality of Spark SQL. Missing Count/Rate is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. Non Zero Count/Rate is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a supporting function nonzeroCount_computation. Under the hood, it leverage Multivariate Statistical Summary of Spark MLlib. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct] Expand source code def measures_of_counts ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Counts function computes different count metrics for each column. It returns a Spark DataFrame with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. - Fill Count/Rate is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary functionality of Spark SQL. - Missing Count/Rate is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. - Non Zero Count/Rate is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a supporting function nonzeroCount_computation. Under the hood, it leverage Multivariate Statistical Summary of Spark MLlib. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"count\" ), \"summary\" ) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), F . col ( \"count\" ) . cast ( T . LongType ()) . alias ( \"fill_count\" ), ) . withColumn ( \"fill_pct\" , F . round ( F . col ( \"fill_count\" ) / F . lit ( idf . count ()), 4 )) . withColumn ( \"missing_count\" , F . lit ( idf . count ()) - F . col ( \"fill_count\" ) . cast ( T . LongType ()) ) . withColumn ( \"missing_pct\" , F . round ( 1 - F . col ( \"fill_pct\" ), 4 )) . join ( nonzeroCount_computation ( spark , idf , num_cols ), \"attribute\" , \"full_outer\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_dispersion ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The Measures of Dispersion function provides statistics that describe the spread of a numerical attribute. Alternatively, these statistics are also known as measures of spread. It returns a Spark DataFrame with schema \u2013 attribute, stddev, variance, cov, IQR, range. Standard Deviation (stddev) measures how concentrated an attribute is around the mean or average. It leverages \u2018stddev\u2019 statistic from summary functionality of Spark SQL. Variance is the squared value of Standard Deviation. Coefficient of Variance (cov) is computed as ratio of Standard Deviation & Mean. It leverages \u2018stddev\u2019 and \u2018mean\u2019 statistic from the summary functionality of Spark SQL. Interquartile Range (IQR): It describes the difference between the third quartile (75th percentile) and the first quartile (25th percentile), telling us about the range where middle half values are seen. It leverage \u201825%\u2019 and \u201875%\u2019 statistics from the summary functionality of Spark SQL. Range is simply the difference between the maximum value and the minimum value. It leverage \u2018min\u2019 and \u2018max\u2019 statistics from the summary functionality of Spark Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, stddev, variance, cov, IQR, range] Expand source code def measures_of_dispersion ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Dispersion function provides statistics that describe the spread of a numerical attribute. Alternatively, these statistics are also known as measures of spread. It returns a Spark DataFrame with schema \u2013 attribute, stddev, variance, cov, IQR, range. - Standard Deviation (stddev) measures how concentrated an attribute is around the mean or average. It leverages \u2018stddev\u2019 statistic from summary functionality of Spark SQL. - Variance is the squared value of Standard Deviation. - Coefficient of Variance (cov) is computed as ratio of Standard Deviation & Mean. It leverages \u2018stddev\u2019 and \u2018mean\u2019 statistic from the summary functionality of Spark SQL. - Interquartile Range (IQR): It describes the difference between the third quartile (75th percentile) and the first quartile (25th percentile), telling us about the range where middle half values are seen. It leverage \u201825%\u2019 and \u201875%\u2019 statistics from the summary functionality of Spark SQL. - Range is simply the difference between the maximum value and the minimum value. It leverage \u2018min\u2019 and \u2018max\u2019 statistics from the summary functionality of Spark Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, stddev, variance, cov, IQR, range] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Dispersion Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"stddev\" , T . StringType (), True ), T . StructField ( \"variance\" , T . StringType (), True ), T . StructField ( \"cov\" , T . StringType (), True ), T . StructField ( \"IQR\" , T . StringType (), True ), T . StructField ( \"range\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"stddev\" , \"min\" , \"max\" , \"mean\" , \"25%\" , \"75%\" ), \"summary\" , ) . withColumn ( \"stddev\" , F . round ( F . col ( \"stddev\" ) . cast ( T . DoubleType ()), 4 )) . withColumn ( \"variance\" , F . round ( F . col ( \"stddev\" ) * F . col ( \"stddev\" ), 4 )) . withColumn ( \"range\" , F . round ( F . col ( \"max\" ) - F . col ( \"min\" ), 4 )) . withColumn ( \"cov\" , F . round ( F . col ( \"stddev\" ) / F . col ( \"mean\" ), 4 )) . withColumn ( \"IQR\" , F . round ( F . col ( \"75%\" ) - F . col ( \"25%\" ), 4 )) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), \"stddev\" , \"variance\" , \"cov\" , \"IQR\" , \"range\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_percentiles ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The Measures of Percentiles function provides statistics at different percentiles. Nth percentile can be interpreted as N% of rows having values lesser than or equal to Nth percentile value. It is prominently used for quick detection of skewness or outlier. Alternatively, these statistics are also known as measures of position. These statistics are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max. It leverage \u2018N%\u2019 statistics from summary functionality of Spark SQL where N is 0 for min and 100 for max. Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max] Expand source code def measures_of_percentiles ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Percentiles function provides statistics at different percentiles. Nth percentile can be interpreted as N% of rows having values lesser than or equal to Nth percentile value. It is prominently used for quick detection of skewness or outlier. Alternatively, these statistics are also known as measures of position. These statistics are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max. It leverage \u2018N%\u2019 statistics from summary functionality of Spark SQL where N is 0 for min and 100 for max. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Percentiles Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"min\" , T . StringType (), True ), T . StructField ( \"1%\" , T . StringType (), True ), T . StructField ( \"5%\" , T . StringType (), True ), T . StructField ( \"10%\" , T . StringType (), True ), T . StructField ( \"25%\" , T . StringType (), True ), T . StructField ( \"50%\" , T . StringType (), True ), T . StructField ( \"75%\" , T . StringType (), True ), T . StructField ( \"90%\" , T . StringType (), True ), T . StructField ( \"95%\" , T . StringType (), True ), T . StructField ( \"99%\" , T . StringType (), True ), T . StructField ( \"max\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf stats = [ \"min\" , \"1%\" , \"5%\" , \"10%\" , \"25%\" , \"50%\" , \"75%\" , \"90%\" , \"95%\" , \"99%\" , \"max\" ] odf = transpose_dataframe ( idf . select ( list_of_cols ) . summary ( * stats ), \"summary\" ) . withColumnRenamed ( \"key\" , \"attribute\" ) for i in odf . columns : if i != \"attribute\" : odf = odf . withColumn ( i , F . round ( F . col ( i ) . cast ( \"Double\" ), 4 )) odf = odf . select ([ \"attribute\" ] + stats ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_shape ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The Measures of Shapes function provides statistics related to the shape of an attribute's distribution. Alternatively, these statistics are also known as measures of the moment and are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, skewness, kurtosis. Skewness describes how much-skewed values are relative to a perfect bell curve observed in normal distribution and the direction of skew. If the majority of the values are at the left and the right tail is longer, we say that the distribution is skewed right or positively skewed; if the peak is toward the right and the left tail is longer, we say that the distribution is skewed left or negatively skewed. It leverage skewness functionality of Spark SQL. (Excess) Kurtosis describes how tall and sharp the central peak is relative to a perfect bell curve observed in the normal distribution. The reference standard is a normal distribution, which has a kurtosis of 3. In token of this, often, the excess kurtosis is presented: excess kurtosis is simply kurtosis\u22123. Higher (positive) values indicate a higher, sharper peak; lower (negative) values indicate a less distinct peak. It leverages kurtosis functionality of Spark SQL. Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, skewness, kurtosis] Expand source code def measures_of_shape ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Shapes function provides statistics related to the shape of an attribute's distribution. Alternatively, these statistics are also known as measures of the moment and are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, skewness, kurtosis. - Skewness describes how much-skewed values are relative to a perfect bell curve observed in normal distribution and the direction of skew. If the majority of the values are at the left and the right tail is longer, we say that the distribution is skewed right or positively skewed; if the peak is toward the right and the left tail is longer, we say that the distribution is skewed left or negatively skewed. It leverage skewness functionality of Spark SQL. - (Excess) Kurtosis describes how tall and sharp the central peak is relative to a perfect bell curve observed in the normal distribution. The reference standard is a normal distribution, which has a kurtosis of 3. In token of this, often, the excess kurtosis is presented: excess kurtosis is simply kurtosis\u22123. Higher (positive) values indicate a higher, sharper peak; lower (negative) values indicate a less distinct peak. It leverages kurtosis functionality of Spark SQL. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, skewness, kurtosis] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Skewness/Kurtosis Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"skewness\" , T . StringType (), True ), T . StructField ( \"kurtosis\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf exprs = [ f ( F . col ( c )) for f in [ F . skewness , F . kurtosis ] for c in list_of_cols ] list_result = idf . groupby () . agg ( * exprs ) . rdd . flatMap ( lambda x : x ) . collect () shapes = [] for i in range ( int ( len ( list_result ) / 2 )): shapes . append ( [ list_of_cols [ i ], list_result [ i ], list_result [ i + int ( len ( list_result ) / 2 )], ] ) odf = ( spark . createDataFrame ( shapes , schema = ( \"attribute\" , \"skewness\" , \"kurtosis\" )) . withColumn ( \"skewness\" , F . round ( F . col ( \"skewness\" ), 4 )) . withColumn ( \"kurtosis\" , F . round ( F . col ( \"kurtosis\" ), 4 )) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def missingCount_computation ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, missing_count, missing_pct] Expand source code def missingCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, missing_count, missing_pct] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) idf_stats = idf . select ( list_of_cols ) . summary ( \"count\" ) odf = ( transpose_dataframe ( idf_stats , \"summary\" ) . withColumn ( \"missing_count\" , F . lit ( idf . count ()) - F . col ( \"count\" ) . cast ( T . LongType ()) ) . withColumn ( \"missing_pct\" , F . round ( F . col ( \"missing_count\" ) / F . lit ( idf . count ()), 4 ) ) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), \"missing_count\" , \"missing_pct\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def mode_computation ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) Parameters spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, mode, mode_rows] In case there is tie between multiple values, one value is randomly picked as mode. Expand source code def mode_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, mode, mode_rows] In case there is tie between multiple values, one value is randomly picked as mode. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Mode Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf list_df = [] for col in list_of_cols : out_df = ( idf . select ( col ) . dropna () . groupby ( col ) . count () . orderBy ( \"count\" , ascending = False ) . limit ( 1 ) . select ( F . lit ( col ) . alias ( \"attribute\" ), F . col ( col ) . alias ( \"mode\" ), F . col ( \"count\" ) . alias ( \"mode_rows\" ), ) ) list_df . append ( out_df ) def unionAll ( dfs ): first , * _ = dfs schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . LongType (), True ), ] ) return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), schema ) odf = unionAll ( list_df ) if print_impact : odf . show ( len ( list_of_cols )) return odf def nonzeroCount_computation ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, nonzero_count, nonzero_pct] Expand source code def nonzeroCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, nonzero_count, nonzero_pct] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Non-Zero Count Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"nonzero_count\" , T . StringType (), True ), T . StructField ( \"nonzero_pct\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf tmp = idf . select ( list_of_cols ) . fillna ( 0 ) . rdd . map ( lambda row : Vectors . dense ( row )) nonzero_count = Statistics . colStats ( tmp ) . numNonzeros () odf = spark . createDataFrame ( zip ( list_of_cols , [ int ( i ) for i in nonzero_count ]), schema = ( \"attribute\" , \"nonzero_count\" ), ) . withColumn ( \"nonzero_pct\" , F . round ( F . col ( \"nonzero_count\" ) / F . lit ( idf . count ()), 4 )) if print_impact : odf . show ( len ( list_of_cols )) return odf def uniqueCount_computation ( spark, idf, list_of_cols='all', drop_cols=[], compute_approx_unique_count=False, rsd=None, print_impact=False) Parameters spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) compute_approx_unique_count boolean, optional This flag tells the function whether to compute approximate unique count or exact unique count (Default value = False) rsd float, optional This is used when compute_approx_unique_count is True. This is the maximum relative standard deviation allowed (default = 0.05). For rsd < 0.01, it is more efficient to use :func: countDistinct print_impact True, False This argument is to print out the statistics.(Default value = False) Returns DataFrame [attribute, unique_values] Expand source code def uniqueCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], compute_approx_unique_count = False , rsd = None , print_impact = False , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) compute_approx_unique_count boolean, optional This flag tells the function whether to compute approximate unique count or exact unique count (Default value = False) rsd float, optional This is used when compute_approx_unique_count is True. This is the maximum relative standard deviation allowed (default = 0.05). For rsd < 0.01, it is more efficient to use :func:`countDistinct` print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, unique_values] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if rsd != None and rsd < 0 : raise ValueError ( \"rsd value can not be less than 0 (default value is 0.05)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Unique Count Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf if compute_approx_unique_count : uniquevalue_count = idf . agg ( * ( F . approx_count_distinct ( F . col ( i ), rsd ) . alias ( i ) for i in list_of_cols ) ) else : uniquevalue_count = idf . agg ( * ( F . countDistinct ( F . col ( i )) . alias ( i ) for i in list_of_cols ) ) odf = spark . createDataFrame ( zip ( list_of_cols , uniquevalue_count . rdd . map ( list ) . collect ()[ 0 ]), schema = ( \"attribute\" , \"unique_values\" ), ) if print_impact : odf . show ( len ( list_of_cols )) return odf","title":"<code>stats_generator</code>"},{"location":"api/data_analyzer/stats_generator.html#stats_generator","text":"This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are split into different metric types, and each function below corresponds to one metric type: global_summary measures_of_counts measures_of_centralTendency measures_of_cardinality measures_of_dispersion measures_of_percentiles measures_of_shape Above primary functions are supported by below functions, which can be used independently as well: missingCount_computation nonzeroCount_computation mode_computation uniqueCount_computation Expand source code # coding=utf-8 \"\"\" This module generates all the descriptive statistics related to the ingested data. Descriptive statistics are split into different metric types, and each function below corresponds to one metric type: - global_summary - measures_of_counts - measures_of_centralTendency - measures_of_cardinality - measures_of_dispersion - measures_of_percentiles - measures_of_shape Above primary functions are supported by below functions, which can be used independently as well: - missingCount_computation - nonzeroCount_computation - mode_computation - uniqueCount_computation \"\"\" import warnings from pyspark.mllib.linalg import Vectors from pyspark.mllib.stat import Statistics from pyspark.sql import functions as F from pyspark.sql import types as T import pyspark from anovos.shared.utils import attributeType_segregation , transpose_dataframe def global_summary ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The global summary function computes the universal statistics/metrics and returns a Spark DataFrame with schema \u2013 metric, value. The metrics computed in this function - No. of rows, No. of columns, No. of categorical columns along with column names, No. of numerical columns along with the column names, No. of non-numerical non-categorical columns such as date type, array type etc. along with column names. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [metric, value] \"\"\" if list_of_cols == \"all\" : list_of_cols = idf . columns if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) row_count = idf . count () col_count = len ( list_of_cols ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) numcol_count = len ( num_cols ) catcol_count = len ( cat_cols ) othercol_count = len ( other_cols ) if print_impact : print ( \"No. of Rows: %s \" % \" {0:,} \" . format ( row_count )) print ( \"No. of Columns: %s \" % \" {0:,} \" . format ( col_count )) print ( \"Numerical Columns: %s \" % \" {0:,} \" . format ( numcol_count )) if numcol_count > 0 : print ( num_cols ) print ( \"Categorical Columns: %s \" % \" {0:,} \" . format ( catcol_count )) if catcol_count > 0 : print ( cat_cols ) if othercol_count > 0 : print ( \"Other Columns: %s \" % \" {0:,} \" . format ( othercol_count )) print ( other_cols ) odf = spark . createDataFrame ( [ [ \"rows_count\" , str ( row_count )], [ \"columns_count\" , str ( col_count )], [ \"numcols_count\" , str ( numcol_count )], [ \"numcols_name\" , \", \" . join ( num_cols )], [ \"catcols_count\" , str ( catcol_count )], [ \"catcols_name\" , \", \" . join ( cat_cols )], [ \"othercols_count\" , str ( othercol_count )], [ \"othercols_name\" , \", \" . join ( other_cols )], ], schema = [ \"metric\" , \"value\" ], ) return odf def missingCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, missing_count, missing_pct] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) idf_stats = idf . select ( list_of_cols ) . summary ( \"count\" ) odf = ( transpose_dataframe ( idf_stats , \"summary\" ) . withColumn ( \"missing_count\" , F . lit ( idf . count ()) - F . col ( \"count\" ) . cast ( T . LongType ()) ) . withColumn ( \"missing_pct\" , F . round ( F . col ( \"missing_count\" ) / F . lit ( idf . count ()), 4 ) ) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), \"missing_count\" , \"missing_pct\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def nonzeroCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, nonzero_count, nonzero_pct] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Non-Zero Count Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"nonzero_count\" , T . StringType (), True ), T . StructField ( \"nonzero_pct\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf tmp = idf . select ( list_of_cols ) . fillna ( 0 ) . rdd . map ( lambda row : Vectors . dense ( row )) nonzero_count = Statistics . colStats ( tmp ) . numNonzeros () odf = spark . createDataFrame ( zip ( list_of_cols , [ int ( i ) for i in nonzero_count ]), schema = ( \"attribute\" , \"nonzero_count\" ), ) . withColumn ( \"nonzero_pct\" , F . round ( F . col ( \"nonzero_count\" ) / F . lit ( idf . count ()), 4 )) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_counts ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Counts function computes different count metrics for each column. It returns a Spark DataFrame with schema \u2013 attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct. - Fill Count/Rate is defined as number of rows with non-null values in a column both in terms of absolute count and its proportion to row count. It leverages count statistic from summary functionality of Spark SQL. - Missing Count/Rate is defined as null (or missing) values seen in a column both in terms of absolute count and its proportion to row count. It is directly derivable from Fill Count/Rate. - Non Zero Count/Rate is defined as non-zero values seen in a numerical column both in terms of absolute count and its proportion to row count. For categorical column, it will show null value. Also, it uses a supporting function nonzeroCount_computation. Under the hood, it leverage Multivariate Statistical Summary of Spark MLlib. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, fill_count, fill_pct, missing_count, missing_pct, nonzero_count, nonzero_pct] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"count\" ), \"summary\" ) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), F . col ( \"count\" ) . cast ( T . LongType ()) . alias ( \"fill_count\" ), ) . withColumn ( \"fill_pct\" , F . round ( F . col ( \"fill_count\" ) / F . lit ( idf . count ()), 4 )) . withColumn ( \"missing_count\" , F . lit ( idf . count ()) - F . col ( \"fill_count\" ) . cast ( T . LongType ()) ) . withColumn ( \"missing_pct\" , F . round ( 1 - F . col ( \"fill_pct\" ), 4 )) . join ( nonzeroCount_computation ( spark , idf , num_cols ), \"attribute\" , \"full_outer\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def mode_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, mode, mode_rows] In case there is tie between multiple values, one value is randomly picked as mode. \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Mode Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf list_df = [] for col in list_of_cols : out_df = ( idf . select ( col ) . dropna () . groupby ( col ) . count () . orderBy ( \"count\" , ascending = False ) . limit ( 1 ) . select ( F . lit ( col ) . alias ( \"attribute\" ), F . col ( col ) . alias ( \"mode\" ), F . col ( \"count\" ) . alias ( \"mode_rows\" ), ) ) list_df . append ( out_df ) def unionAll ( dfs ): first , * _ = dfs schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"mode\" , T . StringType (), True ), T . StructField ( \"mode_rows\" , T . LongType (), True ), ] ) return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), schema ) odf = unionAll ( list_df ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_centralTendency ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Central Tendency function provides summary statistics that represents the centre point or most likely value of an attribute. It returns a Spark DataFrame with schema \u2013 attribute, mean, median, mode, mode_rows, mode_pct. - Mean is arithmetic average of a column i.e. sum of all values seen in the column divided by the number of rows. It leverage mean statistic from summary functionality of Spark SQL. Mean is calculated only for numerical columns. - Median is 50th percentile or middle value in a column when the values are arranged in ascending or descending order. It leverage \u201850%\u2019 statistic from summary functionality of Spark SQL. Median is calculated only for numerical columns. - Mode is most frequently seen value in a column. Mode is calculated only for discrete columns (categorical + Integer/Long columns). - Mode Rows is the numer of rows seen with Mode value. Mode Rows is calculated only for discrete columns (categorical + Integer/Long columns). - Mode Pct is defined as Mode Rows divided by non-null values seen in a column. Mode Pct is calculated only for discrete columns (categorical + Integer/Long columns). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, mean, median, mode, mode_rows, mode_pct] \"\"\" num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) df_mode_compute = mode_computation ( spark , idf , list_of_cols ) summary_lst = [] for col in list_of_cols : summary_col = ( idf . select ( col ) . summary ( \"mean\" , \"50%\" , \"count\" ) . rdd . map ( lambda x : x [ 1 ]) . collect () ) summary_col = [ str ( i ) for i in summary_col if type ( i ) != \"str\" ] summary_col . insert ( 0 , col ) summary_lst . append ( summary_col ) summary_df = spark . createDataFrame ( summary_lst , schema = ( \"key\" , \"mean\" , \"50%\" , \"count\" ), ) odf = ( summary_df . withColumn ( \"mean\" , F . when ( F . col ( \"key\" ) . isin ( num_cols ), F . round ( F . col ( \"mean\" ) . cast ( T . DoubleType ()), 4 ), ) . otherwise ( None ), ) . withColumn ( \"median\" , F . when ( F . col ( \"key\" ) . isin ( num_cols ), F . round ( F . col ( \"50%\" ) . cast ( T . DoubleType ()), 4 ), ) . otherwise ( None ), ) . withColumnRenamed ( \"key\" , \"attribute\" ) . join ( df_mode_compute , \"attribute\" , \"full_outer\" ) . withColumn ( \"mode_pct\" , F . round ( F . col ( \"mode_rows\" ) / F . col ( \"count\" ) . cast ( T . DoubleType ()), 4 ), ) . select ( \"attribute\" , \"mean\" , \"median\" , \"mode\" , \"mode_rows\" , \"mode_pct\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def uniqueCount_computation ( spark , idf , list_of_cols = \"all\" , drop_cols = [], compute_approx_unique_count = False , rsd = None , print_impact = False , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) compute_approx_unique_count boolean, optional This flag tells the function whether to compute approximate unique count or exact unique count (Default value = False) rsd float, optional This is used when compute_approx_unique_count is True. This is the maximum relative standard deviation allowed (default = 0.05). For rsd < 0.01, it is more efficient to use :func:`countDistinct` print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, unique_values] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if rsd != None and rsd < 0 : raise ValueError ( \"rsd value can not be less than 0 (default value is 0.05)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Unique Count Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf if compute_approx_unique_count : uniquevalue_count = idf . agg ( * ( F . approx_count_distinct ( F . col ( i ), rsd ) . alias ( i ) for i in list_of_cols ) ) else : uniquevalue_count = idf . agg ( * ( F . countDistinct ( F . col ( i )) . alias ( i ) for i in list_of_cols ) ) odf = spark . createDataFrame ( zip ( list_of_cols , uniquevalue_count . rdd . map ( list ) . collect ()[ 0 ]), schema = ( \"attribute\" , \"unique_values\" ), ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_cardinality ( spark , idf , list_of_cols = \"all\" , drop_cols = [], use_approx_unique_count = True , rsd = None , print_impact = False , ): \"\"\" The Measures of Cardinality function provides statistics that are related to unique values seen in an attribute. These statistics are calculated only for discrete columns (categorical + Integer/Long columns). It returns a Spark Dataframe with schema \u2013 attribute, unique_values, IDness. - Unique Value is defined as a distinct value count of a column. It relies on a supporting function uniqueCount_computation for its computation and leverages the countDistinct/approx_count_distinct functionality of Spark SQL. - IDness is calculated as Unique Values divided by non-null values seen in a column. Non-null values count is used instead of total count because too many null values can give misleading results even if the column have all unique values (except null). It uses supporting functions - uniqueCount_computation and missingCount_computation. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of Discrete (Categorical + Integer) columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all discrete columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) use_approx_unique_count boolean, optional This flag tells the function whether to use approximate unique count to compute the IDness or use exact unique count (Default value = True) rsd float, optional This is used when use_approx_unique_count is True. This is the maximum relative standard deviation allowed (default = 0.05). For rsd < 0.01, it is more efficient to set use_approx_unique_count as False print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, unique_values, IDness] \"\"\" if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if rsd != None and rsd < 0 : raise ValueError ( \"rsd value can not be less than 0 (default value is 0.05)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Cardinality Computation - No discrete column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"unique_values\" , T . StringType (), True ), T . StructField ( \"IDness\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf odf = ( uniqueCount_computation ( spark , idf , list_of_cols , compute_approx_unique_count = use_approx_unique_count , rsd = rsd , ) . join ( missingCount_computation ( spark , idf , list_of_cols ), \"attribute\" , \"full_outer\" , ) . withColumn ( \"IDness\" , F . round ( F . col ( \"unique_values\" ) / ( F . lit ( idf . count ()) - F . col ( \"missing_count\" )), 4 , ), ) . select ( \"attribute\" , \"unique_values\" , \"IDness\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_dispersion ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Dispersion function provides statistics that describe the spread of a numerical attribute. Alternatively, these statistics are also known as measures of spread. It returns a Spark DataFrame with schema \u2013 attribute, stddev, variance, cov, IQR, range. - Standard Deviation (stddev) measures how concentrated an attribute is around the mean or average. It leverages \u2018stddev\u2019 statistic from summary functionality of Spark SQL. - Variance is the squared value of Standard Deviation. - Coefficient of Variance (cov) is computed as ratio of Standard Deviation & Mean. It leverages \u2018stddev\u2019 and \u2018mean\u2019 statistic from the summary functionality of Spark SQL. - Interquartile Range (IQR): It describes the difference between the third quartile (75th percentile) and the first quartile (25th percentile), telling us about the range where middle half values are seen. It leverage \u201825%\u2019 and \u201875%\u2019 statistics from the summary functionality of Spark SQL. - Range is simply the difference between the maximum value and the minimum value. It leverage \u2018min\u2019 and \u2018max\u2019 statistics from the summary functionality of Spark Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, stddev, variance, cov, IQR, range] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Dispersion Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"stddev\" , T . StringType (), True ), T . StructField ( \"variance\" , T . StringType (), True ), T . StructField ( \"cov\" , T . StringType (), True ), T . StructField ( \"IQR\" , T . StringType (), True ), T . StructField ( \"range\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf odf = ( transpose_dataframe ( idf . select ( list_of_cols ) . summary ( \"stddev\" , \"min\" , \"max\" , \"mean\" , \"25%\" , \"75%\" ), \"summary\" , ) . withColumn ( \"stddev\" , F . round ( F . col ( \"stddev\" ) . cast ( T . DoubleType ()), 4 )) . withColumn ( \"variance\" , F . round ( F . col ( \"stddev\" ) * F . col ( \"stddev\" ), 4 )) . withColumn ( \"range\" , F . round ( F . col ( \"max\" ) - F . col ( \"min\" ), 4 )) . withColumn ( \"cov\" , F . round ( F . col ( \"stddev\" ) / F . col ( \"mean\" ), 4 )) . withColumn ( \"IQR\" , F . round ( F . col ( \"75%\" ) - F . col ( \"25%\" ), 4 )) . select ( F . col ( \"key\" ) . alias ( \"attribute\" ), \"stddev\" , \"variance\" , \"cov\" , \"IQR\" , \"range\" ) ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_percentiles ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Percentiles function provides statistics at different percentiles. Nth percentile can be interpreted as N% of rows having values lesser than or equal to Nth percentile value. It is prominently used for quick detection of skewness or outlier. Alternatively, these statistics are also known as measures of position. These statistics are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max. It leverage \u2018N%\u2019 statistics from summary functionality of Spark SQL where N is 0 for min and 100 for max. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, min, 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%, max] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Percentiles Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"min\" , T . StringType (), True ), T . StructField ( \"1%\" , T . StringType (), True ), T . StructField ( \"5%\" , T . StringType (), True ), T . StructField ( \"10%\" , T . StringType (), True ), T . StructField ( \"25%\" , T . StringType (), True ), T . StructField ( \"50%\" , T . StringType (), True ), T . StructField ( \"75%\" , T . StringType (), True ), T . StructField ( \"90%\" , T . StringType (), True ), T . StructField ( \"95%\" , T . StringType (), True ), T . StructField ( \"99%\" , T . StringType (), True ), T . StructField ( \"max\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf stats = [ \"min\" , \"1%\" , \"5%\" , \"10%\" , \"25%\" , \"50%\" , \"75%\" , \"90%\" , \"95%\" , \"99%\" , \"max\" ] odf = transpose_dataframe ( idf . select ( list_of_cols ) . summary ( * stats ), \"summary\" ) . withColumnRenamed ( \"key\" , \"attribute\" ) for i in odf . columns : if i != \"attribute\" : odf = odf . withColumn ( i , F . round ( F . col ( i ) . cast ( \"Double\" ), 4 )) odf = odf . select ([ \"attribute\" ] + stats ) if print_impact : odf . show ( len ( list_of_cols )) return odf def measures_of_shape ( spark , idf , list_of_cols = \"all\" , drop_cols = [], print_impact = False ): \"\"\" The Measures of Shapes function provides statistics related to the shape of an attribute's distribution. Alternatively, these statistics are also known as measures of the moment and are computed only for numerical attributes. It returns a Spark Dataframe with schema \u2013 attribute, skewness, kurtosis. - Skewness describes how much-skewed values are relative to a perfect bell curve observed in normal distribution and the direction of skew. If the majority of the values are at the left and the right tail is longer, we say that the distribution is skewed right or positively skewed; if the peak is toward the right and the left tail is longer, we say that the distribution is skewed left or negatively skewed. It leverage skewness functionality of Spark SQL. - (Excess) Kurtosis describes how tall and sharp the central peak is relative to a perfect bell curve observed in the normal distribution. The reference standard is a normal distribution, which has a kurtosis of 3. In token of this, often, the excess kurtosis is presented: excess kurtosis is simply kurtosis\u22123. Higher (positive) values indicate a higher, sharper peak; lower (negative) values indicate a less distinct peak. It leverages kurtosis functionality of Spark SQL. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to analyse e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) print_impact True, False This argument is to print out the statistics.(Default value = False) Returns ------- DataFrame [attribute, skewness, kurtosis] \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Skewness/Kurtosis Computation - No numerical column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"skewness\" , T . StringType (), True ), T . StructField ( \"kurtosis\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf exprs = [ f ( F . col ( c )) for f in [ F . skewness , F . kurtosis ] for c in list_of_cols ] list_result = idf . groupby () . agg ( * exprs ) . rdd . flatMap ( lambda x : x ) . collect () shapes = [] for i in range ( int ( len ( list_result ) / 2 )): shapes . append ( [ list_of_cols [ i ], list_result [ i ], list_result [ i + int ( len ( list_result ) / 2 )], ] ) odf = ( spark . createDataFrame ( shapes , schema = ( \"attribute\" , \"skewness\" , \"kurtosis\" )) . withColumn ( \"skewness\" , F . round ( F . col ( \"skewness\" ), 4 )) . withColumn ( \"kurtosis\" , F . round ( F . col ( \"kurtosis\" ), 4 )) ) if print_impact : odf . show ( len ( list_of_cols )) return odf","title":"stats_generator"},{"location":"api/data_analyzer/stats_generator.html#functions","text":"def global_summary ( spark, idf, list_of_cols='all', drop_cols=[], print_impact=False) The global summary function computes the universal statistics/metrics and returns a Spark DataFrame with schema \u2013 metric, value. The metrics computed in this function - No. of rows, No. of columns, No. of categorical columns along with column names, No. of numerical columns along with the column names, No. of non-numerical non-categorical columns such as date type, array type etc. along with column names.","title":"Functions"},{"location":"api/data_analyzer/ts_analyzer.html","text":"ts_analyzer This module generates the intermediate output specific to the inspection of Time series analysis. As a part of generation of final output, there are various functions created such as - ts_processed_feats ts_eligiblity_check ts_viz_data ts_analyzer daypart_cat Respective functions have sections containing the detailed definition of the parameters used for computing. Expand source code # coding=utf-8 \"\"\"This module generates the intermediate output specific to the inspection of Time series analysis. As a part of generation of final output, there are various functions created such as - - ts_processed_feats - ts_eligiblity_check - ts_viz_data - ts_analyzer - daypart_cat Respective functions have sections containing the detailed definition of the parameters used for computing. \"\"\" import calendar from anovos.shared.utils import ( attributeType_segregation , ends_with , output_to_local , path_ak8s_modify , ) from anovos.data_analyzer.stats_generator import measures_of_percentiles from anovos.data_ingest.ts_auto_detection import ts_preprocess from anovos.data_transformer.datetime import ( timeUnits_extraction , unix_to_timestamp , lagged_ts , ) import csv import datetime import io import os import re import subprocess import warnings from pathlib import Path import dateutil.parser import numpy as np import pandas as pd import pyspark from loguru import logger from pyspark.sql import Window from pyspark.sql import functions as F from pyspark.sql import types as T from statsmodels.tsa.seasonal import seasonal_decompose def daypart_cat ( column ): \"\"\" This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value. Parameters ---------- column Reads the column containing the hour part and converts into respective day part Returns ------- String \"\"\" # calculate hour buckets after adding local timezone if column is None : return \"Missing_NA\" elif ( column >= 4 ) and ( column < 7 ): return \"early_hours\" elif ( column >= 10 ) and ( column < 17 ): return \"work_hours\" elif ( column >= 23 ) or ( column < 4 ): return \"late_hours\" elif (( column >= 7 ) and ( column < 10 )) or (( column >= 17 ) and ( column < 20 )): return \"commuting_hours\" else : return \"other_hours\" f_daypart_cat = F . udf ( daypart_cat , T . StringType ()) def ts_processed_feats ( idf , col , id_col , tz , cnt_row , cnt_unique_id ): \"\"\" This function helps to extract time units from the input dataframe on a processed column being timestamp / date. Parameters ---------- idf Input dataframe col Column belonging to timestamp / date id_col ID column tz Timezone offset cnt_row Count of rows present in the Input dataframe cnt_unique_id Count of unique records present in the Input dataframe Returns ------- DataFrame \"\"\" if cnt_row == cnt_unique_id : odf = ( timeUnits_extraction ( idf , col , \"all\" , output_mode = \"append\" , ) . withColumn ( \"yyyymmdd_col\" , F . to_date ( col )) . orderBy ( \"yyyymmdd_col\" ) . withColumn ( \"daypart_cat\" , f_daypart_cat ( F . col ( col + \"_hour\" ))) . withColumn ( \"week_cat\" , F . when ( F . col ( col + \"_dayofweek\" ) > 5 , F . lit ( \"weekend\" )) . otherwise ( \"weekday\" ), ) . withColumnRenamed ( col + \"_dayofweek\" , \"dow\" ) ) return odf else : odf = ( timeUnits_extraction ( idf , col , \"all\" , output_mode = \"append\" , ) . withColumn ( \"yyyymmdd_col\" , F . to_date ( col )) . orderBy ( id_col , \"yyyymmdd_col\" ) . withColumn ( \"daypart_cat\" , f_daypart_cat ( F . col ( col + \"_hour\" ))) . withColumn ( \"week_cat\" , F . when ( F . col ( col + \"_dayofweek\" ) > 5 , F . lit ( \"weekend\" )) . otherwise ( \"weekday\" ), ) . withColumnRenamed ( col + \"_dayofweek\" , \"dow\" ) ) return odf def ts_eligiblity_check ( spark , idf , id_col , opt = 1 , tz_offset = \"local\" ): \"\"\" This function helps to extract various metrics which can help to understand the nature of timestamp / date column for a given dataset. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column opt Option to choose between [1,2]. 1 is kept as default. Based on the user input, the specific aggregation of data will happen. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". Returns ------- DataFrame \"\"\" lagged_df = lagged_ts ( idf . select ( \"yyyymmdd_col\" ) . distinct () . orderBy ( \"yyyymmdd_col\" ), \"yyyymmdd_col\" , lag = 1 , tsdiff_unit = \"days\" , output_mode = \"append\" , ) . orderBy ( \"yyyymmdd_col\" ) diff_lagged_df = list ( np . around ( lagged_df . withColumn ( \"daydiff\" , F . datediff ( \"yyyymmdd_col\" , \"yyyymmdd_col_lag1\" ) ) . where ( F . col ( \"daydiff\" ) . isNotNull ()) . groupBy () . agg ( F . mean ( \"daydiff\" ) . alias ( \"mean\" ), F . variance ( \"daydiff\" ) . alias ( \"variance\" ), F . stddev ( \"daydiff\" ) . alias ( \"stdev\" ), ) . withColumn ( \"coef_of_var_lag\" , F . col ( \"stdev\" ) / F . col ( \"mean\" )) . rdd . flatMap ( lambda x : x ) . collect (), 3 , ) ) p1 = measures_of_percentiles ( spark , idf . groupBy ( id_col ) . agg ( F . countDistinct ( \"yyyymmdd_col\" ) . alias ( \"id_date_pair\" )), list_of_cols = \"id_date_pair\" , ) p2 = measures_of_percentiles ( spark , idf . groupBy ( \"yyyymmdd_col\" ) . agg ( F . countDistinct ( id_col ) . alias ( \"date_id_pair\" )), list_of_cols = \"date_id_pair\" , ) if opt == 1 : odf = p1 . union ( p2 ) . toPandas () return odf else : odf = idf m = ( odf . groupBy ( \"yyyymmdd_col\" ) . count () . orderBy ( \"count\" , ascending = False ) . collect () ) mode = str ( m [ 0 ][ 0 ]) + \" [\" + str ( m [ 0 ][ 1 ]) + \"]\" missing_vals = odf . where ( F . col ( \"yyyymmdd_col\" ) . isNull ()) . count () odf = ( odf . groupBy () . agg ( F . countDistinct ( \"yyyymmdd_col\" ) . alias ( \"count_unique_dates\" ), F . min ( \"yyyymmdd_col\" ) . alias ( \"min_date\" ), F . max ( \"yyyymmdd_col\" ) . alias ( \"max_date\" ), ) . withColumn ( \"modal_date\" , F . lit ( mode )) . withColumn ( \"date_diff\" , F . datediff ( \"max_date\" , \"min_date\" )) . withColumn ( \"missing_date\" , F . lit ( missing_vals )) . withColumn ( \"mean\" , F . lit ( diff_lagged_df [ 0 ])) . withColumn ( \"variance\" , F . lit ( diff_lagged_df [ 1 ])) . withColumn ( \"stdev\" , F . lit ( diff_lagged_df [ 2 ])) . withColumn ( \"cov\" , F . lit ( diff_lagged_df [ 3 ])) . toPandas () ) return odf def ts_viz_data ( idf , x_col , y_col , id_col , tz_offset = \"local\" , output_mode = \"append\" , output_type = \"daily\" , n_cat = 10 , ): \"\"\" This function helps to produce the processed dataframe with the relevant aggregation at the time frequency chosen for a given column as seen against the timestamp / date column. Parameters ---------- idf Input Dataframe x_col Timestamp / Date column as set in the X-Axis y_col Numerical & Categorical column as set in the Y-Axis id_col ID Column tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. n_cat For categorical columns whose cardinality is beyond N, the Top N categories are chosen, beyond which the categories are grouped as Others. Returns ------- DataFrame \"\"\" y_col_org = y_col y_col = y_col . replace ( \"-\" , \"_\" ) idf = idf . withColumnRenamed ( y_col_org , y_col ) for i in idf . dtypes : if y_col == i [ 0 ]: y_col_dtype = i [ 1 ] if y_col_dtype == \"string\" : top_cat = list ( idf . groupBy ( y_col ) . count () . orderBy ( \"count\" , ascending = False ) . limit ( int ( n_cat )) . select ( y_col ) . toPandas ()[ y_col ] . values ) idf = idf . withColumn ( y_col , F . when ( F . col ( y_col ) . isin ( top_cat ), F . col ( y_col )) . otherwise ( F . lit ( \"Others\" )), ) if output_type == \"daily\" : odf = ( idf . groupBy ( y_col , \"yyyymmdd_col\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"yyyymmdd_col\" ) . withColumnRenamed ( \"yyyymmdd_col\" , x_col ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"hourly\" : odf = ( idf . groupBy ( y_col , \"daypart_cat\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"daypart_cat\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"weekly\" : odf = ( idf . groupBy ( y_col , \"dow\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"dow\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) return odf else : if output_type == \"daily\" : odf = ( idf . groupBy ( \"yyyymmdd_col\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"yyyymmdd_col\" ) . withColumnRenamed ( \"yyyymmdd_col\" , x_col ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"hourly\" : odf = ( idf . groupBy ( \"daypart_cat\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"daypart_cat\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"weekly\" : odf = ( idf . groupBy ( \"dow\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"dow\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) return odf def ts_analyzer ( spark , idf , id_col , max_days , output_path , output_type = \"daily\" , tz_offset = \"local\" , run_type = \"local\" , auth_key = \"NA\" , ): \"\"\" This function helps to produce the processed output in an aggregate form considering the input dataframe with processed timestamp / date column. The aggregation happens across Mean, Median, Min & Max for the Numerical / Categorical column. Parameters ---------- spark Spark session idf Input Dataframe id_col ID Column max_days Max days upto which the data will be aggregated. If we've a dataset containing a timestamp / date field with very high number of unique dates (Let's say beyond 20 years worth of daily data), a maximum days value chosen basis which the latest output is displayed. output_path Output path where the intermediate data is going to be saved output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"local\" or \"emr\" or \"databricks\" or \"ak8s\" basis the user flexibility. Default option is set as \"Local\". auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns ------- Output[CSV] \"\"\" if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) num_cols = [ x for x in num_cols if x not in [ id_col ]] cat_cols = [ x for x in cat_cols if x not in [ id_col ]] ts_loop_cols_post = [ x [ 0 ] for x in idf . dtypes if x [ 1 ] in [ \"timestamp\" , \"date\" ]] cnt_row = idf . count () cnt_unique_id = idf . select ( id_col ) . distinct () . count () for i in ts_loop_cols_post : ts_processed_feat_df = ts_processed_feats ( idf , i , id_col , tz_offset , cnt_row , cnt_unique_id ) ts_processed_feat_df . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) # for j in range(1, 3): # f = ts_eligiblity_check( # spark, # ts_processed_feat_df, # id_col=id_col, # opt=j, # tz_offset=tz_offset, # ) # f.to_csv( # ends_with(local_path) + \"stats_\" + str(i) + \"_\" + str(j) + \".csv\", # index=False, # ) f1 = ts_eligiblity_check ( spark , ts_processed_feat_df , id_col = id_col , opt = 1 , tz_offset = tz_offset ) f1 . to_csv ( ends_with ( local_path ) + \"stats_\" + str ( i ) + \"_\" + str ( 1 ) + \".csv\" , index = False , ) f2 = ts_eligiblity_check ( spark , ts_processed_feat_df , id_col = id_col , opt = 2 , tz_offset = tz_offset ) f2 . to_csv ( ends_with ( local_path ) + \"stats_\" + str ( i ) + \"_\" + str ( 2 ) + \".csv\" , index = False , ) for k in [ num_cols , cat_cols ]: for l in k : for m in [ output_type ]: f = ( ts_viz_data ( ts_processed_feat_df , i , l , id_col = id_col , tz_offset = tz_offset , output_mode = \"append\" , output_type = m , n_cat = 10 , ) . tail ( int ( max_days )) . dropna () ) f . to_csv ( ends_with ( local_path ) + i + \"_\" + l + \"_\" + m + \".csv\" , index = False , ) ts_processed_feat_df . unpersist () if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( output_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( output_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) Functions def daypart_cat ( column) This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value. Parameters column Reads the column containing the hour part and converts into respective day part Returns String Expand source code def daypart_cat ( column ): \"\"\" This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value. Parameters ---------- column Reads the column containing the hour part and converts into respective day part Returns ------- String \"\"\" # calculate hour buckets after adding local timezone if column is None : return \"Missing_NA\" elif ( column >= 4 ) and ( column < 7 ): return \"early_hours\" elif ( column >= 10 ) and ( column < 17 ): return \"work_hours\" elif ( column >= 23 ) or ( column < 4 ): return \"late_hours\" elif (( column >= 7 ) and ( column < 10 )) or (( column >= 17 ) and ( column < 20 )): return \"commuting_hours\" else : return \"other_hours\" def f_daypart_cat ( column) This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value. Parameters column Reads the column containing the hour part and converts into respective day part Returns String Expand source code def daypart_cat ( column ): \"\"\" This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value. Parameters ---------- column Reads the column containing the hour part and converts into respective day part Returns ------- String \"\"\" # calculate hour buckets after adding local timezone if column is None : return \"Missing_NA\" elif ( column >= 4 ) and ( column < 7 ): return \"early_hours\" elif ( column >= 10 ) and ( column < 17 ): return \"work_hours\" elif ( column >= 23 ) or ( column < 4 ): return \"late_hours\" elif (( column >= 7 ) and ( column < 10 )) or (( column >= 17 ) and ( column < 20 )): return \"commuting_hours\" else : return \"other_hours\" def ts_analyzer ( spark, idf, id_col, max_days, output_path, output_type='daily', tz_offset='local', run_type='local', auth_key='NA') This function helps to produce the processed output in an aggregate form considering the input dataframe with processed timestamp / date column. The aggregation happens across Mean, Median, Min & Max for the Numerical / Categorical column. Parameters spark Spark session idf Input Dataframe id_col ID Column max_days Max days upto which the data will be aggregated. If we've a dataset containing a timestamp / date field with very high number of unique dates (Let's say beyond 20 years worth of daily data), a maximum days value chosen basis which the latest output is displayed. output_path Output path where the intermediate data is going to be saved output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"local\" or \"emr\" or \"databricks\" or \"ak8s\" basis the user flexibility. Default option is set as \"Local\". auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns Output[CSV] Expand source code def ts_analyzer ( spark , idf , id_col , max_days , output_path , output_type = \"daily\" , tz_offset = \"local\" , run_type = \"local\" , auth_key = \"NA\" , ): \"\"\" This function helps to produce the processed output in an aggregate form considering the input dataframe with processed timestamp / date column. The aggregation happens across Mean, Median, Min & Max for the Numerical / Categorical column. Parameters ---------- spark Spark session idf Input Dataframe id_col ID Column max_days Max days upto which the data will be aggregated. If we've a dataset containing a timestamp / date field with very high number of unique dates (Let's say beyond 20 years worth of daily data), a maximum days value chosen basis which the latest output is displayed. output_path Output path where the intermediate data is going to be saved output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"local\" or \"emr\" or \"databricks\" or \"ak8s\" basis the user flexibility. Default option is set as \"Local\". auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns ------- Output[CSV] \"\"\" if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) num_cols = [ x for x in num_cols if x not in [ id_col ]] cat_cols = [ x for x in cat_cols if x not in [ id_col ]] ts_loop_cols_post = [ x [ 0 ] for x in idf . dtypes if x [ 1 ] in [ \"timestamp\" , \"date\" ]] cnt_row = idf . count () cnt_unique_id = idf . select ( id_col ) . distinct () . count () for i in ts_loop_cols_post : ts_processed_feat_df = ts_processed_feats ( idf , i , id_col , tz_offset , cnt_row , cnt_unique_id ) ts_processed_feat_df . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) # for j in range(1, 3): # f = ts_eligiblity_check( # spark, # ts_processed_feat_df, # id_col=id_col, # opt=j, # tz_offset=tz_offset, # ) # f.to_csv( # ends_with(local_path) + \"stats_\" + str(i) + \"_\" + str(j) + \".csv\", # index=False, # ) f1 = ts_eligiblity_check ( spark , ts_processed_feat_df , id_col = id_col , opt = 1 , tz_offset = tz_offset ) f1 . to_csv ( ends_with ( local_path ) + \"stats_\" + str ( i ) + \"_\" + str ( 1 ) + \".csv\" , index = False , ) f2 = ts_eligiblity_check ( spark , ts_processed_feat_df , id_col = id_col , opt = 2 , tz_offset = tz_offset ) f2 . to_csv ( ends_with ( local_path ) + \"stats_\" + str ( i ) + \"_\" + str ( 2 ) + \".csv\" , index = False , ) for k in [ num_cols , cat_cols ]: for l in k : for m in [ output_type ]: f = ( ts_viz_data ( ts_processed_feat_df , i , l , id_col = id_col , tz_offset = tz_offset , output_mode = \"append\" , output_type = m , n_cat = 10 , ) . tail ( int ( max_days )) . dropna () ) f . to_csv ( ends_with ( local_path ) + i + \"_\" + l + \"_\" + m + \".csv\" , index = False , ) ts_processed_feat_df . unpersist () if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( output_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( output_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) def ts_eligiblity_check ( spark, idf, id_col, opt=1, tz_offset='local') This function helps to extract various metrics which can help to understand the nature of timestamp / date column for a given dataset. Parameters spark Spark session idf Input dataframe id_col ID Column opt Option to choose between [1,2]. 1 is kept as default. Based on the user input, the specific aggregation of data will happen. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". Returns DataFrame Expand source code def ts_eligiblity_check ( spark , idf , id_col , opt = 1 , tz_offset = \"local\" ): \"\"\" This function helps to extract various metrics which can help to understand the nature of timestamp / date column for a given dataset. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column opt Option to choose between [1,2]. 1 is kept as default. Based on the user input, the specific aggregation of data will happen. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". Returns ------- DataFrame \"\"\" lagged_df = lagged_ts ( idf . select ( \"yyyymmdd_col\" ) . distinct () . orderBy ( \"yyyymmdd_col\" ), \"yyyymmdd_col\" , lag = 1 , tsdiff_unit = \"days\" , output_mode = \"append\" , ) . orderBy ( \"yyyymmdd_col\" ) diff_lagged_df = list ( np . around ( lagged_df . withColumn ( \"daydiff\" , F . datediff ( \"yyyymmdd_col\" , \"yyyymmdd_col_lag1\" ) ) . where ( F . col ( \"daydiff\" ) . isNotNull ()) . groupBy () . agg ( F . mean ( \"daydiff\" ) . alias ( \"mean\" ), F . variance ( \"daydiff\" ) . alias ( \"variance\" ), F . stddev ( \"daydiff\" ) . alias ( \"stdev\" ), ) . withColumn ( \"coef_of_var_lag\" , F . col ( \"stdev\" ) / F . col ( \"mean\" )) . rdd . flatMap ( lambda x : x ) . collect (), 3 , ) ) p1 = measures_of_percentiles ( spark , idf . groupBy ( id_col ) . agg ( F . countDistinct ( \"yyyymmdd_col\" ) . alias ( \"id_date_pair\" )), list_of_cols = \"id_date_pair\" , ) p2 = measures_of_percentiles ( spark , idf . groupBy ( \"yyyymmdd_col\" ) . agg ( F . countDistinct ( id_col ) . alias ( \"date_id_pair\" )), list_of_cols = \"date_id_pair\" , ) if opt == 1 : odf = p1 . union ( p2 ) . toPandas () return odf else : odf = idf m = ( odf . groupBy ( \"yyyymmdd_col\" ) . count () . orderBy ( \"count\" , ascending = False ) . collect () ) mode = str ( m [ 0 ][ 0 ]) + \" [\" + str ( m [ 0 ][ 1 ]) + \"]\" missing_vals = odf . where ( F . col ( \"yyyymmdd_col\" ) . isNull ()) . count () odf = ( odf . groupBy () . agg ( F . countDistinct ( \"yyyymmdd_col\" ) . alias ( \"count_unique_dates\" ), F . min ( \"yyyymmdd_col\" ) . alias ( \"min_date\" ), F . max ( \"yyyymmdd_col\" ) . alias ( \"max_date\" ), ) . withColumn ( \"modal_date\" , F . lit ( mode )) . withColumn ( \"date_diff\" , F . datediff ( \"max_date\" , \"min_date\" )) . withColumn ( \"missing_date\" , F . lit ( missing_vals )) . withColumn ( \"mean\" , F . lit ( diff_lagged_df [ 0 ])) . withColumn ( \"variance\" , F . lit ( diff_lagged_df [ 1 ])) . withColumn ( \"stdev\" , F . lit ( diff_lagged_df [ 2 ])) . withColumn ( \"cov\" , F . lit ( diff_lagged_df [ 3 ])) . toPandas () ) return odf def ts_processed_feats ( idf, col, id_col, tz, cnt_row, cnt_unique_id) This function helps to extract time units from the input dataframe on a processed column being timestamp / date. Parameters idf Input dataframe col Column belonging to timestamp / date id_col ID column tz Timezone offset cnt_row Count of rows present in the Input dataframe cnt_unique_id Count of unique records present in the Input dataframe Returns DataFrame Expand source code def ts_processed_feats ( idf , col , id_col , tz , cnt_row , cnt_unique_id ): \"\"\" This function helps to extract time units from the input dataframe on a processed column being timestamp / date. Parameters ---------- idf Input dataframe col Column belonging to timestamp / date id_col ID column tz Timezone offset cnt_row Count of rows present in the Input dataframe cnt_unique_id Count of unique records present in the Input dataframe Returns ------- DataFrame \"\"\" if cnt_row == cnt_unique_id : odf = ( timeUnits_extraction ( idf , col , \"all\" , output_mode = \"append\" , ) . withColumn ( \"yyyymmdd_col\" , F . to_date ( col )) . orderBy ( \"yyyymmdd_col\" ) . withColumn ( \"daypart_cat\" , f_daypart_cat ( F . col ( col + \"_hour\" ))) . withColumn ( \"week_cat\" , F . when ( F . col ( col + \"_dayofweek\" ) > 5 , F . lit ( \"weekend\" )) . otherwise ( \"weekday\" ), ) . withColumnRenamed ( col + \"_dayofweek\" , \"dow\" ) ) return odf else : odf = ( timeUnits_extraction ( idf , col , \"all\" , output_mode = \"append\" , ) . withColumn ( \"yyyymmdd_col\" , F . to_date ( col )) . orderBy ( id_col , \"yyyymmdd_col\" ) . withColumn ( \"daypart_cat\" , f_daypart_cat ( F . col ( col + \"_hour\" ))) . withColumn ( \"week_cat\" , F . when ( F . col ( col + \"_dayofweek\" ) > 5 , F . lit ( \"weekend\" )) . otherwise ( \"weekday\" ), ) . withColumnRenamed ( col + \"_dayofweek\" , \"dow\" ) ) return odf def ts_viz_data ( idf, x_col, y_col, id_col, tz_offset='local', output_mode='append', output_type='daily', n_cat=10) This function helps to produce the processed dataframe with the relevant aggregation at the time frequency chosen for a given column as seen against the timestamp / date column. Parameters idf Input Dataframe x_col Timestamp / Date column as set in the X-Axis y_col Numerical & Categorical column as set in the Y-Axis id_col ID Column tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. n_cat For categorical columns whose cardinality is beyond N, the Top N categories are chosen, beyond which the categories are grouped as Others. Returns DataFrame Expand source code def ts_viz_data ( idf , x_col , y_col , id_col , tz_offset = \"local\" , output_mode = \"append\" , output_type = \"daily\" , n_cat = 10 , ): \"\"\" This function helps to produce the processed dataframe with the relevant aggregation at the time frequency chosen for a given column as seen against the timestamp / date column. Parameters ---------- idf Input Dataframe x_col Timestamp / Date column as set in the X-Axis y_col Numerical & Categorical column as set in the Y-Axis id_col ID Column tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. n_cat For categorical columns whose cardinality is beyond N, the Top N categories are chosen, beyond which the categories are grouped as Others. Returns ------- DataFrame \"\"\" y_col_org = y_col y_col = y_col . replace ( \"-\" , \"_\" ) idf = idf . withColumnRenamed ( y_col_org , y_col ) for i in idf . dtypes : if y_col == i [ 0 ]: y_col_dtype = i [ 1 ] if y_col_dtype == \"string\" : top_cat = list ( idf . groupBy ( y_col ) . count () . orderBy ( \"count\" , ascending = False ) . limit ( int ( n_cat )) . select ( y_col ) . toPandas ()[ y_col ] . values ) idf = idf . withColumn ( y_col , F . when ( F . col ( y_col ) . isin ( top_cat ), F . col ( y_col )) . otherwise ( F . lit ( \"Others\" )), ) if output_type == \"daily\" : odf = ( idf . groupBy ( y_col , \"yyyymmdd_col\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"yyyymmdd_col\" ) . withColumnRenamed ( \"yyyymmdd_col\" , x_col ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"hourly\" : odf = ( idf . groupBy ( y_col , \"daypart_cat\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"daypart_cat\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"weekly\" : odf = ( idf . groupBy ( y_col , \"dow\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"dow\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) return odf else : if output_type == \"daily\" : odf = ( idf . groupBy ( \"yyyymmdd_col\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"yyyymmdd_col\" ) . withColumnRenamed ( \"yyyymmdd_col\" , x_col ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"hourly\" : odf = ( idf . groupBy ( \"daypart_cat\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"daypart_cat\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"weekly\" : odf = ( idf . groupBy ( \"dow\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"dow\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) return odf","title":"<code>ts_analyzer</code>"},{"location":"api/data_analyzer/ts_analyzer.html#ts_analyzer","text":"This module generates the intermediate output specific to the inspection of Time series analysis. As a part of generation of final output, there are various functions created such as - ts_processed_feats ts_eligiblity_check ts_viz_data ts_analyzer daypart_cat Respective functions have sections containing the detailed definition of the parameters used for computing. Expand source code # coding=utf-8 \"\"\"This module generates the intermediate output specific to the inspection of Time series analysis. As a part of generation of final output, there are various functions created such as - - ts_processed_feats - ts_eligiblity_check - ts_viz_data - ts_analyzer - daypart_cat Respective functions have sections containing the detailed definition of the parameters used for computing. \"\"\" import calendar from anovos.shared.utils import ( attributeType_segregation , ends_with , output_to_local , path_ak8s_modify , ) from anovos.data_analyzer.stats_generator import measures_of_percentiles from anovos.data_ingest.ts_auto_detection import ts_preprocess from anovos.data_transformer.datetime import ( timeUnits_extraction , unix_to_timestamp , lagged_ts , ) import csv import datetime import io import os import re import subprocess import warnings from pathlib import Path import dateutil.parser import numpy as np import pandas as pd import pyspark from loguru import logger from pyspark.sql import Window from pyspark.sql import functions as F from pyspark.sql import types as T from statsmodels.tsa.seasonal import seasonal_decompose def daypart_cat ( column ): \"\"\" This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value. Parameters ---------- column Reads the column containing the hour part and converts into respective day part Returns ------- String \"\"\" # calculate hour buckets after adding local timezone if column is None : return \"Missing_NA\" elif ( column >= 4 ) and ( column < 7 ): return \"early_hours\" elif ( column >= 10 ) and ( column < 17 ): return \"work_hours\" elif ( column >= 23 ) or ( column < 4 ): return \"late_hours\" elif (( column >= 7 ) and ( column < 10 )) or (( column >= 17 ) and ( column < 20 )): return \"commuting_hours\" else : return \"other_hours\" f_daypart_cat = F . udf ( daypart_cat , T . StringType ()) def ts_processed_feats ( idf , col , id_col , tz , cnt_row , cnt_unique_id ): \"\"\" This function helps to extract time units from the input dataframe on a processed column being timestamp / date. Parameters ---------- idf Input dataframe col Column belonging to timestamp / date id_col ID column tz Timezone offset cnt_row Count of rows present in the Input dataframe cnt_unique_id Count of unique records present in the Input dataframe Returns ------- DataFrame \"\"\" if cnt_row == cnt_unique_id : odf = ( timeUnits_extraction ( idf , col , \"all\" , output_mode = \"append\" , ) . withColumn ( \"yyyymmdd_col\" , F . to_date ( col )) . orderBy ( \"yyyymmdd_col\" ) . withColumn ( \"daypart_cat\" , f_daypart_cat ( F . col ( col + \"_hour\" ))) . withColumn ( \"week_cat\" , F . when ( F . col ( col + \"_dayofweek\" ) > 5 , F . lit ( \"weekend\" )) . otherwise ( \"weekday\" ), ) . withColumnRenamed ( col + \"_dayofweek\" , \"dow\" ) ) return odf else : odf = ( timeUnits_extraction ( idf , col , \"all\" , output_mode = \"append\" , ) . withColumn ( \"yyyymmdd_col\" , F . to_date ( col )) . orderBy ( id_col , \"yyyymmdd_col\" ) . withColumn ( \"daypart_cat\" , f_daypart_cat ( F . col ( col + \"_hour\" ))) . withColumn ( \"week_cat\" , F . when ( F . col ( col + \"_dayofweek\" ) > 5 , F . lit ( \"weekend\" )) . otherwise ( \"weekday\" ), ) . withColumnRenamed ( col + \"_dayofweek\" , \"dow\" ) ) return odf def ts_eligiblity_check ( spark , idf , id_col , opt = 1 , tz_offset = \"local\" ): \"\"\" This function helps to extract various metrics which can help to understand the nature of timestamp / date column for a given dataset. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column opt Option to choose between [1,2]. 1 is kept as default. Based on the user input, the specific aggregation of data will happen. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". Returns ------- DataFrame \"\"\" lagged_df = lagged_ts ( idf . select ( \"yyyymmdd_col\" ) . distinct () . orderBy ( \"yyyymmdd_col\" ), \"yyyymmdd_col\" , lag = 1 , tsdiff_unit = \"days\" , output_mode = \"append\" , ) . orderBy ( \"yyyymmdd_col\" ) diff_lagged_df = list ( np . around ( lagged_df . withColumn ( \"daydiff\" , F . datediff ( \"yyyymmdd_col\" , \"yyyymmdd_col_lag1\" ) ) . where ( F . col ( \"daydiff\" ) . isNotNull ()) . groupBy () . agg ( F . mean ( \"daydiff\" ) . alias ( \"mean\" ), F . variance ( \"daydiff\" ) . alias ( \"variance\" ), F . stddev ( \"daydiff\" ) . alias ( \"stdev\" ), ) . withColumn ( \"coef_of_var_lag\" , F . col ( \"stdev\" ) / F . col ( \"mean\" )) . rdd . flatMap ( lambda x : x ) . collect (), 3 , ) ) p1 = measures_of_percentiles ( spark , idf . groupBy ( id_col ) . agg ( F . countDistinct ( \"yyyymmdd_col\" ) . alias ( \"id_date_pair\" )), list_of_cols = \"id_date_pair\" , ) p2 = measures_of_percentiles ( spark , idf . groupBy ( \"yyyymmdd_col\" ) . agg ( F . countDistinct ( id_col ) . alias ( \"date_id_pair\" )), list_of_cols = \"date_id_pair\" , ) if opt == 1 : odf = p1 . union ( p2 ) . toPandas () return odf else : odf = idf m = ( odf . groupBy ( \"yyyymmdd_col\" ) . count () . orderBy ( \"count\" , ascending = False ) . collect () ) mode = str ( m [ 0 ][ 0 ]) + \" [\" + str ( m [ 0 ][ 1 ]) + \"]\" missing_vals = odf . where ( F . col ( \"yyyymmdd_col\" ) . isNull ()) . count () odf = ( odf . groupBy () . agg ( F . countDistinct ( \"yyyymmdd_col\" ) . alias ( \"count_unique_dates\" ), F . min ( \"yyyymmdd_col\" ) . alias ( \"min_date\" ), F . max ( \"yyyymmdd_col\" ) . alias ( \"max_date\" ), ) . withColumn ( \"modal_date\" , F . lit ( mode )) . withColumn ( \"date_diff\" , F . datediff ( \"max_date\" , \"min_date\" )) . withColumn ( \"missing_date\" , F . lit ( missing_vals )) . withColumn ( \"mean\" , F . lit ( diff_lagged_df [ 0 ])) . withColumn ( \"variance\" , F . lit ( diff_lagged_df [ 1 ])) . withColumn ( \"stdev\" , F . lit ( diff_lagged_df [ 2 ])) . withColumn ( \"cov\" , F . lit ( diff_lagged_df [ 3 ])) . toPandas () ) return odf def ts_viz_data ( idf , x_col , y_col , id_col , tz_offset = \"local\" , output_mode = \"append\" , output_type = \"daily\" , n_cat = 10 , ): \"\"\" This function helps to produce the processed dataframe with the relevant aggregation at the time frequency chosen for a given column as seen against the timestamp / date column. Parameters ---------- idf Input Dataframe x_col Timestamp / Date column as set in the X-Axis y_col Numerical & Categorical column as set in the Y-Axis id_col ID Column tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. n_cat For categorical columns whose cardinality is beyond N, the Top N categories are chosen, beyond which the categories are grouped as Others. Returns ------- DataFrame \"\"\" y_col_org = y_col y_col = y_col . replace ( \"-\" , \"_\" ) idf = idf . withColumnRenamed ( y_col_org , y_col ) for i in idf . dtypes : if y_col == i [ 0 ]: y_col_dtype = i [ 1 ] if y_col_dtype == \"string\" : top_cat = list ( idf . groupBy ( y_col ) . count () . orderBy ( \"count\" , ascending = False ) . limit ( int ( n_cat )) . select ( y_col ) . toPandas ()[ y_col ] . values ) idf = idf . withColumn ( y_col , F . when ( F . col ( y_col ) . isin ( top_cat ), F . col ( y_col )) . otherwise ( F . lit ( \"Others\" )), ) if output_type == \"daily\" : odf = ( idf . groupBy ( y_col , \"yyyymmdd_col\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"yyyymmdd_col\" ) . withColumnRenamed ( \"yyyymmdd_col\" , x_col ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"hourly\" : odf = ( idf . groupBy ( y_col , \"daypart_cat\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"daypart_cat\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"weekly\" : odf = ( idf . groupBy ( y_col , \"dow\" ) . agg ( F . count ( y_col ) . alias ( \"count\" )) . orderBy ( \"dow\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) return odf else : if output_type == \"daily\" : odf = ( idf . groupBy ( \"yyyymmdd_col\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"yyyymmdd_col\" ) . withColumnRenamed ( \"yyyymmdd_col\" , x_col ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"hourly\" : odf = ( idf . groupBy ( \"daypart_cat\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"daypart_cat\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) elif output_type == \"weekly\" : odf = ( idf . groupBy ( \"dow\" ) . agg ( F . min ( y_col ) . alias ( \"min\" ), F . max ( y_col ) . alias ( \"max\" ), F . mean ( y_col ) . alias ( \"mean\" ), F . expr ( \"percentile(\" + y_col + \", array(0.5))\" )[ 0 ] . alias ( \"median\" ), ) . orderBy ( \"dow\" ) . withColumnRenamed ( y_col , y_col_org ) . toPandas () ) return odf def ts_analyzer ( spark , idf , id_col , max_days , output_path , output_type = \"daily\" , tz_offset = \"local\" , run_type = \"local\" , auth_key = \"NA\" , ): \"\"\" This function helps to produce the processed output in an aggregate form considering the input dataframe with processed timestamp / date column. The aggregation happens across Mean, Median, Min & Max for the Numerical / Categorical column. Parameters ---------- spark Spark session idf Input Dataframe id_col ID Column max_days Max days upto which the data will be aggregated. If we've a dataset containing a timestamp / date field with very high number of unique dates (Let's say beyond 20 years worth of daily data), a maximum days value chosen basis which the latest output is displayed. output_path Output path where the intermediate data is going to be saved output_type Option to choose between \"Daily\" or \"Weekly\" or \"Hourly\". Daily is chosen as default. If \"Daily\" is selected as the output type, the daily view is populated ; If it's \"Hourly\", the view is shown at a Day part level. However, if it's \"Weekly\", then the display it per individual week days (1-7) as captured. tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"local\" or \"emr\" or \"databricks\" or \"ak8s\" basis the user flexibility. Default option is set as \"Local\". auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns ------- Output[CSV] \"\"\" if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) num_cols = [ x for x in num_cols if x not in [ id_col ]] cat_cols = [ x for x in cat_cols if x not in [ id_col ]] ts_loop_cols_post = [ x [ 0 ] for x in idf . dtypes if x [ 1 ] in [ \"timestamp\" , \"date\" ]] cnt_row = idf . count () cnt_unique_id = idf . select ( id_col ) . distinct () . count () for i in ts_loop_cols_post : ts_processed_feat_df = ts_processed_feats ( idf , i , id_col , tz_offset , cnt_row , cnt_unique_id ) ts_processed_feat_df . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) # for j in range(1, 3): # f = ts_eligiblity_check( # spark, # ts_processed_feat_df, # id_col=id_col, # opt=j, # tz_offset=tz_offset, # ) # f.to_csv( # ends_with(local_path) + \"stats_\" + str(i) + \"_\" + str(j) + \".csv\", # index=False, # ) f1 = ts_eligiblity_check ( spark , ts_processed_feat_df , id_col = id_col , opt = 1 , tz_offset = tz_offset ) f1 . to_csv ( ends_with ( local_path ) + \"stats_\" + str ( i ) + \"_\" + str ( 1 ) + \".csv\" , index = False , ) f2 = ts_eligiblity_check ( spark , ts_processed_feat_df , id_col = id_col , opt = 2 , tz_offset = tz_offset ) f2 . to_csv ( ends_with ( local_path ) + \"stats_\" + str ( i ) + \"_\" + str ( 2 ) + \".csv\" , index = False , ) for k in [ num_cols , cat_cols ]: for l in k : for m in [ output_type ]: f = ( ts_viz_data ( ts_processed_feat_df , i , l , id_col = id_col , tz_offset = tz_offset , output_mode = \"append\" , output_type = m , n_cat = 10 , ) . tail ( int ( max_days )) . dropna () ) f . to_csv ( ends_with ( local_path ) + i + \"_\" + l + \"_\" + m + \".csv\" , index = False , ) ts_processed_feat_df . unpersist () if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( output_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( output_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ])","title":"ts_analyzer"},{"location":"api/data_analyzer/ts_analyzer.html#functions","text":"def daypart_cat ( column) This functioin helps to convert the input hour part into the respective day parts. The different dayparts are Early Hours, Work Hours, Late Hours, Commuting Hours, Other Hours based on the hour value.","title":"Functions"},{"location":"api/data_ingest/_index.html","text":"Overview Sub-modules anovos.data_ingest.data_ingest This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic \u2026 anovos.data_ingest.data_sampling anovos.data_ingest.geo_auto_detection This module help to automatically identify the latitude, longitude as well as geohash columns present in the analysis data through some intelligent \u2026 anovos.data_ingest.ts_auto_detection This module help produce the output containing a transformation through auto timestamp / date detection by reading the ingested dataframe from source \u2026","title":"Overview"},{"location":"api/data_ingest/_index.html#overview","text":"","title":"Overview"},{"location":"api/data_ingest/_index.html#sub-modules","text":"anovos.data_ingest.data_ingest This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic \u2026 anovos.data_ingest.data_sampling anovos.data_ingest.geo_auto_detection This module help to automatically identify the latitude, longitude as well as geohash columns present in the analysis data through some intelligent \u2026 anovos.data_ingest.ts_auto_detection This module help produce the output containing a transformation through auto timestamp / date detection by reading the ingested dataframe from source \u2026","title":"Sub-modules"},{"location":"api/data_ingest/data_ingest.html","text":"data_ingest This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic ETL actions such as selecting, deleting, renaming and/or recasting columns. List of functions included in this module are: - read_dataset - write_dataset - concatenate_dataset - join_dataset - delete_column - select_column - rename_column - recast_column Expand source code # coding=utf-8 \"\"\" This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic ETL actions such as selecting, deleting, renaming and/or recasting columns. List of functions included in this module are: - read_dataset - write_dataset - concatenate_dataset - join_dataset - delete_column - select_column - rename_column - recast_column \"\"\" import warnings import pyspark.sql.functions as F from pyspark.sql import DataFrame from pyspark.sql import types as T from anovos.shared.utils import attributeType_segregation , pairwise_reduce def read_dataset ( spark , file_path , file_type , file_configs = {}): \"\"\" This function reads the input data path and return a Spark DataFrame. Under the hood, this function is based on generic Load functionality of Spark SQL. Parameters ---------- spark Spark Session file_path Path to input data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This optional argument is passed in a dictionary format as key/value pairs e.g. {\"header\": \"True\",\"delimiter\": \"|\",\"inferSchema\": \"True\"} for csv files. All the key/value pairs in this argument are passed as options to DataFrameReader, which is created using SparkSession.read. (Default value = {}) Returns ------- DataFrame \"\"\" odf = spark . read . format ( file_type ) . options ( ** file_configs ) . load ( file_path ) return odf def write_dataset ( idf , file_path , file_type , file_configs = {}, column_order = []): \"\"\" This function saves the Spark DataFrame in the user-provided output path. Like read_dataset, this function is based on the generic Save functionality of Spark SQL. Parameters ---------- idf Input Dataframe i.e. Spark DataFrame to be saved file_path Path to output data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This argument is passed in dictionary format as key/value pairs. Some of the potential keys are header, delimiter, mode, compression, repartition. compression options - uncompressed, gzip (doesn't work with avro), snappy (only valid for parquet) mode options - error (default), overwrite, append repartition - None (automatic partitioning) or an integer value () e.g. {\"header\":\"True\", \"delimiter\":\",\",'compression':'snappy','mode':'overwrite','repartition':'10'}. All the key/value pairs (except repartition, mode) written in this argument are passed as options to DataFrameWriter is available using Dataset.write operator. If the number of repartitions mentioned through this argument is less than the existing DataFrame partitions, then the coalesce operation is used instead of the repartition operation to make the execution work. This is because the coalesce operation doesn\u2019t require any shuffling like repartition which is known to be an expensive step. column_order list of columns in the order in which Dataframe is to be written. If None or [] is specified, then the default order is applied. \"\"\" if not column_order : column_order = idf . columns else : if len ( column_order ) != len ( idf . columns ): raise ValueError ( \"Count of column(s) specified in column_order argument do not match Dataframe\" ) diff_cols = [ x for x in column_order if x not in set ( idf . columns )] if diff_cols : raise ValueError ( \"Column(s) specified in column_order argument not found in Dataframe: \" + str ( diff_cols ) ) mode = file_configs [ \"mode\" ] if \"mode\" in file_configs else \"error\" repartition = ( int ( file_configs [ \"repartition\" ]) if \"repartition\" in file_configs else None ) if repartition is None : idf . select ( column_order ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) else : exist_parts = idf . rdd . getNumPartitions () req_parts = int ( repartition ) if req_parts > exist_parts : idf . select ( column_order ) . repartition ( req_parts ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) else : idf . select ( column_order ) . coalesce ( req_parts ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) def concatenate_dataset ( * idfs , method_type = \"name\" ): \"\"\" This function combines multiple dataframes into a single dataframe. A pairwise concatenation is performed on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function leverages union functionality of Spark SQL. Parameters ---------- *idfs All dataframes to be concatenated (with the first dataframe columns) method_type \"index\", \"name\". This argument needs to be passed as a keyword argument. The \u201cindex\u201d method concatenates the dataframes by the column index (without shuffling columns). If the sequence of column is not fixed among the dataframe, this method should be avoided. The \u201cname\u201d method concatenates after shuffling and arranging columns as per the first dataframe order. First dataframe passed under idfs will define the final columns in the concatenated dataframe, and will throw error if any column in first dataframe is not available in any of other dataframes. (Default value = \"name\") Returns ------- DataFrame Concatenated dataframe \"\"\" if method_type not in [ \"index\" , \"name\" ]: raise TypeError ( \"Invalid input for concatenate_dataset method\" ) if method_type == \"name\" : odf = pairwise_reduce ( lambda idf1 , idf2 : idf1 . union ( idf2 . select ( idf1 . columns )), idfs ) # odf = reduce(DataFrame.unionByName, idfs) # only if exact no. of columns else : odf = pairwise_reduce ( DataFrame . union , idfs ) return odf def join_dataset ( * idfs , join_cols , join_type ): \"\"\" This function joins multiple dataframes into a single dataframe by joining key column(s). For optimization, Pairwise joining is done on the dataframes, instead of joining individual dataframes to the bigger dataframe. This function leverages join functionality of Spark SQL. Parameters ---------- idfs All dataframes to be joined join_cols Key column(s) to join all dataframes together. In case of multiple key columns to join, they can be passed in a list format or a string format where different column names are separated by pipe delimiter \u201c|\u201d e.g. \"col1|col2\". join_type \"inner\", \u201cfull\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cleft_semi\u201d, \u201cleft_anti\u201d Returns ------- DataFrame Joined dataframe \"\"\" if isinstance ( join_cols , str ): join_cols = [ x . strip () for x in join_cols . split ( \"|\" )] list_of_df_cols = [ x . columns for x in idfs ] list_of_all_cols = [ x for sublist in list_of_df_cols for x in sublist ] list_of_nonjoin_cols = [ x for x in list_of_all_cols if x not in join_cols ] if len ( list_of_nonjoin_cols ) != ( len ( list_of_all_cols ) - ( len ( list_of_df_cols ) * len ( join_cols )) ): raise ValueError ( \"Specified join_cols do not match all the Input Dataframe(s)\" ) if len ( list_of_nonjoin_cols ) != len ( set ( list_of_nonjoin_cols )): raise ValueError ( \"Duplicate column(s) present in non joining column(s) in Input Dataframe(s)\" ) odf = pairwise_reduce ( lambda idf1 , idf2 : idf1 . join ( idf2 , join_cols , join_type ), idfs ) return odf def delete_column ( idf , list_of_cols , print_impact = False ): \"\"\" This function is used to delete specific columns from the input data. It is executed using drop functionality of Spark SQL. It is advisable to use this function if the number of columns to delete is lesser than the number of columns to select; otherwise, it is recommended to use select_column. Parameters ---------- idf Input Dataframe list_of_cols List of columns to delete e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns ------- DataFrame Dataframe after dropping columns \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] list_of_cols = list ( set ( list_of_cols )) odf = idf . drop ( * list_of_cols ) if print_impact : print ( \"Before: \\n No. of Columns- \" , len ( idf . columns )) print ( idf . columns ) print ( \"After: \\n No. of Columns- \" , len ( odf . columns )) print ( odf . columns ) return odf def select_column ( idf , list_of_cols , print_impact = False ): \"\"\" This function is used to select specific columns from the input data. It is executed using select operation of spark dataframe. It is advisable to use this function if the number of columns to select is lesser than the number of columns to drop; otherwise, it is recommended to use delete_column. Parameters ---------- idf Input Dataframe list_of_cols List of columns to select e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns ------- DataFrame Dataframe with the selected columns \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] list_of_cols = list ( set ( list_of_cols )) odf = idf . select ( list_of_cols ) if print_impact : print ( \"Before: \\n No. of Columns-\" , len ( idf . columns )) print ( idf . columns ) print ( \" \\n After: \\n No. of Columns-\" , len ( odf . columns )) print ( odf . columns ) return odf def rename_column ( idf , list_of_cols , list_of_newcols , print_impact = False ): \"\"\" This function is used to rename the columns of the input data. Multiple columns can be renamed; however, the sequence they passed as an argument is critical and must be consistent between list_of_cols and list_of_newcols. Parameters ---------- idf Input Dataframe list_of_cols List of old column names e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_newcols List of corresponding new column names e.g., [\"newcol1\",\"newcol2\"]. Alternatively, new column names can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"newcol1|newcol2\". First element in list_of_cols will be original column name, and corresponding first column in list_of_newcols will be new column name. print_impact True, False This argument is to compare column names before and after the operation. (Default value = False) Returns ------- DataFrame Dataframe with revised column names \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( list_of_newcols , str ): list_of_newcols = [ x . strip () for x in list_of_newcols . split ( \"|\" )] mapping = dict ( zip ( list_of_cols , list_of_newcols )) odf = idf . select ([ F . col ( i ) . alias ( mapping . get ( i , i )) for i in idf . columns ]) if print_impact : print ( \"Before: \\n No. of Columns- \" , len ( idf . columns )) print ( idf . columns ) print ( \"After: \\n No. of Columns- \" , len ( odf . columns )) print ( odf . columns ) return odf def recast_column ( idf , list_of_cols , list_of_dtypes , print_impact = False ): \"\"\" This function is used to modify the datatype of columns. Multiple columns can be cast; however, the sequence they passed as argument is critical and needs to be consistent between list_of_cols and list_of_dtypes. Parameters ---------- idf Input Dataframe list_of_cols List of columns to cast e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_dtypes List of corresponding datatypes e.g., [\"type1\",\"type2\"]. Alternatively, datatypes can be specified in a string format, where they are separated by pipe delimiter \u201c|\u201d e.g., \"type1|type2\". First element in list_of_cols will column name and corresponding element in list_of_dtypes will be new datatypes such as \"float\", \"integer\", \"long\", \"string\", \"double\", decimal\" etc. Datatypes are case insensitive e.g. float or Float are treated as same. print_impact True, False This argument is to compare schema before and after the operation. (Default value = False) Returns ------- DataFrame Dataframe with revised datatypes \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( list_of_dtypes , str ): list_of_dtypes = [ x . strip () for x in list_of_dtypes . split ( \"|\" )] odf = idf for i , j in zip ( list_of_cols , list_of_dtypes ): odf = odf . withColumn ( i , F . col ( i ) . cast ( j )) if print_impact : print ( \"Before: \" ) idf . printSchema () print ( \"After: \" ) odf . printSchema () return odf def recommend_type ( spark , idf , list_of_cols = \"all\" , drop_cols = [], dynamic_threshold = 0.01 , static_threshold = 100 , ): \"\"\" This function is to recommend the form and datatype of columns. Cardinality of each column will be measured, then both dynamic_threshold and static_threshold will be used to determine the recommended form and datatype for each column. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to cast e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = 'all') drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of strata_cols, when we need to consider all columns except a few handful of them. (Default value = []) dynamic_threshold Cardinality threshold to determine columns recommended form and datatype. If the column's unique values < column total records * dynamic_threshold, column will be recommended as categorical, and in string datatype. Else, column will be recommended as numerical, and in double datatype In recommend_type, we will use the general threshold equals to the minimum of dynamic_threshold and static_threshold. (Default value = 0.01) static_threshold Cardinality threshold to determine columns recommended form and datatype. If the column's unique values < static_threshold, column will be recommended as categorical, and in string datatype. Else, column will be recommended as numerical, and in double datatype In recommend_type, we will use the general threshold equals to the minimum of dynamic_threshold and static_threshold. (Default value = 100) Returns ------- DataFrame Dataframe with attributes and their original/recommended form and datatype \"\"\" if list_of_cols == \"all\" : list_of_cols = idf . columns if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No recommend_attributeType analysis - No column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"original_form\" , T . StringType (), True ), T . StructField ( \"original_dataType\" , T . StringType (), True ), T . StructField ( \"recommended_form\" , T . StringType (), True ), T . StructField ( \"recommended_dataType\" , T . StringType (), True ), T . StructField ( \"distinct_value_count\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf if type ( dynamic_threshold ) != float : raise TypeError ( \"Invalid input for dynamic_threshold: float type only\" ) if dynamic_threshold <= 0 or dynamic_threshold > 1 : raise TypeError ( \"Invalid input for dynamic_threshold: Value need to be between 0 and 1\" ) if type ( static_threshold ) != int : raise TypeError ( \"Invalid input for static_threshold: int type only\" ) def min_val ( val1 , val2 ): if val1 > val2 : return val2 else : return val1 num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) rec_num_cols = [] rec_cat_cols = [] for col in num_cols : if idf . select ( col ) . distinct () . na . drop () . count () < min_val ( ( dynamic_threshold * idf . select ( col ) . na . drop () . count ()), static_threshold ): rec_cat_cols . append ( col ) for col in cat_cols : idf_inter = ( idf . na . drop ( subset = col ) . withColumn ( col , idf [ col ] . cast ( \"double\" )) . select ( col ) ) if ( idf_inter . distinct () . na . drop () . count () == idf_inter . distinct () . count () and idf_inter . distinct () . na . drop () . count () != 0 ): if idf . select ( col ) . distinct () . na . drop () . count () >= min_val ( ( dynamic_threshold * idf . select ( col ) . na . drop () . count ()), static_threshold , ): rec_num_cols . append ( col ) rec_cols = rec_num_cols + rec_cat_cols ori_form = [] ori_type = [] rec_form = [] rec_type = [] num_dist_val = [] if len ( rec_cols ) > 0 : for col in rec_cols : if col in rec_num_cols : ori_form . append ( \"categorical\" ) ori_type . append ( idf . select ( col ) . dtypes [ 0 ][ 1 ]) rec_form . append ( \"numerical\" ) rec_type . append ( \"double\" ) num_dist_val . append ( idf . select ( col ) . distinct () . count ()) else : ori_form . append ( \"numerical\" ) ori_type . append ( idf . select ( col ) . dtypes [ 0 ][ 1 ]) rec_form . append ( \"categorical\" ) rec_type . append ( \"string\" ) num_dist_val . append ( idf . select ( col ) . distinct () . count ()) odf_rec = spark . createDataFrame ( zip ( rec_cols , ori_form , ori_type , rec_form , rec_type , num_dist_val ), schema = ( \"attribute\" , \"original_form\" , \"original_dataType\" , \"recommended_form\" , \"recommended_dataType\" , \"distinct_value_count\" , ), ) return odf_rec else : warnings . warn ( \"No column type change recommendation is made\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"original_form\" , T . StringType (), True ), T . StructField ( \"original_dataType\" , T . StringType (), True ), T . StructField ( \"recommended_form\" , T . StringType (), True ), T . StructField ( \"recommended_dataType\" , T . StringType (), True ), T . StructField ( \"distinct_value_count\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf Functions def concatenate_dataset ( *idfs, method_type='name') This function combines multiple dataframes into a single dataframe. A pairwise concatenation is performed on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function leverages union functionality of Spark SQL. Parameters *idfs All dataframes to be concatenated (with the first dataframe columns) method_type \"index\", \"name\". This argument needs to be passed as a keyword argument. The \u201cindex\u201d method concatenates the dataframes by the column index (without shuffling columns). If the sequence of column is not fixed among the dataframe, this method should be avoided. The \u201cname\u201d method concatenates after shuffling and arranging columns as per the first dataframe order. First dataframe passed under idfs will define the final columns in the concatenated dataframe, and will throw error if any column in first dataframe is not available in any of other dataframes. (Default value = \"name\") Returns DataFrame Concatenated dataframe Expand source code def concatenate_dataset ( * idfs , method_type = \"name\" ): \"\"\" This function combines multiple dataframes into a single dataframe. A pairwise concatenation is performed on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function leverages union functionality of Spark SQL. Parameters ---------- *idfs All dataframes to be concatenated (with the first dataframe columns) method_type \"index\", \"name\". This argument needs to be passed as a keyword argument. The \u201cindex\u201d method concatenates the dataframes by the column index (without shuffling columns). If the sequence of column is not fixed among the dataframe, this method should be avoided. The \u201cname\u201d method concatenates after shuffling and arranging columns as per the first dataframe order. First dataframe passed under idfs will define the final columns in the concatenated dataframe, and will throw error if any column in first dataframe is not available in any of other dataframes. (Default value = \"name\") Returns ------- DataFrame Concatenated dataframe \"\"\" if method_type not in [ \"index\" , \"name\" ]: raise TypeError ( \"Invalid input for concatenate_dataset method\" ) if method_type == \"name\" : odf = pairwise_reduce ( lambda idf1 , idf2 : idf1 . union ( idf2 . select ( idf1 . columns )), idfs ) # odf = reduce(DataFrame.unionByName, idfs) # only if exact no. of columns else : odf = pairwise_reduce ( DataFrame . union , idfs ) return odf def delete_column ( idf, list_of_cols, print_impact=False) This function is used to delete specific columns from the input data. It is executed using drop functionality of Spark SQL. It is advisable to use this function if the number of columns to delete is lesser than the number of columns to select; otherwise, it is recommended to use select_column. Parameters idf Input Dataframe list_of_cols List of columns to delete e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns DataFrame Dataframe after dropping columns Expand source code def delete_column ( idf , list_of_cols , print_impact = False ): \"\"\" This function is used to delete specific columns from the input data. It is executed using drop functionality of Spark SQL. It is advisable to use this function if the number of columns to delete is lesser than the number of columns to select; otherwise, it is recommended to use select_column. Parameters ---------- idf Input Dataframe list_of_cols List of columns to delete e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns ------- DataFrame Dataframe after dropping columns \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] list_of_cols = list ( set ( list_of_cols )) odf = idf . drop ( * list_of_cols ) if print_impact : print ( \"Before: \\n No. of Columns- \" , len ( idf . columns )) print ( idf . columns ) print ( \"After: \\n No. of Columns- \" , len ( odf . columns )) print ( odf . columns ) return odf def join_dataset ( *idfs, join_cols, join_type) This function joins multiple dataframes into a single dataframe by joining key column(s). For optimization, Pairwise joining is done on the dataframes, instead of joining individual dataframes to the bigger dataframe. This function leverages join functionality of Spark SQL. Parameters idfs All dataframes to be joined join_cols Key column(s) to join all dataframes together. In case of multiple key columns to join, they can be passed in a list format or a string format where different column names are separated by pipe delimiter \u201c|\u201d e.g. \"col1|col2\". join_type \"inner\", \u201cfull\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cleft_semi\u201d, \u201cleft_anti\u201d Returns DataFrame Joined dataframe Expand source code def join_dataset ( * idfs , join_cols , join_type ): \"\"\" This function joins multiple dataframes into a single dataframe by joining key column(s). For optimization, Pairwise joining is done on the dataframes, instead of joining individual dataframes to the bigger dataframe. This function leverages join functionality of Spark SQL. Parameters ---------- idfs All dataframes to be joined join_cols Key column(s) to join all dataframes together. In case of multiple key columns to join, they can be passed in a list format or a string format where different column names are separated by pipe delimiter \u201c|\u201d e.g. \"col1|col2\". join_type \"inner\", \u201cfull\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cleft_semi\u201d, \u201cleft_anti\u201d Returns ------- DataFrame Joined dataframe \"\"\" if isinstance ( join_cols , str ): join_cols = [ x . strip () for x in join_cols . split ( \"|\" )] list_of_df_cols = [ x . columns for x in idfs ] list_of_all_cols = [ x for sublist in list_of_df_cols for x in sublist ] list_of_nonjoin_cols = [ x for x in list_of_all_cols if x not in join_cols ] if len ( list_of_nonjoin_cols ) != ( len ( list_of_all_cols ) - ( len ( list_of_df_cols ) * len ( join_cols )) ): raise ValueError ( \"Specified join_cols do not match all the Input Dataframe(s)\" ) if len ( list_of_nonjoin_cols ) != len ( set ( list_of_nonjoin_cols )): raise ValueError ( \"Duplicate column(s) present in non joining column(s) in Input Dataframe(s)\" ) odf = pairwise_reduce ( lambda idf1 , idf2 : idf1 . join ( idf2 , join_cols , join_type ), idfs ) return odf def read_dataset ( spark, file_path, file_type, file_configs={}) This function reads the input data path and return a Spark DataFrame. Under the hood, this function is based on generic Load functionality of Spark SQL. Parameters spark Spark Session file_path Path to input data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (\u2013packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This optional argument is passed in a dictionary format as key/value pairs e.g. {\"header\": \"True\",\"delimiter\": \"|\",\"inferSchema\": \"True\"} for csv files. All the key/value pairs in this argument are passed as options to DataFrameReader, which is created using SparkSession.read. (Default value = {}) Returns DataFrame Expand source code def read_dataset ( spark , file_path , file_type , file_configs = {}): \"\"\" This function reads the input data path and return a Spark DataFrame. Under the hood, this function is based on generic Load functionality of Spark SQL. Parameters ---------- spark Spark Session file_path Path to input data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This optional argument is passed in a dictionary format as key/value pairs e.g. {\"header\": \"True\",\"delimiter\": \"|\",\"inferSchema\": \"True\"} for csv files. All the key/value pairs in this argument are passed as options to DataFrameReader, which is created using SparkSession.read. (Default value = {}) Returns ------- DataFrame \"\"\" odf = spark . read . format ( file_type ) . options ( ** file_configs ) . load ( file_path ) return odf def recast_column ( idf, list_of_cols, list_of_dtypes, print_impact=False) This function is used to modify the datatype of columns. Multiple columns can be cast; however, the sequence they passed as argument is critical and needs to be consistent between list_of_cols and list_of_dtypes. Parameters idf Input Dataframe list_of_cols List of columns to cast e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_dtypes List of corresponding datatypes e.g., [\"type1\",\"type2\"]. Alternatively, datatypes can be specified in a string format, where they are separated by pipe delimiter \u201c|\u201d e.g., \"type1|type2\". First element in list_of_cols will column name and corresponding element in list_of_dtypes will be new datatypes such as \"float\", \"integer\", \"long\", \"string\", \"double\", decimal\" etc. Datatypes are case insensitive e.g. float or Float are treated as same. print_impact True, False This argument is to compare schema before and after the operation. (Default value = False) Returns DataFrame Dataframe with revised datatypes Expand source code def recast_column ( idf , list_of_cols , list_of_dtypes , print_impact = False ): \"\"\" This function is used to modify the datatype of columns. Multiple columns can be cast; however, the sequence they passed as argument is critical and needs to be consistent between list_of_cols and list_of_dtypes. Parameters ---------- idf Input Dataframe list_of_cols List of columns to cast e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_dtypes List of corresponding datatypes e.g., [\"type1\",\"type2\"]. Alternatively, datatypes can be specified in a string format, where they are separated by pipe delimiter \u201c|\u201d e.g., \"type1|type2\". First element in list_of_cols will column name and corresponding element in list_of_dtypes will be new datatypes such as \"float\", \"integer\", \"long\", \"string\", \"double\", decimal\" etc. Datatypes are case insensitive e.g. float or Float are treated as same. print_impact True, False This argument is to compare schema before and after the operation. (Default value = False) Returns ------- DataFrame Dataframe with revised datatypes \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( list_of_dtypes , str ): list_of_dtypes = [ x . strip () for x in list_of_dtypes . split ( \"|\" )] odf = idf for i , j in zip ( list_of_cols , list_of_dtypes ): odf = odf . withColumn ( i , F . col ( i ) . cast ( j )) if print_impact : print ( \"Before: \" ) idf . printSchema () print ( \"After: \" ) odf . printSchema () return odf def recommend_type ( spark, idf, list_of_cols='all', drop_cols=[], dynamic_threshold=0.01, static_threshold=100) This function is to recommend the form and datatype of columns. Cardinality of each column will be measured, then both dynamic_threshold and static_threshold will be used to determine the recommended form and datatype for each column. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to cast e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = 'all') drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of strata_cols, when we need to consider all columns except a few handful of them. (Default value = []) dynamic_threshold Cardinality threshold to determine columns recommended form and datatype. If the column's unique values < column total records * dynamic_threshold, column will be recommended as categorical, and in string datatype. Else, column will be recommended as numerical, and in double datatype In recommend_type, we will use the general threshold equals to the minimum of dynamic_threshold and static_threshold. (Default value = 0.01) static_threshold Cardinality threshold to determine columns recommended form and datatype. If the column's unique values < static_threshold, column will be recommended as categorical, and in string datatype. Else, column will be recommended as numerical, and in double datatype In recommend_type, we will use the general threshold equals to the minimum of dynamic_threshold and static_threshold. (Default value = 100) Returns DataFrame Dataframe with attributes and their original/recommended form and datatype Expand source code def recommend_type ( spark , idf , list_of_cols = \"all\" , drop_cols = [], dynamic_threshold = 0.01 , static_threshold = 100 , ): \"\"\" This function is to recommend the form and datatype of columns. Cardinality of each column will be measured, then both dynamic_threshold and static_threshold will be used to determine the recommended form and datatype for each column. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to cast e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = 'all') drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of strata_cols, when we need to consider all columns except a few handful of them. (Default value = []) dynamic_threshold Cardinality threshold to determine columns recommended form and datatype. If the column's unique values < column total records * dynamic_threshold, column will be recommended as categorical, and in string datatype. Else, column will be recommended as numerical, and in double datatype In recommend_type, we will use the general threshold equals to the minimum of dynamic_threshold and static_threshold. (Default value = 0.01) static_threshold Cardinality threshold to determine columns recommended form and datatype. If the column's unique values < static_threshold, column will be recommended as categorical, and in string datatype. Else, column will be recommended as numerical, and in double datatype In recommend_type, we will use the general threshold equals to the minimum of dynamic_threshold and static_threshold. (Default value = 100) Returns ------- DataFrame Dataframe with attributes and their original/recommended form and datatype \"\"\" if list_of_cols == \"all\" : list_of_cols = idf . columns if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No recommend_attributeType analysis - No column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"original_form\" , T . StringType (), True ), T . StructField ( \"original_dataType\" , T . StringType (), True ), T . StructField ( \"recommended_form\" , T . StringType (), True ), T . StructField ( \"recommended_dataType\" , T . StringType (), True ), T . StructField ( \"distinct_value_count\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf if type ( dynamic_threshold ) != float : raise TypeError ( \"Invalid input for dynamic_threshold: float type only\" ) if dynamic_threshold <= 0 or dynamic_threshold > 1 : raise TypeError ( \"Invalid input for dynamic_threshold: Value need to be between 0 and 1\" ) if type ( static_threshold ) != int : raise TypeError ( \"Invalid input for static_threshold: int type only\" ) def min_val ( val1 , val2 ): if val1 > val2 : return val2 else : return val1 num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) rec_num_cols = [] rec_cat_cols = [] for col in num_cols : if idf . select ( col ) . distinct () . na . drop () . count () < min_val ( ( dynamic_threshold * idf . select ( col ) . na . drop () . count ()), static_threshold ): rec_cat_cols . append ( col ) for col in cat_cols : idf_inter = ( idf . na . drop ( subset = col ) . withColumn ( col , idf [ col ] . cast ( \"double\" )) . select ( col ) ) if ( idf_inter . distinct () . na . drop () . count () == idf_inter . distinct () . count () and idf_inter . distinct () . na . drop () . count () != 0 ): if idf . select ( col ) . distinct () . na . drop () . count () >= min_val ( ( dynamic_threshold * idf . select ( col ) . na . drop () . count ()), static_threshold , ): rec_num_cols . append ( col ) rec_cols = rec_num_cols + rec_cat_cols ori_form = [] ori_type = [] rec_form = [] rec_type = [] num_dist_val = [] if len ( rec_cols ) > 0 : for col in rec_cols : if col in rec_num_cols : ori_form . append ( \"categorical\" ) ori_type . append ( idf . select ( col ) . dtypes [ 0 ][ 1 ]) rec_form . append ( \"numerical\" ) rec_type . append ( \"double\" ) num_dist_val . append ( idf . select ( col ) . distinct () . count ()) else : ori_form . append ( \"numerical\" ) ori_type . append ( idf . select ( col ) . dtypes [ 0 ][ 1 ]) rec_form . append ( \"categorical\" ) rec_type . append ( \"string\" ) num_dist_val . append ( idf . select ( col ) . distinct () . count ()) odf_rec = spark . createDataFrame ( zip ( rec_cols , ori_form , ori_type , rec_form , rec_type , num_dist_val ), schema = ( \"attribute\" , \"original_form\" , \"original_dataType\" , \"recommended_form\" , \"recommended_dataType\" , \"distinct_value_count\" , ), ) return odf_rec else : warnings . warn ( \"No column type change recommendation is made\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"original_form\" , T . StringType (), True ), T . StructField ( \"original_dataType\" , T . StringType (), True ), T . StructField ( \"recommended_form\" , T . StringType (), True ), T . StructField ( \"recommended_dataType\" , T . StringType (), True ), T . StructField ( \"distinct_value_count\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf def rename_column ( idf, list_of_cols, list_of_newcols, print_impact=False) This function is used to rename the columns of the input data. Multiple columns can be renamed; however, the sequence they passed as an argument is critical and must be consistent between list_of_cols and list_of_newcols. Parameters idf Input Dataframe list_of_cols List of old column names e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_newcols List of corresponding new column names e.g., [\"newcol1\",\"newcol2\"]. Alternatively, new column names can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"newcol1|newcol2\". First element in list_of_cols will be original column name, and corresponding first column in list_of_newcols will be new column name. print_impact True, False This argument is to compare column names before and after the operation. (Default value = False) Returns DataFrame Dataframe with revised column names Expand source code def rename_column ( idf , list_of_cols , list_of_newcols , print_impact = False ): \"\"\" This function is used to rename the columns of the input data. Multiple columns can be renamed; however, the sequence they passed as an argument is critical and must be consistent between list_of_cols and list_of_newcols. Parameters ---------- idf Input Dataframe list_of_cols List of old column names e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_newcols List of corresponding new column names e.g., [\"newcol1\",\"newcol2\"]. Alternatively, new column names can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"newcol1|newcol2\". First element in list_of_cols will be original column name, and corresponding first column in list_of_newcols will be new column name. print_impact True, False This argument is to compare column names before and after the operation. (Default value = False) Returns ------- DataFrame Dataframe with revised column names \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( list_of_newcols , str ): list_of_newcols = [ x . strip () for x in list_of_newcols . split ( \"|\" )] mapping = dict ( zip ( list_of_cols , list_of_newcols )) odf = idf . select ([ F . col ( i ) . alias ( mapping . get ( i , i )) for i in idf . columns ]) if print_impact : print ( \"Before: \\n No. of Columns- \" , len ( idf . columns )) print ( idf . columns ) print ( \"After: \\n No. of Columns- \" , len ( odf . columns )) print ( odf . columns ) return odf def select_column ( idf, list_of_cols, print_impact=False) This function is used to select specific columns from the input data. It is executed using select operation of spark dataframe. It is advisable to use this function if the number of columns to select is lesser than the number of columns to drop; otherwise, it is recommended to use delete_column. Parameters idf Input Dataframe list_of_cols List of columns to select e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns DataFrame Dataframe with the selected columns Expand source code def select_column ( idf , list_of_cols , print_impact = False ): \"\"\" This function is used to select specific columns from the input data. It is executed using select operation of spark dataframe. It is advisable to use this function if the number of columns to select is lesser than the number of columns to drop; otherwise, it is recommended to use delete_column. Parameters ---------- idf Input Dataframe list_of_cols List of columns to select e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns ------- DataFrame Dataframe with the selected columns \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] list_of_cols = list ( set ( list_of_cols )) odf = idf . select ( list_of_cols ) if print_impact : print ( \"Before: \\n No. of Columns-\" , len ( idf . columns )) print ( idf . columns ) print ( \" \\n After: \\n No. of Columns-\" , len ( odf . columns )) print ( odf . columns ) return odf def write_dataset ( idf, file_path, file_type, file_configs={}, column_order=[]) This function saves the Spark DataFrame in the user-provided output path. Like read_dataset, this function is based on the generic Save functionality of Spark SQL. Parameters idf Input Dataframe i.e. Spark DataFrame to be saved file_path Path to output data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (\u2013packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This argument is passed in dictionary format as key/value pairs. Some of the potential keys are header, delimiter, mode, compression, repartition. compression options - uncompressed, gzip (doesn't work with avro), snappy (only valid for parquet) mode options - error (default), overwrite, append repartition - None (automatic partitioning) or an integer value () e.g. {\"header\":\"True\", \"delimiter\":\",\",'compression':'snappy','mode':'overwrite','repartition':'10'}. All the key/value pairs (except repartition, mode) written in this argument are passed as options to DataFrameWriter is available using Dataset.write operator. If the number of repartitions mentioned through this argument is less than the existing DataFrame partitions, then the coalesce operation is used instead of the repartition operation to make the execution work. This is because the coalesce operation doesn\u2019t require any shuffling like repartition which is known to be an expensive step. column_order list of columns in the order in which Dataframe is to be written. If None or [] is specified, then the default order is applied. Expand source code def write_dataset ( idf , file_path , file_type , file_configs = {}, column_order = []): \"\"\" This function saves the Spark DataFrame in the user-provided output path. Like read_dataset, this function is based on the generic Save functionality of Spark SQL. Parameters ---------- idf Input Dataframe i.e. Spark DataFrame to be saved file_path Path to output data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This argument is passed in dictionary format as key/value pairs. Some of the potential keys are header, delimiter, mode, compression, repartition. compression options - uncompressed, gzip (doesn't work with avro), snappy (only valid for parquet) mode options - error (default), overwrite, append repartition - None (automatic partitioning) or an integer value () e.g. {\"header\":\"True\", \"delimiter\":\",\",'compression':'snappy','mode':'overwrite','repartition':'10'}. All the key/value pairs (except repartition, mode) written in this argument are passed as options to DataFrameWriter is available using Dataset.write operator. If the number of repartitions mentioned through this argument is less than the existing DataFrame partitions, then the coalesce operation is used instead of the repartition operation to make the execution work. This is because the coalesce operation doesn\u2019t require any shuffling like repartition which is known to be an expensive step. column_order list of columns in the order in which Dataframe is to be written. If None or [] is specified, then the default order is applied. \"\"\" if not column_order : column_order = idf . columns else : if len ( column_order ) != len ( idf . columns ): raise ValueError ( \"Count of column(s) specified in column_order argument do not match Dataframe\" ) diff_cols = [ x for x in column_order if x not in set ( idf . columns )] if diff_cols : raise ValueError ( \"Column(s) specified in column_order argument not found in Dataframe: \" + str ( diff_cols ) ) mode = file_configs [ \"mode\" ] if \"mode\" in file_configs else \"error\" repartition = ( int ( file_configs [ \"repartition\" ]) if \"repartition\" in file_configs else None ) if repartition is None : idf . select ( column_order ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) else : exist_parts = idf . rdd . getNumPartitions () req_parts = int ( repartition ) if req_parts > exist_parts : idf . select ( column_order ) . repartition ( req_parts ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) else : idf . select ( column_order ) . coalesce ( req_parts ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode )","title":"<code>data_ingest</code>"},{"location":"api/data_ingest/data_ingest.html#data_ingest","text":"This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic ETL actions such as selecting, deleting, renaming and/or recasting columns. List of functions included in this module are: - read_dataset - write_dataset - concatenate_dataset - join_dataset - delete_column - select_column - rename_column - recast_column Expand source code # coding=utf-8 \"\"\" This module consists of functions to read the dataset as Spark DataFrame, concatenate/join with other functions (if required), and perform some basic ETL actions such as selecting, deleting, renaming and/or recasting columns. List of functions included in this module are: - read_dataset - write_dataset - concatenate_dataset - join_dataset - delete_column - select_column - rename_column - recast_column \"\"\" import warnings import pyspark.sql.functions as F from pyspark.sql import DataFrame from pyspark.sql import types as T from anovos.shared.utils import attributeType_segregation , pairwise_reduce def read_dataset ( spark , file_path , file_type , file_configs = {}): \"\"\" This function reads the input data path and return a Spark DataFrame. Under the hood, this function is based on generic Load functionality of Spark SQL. Parameters ---------- spark Spark Session file_path Path to input data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This optional argument is passed in a dictionary format as key/value pairs e.g. {\"header\": \"True\",\"delimiter\": \"|\",\"inferSchema\": \"True\"} for csv files. All the key/value pairs in this argument are passed as options to DataFrameReader, which is created using SparkSession.read. (Default value = {}) Returns ------- DataFrame \"\"\" odf = spark . read . format ( file_type ) . options ( ** file_configs ) . load ( file_path ) return odf def write_dataset ( idf , file_path , file_type , file_configs = {}, column_order = []): \"\"\" This function saves the Spark DataFrame in the user-provided output path. Like read_dataset, this function is based on the generic Save functionality of Spark SQL. Parameters ---------- idf Input Dataframe i.e. Spark DataFrame to be saved file_path Path to output data (directory or filename). Compatible with local path and s3 path (when running in AWS environment). file_type \"csv\", \"parquet\", \"avro\", \"json\". Avro data source requires an external package to run, which can be configured with spark-submit (--packages org.apache.spark:spark-avro_2.11:2.4.0). file_configs This argument is passed in dictionary format as key/value pairs. Some of the potential keys are header, delimiter, mode, compression, repartition. compression options - uncompressed, gzip (doesn't work with avro), snappy (only valid for parquet) mode options - error (default), overwrite, append repartition - None (automatic partitioning) or an integer value () e.g. {\"header\":\"True\", \"delimiter\":\",\",'compression':'snappy','mode':'overwrite','repartition':'10'}. All the key/value pairs (except repartition, mode) written in this argument are passed as options to DataFrameWriter is available using Dataset.write operator. If the number of repartitions mentioned through this argument is less than the existing DataFrame partitions, then the coalesce operation is used instead of the repartition operation to make the execution work. This is because the coalesce operation doesn\u2019t require any shuffling like repartition which is known to be an expensive step. column_order list of columns in the order in which Dataframe is to be written. If None or [] is specified, then the default order is applied. \"\"\" if not column_order : column_order = idf . columns else : if len ( column_order ) != len ( idf . columns ): raise ValueError ( \"Count of column(s) specified in column_order argument do not match Dataframe\" ) diff_cols = [ x for x in column_order if x not in set ( idf . columns )] if diff_cols : raise ValueError ( \"Column(s) specified in column_order argument not found in Dataframe: \" + str ( diff_cols ) ) mode = file_configs [ \"mode\" ] if \"mode\" in file_configs else \"error\" repartition = ( int ( file_configs [ \"repartition\" ]) if \"repartition\" in file_configs else None ) if repartition is None : idf . select ( column_order ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) else : exist_parts = idf . rdd . getNumPartitions () req_parts = int ( repartition ) if req_parts > exist_parts : idf . select ( column_order ) . repartition ( req_parts ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) else : idf . select ( column_order ) . coalesce ( req_parts ) . write . format ( file_type ) . options ( ** file_configs ) . save ( file_path , mode = mode ) def concatenate_dataset ( * idfs , method_type = \"name\" ): \"\"\" This function combines multiple dataframes into a single dataframe. A pairwise concatenation is performed on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function leverages union functionality of Spark SQL. Parameters ---------- *idfs All dataframes to be concatenated (with the first dataframe columns) method_type \"index\", \"name\". This argument needs to be passed as a keyword argument. The \u201cindex\u201d method concatenates the dataframes by the column index (without shuffling columns). If the sequence of column is not fixed among the dataframe, this method should be avoided. The \u201cname\u201d method concatenates after shuffling and arranging columns as per the first dataframe order. First dataframe passed under idfs will define the final columns in the concatenated dataframe, and will throw error if any column in first dataframe is not available in any of other dataframes. (Default value = \"name\") Returns ------- DataFrame Concatenated dataframe \"\"\" if method_type not in [ \"index\" , \"name\" ]: raise TypeError ( \"Invalid input for concatenate_dataset method\" ) if method_type == \"name\" : odf = pairwise_reduce ( lambda idf1 , idf2 : idf1 . union ( idf2 . select ( idf1 . columns )), idfs ) # odf = reduce(DataFrame.unionByName, idfs) # only if exact no. of columns else : odf = pairwise_reduce ( DataFrame . union , idfs ) return odf def join_dataset ( * idfs , join_cols , join_type ): \"\"\" This function joins multiple dataframes into a single dataframe by joining key column(s). For optimization, Pairwise joining is done on the dataframes, instead of joining individual dataframes to the bigger dataframe. This function leverages join functionality of Spark SQL. Parameters ---------- idfs All dataframes to be joined join_cols Key column(s) to join all dataframes together. In case of multiple key columns to join, they can be passed in a list format or a string format where different column names are separated by pipe delimiter \u201c|\u201d e.g. \"col1|col2\". join_type \"inner\", \u201cfull\u201d, \u201cleft\u201d, \u201cright\u201d, \u201cleft_semi\u201d, \u201cleft_anti\u201d Returns ------- DataFrame Joined dataframe \"\"\" if isinstance ( join_cols , str ): join_cols = [ x . strip () for x in join_cols . split ( \"|\" )] list_of_df_cols = [ x . columns for x in idfs ] list_of_all_cols = [ x for sublist in list_of_df_cols for x in sublist ] list_of_nonjoin_cols = [ x for x in list_of_all_cols if x not in join_cols ] if len ( list_of_nonjoin_cols ) != ( len ( list_of_all_cols ) - ( len ( list_of_df_cols ) * len ( join_cols )) ): raise ValueError ( \"Specified join_cols do not match all the Input Dataframe(s)\" ) if len ( list_of_nonjoin_cols ) != len ( set ( list_of_nonjoin_cols )): raise ValueError ( \"Duplicate column(s) present in non joining column(s) in Input Dataframe(s)\" ) odf = pairwise_reduce ( lambda idf1 , idf2 : idf1 . join ( idf2 , join_cols , join_type ), idfs ) return odf def delete_column ( idf , list_of_cols , print_impact = False ): \"\"\" This function is used to delete specific columns from the input data. It is executed using drop functionality of Spark SQL. It is advisable to use this function if the number of columns to delete is lesser than the number of columns to select; otherwise, it is recommended to use select_column. Parameters ---------- idf Input Dataframe list_of_cols List of columns to delete e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns ------- DataFrame Dataframe after dropping columns \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] list_of_cols = list ( set ( list_of_cols )) odf = idf . drop ( * list_of_cols ) if print_impact : print ( \"Before: \\n No. of Columns- \" , len ( idf . columns )) print ( idf . columns ) print ( \"After: \\n No. of Columns- \" , len ( odf . columns )) print ( odf . columns ) return odf def select_column ( idf , list_of_cols , print_impact = False ): \"\"\" This function is used to select specific columns from the input data. It is executed using select operation of spark dataframe. It is advisable to use this function if the number of columns to select is lesser than the number of columns to drop; otherwise, it is recommended to use delete_column. Parameters ---------- idf Input Dataframe list_of_cols List of columns to select e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". print_impact True, False This argument is to compare number of columns before and after the operation.(Default value = False) Returns ------- DataFrame Dataframe with the selected columns \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] list_of_cols = list ( set ( list_of_cols )) odf = idf . select ( list_of_cols ) if print_impact : print ( \"Before: \\n No. of Columns-\" , len ( idf . columns )) print ( idf . columns ) print ( \" \\n After: \\n No. of Columns-\" , len ( odf . columns )) print ( odf . columns ) return odf def rename_column ( idf , list_of_cols , list_of_newcols , print_impact = False ): \"\"\" This function is used to rename the columns of the input data. Multiple columns can be renamed; however, the sequence they passed as an argument is critical and must be consistent between list_of_cols and list_of_newcols. Parameters ---------- idf Input Dataframe list_of_cols List of old column names e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_newcols List of corresponding new column names e.g., [\"newcol1\",\"newcol2\"]. Alternatively, new column names can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"newcol1|newcol2\". First element in list_of_cols will be original column name, and corresponding first column in list_of_newcols will be new column name. print_impact True, False This argument is to compare column names before and after the operation. (Default value = False) Returns ------- DataFrame Dataframe with revised column names \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( list_of_newcols , str ): list_of_newcols = [ x . strip () for x in list_of_newcols . split ( \"|\" )] mapping = dict ( zip ( list_of_cols , list_of_newcols )) odf = idf . select ([ F . col ( i ) . alias ( mapping . get ( i , i )) for i in idf . columns ]) if print_impact : print ( \"Before: \\n No. of Columns- \" , len ( idf . columns )) print ( idf . columns ) print ( \"After: \\n No. of Columns- \" , len ( odf . columns )) print ( odf . columns ) return odf def recast_column ( idf , list_of_cols , list_of_dtypes , print_impact = False ): \"\"\" This function is used to modify the datatype of columns. Multiple columns can be cast; however, the sequence they passed as argument is critical and needs to be consistent between list_of_cols and list_of_dtypes. Parameters ---------- idf Input Dataframe list_of_cols List of columns to cast e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_dtypes List of corresponding datatypes e.g., [\"type1\",\"type2\"]. Alternatively, datatypes can be specified in a string format, where they are separated by pipe delimiter \u201c|\u201d e.g., \"type1|type2\". First element in list_of_cols will column name and corresponding element in list_of_dtypes will be new datatypes such as \"float\", \"integer\", \"long\", \"string\", \"double\", decimal\" etc. Datatypes are case insensitive e.g. float or Float are treated as same. print_impact True, False This argument is to compare schema before and after the operation. (Default value = False) Returns ------- DataFrame Dataframe with revised datatypes \"\"\" if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( list_of_dtypes , str ): list_of_dtypes = [ x . strip () for x in list_of_dtypes . split ( \"|\" )] odf = idf for i , j in zip ( list_of_cols , list_of_dtypes ): odf = odf . withColumn ( i , F . col ( i ) . cast ( j )) if print_impact : print ( \"Before: \" ) idf . printSchema () print ( \"After: \" ) odf . printSchema () return odf def recommend_type ( spark , idf , list_of_cols = \"all\" , drop_cols = [], dynamic_threshold = 0.01 , static_threshold = 100 , ): \"\"\" This function is to recommend the form and datatype of columns. Cardinality of each column will be measured, then both dynamic_threshold and static_threshold will be used to determine the recommended form and datatype for each column. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to cast e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = 'all') drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of strata_cols, when we need to consider all columns except a few handful of them. (Default value = []) dynamic_threshold Cardinality threshold to determine columns recommended form and datatype. If the column's unique values < column total records * dynamic_threshold, column will be recommended as categorical, and in string datatype. Else, column will be recommended as numerical, and in double datatype In recommend_type, we will use the general threshold equals to the minimum of dynamic_threshold and static_threshold. (Default value = 0.01) static_threshold Cardinality threshold to determine columns recommended form and datatype. If the column's unique values < static_threshold, column will be recommended as categorical, and in string datatype. Else, column will be recommended as numerical, and in double datatype In recommend_type, we will use the general threshold equals to the minimum of dynamic_threshold and static_threshold. (Default value = 100) Returns ------- DataFrame Dataframe with attributes and their original/recommended form and datatype \"\"\" if list_of_cols == \"all\" : list_of_cols = idf . columns if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No recommend_attributeType analysis - No column(s) to analyze\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"original_form\" , T . StringType (), True ), T . StructField ( \"original_dataType\" , T . StringType (), True ), T . StructField ( \"recommended_form\" , T . StringType (), True ), T . StructField ( \"recommended_dataType\" , T . StringType (), True ), T . StructField ( \"distinct_value_count\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf if type ( dynamic_threshold ) != float : raise TypeError ( \"Invalid input for dynamic_threshold: float type only\" ) if dynamic_threshold <= 0 or dynamic_threshold > 1 : raise TypeError ( \"Invalid input for dynamic_threshold: Value need to be between 0 and 1\" ) if type ( static_threshold ) != int : raise TypeError ( \"Invalid input for static_threshold: int type only\" ) def min_val ( val1 , val2 ): if val1 > val2 : return val2 else : return val1 num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) rec_num_cols = [] rec_cat_cols = [] for col in num_cols : if idf . select ( col ) . distinct () . na . drop () . count () < min_val ( ( dynamic_threshold * idf . select ( col ) . na . drop () . count ()), static_threshold ): rec_cat_cols . append ( col ) for col in cat_cols : idf_inter = ( idf . na . drop ( subset = col ) . withColumn ( col , idf [ col ] . cast ( \"double\" )) . select ( col ) ) if ( idf_inter . distinct () . na . drop () . count () == idf_inter . distinct () . count () and idf_inter . distinct () . na . drop () . count () != 0 ): if idf . select ( col ) . distinct () . na . drop () . count () >= min_val ( ( dynamic_threshold * idf . select ( col ) . na . drop () . count ()), static_threshold , ): rec_num_cols . append ( col ) rec_cols = rec_num_cols + rec_cat_cols ori_form = [] ori_type = [] rec_form = [] rec_type = [] num_dist_val = [] if len ( rec_cols ) > 0 : for col in rec_cols : if col in rec_num_cols : ori_form . append ( \"categorical\" ) ori_type . append ( idf . select ( col ) . dtypes [ 0 ][ 1 ]) rec_form . append ( \"numerical\" ) rec_type . append ( \"double\" ) num_dist_val . append ( idf . select ( col ) . distinct () . count ()) else : ori_form . append ( \"numerical\" ) ori_type . append ( idf . select ( col ) . dtypes [ 0 ][ 1 ]) rec_form . append ( \"categorical\" ) rec_type . append ( \"string\" ) num_dist_val . append ( idf . select ( col ) . distinct () . count ()) odf_rec = spark . createDataFrame ( zip ( rec_cols , ori_form , ori_type , rec_form , rec_type , num_dist_val ), schema = ( \"attribute\" , \"original_form\" , \"original_dataType\" , \"recommended_form\" , \"recommended_dataType\" , \"distinct_value_count\" , ), ) return odf_rec else : warnings . warn ( \"No column type change recommendation is made\" ) schema = T . StructType ( [ T . StructField ( \"attribute\" , T . StringType (), True ), T . StructField ( \"original_form\" , T . StringType (), True ), T . StructField ( \"original_dataType\" , T . StringType (), True ), T . StructField ( \"recommended_form\" , T . StringType (), True ), T . StructField ( \"recommended_dataType\" , T . StringType (), True ), T . StructField ( \"distinct_value_count\" , T . StringType (), True ), ] ) odf = spark . sparkContext . emptyRDD () . toDF ( schema ) return odf","title":"data_ingest"},{"location":"api/data_ingest/data_ingest.html#functions","text":"def concatenate_dataset ( *idfs, method_type='name') This function combines multiple dataframes into a single dataframe. A pairwise concatenation is performed on the dataframes, instead of adding one dataframe at a time to the bigger dataframe. This function leverages union functionality of Spark SQL.","title":"Functions"},{"location":"api/data_ingest/data_sampling.html","text":"data_sampling Expand source code import warnings from pyspark.sql import functions as F from anovos.shared.utils import attributeType_segregation def data_sample ( idf , strata_cols = \"all\" , drop_cols = [], fraction = 0.1 , method_type = \"random\" , stratified_type = \"population\" , seed_value = 12 , unique_threshold = 0.5 , ): \"\"\" This is a method focus on under-sampling necessary data through multiple methods. It covers two popular sampling techniques - stratified sampling and random sampling In stratified sampling, we sample out data based on the presence of strata, determined by strata_cols. Inside stratified sampling, there are 2 sub-methods, called \"population\" and \"balanced\", determined by stratified_type. \"Population\" stratified sampling method is Proportionate Allocation sampling strategy, uses a sampling fraction in each of the strata that is proportional to that of the original dataframe. On the other hand, \"Balanced\" stratified sampling method is Optimum Allocation sampling strategy, meaning the sampling fraction of each stratum is not proportional to their occurrence in the original dataframe. Instead, the strata will have an equal number of all stratum available. In random sampling, we sample out data randomly, purely depends on the fraction, and seed_value that is being inputted Parameters ---------- idf Input Dataframe strata_cols List of columns to be treated as strata e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of strata_cols, when we need to consider all columns except a few handful of them. (Default value = []) fraction : float Fraction of the data to be sampled out. (Default value = 0.1) method_type : str \"stratified\" for Stratified sampling, \"random\" for Random Sampling. (Default value = \"random\") stratified_type : str \"population\" for Proportionate Stratified Sampling, \"balanced\" for Optimum Stratified Sampling seed_value : int Seed value for sampling function. (Default value = 12) unique_threshold : float or int Defines threshold to skip columns with higher cardinality values from encoding. If unique_threshold < 1, meaning that if any column has unique records > unique_threshold * total records, it will be considered as high cardinality column, thus not fit to be in strata_cols If unique_threshold > 1, meaning that if any column has unique records > unique_threshold, it will be considered as high cardinality column, thus not fit to be in strata_cols. (Default value = 0.5) Returns ------- DataFrame Sampled Dataframe \"\"\" if type ( fraction ) != float and type ( fraction ) != int : raise TypeError ( \"Invalid input for fraction\" ) if fraction <= 0 or fraction > 1 : raise TypeError ( \"Invalid input for fraction: fraction value is between 0 and 1\" ) if type ( seed_value ) != int : raise TypeError ( \"Invalid input for seed_value\" ) if method_type not in [ \"stratified\" , \"random\" ]: raise TypeError ( \"Invalid input for data_sample method_type\" ) if method_type == \"stratified\" : if type ( unique_threshold ) != float and type ( unique_threshold ) != int : raise TypeError ( \"Invalid input for unique_threshold\" ) if unique_threshold > 1 and type ( unique_threshold ) != int : raise TypeError ( \"Invalid input for unique_threshold: unique_threshold can only be integer if larger than 1\" ) if unique_threshold <= 0 : raise TypeError ( \"Invalid input for unique_threshold: unique_threshold value is either between 0 and 1, or an integer > 1\" ) if stratified_type not in [ \"population\" , \"balanced\" ]: raise TypeError ( \"Invalid input for stratified_type\" ) if strata_cols == \"all\" : strata_cols = idf . columns if isinstance ( strata_cols , str ): strata_cols = [ x . strip () for x in strata_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] strata_cols = list ( set ([ e for e in strata_cols if e not in drop_cols ])) if len ( strata_cols ) == 0 : raise TypeError ( \"Missing strata_cols value\" ) skip_cols = [] for col in strata_cols : if col not in idf . columns : raise TypeError ( \"Invalid input for strata_cols: \" + col + \" does not exist\" ) if method_type == \"stratified\" : if unique_threshold <= 1 : if float ( idf . select ( col ) . distinct () . count () ) > unique_threshold * float ( idf . select ( col ) . count ()): skip_cols . append ( col ) else : if float ( idf . select ( col ) . distinct () . count ()) > unique_threshold : skip_cols . append ( col ) if skip_cols : warnings . warn ( \"Columns dropped from strata due to high cardinality: \" + \",\" . join ( skip_cols ) ) strata_cols = list ( set ([ e for e in strata_cols if e not in skip_cols ])) if len ( strata_cols ) == 0 : warnings . warn ( \"No Stratified Sampling Computation - No strata column(s) to sample\" ) return idf if method_type == \"stratified\" : sample_df = idf . na . drop ( subset = strata_cols ) . withColumn ( \"merge\" , F . concat ( * strata_cols ) ) fractions = ( sample_df . select ( \"merge\" ) . distinct () . withColumn ( \"fraction\" , F . lit ( fraction )) . rdd . collectAsMap () ) if stratified_type == \"population\" : odf = sample_df . stat . sampleBy ( \"merge\" , fractions , seed_value ) . drop ( \"merge\" ) else : count_dict = ( sample_df . groupby ( \"merge\" ) . count () . orderBy ( \"count\" ) . rdd . collectAsMap () ) smallest_count = int ( count_dict [ list ( count_dict . keys ())[ 0 ]]) for key in fractions . keys (): fractions [ key ] = float ( fraction * smallest_count / int ( count_dict [ key ])) odf = sample_df . stat . sampleBy ( \"merge\" , fractions , seed_value ) . drop ( \"merge\" ) else : odf = idf . sample ( withReplacement = False , fraction = fraction , seed = seed_value ) return odf Functions def data_sample ( idf, strata_cols='all', drop_cols=[], fraction=0.1, method_type='random', stratified_type='population', seed_value=12, unique_threshold=0.5) This is a method focus on under-sampling necessary data through multiple methods. It covers two popular sampling techniques - stratified sampling and random sampling In stratified sampling, we sample out data based on the presence of strata, determined by strata_cols. Inside stratified sampling, there are 2 sub-methods, called \"population\" and \"balanced\", determined by stratified_type. \"Population\" stratified sampling method is Proportionate Allocation sampling strategy, uses a sampling fraction in each of the strata that is proportional to that of the original dataframe. On the other hand, \"Balanced\" stratified sampling method is Optimum Allocation sampling strategy, meaning the sampling fraction of each stratum is not proportional to their occurrence in the original dataframe. Instead, the strata will have an equal number of all stratum available. In random sampling, we sample out data randomly, purely depends on the fraction, and seed_value that is being inputted Parameters idf Input Dataframe strata_cols List of columns to be treated as strata e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of strata_cols, when we need to consider all columns except a few handful of them. (Default value = []) fraction :\u2002 float Fraction of the data to be sampled out. (Default value = 0.1) method_type :\u2002 str \"stratified\" for Stratified sampling, \"random\" for Random Sampling. (Default value = \"random\") stratified_type :\u2002 str \"population\" for Proportionate Stratified Sampling, \"balanced\" for Optimum Stratified Sampling seed_value :\u2002 int Seed value for sampling function. (Default value = 12) unique_threshold :\u2002 float or int Defines threshold to skip columns with higher cardinality values from encoding. If unique_threshold < 1, meaning that if any column has unique records > unique_threshold * total records, it will be considered as high cardinality column, thus not fit to be in strata_cols If unique_threshold > 1, meaning that if any column has unique records > unique_threshold, it will be considered as high cardinality column, thus not fit to be in strata_cols. (Default value = 0.5) Returns DataFrame Sampled Dataframe Expand source code def data_sample ( idf , strata_cols = \"all\" , drop_cols = [], fraction = 0.1 , method_type = \"random\" , stratified_type = \"population\" , seed_value = 12 , unique_threshold = 0.5 , ): \"\"\" This is a method focus on under-sampling necessary data through multiple methods. It covers two popular sampling techniques - stratified sampling and random sampling In stratified sampling, we sample out data based on the presence of strata, determined by strata_cols. Inside stratified sampling, there are 2 sub-methods, called \"population\" and \"balanced\", determined by stratified_type. \"Population\" stratified sampling method is Proportionate Allocation sampling strategy, uses a sampling fraction in each of the strata that is proportional to that of the original dataframe. On the other hand, \"Balanced\" stratified sampling method is Optimum Allocation sampling strategy, meaning the sampling fraction of each stratum is not proportional to their occurrence in the original dataframe. Instead, the strata will have an equal number of all stratum available. In random sampling, we sample out data randomly, purely depends on the fraction, and seed_value that is being inputted Parameters ---------- idf Input Dataframe strata_cols List of columns to be treated as strata e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of strata_cols, when we need to consider all columns except a few handful of them. (Default value = []) fraction : float Fraction of the data to be sampled out. (Default value = 0.1) method_type : str \"stratified\" for Stratified sampling, \"random\" for Random Sampling. (Default value = \"random\") stratified_type : str \"population\" for Proportionate Stratified Sampling, \"balanced\" for Optimum Stratified Sampling seed_value : int Seed value for sampling function. (Default value = 12) unique_threshold : float or int Defines threshold to skip columns with higher cardinality values from encoding. If unique_threshold < 1, meaning that if any column has unique records > unique_threshold * total records, it will be considered as high cardinality column, thus not fit to be in strata_cols If unique_threshold > 1, meaning that if any column has unique records > unique_threshold, it will be considered as high cardinality column, thus not fit to be in strata_cols. (Default value = 0.5) Returns ------- DataFrame Sampled Dataframe \"\"\" if type ( fraction ) != float and type ( fraction ) != int : raise TypeError ( \"Invalid input for fraction\" ) if fraction <= 0 or fraction > 1 : raise TypeError ( \"Invalid input for fraction: fraction value is between 0 and 1\" ) if type ( seed_value ) != int : raise TypeError ( \"Invalid input for seed_value\" ) if method_type not in [ \"stratified\" , \"random\" ]: raise TypeError ( \"Invalid input for data_sample method_type\" ) if method_type == \"stratified\" : if type ( unique_threshold ) != float and type ( unique_threshold ) != int : raise TypeError ( \"Invalid input for unique_threshold\" ) if unique_threshold > 1 and type ( unique_threshold ) != int : raise TypeError ( \"Invalid input for unique_threshold: unique_threshold can only be integer if larger than 1\" ) if unique_threshold <= 0 : raise TypeError ( \"Invalid input for unique_threshold: unique_threshold value is either between 0 and 1, or an integer > 1\" ) if stratified_type not in [ \"population\" , \"balanced\" ]: raise TypeError ( \"Invalid input for stratified_type\" ) if strata_cols == \"all\" : strata_cols = idf . columns if isinstance ( strata_cols , str ): strata_cols = [ x . strip () for x in strata_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] strata_cols = list ( set ([ e for e in strata_cols if e not in drop_cols ])) if len ( strata_cols ) == 0 : raise TypeError ( \"Missing strata_cols value\" ) skip_cols = [] for col in strata_cols : if col not in idf . columns : raise TypeError ( \"Invalid input for strata_cols: \" + col + \" does not exist\" ) if method_type == \"stratified\" : if unique_threshold <= 1 : if float ( idf . select ( col ) . distinct () . count () ) > unique_threshold * float ( idf . select ( col ) . count ()): skip_cols . append ( col ) else : if float ( idf . select ( col ) . distinct () . count ()) > unique_threshold : skip_cols . append ( col ) if skip_cols : warnings . warn ( \"Columns dropped from strata due to high cardinality: \" + \",\" . join ( skip_cols ) ) strata_cols = list ( set ([ e for e in strata_cols if e not in skip_cols ])) if len ( strata_cols ) == 0 : warnings . warn ( \"No Stratified Sampling Computation - No strata column(s) to sample\" ) return idf if method_type == \"stratified\" : sample_df = idf . na . drop ( subset = strata_cols ) . withColumn ( \"merge\" , F . concat ( * strata_cols ) ) fractions = ( sample_df . select ( \"merge\" ) . distinct () . withColumn ( \"fraction\" , F . lit ( fraction )) . rdd . collectAsMap () ) if stratified_type == \"population\" : odf = sample_df . stat . sampleBy ( \"merge\" , fractions , seed_value ) . drop ( \"merge\" ) else : count_dict = ( sample_df . groupby ( \"merge\" ) . count () . orderBy ( \"count\" ) . rdd . collectAsMap () ) smallest_count = int ( count_dict [ list ( count_dict . keys ())[ 0 ]]) for key in fractions . keys (): fractions [ key ] = float ( fraction * smallest_count / int ( count_dict [ key ])) odf = sample_df . stat . sampleBy ( \"merge\" , fractions , seed_value ) . drop ( \"merge\" ) else : odf = idf . sample ( withReplacement = False , fraction = fraction , seed = seed_value ) return odf","title":"<code>data_sampling</code>"},{"location":"api/data_ingest/data_sampling.html#data_sampling","text":"Expand source code import warnings from pyspark.sql import functions as F from anovos.shared.utils import attributeType_segregation def data_sample ( idf , strata_cols = \"all\" , drop_cols = [], fraction = 0.1 , method_type = \"random\" , stratified_type = \"population\" , seed_value = 12 , unique_threshold = 0.5 , ): \"\"\" This is a method focus on under-sampling necessary data through multiple methods. It covers two popular sampling techniques - stratified sampling and random sampling In stratified sampling, we sample out data based on the presence of strata, determined by strata_cols. Inside stratified sampling, there are 2 sub-methods, called \"population\" and \"balanced\", determined by stratified_type. \"Population\" stratified sampling method is Proportionate Allocation sampling strategy, uses a sampling fraction in each of the strata that is proportional to that of the original dataframe. On the other hand, \"Balanced\" stratified sampling method is Optimum Allocation sampling strategy, meaning the sampling fraction of each stratum is not proportional to their occurrence in the original dataframe. Instead, the strata will have an equal number of all stratum available. In random sampling, we sample out data randomly, purely depends on the fraction, and seed_value that is being inputted Parameters ---------- idf Input Dataframe strata_cols List of columns to be treated as strata e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of strata_cols, when we need to consider all columns except a few handful of them. (Default value = []) fraction : float Fraction of the data to be sampled out. (Default value = 0.1) method_type : str \"stratified\" for Stratified sampling, \"random\" for Random Sampling. (Default value = \"random\") stratified_type : str \"population\" for Proportionate Stratified Sampling, \"balanced\" for Optimum Stratified Sampling seed_value : int Seed value for sampling function. (Default value = 12) unique_threshold : float or int Defines threshold to skip columns with higher cardinality values from encoding. If unique_threshold < 1, meaning that if any column has unique records > unique_threshold * total records, it will be considered as high cardinality column, thus not fit to be in strata_cols If unique_threshold > 1, meaning that if any column has unique records > unique_threshold, it will be considered as high cardinality column, thus not fit to be in strata_cols. (Default value = 0.5) Returns ------- DataFrame Sampled Dataframe \"\"\" if type ( fraction ) != float and type ( fraction ) != int : raise TypeError ( \"Invalid input for fraction\" ) if fraction <= 0 or fraction > 1 : raise TypeError ( \"Invalid input for fraction: fraction value is between 0 and 1\" ) if type ( seed_value ) != int : raise TypeError ( \"Invalid input for seed_value\" ) if method_type not in [ \"stratified\" , \"random\" ]: raise TypeError ( \"Invalid input for data_sample method_type\" ) if method_type == \"stratified\" : if type ( unique_threshold ) != float and type ( unique_threshold ) != int : raise TypeError ( \"Invalid input for unique_threshold\" ) if unique_threshold > 1 and type ( unique_threshold ) != int : raise TypeError ( \"Invalid input for unique_threshold: unique_threshold can only be integer if larger than 1\" ) if unique_threshold <= 0 : raise TypeError ( \"Invalid input for unique_threshold: unique_threshold value is either between 0 and 1, or an integer > 1\" ) if stratified_type not in [ \"population\" , \"balanced\" ]: raise TypeError ( \"Invalid input for stratified_type\" ) if strata_cols == \"all\" : strata_cols = idf . columns if isinstance ( strata_cols , str ): strata_cols = [ x . strip () for x in strata_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] strata_cols = list ( set ([ e for e in strata_cols if e not in drop_cols ])) if len ( strata_cols ) == 0 : raise TypeError ( \"Missing strata_cols value\" ) skip_cols = [] for col in strata_cols : if col not in idf . columns : raise TypeError ( \"Invalid input for strata_cols: \" + col + \" does not exist\" ) if method_type == \"stratified\" : if unique_threshold <= 1 : if float ( idf . select ( col ) . distinct () . count () ) > unique_threshold * float ( idf . select ( col ) . count ()): skip_cols . append ( col ) else : if float ( idf . select ( col ) . distinct () . count ()) > unique_threshold : skip_cols . append ( col ) if skip_cols : warnings . warn ( \"Columns dropped from strata due to high cardinality: \" + \",\" . join ( skip_cols ) ) strata_cols = list ( set ([ e for e in strata_cols if e not in skip_cols ])) if len ( strata_cols ) == 0 : warnings . warn ( \"No Stratified Sampling Computation - No strata column(s) to sample\" ) return idf if method_type == \"stratified\" : sample_df = idf . na . drop ( subset = strata_cols ) . withColumn ( \"merge\" , F . concat ( * strata_cols ) ) fractions = ( sample_df . select ( \"merge\" ) . distinct () . withColumn ( \"fraction\" , F . lit ( fraction )) . rdd . collectAsMap () ) if stratified_type == \"population\" : odf = sample_df . stat . sampleBy ( \"merge\" , fractions , seed_value ) . drop ( \"merge\" ) else : count_dict = ( sample_df . groupby ( \"merge\" ) . count () . orderBy ( \"count\" ) . rdd . collectAsMap () ) smallest_count = int ( count_dict [ list ( count_dict . keys ())[ 0 ]]) for key in fractions . keys (): fractions [ key ] = float ( fraction * smallest_count / int ( count_dict [ key ])) odf = sample_df . stat . sampleBy ( \"merge\" , fractions , seed_value ) . drop ( \"merge\" ) else : odf = idf . sample ( withReplacement = False , fraction = fraction , seed = seed_value ) return odf","title":"data_sampling"},{"location":"api/data_ingest/data_sampling.html#functions","text":"def data_sample ( idf, strata_cols='all', drop_cols=[], fraction=0.1, method_type='random', stratified_type='population', seed_value=12, unique_threshold=0.5) This is a method focus on under-sampling necessary data through multiple methods. It covers two popular sampling techniques - stratified sampling and random sampling In stratified sampling, we sample out data based on the presence of strata, determined by strata_cols. Inside stratified sampling, there are 2 sub-methods, called \"population\" and \"balanced\", determined by stratified_type. \"Population\" stratified sampling method is Proportionate Allocation sampling strategy, uses a sampling fraction in each of the strata that is proportional to that of the original dataframe. On the other hand, \"Balanced\" stratified sampling method is Optimum Allocation sampling strategy, meaning the sampling fraction of each stratum is not proportional to their occurrence in the original dataframe. Instead, the strata will have an equal number of all stratum available. In random sampling, we sample out data randomly, purely depends on the fraction, and seed_value that is being inputted","title":"Functions"},{"location":"api/data_ingest/geo_auto_detection.html","text":"geo_auto_detection This module help to automatically identify the latitude, longitude as well as geohash columns present in the analysis data through some intelligent checks provisioned as a part of the module. As a part of generation of the auto detection output, there are various functions created such as - reg_lat_lon conv_str_plus precision_lev geo_to_latlong latlong_to_geo ll_gh_cols Respective functions have sections containing the detailed definition of the parameters used for computing. Expand source code # coding=utf-8 \"\"\"This module help to automatically identify the latitude, longitude as well as geohash columns present in the analysis data through some intelligent checks provisioned as a part of the module. As a part of generation of the auto detection output, there are various functions created such as - - reg_lat_lon - conv_str_plus - precision_lev - geo_to_latlong - latlong_to_geo - ll_gh_cols Respective functions have sections containing the detailed definition of the parameters used for computing. \"\"\" import pygeohash as gh from pyspark.sql import functions as F from pyspark.sql import types as T def reg_lat_lon ( option ): \"\"\" This function helps to produce the relevant regular expression to be used for further processing based on the input field category - latitude / longitude Parameters ---------- option Can be either latitude or longitude basis which the desired regular expression will be produced Returns ------- Regular Expression \"\"\" if option == \"latitude\" : return \"^(\\+|-|)?(?:90(?:(?:\\.0{1,10})?)|(?:[0-9]|[1-8][0-9])(?:(?:\\.[0-9]{1,})?))$\" elif option == \"longitude\" : return \"^(\\+|-)?(?:180(?:(?:\\.0{1,10})?)|(?:[0-9]|[1-9][0-9]|1[0-7][0-9])(?:(?:\\.[0-9]{1,10})?))$\" def conv_str_plus ( col ): \"\"\" This function helps to produce an extra \"+\" to the positive values while negative values are kept as is Parameters ---------- col Analysis column Returns ------- String \"\"\" if col is None : return None elif col < 0 : return col else : return \"+\" + str ( col ) f_conv_str_plus = F . udf ( conv_str_plus , T . StringType ()) def precision_lev ( col ): \"\"\" This function helps to capture the precision level after decimal point. Parameters ---------- col Analysis column Returns ------- String \"\"\" if col is None : return 0 else : x = float ( str ( format ( float ( col ), \".8f\" )) . split ( \".\" )[ 1 ]) if x > 0 : return len ( str ( format ( float ( col ), \".8f\" )) . split ( \".\" )[ 1 ]) else : return 0 f_precision_lev = F . udf ( precision_lev , T . StringType ()) def geo_to_latlong ( x , option ): \"\"\" This function helps to convert geohash to latitude / longitude with the help of pygeohash library. Parameters ---------- x Analysis column option Can be either 0 or 1 basis which the latitude / longitude will be produced Returns ------- Float \"\"\" if x is not None : if option == 0 : try : return [ float ( a ) for a in gh . decode ( x )][ option ] except : pass elif option == 1 : try : return [ float ( a ) for a in gh . decode ( x )][ option ] except : pass else : return None else : return None f_geo_to_latlong = F . udf ( geo_to_latlong , T . FloatType ()) def latlong_to_geo ( lat , long , precision = 9 ): \"\"\" This function helps to convert latitude-longitude to geohash with the help of pygeohash library. Parameters ---------- lat latitude column long longitude column precision precision at which the geohash is converted to Returns ------- Regular String \"\"\" if ( lat is not None ) and ( long is not None ): return gh . encode ( lat , long , precision ) else : return None f_latlong_to_geo = F . udf ( latlong_to_geo , T . StringType ()) def ll_gh_cols ( df , max_records ): \"\"\" This function is the main function to auto-detect latitude, longitude and geohash columns from a given dataset df. To detect latitude and longitude columns, it will check whether \"latitude\" or \"longitude\" appears in the columns. If not, it will calculate the precision level, maximum, standard deviation and mean value of each float or double-type column, and convert to string type by calling \"conv_str_plus\". If the converted string matches regular expression of latitude and the absolute value of maximum is <= 90, then it will be regarded as latitude column. If the converted string matches regular expression of longitude and the absolute value of maximum is > 90, then it will be regarded as longitude column. To detect geohash column, it will calculate the maximum string-length of every string column, and convert it to lat-long pairs by calling \"geo_to_lat_long\". If the conversion is successful and the maximum string-length is between 4 and 12 (exclusive), this string column will be regarded as geohash column. Parameters ---------- df Analysis dataframe max_records Maximum geospatial points analyzed Returns ------- List \"\"\" lat_cols , long_cols , gh_cols = [], [], [] for i in df . dtypes : if i [ 1 ] in ( \"float\" , \"double\" , \"float32\" , \"float64\" ): c = 0 prec_val = ( df . withColumn ( \"__\" , f_precision_lev ( F . col ( i [ 0 ]))) . agg ( F . max ( \"__\" )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) max_val = ( df . withColumn ( \"__\" , F . col ( i [ 0 ])) . agg ( F . max ( \"__\" )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) stddev_val = ( df . agg ( F . stddev ( F . col ( i [ 0 ]))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) mean_val = df . agg ( F . mean ( F . col ( i [ 0 ]))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] if prec_val is None : p = 0 elif prec_val == 0 : p = 0 else : p = prec_val if \"latitude\" in i [ 0 ] . lower (): lat_cols . append ( i [ 0 ]) elif \"longitude\" in i [ 0 ] . lower (): long_cols . append ( i [ 0 ]) elif ( ( int ( p ) > 0 ) & ( max_val <= 180 ) & ( stddev_val >= 1 ) & ( float ( stddev_val ) / float ( mean_val ) < 1 ) ): for j in [ reg_lat_lon ( \"latitude\" ), reg_lat_lon ( \"longitude\" )]: if c == 0 : x = ( df . select ( F . col ( i [ 0 ])) . dropna () . withColumn ( \"_\" , F . regexp_extract ( f_conv_str_plus ( i [ 0 ]), j , 0 ) ) ) max_val = abs ( float ( x . agg ( F . max ( i [ 0 ])) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) ) if ( x . groupBy ( \"_\" ) . count () . count () > 2 ) & ( max_val <= 90 ): lat_cols . append ( i [ 0 ]) c = c + 1 elif ( x . groupBy ( \"_\" ) . count () . count () > 2 ) & ( max_val > 90 ): long_cols . append ( i [ 0 ]) c = c + 1 elif i [ 1 ] in ( \"string\" , \"object\" ): x = ( df . select ( F . col ( i [ 0 ])) . dropna () . limit ( max_records ) . withColumn ( \"len_gh\" , F . length ( F . col ( i [ 0 ]))) ) x_ = x . agg ( F . max ( \"len_gh\" )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] try : if ( x_ > 4 ) & ( x_ < 12 ): if ( x . withColumn ( \"_\" , f_geo_to_latlong ( i [ 0 ], F . lit ( 0 ))) . groupBy ( \"_\" ) . count () . count () > 2 ): gh_cols . append ( i [ 0 ]) else : pass except : pass if len ( lat_cols ) != len ( long_cols ): lat_cols , long_cols = [], [] return lat_cols , long_cols , gh_cols Functions def conv_str_plus ( col) This function helps to produce an extra \"+\" to the positive values while negative values are kept as is Parameters col Analysis column Returns String Expand source code def conv_str_plus ( col ): \"\"\" This function helps to produce an extra \"+\" to the positive values while negative values are kept as is Parameters ---------- col Analysis column Returns ------- String \"\"\" if col is None : return None elif col < 0 : return col else : return \"+\" + str ( col ) def f_conv_str_plus ( col) This function helps to produce an extra \"+\" to the positive values while negative values are kept as is Parameters col Analysis column Returns String Expand source code def conv_str_plus ( col ): \"\"\" This function helps to produce an extra \"+\" to the positive values while negative values are kept as is Parameters ---------- col Analysis column Returns ------- String \"\"\" if col is None : return None elif col < 0 : return col else : return \"+\" + str ( col ) def f_geo_to_latlong ( x, option) This function helps to convert geohash to latitude / longitude with the help of pygeohash library. Parameters x Analysis column option Can be either 0 or 1 basis which the latitude / longitude will be produced Returns Float Expand source code def geo_to_latlong ( x , option ): \"\"\" This function helps to convert geohash to latitude / longitude with the help of pygeohash library. Parameters ---------- x Analysis column option Can be either 0 or 1 basis which the latitude / longitude will be produced Returns ------- Float \"\"\" if x is not None : if option == 0 : try : return [ float ( a ) for a in gh . decode ( x )][ option ] except : pass elif option == 1 : try : return [ float ( a ) for a in gh . decode ( x )][ option ] except : pass else : return None else : return None def f_latlong_to_geo ( lat, long, precision=9) This function helps to convert latitude-longitude to geohash with the help of pygeohash library. Parameters lat latitude column long longitude column precision precision at which the geohash is converted to Returns Regular String Expand source code def latlong_to_geo ( lat , long , precision = 9 ): \"\"\" This function helps to convert latitude-longitude to geohash with the help of pygeohash library. Parameters ---------- lat latitude column long longitude column precision precision at which the geohash is converted to Returns ------- Regular String \"\"\" if ( lat is not None ) and ( long is not None ): return gh . encode ( lat , long , precision ) else : return None def f_precision_lev ( col) This function helps to capture the precision level after decimal point. Parameters col Analysis column Returns String Expand source code def precision_lev ( col ): \"\"\" This function helps to capture the precision level after decimal point. Parameters ---------- col Analysis column Returns ------- String \"\"\" if col is None : return 0 else : x = float ( str ( format ( float ( col ), \".8f\" )) . split ( \".\" )[ 1 ]) if x > 0 : return len ( str ( format ( float ( col ), \".8f\" )) . split ( \".\" )[ 1 ]) else : return 0 def geo_to_latlong ( x, option) This function helps to convert geohash to latitude / longitude with the help of pygeohash library. Parameters x Analysis column option Can be either 0 or 1 basis which the latitude / longitude will be produced Returns Float Expand source code def geo_to_latlong ( x , option ): \"\"\" This function helps to convert geohash to latitude / longitude with the help of pygeohash library. Parameters ---------- x Analysis column option Can be either 0 or 1 basis which the latitude / longitude will be produced Returns ------- Float \"\"\" if x is not None : if option == 0 : try : return [ float ( a ) for a in gh . decode ( x )][ option ] except : pass elif option == 1 : try : return [ float ( a ) for a in gh . decode ( x )][ option ] except : pass else : return None else : return None def latlong_to_geo ( lat, long, precision=9) This function helps to convert latitude-longitude to geohash with the help of pygeohash library. Parameters lat latitude column long longitude column precision precision at which the geohash is converted to Returns Regular String Expand source code def latlong_to_geo ( lat , long , precision = 9 ): \"\"\" This function helps to convert latitude-longitude to geohash with the help of pygeohash library. Parameters ---------- lat latitude column long longitude column precision precision at which the geohash is converted to Returns ------- Regular String \"\"\" if ( lat is not None ) and ( long is not None ): return gh . encode ( lat , long , precision ) else : return None def ll_gh_cols ( df, max_records) This function is the main function to auto-detect latitude, longitude and geohash columns from a given dataset df. To detect latitude and longitude columns, it will check whether \"latitude\" or \"longitude\" appears in the columns. If not, it will calculate the precision level, maximum, standard deviation and mean value of each float or double-type column, and convert to string type by calling \"conv_str_plus\". If the converted string matches regular expression of latitude and the absolute value of maximum is <= 90, then it will be regarded as latitude column. If the converted string matches regular expression of longitude and the absolute value of maximum is > 90, then it will be regarded as longitude column. To detect geohash column, it will calculate the maximum string-length of every string column, and convert it to lat-long pairs by calling \"geo_to_lat_long\". If the conversion is successful and the maximum string-length is between 4 and 12 (exclusive), this string column will be regarded as geohash column. Parameters df Analysis dataframe max_records Maximum geospatial points analyzed Returns List Expand source code def ll_gh_cols ( df , max_records ): \"\"\" This function is the main function to auto-detect latitude, longitude and geohash columns from a given dataset df. To detect latitude and longitude columns, it will check whether \"latitude\" or \"longitude\" appears in the columns. If not, it will calculate the precision level, maximum, standard deviation and mean value of each float or double-type column, and convert to string type by calling \"conv_str_plus\". If the converted string matches regular expression of latitude and the absolute value of maximum is <= 90, then it will be regarded as latitude column. If the converted string matches regular expression of longitude and the absolute value of maximum is > 90, then it will be regarded as longitude column. To detect geohash column, it will calculate the maximum string-length of every string column, and convert it to lat-long pairs by calling \"geo_to_lat_long\". If the conversion is successful and the maximum string-length is between 4 and 12 (exclusive), this string column will be regarded as geohash column. Parameters ---------- df Analysis dataframe max_records Maximum geospatial points analyzed Returns ------- List \"\"\" lat_cols , long_cols , gh_cols = [], [], [] for i in df . dtypes : if i [ 1 ] in ( \"float\" , \"double\" , \"float32\" , \"float64\" ): c = 0 prec_val = ( df . withColumn ( \"__\" , f_precision_lev ( F . col ( i [ 0 ]))) . agg ( F . max ( \"__\" )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) max_val = ( df . withColumn ( \"__\" , F . col ( i [ 0 ])) . agg ( F . max ( \"__\" )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) stddev_val = ( df . agg ( F . stddev ( F . col ( i [ 0 ]))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) mean_val = df . agg ( F . mean ( F . col ( i [ 0 ]))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] if prec_val is None : p = 0 elif prec_val == 0 : p = 0 else : p = prec_val if \"latitude\" in i [ 0 ] . lower (): lat_cols . append ( i [ 0 ]) elif \"longitude\" in i [ 0 ] . lower (): long_cols . append ( i [ 0 ]) elif ( ( int ( p ) > 0 ) & ( max_val <= 180 ) & ( stddev_val >= 1 ) & ( float ( stddev_val ) / float ( mean_val ) < 1 ) ): for j in [ reg_lat_lon ( \"latitude\" ), reg_lat_lon ( \"longitude\" )]: if c == 0 : x = ( df . select ( F . col ( i [ 0 ])) . dropna () . withColumn ( \"_\" , F . regexp_extract ( f_conv_str_plus ( i [ 0 ]), j , 0 ) ) ) max_val = abs ( float ( x . agg ( F . max ( i [ 0 ])) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) ) if ( x . groupBy ( \"_\" ) . count () . count () > 2 ) & ( max_val <= 90 ): lat_cols . append ( i [ 0 ]) c = c + 1 elif ( x . groupBy ( \"_\" ) . count () . count () > 2 ) & ( max_val > 90 ): long_cols . append ( i [ 0 ]) c = c + 1 elif i [ 1 ] in ( \"string\" , \"object\" ): x = ( df . select ( F . col ( i [ 0 ])) . dropna () . limit ( max_records ) . withColumn ( \"len_gh\" , F . length ( F . col ( i [ 0 ]))) ) x_ = x . agg ( F . max ( \"len_gh\" )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] try : if ( x_ > 4 ) & ( x_ < 12 ): if ( x . withColumn ( \"_\" , f_geo_to_latlong ( i [ 0 ], F . lit ( 0 ))) . groupBy ( \"_\" ) . count () . count () > 2 ): gh_cols . append ( i [ 0 ]) else : pass except : pass if len ( lat_cols ) != len ( long_cols ): lat_cols , long_cols = [], [] return lat_cols , long_cols , gh_cols def precision_lev ( col) This function helps to capture the precision level after decimal point. Parameters col Analysis column Returns String Expand source code def precision_lev ( col ): \"\"\" This function helps to capture the precision level after decimal point. Parameters ---------- col Analysis column Returns ------- String \"\"\" if col is None : return 0 else : x = float ( str ( format ( float ( col ), \".8f\" )) . split ( \".\" )[ 1 ]) if x > 0 : return len ( str ( format ( float ( col ), \".8f\" )) . split ( \".\" )[ 1 ]) else : return 0 def reg_lat_lon ( option) This function helps to produce the relevant regular expression to be used for further processing based on the input field category - latitude / longitude Parameters option Can be either latitude or longitude basis which the desired regular expression will be produced Returns Regular Expression Expand source code def reg_lat_lon ( option ): \"\"\" This function helps to produce the relevant regular expression to be used for further processing based on the input field category - latitude / longitude Parameters ---------- option Can be either latitude or longitude basis which the desired regular expression will be produced Returns ------- Regular Expression \"\"\" if option == \"latitude\" : return \"^(\\+|-|)?(?:90(?:(?:\\.0{1,10})?)|(?:[0-9]|[1-8][0-9])(?:(?:\\.[0-9]{1,})?))$\" elif option == \"longitude\" : return \"^(\\+|-)?(?:180(?:(?:\\.0{1,10})?)|(?:[0-9]|[1-9][0-9]|1[0-7][0-9])(?:(?:\\.[0-9]{1,10})?))$\"","title":"<code>geo_auto_detection</code>"},{"location":"api/data_ingest/geo_auto_detection.html#geo_auto_detection","text":"This module help to automatically identify the latitude, longitude as well as geohash columns present in the analysis data through some intelligent checks provisioned as a part of the module. As a part of generation of the auto detection output, there are various functions created such as - reg_lat_lon conv_str_plus precision_lev geo_to_latlong latlong_to_geo ll_gh_cols Respective functions have sections containing the detailed definition of the parameters used for computing. Expand source code # coding=utf-8 \"\"\"This module help to automatically identify the latitude, longitude as well as geohash columns present in the analysis data through some intelligent checks provisioned as a part of the module. As a part of generation of the auto detection output, there are various functions created such as - - reg_lat_lon - conv_str_plus - precision_lev - geo_to_latlong - latlong_to_geo - ll_gh_cols Respective functions have sections containing the detailed definition of the parameters used for computing. \"\"\" import pygeohash as gh from pyspark.sql import functions as F from pyspark.sql import types as T def reg_lat_lon ( option ): \"\"\" This function helps to produce the relevant regular expression to be used for further processing based on the input field category - latitude / longitude Parameters ---------- option Can be either latitude or longitude basis which the desired regular expression will be produced Returns ------- Regular Expression \"\"\" if option == \"latitude\" : return \"^(\\+|-|)?(?:90(?:(?:\\.0{1,10})?)|(?:[0-9]|[1-8][0-9])(?:(?:\\.[0-9]{1,})?))$\" elif option == \"longitude\" : return \"^(\\+|-)?(?:180(?:(?:\\.0{1,10})?)|(?:[0-9]|[1-9][0-9]|1[0-7][0-9])(?:(?:\\.[0-9]{1,10})?))$\" def conv_str_plus ( col ): \"\"\" This function helps to produce an extra \"+\" to the positive values while negative values are kept as is Parameters ---------- col Analysis column Returns ------- String \"\"\" if col is None : return None elif col < 0 : return col else : return \"+\" + str ( col ) f_conv_str_plus = F . udf ( conv_str_plus , T . StringType ()) def precision_lev ( col ): \"\"\" This function helps to capture the precision level after decimal point. Parameters ---------- col Analysis column Returns ------- String \"\"\" if col is None : return 0 else : x = float ( str ( format ( float ( col ), \".8f\" )) . split ( \".\" )[ 1 ]) if x > 0 : return len ( str ( format ( float ( col ), \".8f\" )) . split ( \".\" )[ 1 ]) else : return 0 f_precision_lev = F . udf ( precision_lev , T . StringType ()) def geo_to_latlong ( x , option ): \"\"\" This function helps to convert geohash to latitude / longitude with the help of pygeohash library. Parameters ---------- x Analysis column option Can be either 0 or 1 basis which the latitude / longitude will be produced Returns ------- Float \"\"\" if x is not None : if option == 0 : try : return [ float ( a ) for a in gh . decode ( x )][ option ] except : pass elif option == 1 : try : return [ float ( a ) for a in gh . decode ( x )][ option ] except : pass else : return None else : return None f_geo_to_latlong = F . udf ( geo_to_latlong , T . FloatType ()) def latlong_to_geo ( lat , long , precision = 9 ): \"\"\" This function helps to convert latitude-longitude to geohash with the help of pygeohash library. Parameters ---------- lat latitude column long longitude column precision precision at which the geohash is converted to Returns ------- Regular String \"\"\" if ( lat is not None ) and ( long is not None ): return gh . encode ( lat , long , precision ) else : return None f_latlong_to_geo = F . udf ( latlong_to_geo , T . StringType ()) def ll_gh_cols ( df , max_records ): \"\"\" This function is the main function to auto-detect latitude, longitude and geohash columns from a given dataset df. To detect latitude and longitude columns, it will check whether \"latitude\" or \"longitude\" appears in the columns. If not, it will calculate the precision level, maximum, standard deviation and mean value of each float or double-type column, and convert to string type by calling \"conv_str_plus\". If the converted string matches regular expression of latitude and the absolute value of maximum is <= 90, then it will be regarded as latitude column. If the converted string matches regular expression of longitude and the absolute value of maximum is > 90, then it will be regarded as longitude column. To detect geohash column, it will calculate the maximum string-length of every string column, and convert it to lat-long pairs by calling \"geo_to_lat_long\". If the conversion is successful and the maximum string-length is between 4 and 12 (exclusive), this string column will be regarded as geohash column. Parameters ---------- df Analysis dataframe max_records Maximum geospatial points analyzed Returns ------- List \"\"\" lat_cols , long_cols , gh_cols = [], [], [] for i in df . dtypes : if i [ 1 ] in ( \"float\" , \"double\" , \"float32\" , \"float64\" ): c = 0 prec_val = ( df . withColumn ( \"__\" , f_precision_lev ( F . col ( i [ 0 ]))) . agg ( F . max ( \"__\" )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) max_val = ( df . withColumn ( \"__\" , F . col ( i [ 0 ])) . agg ( F . max ( \"__\" )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) stddev_val = ( df . agg ( F . stddev ( F . col ( i [ 0 ]))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) mean_val = df . agg ( F . mean ( F . col ( i [ 0 ]))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] if prec_val is None : p = 0 elif prec_val == 0 : p = 0 else : p = prec_val if \"latitude\" in i [ 0 ] . lower (): lat_cols . append ( i [ 0 ]) elif \"longitude\" in i [ 0 ] . lower (): long_cols . append ( i [ 0 ]) elif ( ( int ( p ) > 0 ) & ( max_val <= 180 ) & ( stddev_val >= 1 ) & ( float ( stddev_val ) / float ( mean_val ) < 1 ) ): for j in [ reg_lat_lon ( \"latitude\" ), reg_lat_lon ( \"longitude\" )]: if c == 0 : x = ( df . select ( F . col ( i [ 0 ])) . dropna () . withColumn ( \"_\" , F . regexp_extract ( f_conv_str_plus ( i [ 0 ]), j , 0 ) ) ) max_val = abs ( float ( x . agg ( F . max ( i [ 0 ])) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) ) if ( x . groupBy ( \"_\" ) . count () . count () > 2 ) & ( max_val <= 90 ): lat_cols . append ( i [ 0 ]) c = c + 1 elif ( x . groupBy ( \"_\" ) . count () . count () > 2 ) & ( max_val > 90 ): long_cols . append ( i [ 0 ]) c = c + 1 elif i [ 1 ] in ( \"string\" , \"object\" ): x = ( df . select ( F . col ( i [ 0 ])) . dropna () . limit ( max_records ) . withColumn ( \"len_gh\" , F . length ( F . col ( i [ 0 ]))) ) x_ = x . agg ( F . max ( \"len_gh\" )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] try : if ( x_ > 4 ) & ( x_ < 12 ): if ( x . withColumn ( \"_\" , f_geo_to_latlong ( i [ 0 ], F . lit ( 0 ))) . groupBy ( \"_\" ) . count () . count () > 2 ): gh_cols . append ( i [ 0 ]) else : pass except : pass if len ( lat_cols ) != len ( long_cols ): lat_cols , long_cols = [], [] return lat_cols , long_cols , gh_cols","title":"geo_auto_detection"},{"location":"api/data_ingest/geo_auto_detection.html#functions","text":"def conv_str_plus ( col) This function helps to produce an extra \"+\" to the positive values while negative values are kept as is","title":"Functions"},{"location":"api/data_ingest/ts_auto_detection.html","text":"ts_auto_detection This module help produce the output containing a transformation through auto timestamp / date detection by reading the ingested dataframe from source. As a part of generation of the auto detection output, there are various functions created such as - regex_date_time_parser ts_loop_cols_pre ts_preprocess Respective functions have sections containing the detailed definition of the parameters used for computing. Expand source code # coding=utf-8 \"\"\"This module help produce the output containing a transformation through auto timestamp / date detection by reading the ingested dataframe from source. As a part of generation of the auto detection output, there are various functions created such as - - regex_date_time_parser - ts_loop_cols_pre - ts_preprocess Respective functions have sections containing the detailed definition of the parameters used for computing. \"\"\" import calendar import csv import datetime import io import os import re import subprocess import warnings from pathlib import Path import dateutil.parser import mlflow import numpy as np import pandas as pd import pyspark from loguru import logger from pyspark.sql import Window from pyspark.sql import functions as F from pyspark.sql import types as T from anovos.data_analyzer.stats_generator import measures_of_percentiles from anovos.data_transformer.datetime import ( lagged_ts , timeUnits_extraction , unix_to_timestamp , ) from anovos.shared.utils import ( attributeType_segregation , ends_with , output_to_local , path_ak8s_modify , ) ###regex based ts parser function def regex_date_time_parser ( spark , idf , id_col , col , tz , val_unique_cat , trans_cat , save_output = None , output_mode = \"replace\" , ): \"\"\" This function helps to produce the transformed output in timestamp (if auto-detected) based on the input data. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column col Column passed for Auto detection of Timestamp / date type tz Timezone offset (Option to chose between options like Local, GMT, UTC). Default option is set as \"Local\". val_unique_cat Maximum character length of the field. trans_cat Custom data type basis which further processing will be conditioned. save_output Output path where the transformed ddata can be saved output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name Returns ------- DataFrame \"\"\" REGEX_PARTS = { \"Y\" : r \"(?:19[4-9]\\d|20[0-3]\\d)\" , # 1940 to 2039 \"y\" : r \"(?:\\d\\d)\" , # 00 to 99 \"m\" : r \"(?:1[012]|0?[1-9])\" , # 0?1 to 12 \"mz\" : r \"(?:1[012]|0[1-9])\" , # 01 to 12 \"B\" : r \"(?:\" r \"D?JAN(?:UAR[IY])?|\" r \"[FP]EB(?:RUAR[IY])?|\" r \"MAC|MAR(?:CH|ET)?|MRT|\" r \"APR(?:IL)?|\" r \"M[EA]I|MAY|\" r \"JUNE?|D?JUNI?|\" r \"JUL(?:Y|AI)?|D?JULI?|\" r \"OG(?:OS)?|AUG(?:UST)?|AGT?(?:USTUS)?|\" r \"SEP(?:T(?:EMBER)?)?|\" r \"O[KC]T(?:OBER)?|\" r \"NO[VP](?:EMBER)?|\" r \"D[EI][SC](?:EMBER)?\" r \")\" , \"d\" : r \"(?:3[01]|[12]\\d|0?[1-9])\" , # 0?1 to 31 \"d_range\" : r \"(?:3[01]|[12]\\d|0?[1-9])(?: ?[-] ?(?:3[01]|[12]\\d|0?[1-9]))?\" , # 14-15 \"dz\" : r \"(?:3[01]|[12]\\d|0[1-9])\" , # 01 to 31 \"j\" : r \"(?:36[0-6]|3[0-5]\\d|[12]\\d\\d|0?[1-9]\\d|0?0?[1-9])\" , # 0?0?1 to 366 \"H\" : r \"(?:2[0-4]|[01]?\\d)\" , # 0?0 to 24 \"HZ\" : r \"(?:2[0-4]|[01]\\d)\" , # 0?0 to 24 \"I\" : r \"(?:1[012]|0?[1-9])\" , # 0?1 to 12 \"M\" : r \"(?:[1-5]\\d|0\\d)\" , # 00 to 59 \"S\" : r \"(?:6[01]|[0-5]\\d)\" , # 00 to 61 (leap second) \"p\" : r \"(?:MIDNI(?:GHT|TE)|AFTERNOON|MORNING|NOON|[MN]N|H(?:(?:OU)?RS?)?|[AP]\\.? ?M\\.?)\" , \"p2\" : r \"(?:MIDNI(?:GHT|TE)|NOON|[AP]\\.? ?M\\.?)\" , \"Z\" : r \"(?:A(?:C(?:DT|ST|T|WST)|DT|E(?:DT|ST)|FT|K(?:DT|ST)|M(?:ST|T)|RT|ST|WST\" r \"|Z(?:O(?:ST|T)|T))|B(?:DT|I(?:OT|T)|OT|R(?:ST|T)|ST|TT)|C(?:AT|CT|DT|E(\" r \"?:ST|T)|H(?:A(?:DT|ST)|O(?:ST|T)|ST|UT)|I(?:ST|T)|KT|L(?:ST|T)|O(?:ST|T\" r \")|ST|T|VT|WST|XT)|D(?:AVT|DUT|FT)|E(?:A(?:S(?:ST|T)|T)|CT|DT|E(?:ST|T)|\" r \"G(?:ST|T)|IT|ST)|F(?:ET|JT|K(?:ST|T)|NT)|G(?:A(?:LT|MT)|ET|FT|I(?:LT|T)\" r \"|MT|ST|YT)|H(?:AEC|DT|KT|MT|OV(?:ST|T)|ST)|I(?:CT|D(?:LW|T)|OT|R(?:DT|K\" r \"T|ST)|ST)|JST|K(?:ALT|GT|OST|RAT|ST)|L(?:HST|INT)|M(?:A(?:GT|RT|WT)|DT|\" r \"E(?:ST|T)|HT|I(?:ST|T)|MT|S(?:K|T)|UT|VT|YT)|N(?:CT|DT|FT|PT|ST|T|UT|Z(\" r \"?:DT|ST))|O(?:MST|RAT)|P(?:DT|ET(?:T)?|GT|H(?:OT|T)|KT|M(?:DT|ST)|ONT|S\" r \"T|Y(?:ST|T))|R(?:ET|OTT)|S(?:A(?:KT|MT|ST)|BT|CT|DT|GT|LST|R(?:ET|T)|ST\" r \"|YOT)|T(?:AHT|FT|HA|JT|KT|LT|MT|OT|RT|VT)|U(?:LA(?:ST|T)|TC|Y(?:ST|T)|Z\" r \"T)|V(?:ET|LAT|O(?:LT|ST)|UT)|W(?:A(?:KT|ST|T)|E(?:ST|T)|IT|ST)|Y(?:AKT|\" r \"EKT))\" , # FROM: en.wikipedia.org/wiki/List_of_time_zone_abbreviations \"z\" : r \"(?:[+-](?:0\\d|1[0-4]):?(?:00|15|30|45))\" , # [+-] 00:00 to 14:45 \"A\" : r \"(?:\" r \"MON(?:DAY)?|(?:IS|SE)N(?:[IE]N)?|\" r \"TUE(?:S(?:DAY)?)?|SEL(?:ASA)?|\" r \"WED(?:NESDAY)?|RABU?|\" r \"THU(?:RS(?:DAY)?)?|KH?A(?:M(?:IS)?)?|\" r \"FRI(?:DAY)?|JUM(?:[AM]A?T)?|\" r \"SAT(?:URDAY)?|SAB(?:TU)?|\" r \"SUN(?:DAY)?|AHA?D|MIN(?:GGU)?\" r \")\" , \"th\" : r \"(?:ST|ND|RD|TH|\u00ba)\" , } REGEX_PATTERNS_PARSERS = { # 14/8/1991 \"dd_mm_YYYY_1\" : r \"(?: {d} / {m} / {Y} )\" , \"dd_d2\" : r \"(?: {d} \\\\ {m} \\\\ {Y} )\" , \"dd_mm_YYYY_3\" : r \"(?: {d} [-] {m} [-] {Y} )\" , \"dd_mm_YYYY_4\" : r \"(?: {d} \\. {m} \\. {Y} )\" , # 'dd_mm_YYYY_5': r\"(?:{d}{m}{Y})\", # too many phone numbers \"dd_mm_YYYY_6\" : r \"(?: {d} ? {m} ? {Y} )\" , \"dd_mm_YYYY_7\" : r \"(?: {dz}{mz}{Y} )\" , # 14/8/91 \"dd_mm_yy_1\" : r \"(?: {d} / {m} / {y} )\" , \"dd_mm_yy_2\" : r \"(?: {d} \\\\ {m} \\\\ {y} )\" , \"dd_mm_yy_3\" : r \"(?: {d} [-] {m} [-] {y} )\" , \"dd_mm_yy_4\" : r \"(?: {d} \\. {m} \\. {y} )\" , # 'dd_mm_yy_5': r\"(?:{dz}{mz}{y})\", # too many phone numbers # 14 Aug, 1991 \"dd_mmm_YYYY_1\" : r \"(?: {d}{th} ? ?/ ? {B} ?/ ? {Y} )\" , \"dd_mmm_YYYY_2\" : r \"(?: {d}{th} ? ? \\\\ ? {B} ? \\\\ ? {Y} )\" , \"dd_mmm_YYYY_3\" : r \"(?: {d}{th} ? ?[-] ? {B} ?[ -] ? {Y} )\" , \"dd_mmm_YYYY_4\" : r \"(?: {d}{th} ? ?[ -]? ? {B} ?,? ? {Y} )\" , \"dd_mmm_YYYY_5\" : r \"(?: {d}{th} ? ?\\. ? {B} ?\\. ? {Y} )\" , # 14 Aug '91 \"dd_mmm_yy_1\" : r \"(?: {d}{th} ? ?/ ? {B} ?/ ?'? {y} )\" , \"dd_mmm_yy_2\" : r \"(?: {d}{th} ? ? \\\\ ? {B} ? \\\\ ?'? {y} )\" , \"dd_mmm_yy_3\" : r \"(?: {d}{th} ? ?[-] ? {B} ?[-] ?'? {y} )\" , \"dd_mmm_yy_4\" : r \"(?: {d}{th} ? ?[ -]? ? {B} ?,? ?'? {y} )\" , \"dd_mmm_yy_5\" : r \"(?: {d}{th} ? ?\\. ? {B} ?\\. ?'? {y} )\" , # 14th Aug \"dd_mmm\" : r \"(?: {d}{th} ? ?[/ \\\\ . -] ? {B} )\" , # 08/14/1991 # WARNING! dateutil set to day first \"mm_dd_YYYY_1\" : r \"(?: {m} / {d} / {Y} )\" , \"mm_dd_YYYY_2\" : r \"(?: {m} \\\\ {d} \\\\ {Y} )\" , \"mm_dd_YYYY_3\" : r \"(?: {m} [-] {d} [-] {Y} )\" , \"mm_dd_YYYY_4\" : r \"(?: {m} {d} {Y} )\" , \"mm_dd_YYYY_5\" : r \"(?: {m} \\. {d} \\. {Y} )\" , \"mm_dd_YYYY_6\" : r \"(?: {mz}{dz}{Y} )\" , # 8/14/91 # WARNING! dateutil set to day first \"mm_dd_yy_1\" : r \"(?: {m} / {d} / {y} )\" , \"mm_dd_yy_2\" : r \"(?: {m} \\\\ {d} \\\\ {y} )\" , \"mm_dd_yy_3\" : r \"(?: {m} [-] {d} [-] {y} )\" , \"mm_dd_yy_4\" : r \"(?: {m} \\. {d} \\. {y} )\" , # 'mm_dd_yy_5': r\"(?:{mz}{dz}{y})\", # too many phone numbers # Aug 14th, 1991 \"mmm_dd_YYYY_1\" : r \"(?: {B} ?/ ? {d}{th} ? ?/ ? {Y} )\" , \"mmm_dd_YYYY_2\" : r \"(?: {B} ? \\\\ ? {d}{th} ? ? \\\\ ? {Y} )\" , \"mmm_dd_YYYY_3\" : r \"(?: {B} ?[-] ? {d}{th} ? ?[ -] ? {Y} )\" , \"mmm_dd_YYYY_4\" : r \"(?: {B} ?[ -]? ? {d}{th} ? ?, ? {Y} )\" , \"mmm_dd_YYYY_5\" : r \"(?: {B} ?\\. ? {d}{th} ? ?\\. ? {Y} )\" , # Aug-14 '91 \"mmm_dd_yy_1\" : r \"(?: {B} ?/ ? {d}{th} ? ?/ ?'? {y} )\" , \"mmm_dd_yy_2\" : r \"(?: {B} ? \\\\ ? {d}{th} ? ? \\\\ ?'? {y} )\" , \"mmm_dd_yy_3\" : r \"(?: {B} ?[-] ? {d}{th} ? ?[-] ?'? {y} )\" , \"mmm_dd_yy_4\" : r \"(?: {B} ?[. -]? ? {d}{th} ?, '? {y} )\" , \"mmm_dd_yy_5\" : r \"(?: {B} ?\\. ? {d}{th} ? ?\\. ?'? {y} )\" , # Aug-14 # WARNING! dateutil assumes current year \"mmm_dd\" : r \"(?: {B} ?[/ \\\\ . -] ? {d}{th} ?)\" , # # Aug-91 # 'mmm_yy': r\"(?:{B} ?[/\\\\. -] ?'{y})\", # too many false positives # August 1991 \"mmm_YYYY\" : r \"(?: {B} ?[/ \\\\ . -] ? {Y} )\" , # many non-useful dates # 1991-8-14 \"YYYY_mm_dd_1\" : r \"(?: {Y} / {m} / {d} )\" , \"YYYY_mm_dd_2\" : r \"(?: {Y} \\\\ {m} \\\\ {d} )\" , \"YYYY_mm_dd_3\" : r \"(?: {Y} [-] {m} [-] {d} )\" , \"YYYY_mm_dd_4\" : r \"(?: {Y} {m} {d} )\" , \"YYYY_mm_dd_5\" : r \"(?: {Y} \\. {m} \\. {d} )\" , \"YYYY_mm_dd_6\" : r \"(?: {Y}{mz}{dz} )\" , # 910814 (ISO 8601) # 'yy_mm_dd_1': r\"(?:{y} {m} {d})\", # too many random numbers \"yy_mm_dd_2\" : r \"(?: {y} / {m} / {d} )\" , \"yy_mm_dd_3\" : r \"(?: {y} \\\\ {m} \\\\ {d} )\" , \"yy_mm_dd_4\" : r \"(?: {y} [-] {m} [-] {d} )\" , \"yy_mm_dd_5\" : r \"(?: {y} \\. {m} \\. {d} )\" , # 'yy_mm_dd_6': r\"(?:{y}{mz}{dz})\", # too many phone numbers # 1991-Aug-14 \"YYYY_mmm_dd_1\" : r \"(?: {Y} ?/ ? {B} ?/ ? {d} )\" , \"YYYY_mmm_dd_2\" : r \"(?: {Y} ? \\\\ ? {B} ? \\\\ ? {d} )\" , \"YYYY_mmm_dd_3\" : r \"(?: {Y} ?[-] ? {B} ?[-] ? {d} )\" , \"YYYY_mmm_dd_4\" : r \"(?: {Y} ? {B} ?[ -]? ? {d}{th} ?)\" , # 91-Aug-14 \"yy_mmm_dd_1\" : r \"(?:'? {y} ?/ ? {B} ?/ ? {d} )\" , \"yy_mmm_dd_2\" : r \"(?:'? {y} ? \\\\ ? {B} ? \\\\ ? {d} )\" , \"yy_mmm_dd_3\" : r \"(?:'? {y} ?[-] ? {B} ?[-] ? {d} )\" , \"yy_mmm_dd_4\" : r \"(?:'? {y} ? {B} ?[ -]? ? {d}{th} ?)\" , # # 1991.226 (Aug 14 = day 226 in 1991) # dateutil fails # 'YYYY_ddd_1': r\"(?:{Y}\\.{j})\", # too many random numbers # 'YYYY_ddd_2': r\"(?:{Y}[-]{j})\", # too many random numbers # time \"HH_MM_SS\" : r \"(?: {H} : {M} : {S} (?: ? {p} )?(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_1\" : r \"(?: {H} : {M} (?: ? {p} )?(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_1b\" : r \"(?: {H} [:. ] {M} (?: ? {p} )(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_2\" : r \"(?:(?<!\\.) {HZ} [. ]? {M} (?: ? {p} )(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_pp\" : r \"(?:(?<!\\.) {H} ? {p2} (?: ?(?:Z| {Z} | {z} ))?)\" , # # 910814094500 (9:45am) # 'yy_mm_dd_HH_MM_SS': r\"(?:{y}{mz}{dz}{H}{M}{S})\", # too many phone numbers # 1991-08-14T09:45:00Z \"YYYY_mm_dd_HH_MM\" : r \"(?: {Y} [-] {m} [-] {d} [T ] {H} : {M} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_mm_dd_HH_MM_SS_1\" : r \"(?: {Y} [-] {m} [-] {d} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_mm_dd_HH_MM_SS_2\" : r \"(?: {Y}{mz}{d} T? {H}{M}{S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_dd_mm_HH_MM_SS_3\" : r \"(?: {Y} [-] {d} [-] {m} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"mm_dd_YYYY_HH_MM_SS_1\" : r \"(?: {m} [-] {d} [-] {Y} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"dd_mm_YYYY_HH_MM_SS_1\" : r \"(?: {d} [-] {m} [-] {Y} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , # # standalone # 'day': r\"{A}\", # too many false positives # 'month': r\"{B}\", # too many false positives # 'year': r\"{Y}\", # too many random numbers # 'timezone': r\"(?:Z|{Z}|{z})\", # too many malay words } # unicode fixes REGEX_FORMATTED = { label : \" \\\\ b\" + pattern . format ( ** REGEX_PARTS ) # fill in the chunks . replace ( \"-]\" , \" \\u2009\\u2010\\u2011\\u2012\\u2013\\u2014 -]\" ) # unicode dashes . replace ( \"'?\" , \"[' \\u2018\\u2019 ]?\" ) # unicode quotes + \" \\\\ b\" for label , pattern in REGEX_PATTERNS_PARSERS . items () } # match emails and urls to avoid returning chunks of them REGEX_FORMATTED [ \"eml\" ] = r \"\"\"[a-zA-Z0-9][^\\s`!@%$^= {} \\[\\]/\\\\\"',()<>:;]+(?:@|%40|\\s+at\\s+|\\s*<\\s*at\\s*>\\s*)[a-zA-Z0-9][-_a-zA-Z0-9~.]+\\.[a-zA-Z]{2,15}\"\"\" REGEX_FORMATTED [ \"url\" ] = r \"\\b(?:(?:https?|ftp|file)://|www\\d?\\.|ftp\\.)[-A-Z0-9+&@#/%=~_|$?!:,.]*[A-Z0-9+&@#/%=~_|$]\" REGEX_FORMATTED [ \"dot\" ] = r \"(?:\\d+\\.){3,}\\d+\" # compile all the regex patterns REGEX_COMPILED = { label : re . compile ( pattern , flags = re . I | re . U ) for label , pattern in REGEX_FORMATTED . items () } if trans_cat == \"dt\" : return idf elif ( trans_cat in [ \"long_c\" , \"bigint_c\" , \"int_c\" ]) & ( int ( val_unique_cat ) in [ 10 , 13 ] ): if int ( val_unique_cat ) == 10 : precision = \"s\" elif int ( val_unique_cat ) == 13 : precision = \"ms\" else : precision = \"ms\" output_df = unix_to_timestamp ( spark , idf , col , precision = precision , tz = tz , output_mode = output_mode ) . orderBy ( id_col , col ) if save_output is not None : output_df . write . parquet ( save_output , mode = \"overwrite\" ) else : return output_df elif trans_cat == \"string\" : list_dates = list ( set ( idf . select ( col ) . rdd . flatMap ( lambda x : x ) . collect ())) def regex_text ( text , longest = True , context_max_len = 999 , dayfirst = False ): # join multiple spaces, convert tabs, strip leading/trailing whitespace if isinstance ( text , str ): pass else : raise ValueError ( \"Incompatible Column Type!!\" ) text = \" \" . join ( text . split ()) matches = [] for regex_label , regex_obj in REGEX_COMPILED . items (): for m in regex_obj . finditer ( text ): context_start = max ( 0 , ( m . start () + m . end () - context_max_len ) // 2 ) context_end = min ( len ( text ), context_start + context_max_len ) context_str = text [ context_start : context_end ] if context_start != 0 : context_str = \" \\u2026 \" + context_str [ 1 :] if context_end != len ( text ): context_str = ( context_str [: - 1 ] + \" \\u2026 \" ) # this is the `...` character parsed_date = None try : with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = dateutil . parser . UnknownTimezoneWarning , ) if \"HH\" in regex_label : if \"dd\" in regex_label or \"YYYY\" in regex_label : matched_text = re . sub ( r \"[ \\\\ ]\" , \"/\" , m . group ()) parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) else : matched_text = re . sub ( r \"H(?:(?:OU)?RS?)?\" , \"\" , m . group (), flags = re . I ) matched_text = re . sub ( r \"MN\" , r \"AM\" , matched_text , flags = re . I ) matched_text = re . sub ( r \"NN\" , r \"PM\" , matched_text , flags = re . I ) matched_text = re . sub ( r \"(\\d)[. ](\\d)\" , r \"\\1:\\2\" , matched_text ) matched_text = f \"1970-01-01 { matched_text } \" parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) elif \"dd\" in regex_label or \"YYYY\" in regex_label : matched_text = re . sub ( r \"[ \\\\ ]\" , \"/\" , m . group ()) parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) except ValueError : pass matches . append ( { \"REGEX_LABEL\" : regex_label , \"MATCH\" : m . group (), \"START\" : m . start (), \"END\" : m . end (), \"MATCH_LEN\" : m . end () - m . start (), \"NORM_TEXT_LEN\" : len ( text ), \"CONTEXT\" : context_str , \"PARSED\" : parsed_date , } ) # narrow to longest match for match in matches : if not longest or all ( ( other [ \"START\" ] >= match [ \"START\" ] and other [ \"END\" ] <= match [ \"END\" ]) or other [ \"START\" ] > match [ \"END\" ] or other [ \"END\" ] < match [ \"START\" ] for other in matches ): # don't return emails or urls if match [ \"REGEX_LABEL\" ] not in { \"eml\" , \"url\" , \"dot\" }: yield match bl = [] file_lines = list_dates for line_num , line in enumerate ( file_lines ): bl_int = [] for match_info in regex_text ( line ): try : ye , mo , da , ho , mi , se = ( match_info [ \"PARSED\" ] . year , match_info [ \"PARSED\" ] . month , match_info [ \"PARSED\" ] . day , match_info [ \"PARSED\" ] . hour , match_info [ \"PARSED\" ] . minute , match_info [ \"PARSED\" ] . second , ) if len ( bl_int ) == 0 : bl_int = [ ye , mo , da , ho , mi , se ] else : if ye == 1970 and mo == 1 and da == 1 : pass if ho + mi + se == 0 : pass if ye > 1970 : bl_int [ 0 ] = ye if mo > 0 and ye != 1970 : bl_int [ 1 ] = mo if da > 0 and ye != 1970 : bl_int [ 2 ] = da if ho > 0 : bl_int [ 3 ] = ho if mi > 0 : bl_int [ 4 ] = mi if se > 0 : bl_int [ 5 ] = se else : pass except : pass bl . append ( [ match_info [ \"CONTEXT\" ], datetime . datetime ( bl_int [ 0 ], bl_int [ 1 ], bl_int [ 2 ], bl_int [ 3 ], bl_int [ 4 ], bl_int [ 5 ], ), ] ) if len ( bl ) >= 1 : columns = [ col , col + \"_ts\" ] # output_df = spark.createDataFrame(spark.parallelize(bl),columns) output_df = spark . createDataFrame ( pd . DataFrame ( bl , columns = columns )) else : return idf elif trans_cat in [ \"string_c\" , \"int_c\" ]: if int ( val_unique_cat ) == 4 : output_df = idf . select ( col ) . withColumn ( col + \"_ts\" , F . col ( col ) . cast ( \"string\" ) . cast ( \"date\" ) ) elif int ( val_unique_cat ) == 6 : output_df = ( idf . select ( col ) . withColumn ( col , F . concat ( col , F . lit ( \"01\" ))) . withColumn ( col + \"_ts\" , F . to_date ( col , \"yyyyMMdd\" )) ) elif int ( val_unique_cat ) == 8 : f = ( idf . select ( F . max ( F . substring ( col , 1 , 4 )), F . max ( F . substring ( col , 5 , 2 )), F . max ( F . substring ( col , 7 , 2 )), ) . rdd . flatMap ( lambda x : x ) . collect () ) if int ( f [ 1 ]) > 12 : frmt = \"yyyyddMM\" elif int ( f [ 2 ]) > 12 : frmt = \"yyyyMMdd\" elif ( ( int ( f [ 0 ]) > 1970 & int ( f [ 0 ]) < 2049 ) & ( int ( f [ 1 ]) > 0 & int ( f [ 1 ]) <= 12 ) & ( int ( f [ 2 ]) > 0 & int ( f [ 2 ]) <= 31 ) ): frmt = \"yyyyMMdd\" elif ( ( int ( f [ 0 ]) > 1970 & int ( f [ 0 ]) < 2049 ) & ( int ( f [ 1 ]) > 0 & int ( f [ 1 ]) <= 31 ) & ( int ( f [ 2 ]) > 0 & int ( f [ 2 ]) <= 12 ) ): frmt = \"yyyyddMM\" else : return idf output_df = idf . select ( F . col ( col ) . cast ( \"string\" )) . withColumn ( col + \"_ts\" , F . to_date ( col , frmt ) ) else : return idf else : return idf # if ((output_df.where(F.col(col + \"_ts\").isNull()).count()) / output_df.count()) > 0.9: # return idf # else: # pass if output_mode == \"replace\" : output_df = ( idf . join ( output_df , col , \"left_outer\" ) . drop ( col ) . withColumnRenamed ( col + \"_ts\" , col ) . orderBy ( id_col , col ) ) elif output_mode == \"append\" : output_df = idf . join ( output_df , col , \"left_outer\" ) . orderBy ( id_col , col + \"_ts\" ) else : return \"Incorrect Output Mode Selected\" if save_output : output_df . write . parquet ( save_output , mode = \"overwrite\" ) else : return output_df def ts_loop_cols_pre ( idf , id_col ): \"\"\" This function helps to analyze the potential columns which can be passed for the time series check. The columns are passed on to the auto-detection block. Parameters ---------- idf Input dataframe id_col ID Column Returns ------- Three lists \"\"\" lc1 , lc2 , lc3 = [], [], [] for i in idf . dtypes : try : col_len = ( idf . select ( F . max ( F . length ( i [ 0 ]))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) except : col_len = 0 if idf . select ( i [ 0 ]) . dropna () . distinct () . count () == 0 : lc1 . append ( i [ 0 ]) lc2 . append ( \"NA\" ) lc3 . append ( col_len ) elif ( ( i [ 0 ] != id_col ) & ( idf . select ( F . length ( i [ 0 ])) . distinct () . count () == 1 ) & ( col_len in [ 4 , 6 , 8 , 10 , 13 ]) ): if i [ 1 ] == \"string\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"string_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"long\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"long_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"bigint\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"bigint_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"int\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"int_c\" ) lc3 . append ( col_len ) elif ( i [ 0 ] != id_col ) & ( i [ 1 ] in [ \"string\" , \"object\" ]): lc1 . append ( i [ 0 ]) lc2 . append ( \"string\" ) lc3 . append ( col_len ) elif ( i [ 0 ] != id_col ) & ( i [ 1 ] in [ \"timestamp\" , \"date\" ]): lc1 . append ( i [ 0 ]) lc2 . append ( \"dt\" ) lc3 . append ( col_len ) else : lc1 . append ( i [ 0 ]) lc2 . append ( \"NA\" ) lc3 . append ( col_len ) return lc1 , lc2 , lc3 def ts_preprocess ( spark , idf , id_col , output_path , tz_offset = \"local\" , run_type = \"local\" , mlflow_config = None , auth_key = \"NA\" , ): \"\"\" This function helps to read the input spark dataframe as source and do all the necessary processing. All the intermediate data created through this step foro the Time Series Analyzer. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column output_path Output path where the data would be saved tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"local\" or \"emr\" or \"databricks\" or \"ak8s\" basis the user flexibility. Default option is set as \"local\". mlflow_config MLflow configuration. If None, all MLflow features are disabled. auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for \"ak8s\" run_type. Returns ------- DataFrame,Output[CSV] \"\"\" if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) ts_loop_col_dtls = ts_loop_cols_pre ( idf , id_col ) l1 , l2 = [], [] for index , i in enumerate ( ts_loop_col_dtls [ 1 ]): if i != \"NA\" : l1 . append ( index ) if i == \"dt\" : l2 . append ( index ) ts_loop_cols = [ ts_loop_col_dtls [ 0 ][ i ] for i in l1 ] pre_exist_ts_cols = [ ts_loop_col_dtls [ 0 ][ i ] for i in l2 ] for i in ts_loop_cols : try : idx = ts_loop_col_dtls [ 0 ] . index ( i ) val_unique_cat = ts_loop_col_dtls [ 2 ][ idx ] trans_cat = ts_loop_col_dtls [ 1 ][ idx ] idf = regex_date_time_parser ( spark , idf , id_col , i , tz_offset , val_unique_cat , trans_cat , save_output = None , output_mode = \"replace\" , ) idf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) except : pass odf = idf . distinct () ts_loop_cols_post = [ x [ 0 ] for x in idf . dtypes if x [ 1 ] in [ \"timestamp\" , \"date\" ]] num_cols = [ x for x in num_cols if x not in [ id_col ] + ts_loop_cols_post ] cat_cols = [ x for x in cat_cols if x not in [ id_col ] + ts_loop_cols_post ] c1 = ts_loop_cols c2 = list ( set ( ts_loop_cols_post ) - set ( pre_exist_ts_cols )) c3 = pre_exist_ts_cols c4 = ts_loop_cols_post f = pd . DataFrame ( [ [ \",\" . join ( idf . columns )], [ \",\" . join ( c1 )], [ \",\" . join ( c2 )], [ \",\" . join ( c3 )], [ \",\" . join ( c4 )], [ \",\" . join ( num_cols )], [ \",\" . join ( cat_cols )], ], columns = [ \"cols\" ], ) f . to_csv ( ends_with ( local_path ) + \"ts_cols_stats.csv\" , index = False ) if mlflow_config is not None : mlflow . log_artifact ( ends_with ( local_path ) + \"ts_cols_stats.csv\" ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( output_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( output_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) return odf Functions def regex_date_time_parser ( spark, idf, id_col, col, tz, val_unique_cat, trans_cat, save_output=None, output_mode='replace') This function helps to produce the transformed output in timestamp (if auto-detected) based on the input data. Parameters spark Spark session idf Input dataframe id_col ID Column col Column passed for Auto detection of Timestamp / date type tz Timezone offset (Option to chose between options like Local, GMT, UTC). Default option is set as \"Local\". val_unique_cat Maximum character length of the field. trans_cat Custom data type basis which further processing will be conditioned. save_output Output path where the transformed ddata can be saved output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name Returns DataFrame Expand source code def regex_date_time_parser ( spark , idf , id_col , col , tz , val_unique_cat , trans_cat , save_output = None , output_mode = \"replace\" , ): \"\"\" This function helps to produce the transformed output in timestamp (if auto-detected) based on the input data. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column col Column passed for Auto detection of Timestamp / date type tz Timezone offset (Option to chose between options like Local, GMT, UTC). Default option is set as \"Local\". val_unique_cat Maximum character length of the field. trans_cat Custom data type basis which further processing will be conditioned. save_output Output path where the transformed ddata can be saved output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name Returns ------- DataFrame \"\"\" REGEX_PARTS = { \"Y\" : r \"(?:19[4-9]\\d|20[0-3]\\d)\" , # 1940 to 2039 \"y\" : r \"(?:\\d\\d)\" , # 00 to 99 \"m\" : r \"(?:1[012]|0?[1-9])\" , # 0?1 to 12 \"mz\" : r \"(?:1[012]|0[1-9])\" , # 01 to 12 \"B\" : r \"(?:\" r \"D?JAN(?:UAR[IY])?|\" r \"[FP]EB(?:RUAR[IY])?|\" r \"MAC|MAR(?:CH|ET)?|MRT|\" r \"APR(?:IL)?|\" r \"M[EA]I|MAY|\" r \"JUNE?|D?JUNI?|\" r \"JUL(?:Y|AI)?|D?JULI?|\" r \"OG(?:OS)?|AUG(?:UST)?|AGT?(?:USTUS)?|\" r \"SEP(?:T(?:EMBER)?)?|\" r \"O[KC]T(?:OBER)?|\" r \"NO[VP](?:EMBER)?|\" r \"D[EI][SC](?:EMBER)?\" r \")\" , \"d\" : r \"(?:3[01]|[12]\\d|0?[1-9])\" , # 0?1 to 31 \"d_range\" : r \"(?:3[01]|[12]\\d|0?[1-9])(?: ?[-] ?(?:3[01]|[12]\\d|0?[1-9]))?\" , # 14-15 \"dz\" : r \"(?:3[01]|[12]\\d|0[1-9])\" , # 01 to 31 \"j\" : r \"(?:36[0-6]|3[0-5]\\d|[12]\\d\\d|0?[1-9]\\d|0?0?[1-9])\" , # 0?0?1 to 366 \"H\" : r \"(?:2[0-4]|[01]?\\d)\" , # 0?0 to 24 \"HZ\" : r \"(?:2[0-4]|[01]\\d)\" , # 0?0 to 24 \"I\" : r \"(?:1[012]|0?[1-9])\" , # 0?1 to 12 \"M\" : r \"(?:[1-5]\\d|0\\d)\" , # 00 to 59 \"S\" : r \"(?:6[01]|[0-5]\\d)\" , # 00 to 61 (leap second) \"p\" : r \"(?:MIDNI(?:GHT|TE)|AFTERNOON|MORNING|NOON|[MN]N|H(?:(?:OU)?RS?)?|[AP]\\.? ?M\\.?)\" , \"p2\" : r \"(?:MIDNI(?:GHT|TE)|NOON|[AP]\\.? ?M\\.?)\" , \"Z\" : r \"(?:A(?:C(?:DT|ST|T|WST)|DT|E(?:DT|ST)|FT|K(?:DT|ST)|M(?:ST|T)|RT|ST|WST\" r \"|Z(?:O(?:ST|T)|T))|B(?:DT|I(?:OT|T)|OT|R(?:ST|T)|ST|TT)|C(?:AT|CT|DT|E(\" r \"?:ST|T)|H(?:A(?:DT|ST)|O(?:ST|T)|ST|UT)|I(?:ST|T)|KT|L(?:ST|T)|O(?:ST|T\" r \")|ST|T|VT|WST|XT)|D(?:AVT|DUT|FT)|E(?:A(?:S(?:ST|T)|T)|CT|DT|E(?:ST|T)|\" r \"G(?:ST|T)|IT|ST)|F(?:ET|JT|K(?:ST|T)|NT)|G(?:A(?:LT|MT)|ET|FT|I(?:LT|T)\" r \"|MT|ST|YT)|H(?:AEC|DT|KT|MT|OV(?:ST|T)|ST)|I(?:CT|D(?:LW|T)|OT|R(?:DT|K\" r \"T|ST)|ST)|JST|K(?:ALT|GT|OST|RAT|ST)|L(?:HST|INT)|M(?:A(?:GT|RT|WT)|DT|\" r \"E(?:ST|T)|HT|I(?:ST|T)|MT|S(?:K|T)|UT|VT|YT)|N(?:CT|DT|FT|PT|ST|T|UT|Z(\" r \"?:DT|ST))|O(?:MST|RAT)|P(?:DT|ET(?:T)?|GT|H(?:OT|T)|KT|M(?:DT|ST)|ONT|S\" r \"T|Y(?:ST|T))|R(?:ET|OTT)|S(?:A(?:KT|MT|ST)|BT|CT|DT|GT|LST|R(?:ET|T)|ST\" r \"|YOT)|T(?:AHT|FT|HA|JT|KT|LT|MT|OT|RT|VT)|U(?:LA(?:ST|T)|TC|Y(?:ST|T)|Z\" r \"T)|V(?:ET|LAT|O(?:LT|ST)|UT)|W(?:A(?:KT|ST|T)|E(?:ST|T)|IT|ST)|Y(?:AKT|\" r \"EKT))\" , # FROM: en.wikipedia.org/wiki/List_of_time_zone_abbreviations \"z\" : r \"(?:[+-](?:0\\d|1[0-4]):?(?:00|15|30|45))\" , # [+-] 00:00 to 14:45 \"A\" : r \"(?:\" r \"MON(?:DAY)?|(?:IS|SE)N(?:[IE]N)?|\" r \"TUE(?:S(?:DAY)?)?|SEL(?:ASA)?|\" r \"WED(?:NESDAY)?|RABU?|\" r \"THU(?:RS(?:DAY)?)?|KH?A(?:M(?:IS)?)?|\" r \"FRI(?:DAY)?|JUM(?:[AM]A?T)?|\" r \"SAT(?:URDAY)?|SAB(?:TU)?|\" r \"SUN(?:DAY)?|AHA?D|MIN(?:GGU)?\" r \")\" , \"th\" : r \"(?:ST|ND|RD|TH|\u00ba)\" , } REGEX_PATTERNS_PARSERS = { # 14/8/1991 \"dd_mm_YYYY_1\" : r \"(?: {d} / {m} / {Y} )\" , \"dd_d2\" : r \"(?: {d} \\\\ {m} \\\\ {Y} )\" , \"dd_mm_YYYY_3\" : r \"(?: {d} [-] {m} [-] {Y} )\" , \"dd_mm_YYYY_4\" : r \"(?: {d} \\. {m} \\. {Y} )\" , # 'dd_mm_YYYY_5': r\"(?:{d}{m}{Y})\", # too many phone numbers \"dd_mm_YYYY_6\" : r \"(?: {d} ? {m} ? {Y} )\" , \"dd_mm_YYYY_7\" : r \"(?: {dz}{mz}{Y} )\" , # 14/8/91 \"dd_mm_yy_1\" : r \"(?: {d} / {m} / {y} )\" , \"dd_mm_yy_2\" : r \"(?: {d} \\\\ {m} \\\\ {y} )\" , \"dd_mm_yy_3\" : r \"(?: {d} [-] {m} [-] {y} )\" , \"dd_mm_yy_4\" : r \"(?: {d} \\. {m} \\. {y} )\" , # 'dd_mm_yy_5': r\"(?:{dz}{mz}{y})\", # too many phone numbers # 14 Aug, 1991 \"dd_mmm_YYYY_1\" : r \"(?: {d}{th} ? ?/ ? {B} ?/ ? {Y} )\" , \"dd_mmm_YYYY_2\" : r \"(?: {d}{th} ? ? \\\\ ? {B} ? \\\\ ? {Y} )\" , \"dd_mmm_YYYY_3\" : r \"(?: {d}{th} ? ?[-] ? {B} ?[ -] ? {Y} )\" , \"dd_mmm_YYYY_4\" : r \"(?: {d}{th} ? ?[ -]? ? {B} ?,? ? {Y} )\" , \"dd_mmm_YYYY_5\" : r \"(?: {d}{th} ? ?\\. ? {B} ?\\. ? {Y} )\" , # 14 Aug '91 \"dd_mmm_yy_1\" : r \"(?: {d}{th} ? ?/ ? {B} ?/ ?'? {y} )\" , \"dd_mmm_yy_2\" : r \"(?: {d}{th} ? ? \\\\ ? {B} ? \\\\ ?'? {y} )\" , \"dd_mmm_yy_3\" : r \"(?: {d}{th} ? ?[-] ? {B} ?[-] ?'? {y} )\" , \"dd_mmm_yy_4\" : r \"(?: {d}{th} ? ?[ -]? ? {B} ?,? ?'? {y} )\" , \"dd_mmm_yy_5\" : r \"(?: {d}{th} ? ?\\. ? {B} ?\\. ?'? {y} )\" , # 14th Aug \"dd_mmm\" : r \"(?: {d}{th} ? ?[/ \\\\ . -] ? {B} )\" , # 08/14/1991 # WARNING! dateutil set to day first \"mm_dd_YYYY_1\" : r \"(?: {m} / {d} / {Y} )\" , \"mm_dd_YYYY_2\" : r \"(?: {m} \\\\ {d} \\\\ {Y} )\" , \"mm_dd_YYYY_3\" : r \"(?: {m} [-] {d} [-] {Y} )\" , \"mm_dd_YYYY_4\" : r \"(?: {m} {d} {Y} )\" , \"mm_dd_YYYY_5\" : r \"(?: {m} \\. {d} \\. {Y} )\" , \"mm_dd_YYYY_6\" : r \"(?: {mz}{dz}{Y} )\" , # 8/14/91 # WARNING! dateutil set to day first \"mm_dd_yy_1\" : r \"(?: {m} / {d} / {y} )\" , \"mm_dd_yy_2\" : r \"(?: {m} \\\\ {d} \\\\ {y} )\" , \"mm_dd_yy_3\" : r \"(?: {m} [-] {d} [-] {y} )\" , \"mm_dd_yy_4\" : r \"(?: {m} \\. {d} \\. {y} )\" , # 'mm_dd_yy_5': r\"(?:{mz}{dz}{y})\", # too many phone numbers # Aug 14th, 1991 \"mmm_dd_YYYY_1\" : r \"(?: {B} ?/ ? {d}{th} ? ?/ ? {Y} )\" , \"mmm_dd_YYYY_2\" : r \"(?: {B} ? \\\\ ? {d}{th} ? ? \\\\ ? {Y} )\" , \"mmm_dd_YYYY_3\" : r \"(?: {B} ?[-] ? {d}{th} ? ?[ -] ? {Y} )\" , \"mmm_dd_YYYY_4\" : r \"(?: {B} ?[ -]? ? {d}{th} ? ?, ? {Y} )\" , \"mmm_dd_YYYY_5\" : r \"(?: {B} ?\\. ? {d}{th} ? ?\\. ? {Y} )\" , # Aug-14 '91 \"mmm_dd_yy_1\" : r \"(?: {B} ?/ ? {d}{th} ? ?/ ?'? {y} )\" , \"mmm_dd_yy_2\" : r \"(?: {B} ? \\\\ ? {d}{th} ? ? \\\\ ?'? {y} )\" , \"mmm_dd_yy_3\" : r \"(?: {B} ?[-] ? {d}{th} ? ?[-] ?'? {y} )\" , \"mmm_dd_yy_4\" : r \"(?: {B} ?[. -]? ? {d}{th} ?, '? {y} )\" , \"mmm_dd_yy_5\" : r \"(?: {B} ?\\. ? {d}{th} ? ?\\. ?'? {y} )\" , # Aug-14 # WARNING! dateutil assumes current year \"mmm_dd\" : r \"(?: {B} ?[/ \\\\ . -] ? {d}{th} ?)\" , # # Aug-91 # 'mmm_yy': r\"(?:{B} ?[/\\\\. -] ?'{y})\", # too many false positives # August 1991 \"mmm_YYYY\" : r \"(?: {B} ?[/ \\\\ . -] ? {Y} )\" , # many non-useful dates # 1991-8-14 \"YYYY_mm_dd_1\" : r \"(?: {Y} / {m} / {d} )\" , \"YYYY_mm_dd_2\" : r \"(?: {Y} \\\\ {m} \\\\ {d} )\" , \"YYYY_mm_dd_3\" : r \"(?: {Y} [-] {m} [-] {d} )\" , \"YYYY_mm_dd_4\" : r \"(?: {Y} {m} {d} )\" , \"YYYY_mm_dd_5\" : r \"(?: {Y} \\. {m} \\. {d} )\" , \"YYYY_mm_dd_6\" : r \"(?: {Y}{mz}{dz} )\" , # 910814 (ISO 8601) # 'yy_mm_dd_1': r\"(?:{y} {m} {d})\", # too many random numbers \"yy_mm_dd_2\" : r \"(?: {y} / {m} / {d} )\" , \"yy_mm_dd_3\" : r \"(?: {y} \\\\ {m} \\\\ {d} )\" , \"yy_mm_dd_4\" : r \"(?: {y} [-] {m} [-] {d} )\" , \"yy_mm_dd_5\" : r \"(?: {y} \\. {m} \\. {d} )\" , # 'yy_mm_dd_6': r\"(?:{y}{mz}{dz})\", # too many phone numbers # 1991-Aug-14 \"YYYY_mmm_dd_1\" : r \"(?: {Y} ?/ ? {B} ?/ ? {d} )\" , \"YYYY_mmm_dd_2\" : r \"(?: {Y} ? \\\\ ? {B} ? \\\\ ? {d} )\" , \"YYYY_mmm_dd_3\" : r \"(?: {Y} ?[-] ? {B} ?[-] ? {d} )\" , \"YYYY_mmm_dd_4\" : r \"(?: {Y} ? {B} ?[ -]? ? {d}{th} ?)\" , # 91-Aug-14 \"yy_mmm_dd_1\" : r \"(?:'? {y} ?/ ? {B} ?/ ? {d} )\" , \"yy_mmm_dd_2\" : r \"(?:'? {y} ? \\\\ ? {B} ? \\\\ ? {d} )\" , \"yy_mmm_dd_3\" : r \"(?:'? {y} ?[-] ? {B} ?[-] ? {d} )\" , \"yy_mmm_dd_4\" : r \"(?:'? {y} ? {B} ?[ -]? ? {d}{th} ?)\" , # # 1991.226 (Aug 14 = day 226 in 1991) # dateutil fails # 'YYYY_ddd_1': r\"(?:{Y}\\.{j})\", # too many random numbers # 'YYYY_ddd_2': r\"(?:{Y}[-]{j})\", # too many random numbers # time \"HH_MM_SS\" : r \"(?: {H} : {M} : {S} (?: ? {p} )?(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_1\" : r \"(?: {H} : {M} (?: ? {p} )?(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_1b\" : r \"(?: {H} [:. ] {M} (?: ? {p} )(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_2\" : r \"(?:(?<!\\.) {HZ} [. ]? {M} (?: ? {p} )(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_pp\" : r \"(?:(?<!\\.) {H} ? {p2} (?: ?(?:Z| {Z} | {z} ))?)\" , # # 910814094500 (9:45am) # 'yy_mm_dd_HH_MM_SS': r\"(?:{y}{mz}{dz}{H}{M}{S})\", # too many phone numbers # 1991-08-14T09:45:00Z \"YYYY_mm_dd_HH_MM\" : r \"(?: {Y} [-] {m} [-] {d} [T ] {H} : {M} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_mm_dd_HH_MM_SS_1\" : r \"(?: {Y} [-] {m} [-] {d} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_mm_dd_HH_MM_SS_2\" : r \"(?: {Y}{mz}{d} T? {H}{M}{S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_dd_mm_HH_MM_SS_3\" : r \"(?: {Y} [-] {d} [-] {m} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"mm_dd_YYYY_HH_MM_SS_1\" : r \"(?: {m} [-] {d} [-] {Y} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"dd_mm_YYYY_HH_MM_SS_1\" : r \"(?: {d} [-] {m} [-] {Y} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , # # standalone # 'day': r\"{A}\", # too many false positives # 'month': r\"{B}\", # too many false positives # 'year': r\"{Y}\", # too many random numbers # 'timezone': r\"(?:Z|{Z}|{z})\", # too many malay words } # unicode fixes REGEX_FORMATTED = { label : \" \\\\ b\" + pattern . format ( ** REGEX_PARTS ) # fill in the chunks . replace ( \"-]\" , \" \\u2009\\u2010\\u2011\\u2012\\u2013\\u2014 -]\" ) # unicode dashes . replace ( \"'?\" , \"[' \\u2018\\u2019 ]?\" ) # unicode quotes + \" \\\\ b\" for label , pattern in REGEX_PATTERNS_PARSERS . items () } # match emails and urls to avoid returning chunks of them REGEX_FORMATTED [ \"eml\" ] = r \"\"\"[a-zA-Z0-9][^\\s`!@%$^= {} \\[\\]/\\\\\"',()<>:;]+(?:@|%40|\\s+at\\s+|\\s*<\\s*at\\s*>\\s*)[a-zA-Z0-9][-_a-zA-Z0-9~.]+\\.[a-zA-Z]{2,15}\"\"\" REGEX_FORMATTED [ \"url\" ] = r \"\\b(?:(?:https?|ftp|file)://|www\\d?\\.|ftp\\.)[-A-Z0-9+&@#/%=~_|$?!:,.]*[A-Z0-9+&@#/%=~_|$]\" REGEX_FORMATTED [ \"dot\" ] = r \"(?:\\d+\\.){3,}\\d+\" # compile all the regex patterns REGEX_COMPILED = { label : re . compile ( pattern , flags = re . I | re . U ) for label , pattern in REGEX_FORMATTED . items () } if trans_cat == \"dt\" : return idf elif ( trans_cat in [ \"long_c\" , \"bigint_c\" , \"int_c\" ]) & ( int ( val_unique_cat ) in [ 10 , 13 ] ): if int ( val_unique_cat ) == 10 : precision = \"s\" elif int ( val_unique_cat ) == 13 : precision = \"ms\" else : precision = \"ms\" output_df = unix_to_timestamp ( spark , idf , col , precision = precision , tz = tz , output_mode = output_mode ) . orderBy ( id_col , col ) if save_output is not None : output_df . write . parquet ( save_output , mode = \"overwrite\" ) else : return output_df elif trans_cat == \"string\" : list_dates = list ( set ( idf . select ( col ) . rdd . flatMap ( lambda x : x ) . collect ())) def regex_text ( text , longest = True , context_max_len = 999 , dayfirst = False ): # join multiple spaces, convert tabs, strip leading/trailing whitespace if isinstance ( text , str ): pass else : raise ValueError ( \"Incompatible Column Type!!\" ) text = \" \" . join ( text . split ()) matches = [] for regex_label , regex_obj in REGEX_COMPILED . items (): for m in regex_obj . finditer ( text ): context_start = max ( 0 , ( m . start () + m . end () - context_max_len ) // 2 ) context_end = min ( len ( text ), context_start + context_max_len ) context_str = text [ context_start : context_end ] if context_start != 0 : context_str = \" \\u2026 \" + context_str [ 1 :] if context_end != len ( text ): context_str = ( context_str [: - 1 ] + \" \\u2026 \" ) # this is the `...` character parsed_date = None try : with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = dateutil . parser . UnknownTimezoneWarning , ) if \"HH\" in regex_label : if \"dd\" in regex_label or \"YYYY\" in regex_label : matched_text = re . sub ( r \"[ \\\\ ]\" , \"/\" , m . group ()) parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) else : matched_text = re . sub ( r \"H(?:(?:OU)?RS?)?\" , \"\" , m . group (), flags = re . I ) matched_text = re . sub ( r \"MN\" , r \"AM\" , matched_text , flags = re . I ) matched_text = re . sub ( r \"NN\" , r \"PM\" , matched_text , flags = re . I ) matched_text = re . sub ( r \"(\\d)[. ](\\d)\" , r \"\\1:\\2\" , matched_text ) matched_text = f \"1970-01-01 { matched_text } \" parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) elif \"dd\" in regex_label or \"YYYY\" in regex_label : matched_text = re . sub ( r \"[ \\\\ ]\" , \"/\" , m . group ()) parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) except ValueError : pass matches . append ( { \"REGEX_LABEL\" : regex_label , \"MATCH\" : m . group (), \"START\" : m . start (), \"END\" : m . end (), \"MATCH_LEN\" : m . end () - m . start (), \"NORM_TEXT_LEN\" : len ( text ), \"CONTEXT\" : context_str , \"PARSED\" : parsed_date , } ) # narrow to longest match for match in matches : if not longest or all ( ( other [ \"START\" ] >= match [ \"START\" ] and other [ \"END\" ] <= match [ \"END\" ]) or other [ \"START\" ] > match [ \"END\" ] or other [ \"END\" ] < match [ \"START\" ] for other in matches ): # don't return emails or urls if match [ \"REGEX_LABEL\" ] not in { \"eml\" , \"url\" , \"dot\" }: yield match bl = [] file_lines = list_dates for line_num , line in enumerate ( file_lines ): bl_int = [] for match_info in regex_text ( line ): try : ye , mo , da , ho , mi , se = ( match_info [ \"PARSED\" ] . year , match_info [ \"PARSED\" ] . month , match_info [ \"PARSED\" ] . day , match_info [ \"PARSED\" ] . hour , match_info [ \"PARSED\" ] . minute , match_info [ \"PARSED\" ] . second , ) if len ( bl_int ) == 0 : bl_int = [ ye , mo , da , ho , mi , se ] else : if ye == 1970 and mo == 1 and da == 1 : pass if ho + mi + se == 0 : pass if ye > 1970 : bl_int [ 0 ] = ye if mo > 0 and ye != 1970 : bl_int [ 1 ] = mo if da > 0 and ye != 1970 : bl_int [ 2 ] = da if ho > 0 : bl_int [ 3 ] = ho if mi > 0 : bl_int [ 4 ] = mi if se > 0 : bl_int [ 5 ] = se else : pass except : pass bl . append ( [ match_info [ \"CONTEXT\" ], datetime . datetime ( bl_int [ 0 ], bl_int [ 1 ], bl_int [ 2 ], bl_int [ 3 ], bl_int [ 4 ], bl_int [ 5 ], ), ] ) if len ( bl ) >= 1 : columns = [ col , col + \"_ts\" ] # output_df = spark.createDataFrame(spark.parallelize(bl),columns) output_df = spark . createDataFrame ( pd . DataFrame ( bl , columns = columns )) else : return idf elif trans_cat in [ \"string_c\" , \"int_c\" ]: if int ( val_unique_cat ) == 4 : output_df = idf . select ( col ) . withColumn ( col + \"_ts\" , F . col ( col ) . cast ( \"string\" ) . cast ( \"date\" ) ) elif int ( val_unique_cat ) == 6 : output_df = ( idf . select ( col ) . withColumn ( col , F . concat ( col , F . lit ( \"01\" ))) . withColumn ( col + \"_ts\" , F . to_date ( col , \"yyyyMMdd\" )) ) elif int ( val_unique_cat ) == 8 : f = ( idf . select ( F . max ( F . substring ( col , 1 , 4 )), F . max ( F . substring ( col , 5 , 2 )), F . max ( F . substring ( col , 7 , 2 )), ) . rdd . flatMap ( lambda x : x ) . collect () ) if int ( f [ 1 ]) > 12 : frmt = \"yyyyddMM\" elif int ( f [ 2 ]) > 12 : frmt = \"yyyyMMdd\" elif ( ( int ( f [ 0 ]) > 1970 & int ( f [ 0 ]) < 2049 ) & ( int ( f [ 1 ]) > 0 & int ( f [ 1 ]) <= 12 ) & ( int ( f [ 2 ]) > 0 & int ( f [ 2 ]) <= 31 ) ): frmt = \"yyyyMMdd\" elif ( ( int ( f [ 0 ]) > 1970 & int ( f [ 0 ]) < 2049 ) & ( int ( f [ 1 ]) > 0 & int ( f [ 1 ]) <= 31 ) & ( int ( f [ 2 ]) > 0 & int ( f [ 2 ]) <= 12 ) ): frmt = \"yyyyddMM\" else : return idf output_df = idf . select ( F . col ( col ) . cast ( \"string\" )) . withColumn ( col + \"_ts\" , F . to_date ( col , frmt ) ) else : return idf else : return idf # if ((output_df.where(F.col(col + \"_ts\").isNull()).count()) / output_df.count()) > 0.9: # return idf # else: # pass if output_mode == \"replace\" : output_df = ( idf . join ( output_df , col , \"left_outer\" ) . drop ( col ) . withColumnRenamed ( col + \"_ts\" , col ) . orderBy ( id_col , col ) ) elif output_mode == \"append\" : output_df = idf . join ( output_df , col , \"left_outer\" ) . orderBy ( id_col , col + \"_ts\" ) else : return \"Incorrect Output Mode Selected\" if save_output : output_df . write . parquet ( save_output , mode = \"overwrite\" ) else : return output_df def ts_loop_cols_pre ( idf, id_col) This function helps to analyze the potential columns which can be passed for the time series check. The columns are passed on to the auto-detection block. Parameters idf Input dataframe id_col ID Column Returns Three lists Expand source code def ts_loop_cols_pre ( idf , id_col ): \"\"\" This function helps to analyze the potential columns which can be passed for the time series check. The columns are passed on to the auto-detection block. Parameters ---------- idf Input dataframe id_col ID Column Returns ------- Three lists \"\"\" lc1 , lc2 , lc3 = [], [], [] for i in idf . dtypes : try : col_len = ( idf . select ( F . max ( F . length ( i [ 0 ]))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) except : col_len = 0 if idf . select ( i [ 0 ]) . dropna () . distinct () . count () == 0 : lc1 . append ( i [ 0 ]) lc2 . append ( \"NA\" ) lc3 . append ( col_len ) elif ( ( i [ 0 ] != id_col ) & ( idf . select ( F . length ( i [ 0 ])) . distinct () . count () == 1 ) & ( col_len in [ 4 , 6 , 8 , 10 , 13 ]) ): if i [ 1 ] == \"string\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"string_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"long\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"long_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"bigint\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"bigint_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"int\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"int_c\" ) lc3 . append ( col_len ) elif ( i [ 0 ] != id_col ) & ( i [ 1 ] in [ \"string\" , \"object\" ]): lc1 . append ( i [ 0 ]) lc2 . append ( \"string\" ) lc3 . append ( col_len ) elif ( i [ 0 ] != id_col ) & ( i [ 1 ] in [ \"timestamp\" , \"date\" ]): lc1 . append ( i [ 0 ]) lc2 . append ( \"dt\" ) lc3 . append ( col_len ) else : lc1 . append ( i [ 0 ]) lc2 . append ( \"NA\" ) lc3 . append ( col_len ) return lc1 , lc2 , lc3 def ts_preprocess ( spark, idf, id_col, output_path, tz_offset='local', run_type='local', mlflow_config=None, auth_key='NA') This function helps to read the input spark dataframe as source and do all the necessary processing. All the intermediate data created through this step foro the Time Series Analyzer. Parameters spark Spark session idf Input dataframe id_col ID Column output_path Output path where the data would be saved tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"local\" or \"emr\" or \"databricks\" or \"ak8s\" basis the user flexibility. Default option is set as \"local\". mlflow_config MLflow configuration. If None, all MLflow features are disabled. auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for \"ak8s\" run_type. Returns DataFrame,Output[CSV] Expand source code def ts_preprocess ( spark , idf , id_col , output_path , tz_offset = \"local\" , run_type = \"local\" , mlflow_config = None , auth_key = \"NA\" , ): \"\"\" This function helps to read the input spark dataframe as source and do all the necessary processing. All the intermediate data created through this step foro the Time Series Analyzer. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column output_path Output path where the data would be saved tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"local\" or \"emr\" or \"databricks\" or \"ak8s\" basis the user flexibility. Default option is set as \"local\". mlflow_config MLflow configuration. If None, all MLflow features are disabled. auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for \"ak8s\" run_type. Returns ------- DataFrame,Output[CSV] \"\"\" if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) ts_loop_col_dtls = ts_loop_cols_pre ( idf , id_col ) l1 , l2 = [], [] for index , i in enumerate ( ts_loop_col_dtls [ 1 ]): if i != \"NA\" : l1 . append ( index ) if i == \"dt\" : l2 . append ( index ) ts_loop_cols = [ ts_loop_col_dtls [ 0 ][ i ] for i in l1 ] pre_exist_ts_cols = [ ts_loop_col_dtls [ 0 ][ i ] for i in l2 ] for i in ts_loop_cols : try : idx = ts_loop_col_dtls [ 0 ] . index ( i ) val_unique_cat = ts_loop_col_dtls [ 2 ][ idx ] trans_cat = ts_loop_col_dtls [ 1 ][ idx ] idf = regex_date_time_parser ( spark , idf , id_col , i , tz_offset , val_unique_cat , trans_cat , save_output = None , output_mode = \"replace\" , ) idf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) except : pass odf = idf . distinct () ts_loop_cols_post = [ x [ 0 ] for x in idf . dtypes if x [ 1 ] in [ \"timestamp\" , \"date\" ]] num_cols = [ x for x in num_cols if x not in [ id_col ] + ts_loop_cols_post ] cat_cols = [ x for x in cat_cols if x not in [ id_col ] + ts_loop_cols_post ] c1 = ts_loop_cols c2 = list ( set ( ts_loop_cols_post ) - set ( pre_exist_ts_cols )) c3 = pre_exist_ts_cols c4 = ts_loop_cols_post f = pd . DataFrame ( [ [ \",\" . join ( idf . columns )], [ \",\" . join ( c1 )], [ \",\" . join ( c2 )], [ \",\" . join ( c3 )], [ \",\" . join ( c4 )], [ \",\" . join ( num_cols )], [ \",\" . join ( cat_cols )], ], columns = [ \"cols\" ], ) f . to_csv ( ends_with ( local_path ) + \"ts_cols_stats.csv\" , index = False ) if mlflow_config is not None : mlflow . log_artifact ( ends_with ( local_path ) + \"ts_cols_stats.csv\" ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( output_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( output_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) return odf","title":"<code>ts_auto_detection</code>"},{"location":"api/data_ingest/ts_auto_detection.html#ts_auto_detection","text":"This module help produce the output containing a transformation through auto timestamp / date detection by reading the ingested dataframe from source. As a part of generation of the auto detection output, there are various functions created such as - regex_date_time_parser ts_loop_cols_pre ts_preprocess Respective functions have sections containing the detailed definition of the parameters used for computing. Expand source code # coding=utf-8 \"\"\"This module help produce the output containing a transformation through auto timestamp / date detection by reading the ingested dataframe from source. As a part of generation of the auto detection output, there are various functions created such as - - regex_date_time_parser - ts_loop_cols_pre - ts_preprocess Respective functions have sections containing the detailed definition of the parameters used for computing. \"\"\" import calendar import csv import datetime import io import os import re import subprocess import warnings from pathlib import Path import dateutil.parser import mlflow import numpy as np import pandas as pd import pyspark from loguru import logger from pyspark.sql import Window from pyspark.sql import functions as F from pyspark.sql import types as T from anovos.data_analyzer.stats_generator import measures_of_percentiles from anovos.data_transformer.datetime import ( lagged_ts , timeUnits_extraction , unix_to_timestamp , ) from anovos.shared.utils import ( attributeType_segregation , ends_with , output_to_local , path_ak8s_modify , ) ###regex based ts parser function def regex_date_time_parser ( spark , idf , id_col , col , tz , val_unique_cat , trans_cat , save_output = None , output_mode = \"replace\" , ): \"\"\" This function helps to produce the transformed output in timestamp (if auto-detected) based on the input data. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column col Column passed for Auto detection of Timestamp / date type tz Timezone offset (Option to chose between options like Local, GMT, UTC). Default option is set as \"Local\". val_unique_cat Maximum character length of the field. trans_cat Custom data type basis which further processing will be conditioned. save_output Output path where the transformed ddata can be saved output_mode Option to choose between Append or Replace. If the option Append is selected, the column names are Appended by \"_ts\" else it's replaced by the original column name Returns ------- DataFrame \"\"\" REGEX_PARTS = { \"Y\" : r \"(?:19[4-9]\\d|20[0-3]\\d)\" , # 1940 to 2039 \"y\" : r \"(?:\\d\\d)\" , # 00 to 99 \"m\" : r \"(?:1[012]|0?[1-9])\" , # 0?1 to 12 \"mz\" : r \"(?:1[012]|0[1-9])\" , # 01 to 12 \"B\" : r \"(?:\" r \"D?JAN(?:UAR[IY])?|\" r \"[FP]EB(?:RUAR[IY])?|\" r \"MAC|MAR(?:CH|ET)?|MRT|\" r \"APR(?:IL)?|\" r \"M[EA]I|MAY|\" r \"JUNE?|D?JUNI?|\" r \"JUL(?:Y|AI)?|D?JULI?|\" r \"OG(?:OS)?|AUG(?:UST)?|AGT?(?:USTUS)?|\" r \"SEP(?:T(?:EMBER)?)?|\" r \"O[KC]T(?:OBER)?|\" r \"NO[VP](?:EMBER)?|\" r \"D[EI][SC](?:EMBER)?\" r \")\" , \"d\" : r \"(?:3[01]|[12]\\d|0?[1-9])\" , # 0?1 to 31 \"d_range\" : r \"(?:3[01]|[12]\\d|0?[1-9])(?: ?[-] ?(?:3[01]|[12]\\d|0?[1-9]))?\" , # 14-15 \"dz\" : r \"(?:3[01]|[12]\\d|0[1-9])\" , # 01 to 31 \"j\" : r \"(?:36[0-6]|3[0-5]\\d|[12]\\d\\d|0?[1-9]\\d|0?0?[1-9])\" , # 0?0?1 to 366 \"H\" : r \"(?:2[0-4]|[01]?\\d)\" , # 0?0 to 24 \"HZ\" : r \"(?:2[0-4]|[01]\\d)\" , # 0?0 to 24 \"I\" : r \"(?:1[012]|0?[1-9])\" , # 0?1 to 12 \"M\" : r \"(?:[1-5]\\d|0\\d)\" , # 00 to 59 \"S\" : r \"(?:6[01]|[0-5]\\d)\" , # 00 to 61 (leap second) \"p\" : r \"(?:MIDNI(?:GHT|TE)|AFTERNOON|MORNING|NOON|[MN]N|H(?:(?:OU)?RS?)?|[AP]\\.? ?M\\.?)\" , \"p2\" : r \"(?:MIDNI(?:GHT|TE)|NOON|[AP]\\.? ?M\\.?)\" , \"Z\" : r \"(?:A(?:C(?:DT|ST|T|WST)|DT|E(?:DT|ST)|FT|K(?:DT|ST)|M(?:ST|T)|RT|ST|WST\" r \"|Z(?:O(?:ST|T)|T))|B(?:DT|I(?:OT|T)|OT|R(?:ST|T)|ST|TT)|C(?:AT|CT|DT|E(\" r \"?:ST|T)|H(?:A(?:DT|ST)|O(?:ST|T)|ST|UT)|I(?:ST|T)|KT|L(?:ST|T)|O(?:ST|T\" r \")|ST|T|VT|WST|XT)|D(?:AVT|DUT|FT)|E(?:A(?:S(?:ST|T)|T)|CT|DT|E(?:ST|T)|\" r \"G(?:ST|T)|IT|ST)|F(?:ET|JT|K(?:ST|T)|NT)|G(?:A(?:LT|MT)|ET|FT|I(?:LT|T)\" r \"|MT|ST|YT)|H(?:AEC|DT|KT|MT|OV(?:ST|T)|ST)|I(?:CT|D(?:LW|T)|OT|R(?:DT|K\" r \"T|ST)|ST)|JST|K(?:ALT|GT|OST|RAT|ST)|L(?:HST|INT)|M(?:A(?:GT|RT|WT)|DT|\" r \"E(?:ST|T)|HT|I(?:ST|T)|MT|S(?:K|T)|UT|VT|YT)|N(?:CT|DT|FT|PT|ST|T|UT|Z(\" r \"?:DT|ST))|O(?:MST|RAT)|P(?:DT|ET(?:T)?|GT|H(?:OT|T)|KT|M(?:DT|ST)|ONT|S\" r \"T|Y(?:ST|T))|R(?:ET|OTT)|S(?:A(?:KT|MT|ST)|BT|CT|DT|GT|LST|R(?:ET|T)|ST\" r \"|YOT)|T(?:AHT|FT|HA|JT|KT|LT|MT|OT|RT|VT)|U(?:LA(?:ST|T)|TC|Y(?:ST|T)|Z\" r \"T)|V(?:ET|LAT|O(?:LT|ST)|UT)|W(?:A(?:KT|ST|T)|E(?:ST|T)|IT|ST)|Y(?:AKT|\" r \"EKT))\" , # FROM: en.wikipedia.org/wiki/List_of_time_zone_abbreviations \"z\" : r \"(?:[+-](?:0\\d|1[0-4]):?(?:00|15|30|45))\" , # [+-] 00:00 to 14:45 \"A\" : r \"(?:\" r \"MON(?:DAY)?|(?:IS|SE)N(?:[IE]N)?|\" r \"TUE(?:S(?:DAY)?)?|SEL(?:ASA)?|\" r \"WED(?:NESDAY)?|RABU?|\" r \"THU(?:RS(?:DAY)?)?|KH?A(?:M(?:IS)?)?|\" r \"FRI(?:DAY)?|JUM(?:[AM]A?T)?|\" r \"SAT(?:URDAY)?|SAB(?:TU)?|\" r \"SUN(?:DAY)?|AHA?D|MIN(?:GGU)?\" r \")\" , \"th\" : r \"(?:ST|ND|RD|TH|\u00ba)\" , } REGEX_PATTERNS_PARSERS = { # 14/8/1991 \"dd_mm_YYYY_1\" : r \"(?: {d} / {m} / {Y} )\" , \"dd_d2\" : r \"(?: {d} \\\\ {m} \\\\ {Y} )\" , \"dd_mm_YYYY_3\" : r \"(?: {d} [-] {m} [-] {Y} )\" , \"dd_mm_YYYY_4\" : r \"(?: {d} \\. {m} \\. {Y} )\" , # 'dd_mm_YYYY_5': r\"(?:{d}{m}{Y})\", # too many phone numbers \"dd_mm_YYYY_6\" : r \"(?: {d} ? {m} ? {Y} )\" , \"dd_mm_YYYY_7\" : r \"(?: {dz}{mz}{Y} )\" , # 14/8/91 \"dd_mm_yy_1\" : r \"(?: {d} / {m} / {y} )\" , \"dd_mm_yy_2\" : r \"(?: {d} \\\\ {m} \\\\ {y} )\" , \"dd_mm_yy_3\" : r \"(?: {d} [-] {m} [-] {y} )\" , \"dd_mm_yy_4\" : r \"(?: {d} \\. {m} \\. {y} )\" , # 'dd_mm_yy_5': r\"(?:{dz}{mz}{y})\", # too many phone numbers # 14 Aug, 1991 \"dd_mmm_YYYY_1\" : r \"(?: {d}{th} ? ?/ ? {B} ?/ ? {Y} )\" , \"dd_mmm_YYYY_2\" : r \"(?: {d}{th} ? ? \\\\ ? {B} ? \\\\ ? {Y} )\" , \"dd_mmm_YYYY_3\" : r \"(?: {d}{th} ? ?[-] ? {B} ?[ -] ? {Y} )\" , \"dd_mmm_YYYY_4\" : r \"(?: {d}{th} ? ?[ -]? ? {B} ?,? ? {Y} )\" , \"dd_mmm_YYYY_5\" : r \"(?: {d}{th} ? ?\\. ? {B} ?\\. ? {Y} )\" , # 14 Aug '91 \"dd_mmm_yy_1\" : r \"(?: {d}{th} ? ?/ ? {B} ?/ ?'? {y} )\" , \"dd_mmm_yy_2\" : r \"(?: {d}{th} ? ? \\\\ ? {B} ? \\\\ ?'? {y} )\" , \"dd_mmm_yy_3\" : r \"(?: {d}{th} ? ?[-] ? {B} ?[-] ?'? {y} )\" , \"dd_mmm_yy_4\" : r \"(?: {d}{th} ? ?[ -]? ? {B} ?,? ?'? {y} )\" , \"dd_mmm_yy_5\" : r \"(?: {d}{th} ? ?\\. ? {B} ?\\. ?'? {y} )\" , # 14th Aug \"dd_mmm\" : r \"(?: {d}{th} ? ?[/ \\\\ . -] ? {B} )\" , # 08/14/1991 # WARNING! dateutil set to day first \"mm_dd_YYYY_1\" : r \"(?: {m} / {d} / {Y} )\" , \"mm_dd_YYYY_2\" : r \"(?: {m} \\\\ {d} \\\\ {Y} )\" , \"mm_dd_YYYY_3\" : r \"(?: {m} [-] {d} [-] {Y} )\" , \"mm_dd_YYYY_4\" : r \"(?: {m} {d} {Y} )\" , \"mm_dd_YYYY_5\" : r \"(?: {m} \\. {d} \\. {Y} )\" , \"mm_dd_YYYY_6\" : r \"(?: {mz}{dz}{Y} )\" , # 8/14/91 # WARNING! dateutil set to day first \"mm_dd_yy_1\" : r \"(?: {m} / {d} / {y} )\" , \"mm_dd_yy_2\" : r \"(?: {m} \\\\ {d} \\\\ {y} )\" , \"mm_dd_yy_3\" : r \"(?: {m} [-] {d} [-] {y} )\" , \"mm_dd_yy_4\" : r \"(?: {m} \\. {d} \\. {y} )\" , # 'mm_dd_yy_5': r\"(?:{mz}{dz}{y})\", # too many phone numbers # Aug 14th, 1991 \"mmm_dd_YYYY_1\" : r \"(?: {B} ?/ ? {d}{th} ? ?/ ? {Y} )\" , \"mmm_dd_YYYY_2\" : r \"(?: {B} ? \\\\ ? {d}{th} ? ? \\\\ ? {Y} )\" , \"mmm_dd_YYYY_3\" : r \"(?: {B} ?[-] ? {d}{th} ? ?[ -] ? {Y} )\" , \"mmm_dd_YYYY_4\" : r \"(?: {B} ?[ -]? ? {d}{th} ? ?, ? {Y} )\" , \"mmm_dd_YYYY_5\" : r \"(?: {B} ?\\. ? {d}{th} ? ?\\. ? {Y} )\" , # Aug-14 '91 \"mmm_dd_yy_1\" : r \"(?: {B} ?/ ? {d}{th} ? ?/ ?'? {y} )\" , \"mmm_dd_yy_2\" : r \"(?: {B} ? \\\\ ? {d}{th} ? ? \\\\ ?'? {y} )\" , \"mmm_dd_yy_3\" : r \"(?: {B} ?[-] ? {d}{th} ? ?[-] ?'? {y} )\" , \"mmm_dd_yy_4\" : r \"(?: {B} ?[. -]? ? {d}{th} ?, '? {y} )\" , \"mmm_dd_yy_5\" : r \"(?: {B} ?\\. ? {d}{th} ? ?\\. ?'? {y} )\" , # Aug-14 # WARNING! dateutil assumes current year \"mmm_dd\" : r \"(?: {B} ?[/ \\\\ . -] ? {d}{th} ?)\" , # # Aug-91 # 'mmm_yy': r\"(?:{B} ?[/\\\\. -] ?'{y})\", # too many false positives # August 1991 \"mmm_YYYY\" : r \"(?: {B} ?[/ \\\\ . -] ? {Y} )\" , # many non-useful dates # 1991-8-14 \"YYYY_mm_dd_1\" : r \"(?: {Y} / {m} / {d} )\" , \"YYYY_mm_dd_2\" : r \"(?: {Y} \\\\ {m} \\\\ {d} )\" , \"YYYY_mm_dd_3\" : r \"(?: {Y} [-] {m} [-] {d} )\" , \"YYYY_mm_dd_4\" : r \"(?: {Y} {m} {d} )\" , \"YYYY_mm_dd_5\" : r \"(?: {Y} \\. {m} \\. {d} )\" , \"YYYY_mm_dd_6\" : r \"(?: {Y}{mz}{dz} )\" , # 910814 (ISO 8601) # 'yy_mm_dd_1': r\"(?:{y} {m} {d})\", # too many random numbers \"yy_mm_dd_2\" : r \"(?: {y} / {m} / {d} )\" , \"yy_mm_dd_3\" : r \"(?: {y} \\\\ {m} \\\\ {d} )\" , \"yy_mm_dd_4\" : r \"(?: {y} [-] {m} [-] {d} )\" , \"yy_mm_dd_5\" : r \"(?: {y} \\. {m} \\. {d} )\" , # 'yy_mm_dd_6': r\"(?:{y}{mz}{dz})\", # too many phone numbers # 1991-Aug-14 \"YYYY_mmm_dd_1\" : r \"(?: {Y} ?/ ? {B} ?/ ? {d} )\" , \"YYYY_mmm_dd_2\" : r \"(?: {Y} ? \\\\ ? {B} ? \\\\ ? {d} )\" , \"YYYY_mmm_dd_3\" : r \"(?: {Y} ?[-] ? {B} ?[-] ? {d} )\" , \"YYYY_mmm_dd_4\" : r \"(?: {Y} ? {B} ?[ -]? ? {d}{th} ?)\" , # 91-Aug-14 \"yy_mmm_dd_1\" : r \"(?:'? {y} ?/ ? {B} ?/ ? {d} )\" , \"yy_mmm_dd_2\" : r \"(?:'? {y} ? \\\\ ? {B} ? \\\\ ? {d} )\" , \"yy_mmm_dd_3\" : r \"(?:'? {y} ?[-] ? {B} ?[-] ? {d} )\" , \"yy_mmm_dd_4\" : r \"(?:'? {y} ? {B} ?[ -]? ? {d}{th} ?)\" , # # 1991.226 (Aug 14 = day 226 in 1991) # dateutil fails # 'YYYY_ddd_1': r\"(?:{Y}\\.{j})\", # too many random numbers # 'YYYY_ddd_2': r\"(?:{Y}[-]{j})\", # too many random numbers # time \"HH_MM_SS\" : r \"(?: {H} : {M} : {S} (?: ? {p} )?(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_1\" : r \"(?: {H} : {M} (?: ? {p} )?(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_1b\" : r \"(?: {H} [:. ] {M} (?: ? {p} )(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_MZ_pp_2\" : r \"(?:(?<!\\.) {HZ} [. ]? {M} (?: ? {p} )(?: ?(?:Z| {Z} | {z} ))?)\" , \"HH_pp\" : r \"(?:(?<!\\.) {H} ? {p2} (?: ?(?:Z| {Z} | {z} ))?)\" , # # 910814094500 (9:45am) # 'yy_mm_dd_HH_MM_SS': r\"(?:{y}{mz}{dz}{H}{M}{S})\", # too many phone numbers # 1991-08-14T09:45:00Z \"YYYY_mm_dd_HH_MM\" : r \"(?: {Y} [-] {m} [-] {d} [T ] {H} : {M} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_mm_dd_HH_MM_SS_1\" : r \"(?: {Y} [-] {m} [-] {d} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_mm_dd_HH_MM_SS_2\" : r \"(?: {Y}{mz}{d} T? {H}{M}{S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"YYYY_dd_mm_HH_MM_SS_3\" : r \"(?: {Y} [-] {d} [-] {m} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"mm_dd_YYYY_HH_MM_SS_1\" : r \"(?: {m} [-] {d} [-] {Y} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , \"dd_mm_YYYY_HH_MM_SS_1\" : r \"(?: {d} [-] {m} [-] {Y} [T ] {H} : {M} : {S} (?: ?(?:Z| {Z} | {z} ))?)\" , # # standalone # 'day': r\"{A}\", # too many false positives # 'month': r\"{B}\", # too many false positives # 'year': r\"{Y}\", # too many random numbers # 'timezone': r\"(?:Z|{Z}|{z})\", # too many malay words } # unicode fixes REGEX_FORMATTED = { label : \" \\\\ b\" + pattern . format ( ** REGEX_PARTS ) # fill in the chunks . replace ( \"-]\" , \" \\u2009\\u2010\\u2011\\u2012\\u2013\\u2014 -]\" ) # unicode dashes . replace ( \"'?\" , \"[' \\u2018\\u2019 ]?\" ) # unicode quotes + \" \\\\ b\" for label , pattern in REGEX_PATTERNS_PARSERS . items () } # match emails and urls to avoid returning chunks of them REGEX_FORMATTED [ \"eml\" ] = r \"\"\"[a-zA-Z0-9][^\\s`!@%$^= {} \\[\\]/\\\\\"',()<>:;]+(?:@|%40|\\s+at\\s+|\\s*<\\s*at\\s*>\\s*)[a-zA-Z0-9][-_a-zA-Z0-9~.]+\\.[a-zA-Z]{2,15}\"\"\" REGEX_FORMATTED [ \"url\" ] = r \"\\b(?:(?:https?|ftp|file)://|www\\d?\\.|ftp\\.)[-A-Z0-9+&@#/%=~_|$?!:,.]*[A-Z0-9+&@#/%=~_|$]\" REGEX_FORMATTED [ \"dot\" ] = r \"(?:\\d+\\.){3,}\\d+\" # compile all the regex patterns REGEX_COMPILED = { label : re . compile ( pattern , flags = re . I | re . U ) for label , pattern in REGEX_FORMATTED . items () } if trans_cat == \"dt\" : return idf elif ( trans_cat in [ \"long_c\" , \"bigint_c\" , \"int_c\" ]) & ( int ( val_unique_cat ) in [ 10 , 13 ] ): if int ( val_unique_cat ) == 10 : precision = \"s\" elif int ( val_unique_cat ) == 13 : precision = \"ms\" else : precision = \"ms\" output_df = unix_to_timestamp ( spark , idf , col , precision = precision , tz = tz , output_mode = output_mode ) . orderBy ( id_col , col ) if save_output is not None : output_df . write . parquet ( save_output , mode = \"overwrite\" ) else : return output_df elif trans_cat == \"string\" : list_dates = list ( set ( idf . select ( col ) . rdd . flatMap ( lambda x : x ) . collect ())) def regex_text ( text , longest = True , context_max_len = 999 , dayfirst = False ): # join multiple spaces, convert tabs, strip leading/trailing whitespace if isinstance ( text , str ): pass else : raise ValueError ( \"Incompatible Column Type!!\" ) text = \" \" . join ( text . split ()) matches = [] for regex_label , regex_obj in REGEX_COMPILED . items (): for m in regex_obj . finditer ( text ): context_start = max ( 0 , ( m . start () + m . end () - context_max_len ) // 2 ) context_end = min ( len ( text ), context_start + context_max_len ) context_str = text [ context_start : context_end ] if context_start != 0 : context_str = \" \\u2026 \" + context_str [ 1 :] if context_end != len ( text ): context_str = ( context_str [: - 1 ] + \" \\u2026 \" ) # this is the `...` character parsed_date = None try : with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = dateutil . parser . UnknownTimezoneWarning , ) if \"HH\" in regex_label : if \"dd\" in regex_label or \"YYYY\" in regex_label : matched_text = re . sub ( r \"[ \\\\ ]\" , \"/\" , m . group ()) parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) else : matched_text = re . sub ( r \"H(?:(?:OU)?RS?)?\" , \"\" , m . group (), flags = re . I ) matched_text = re . sub ( r \"MN\" , r \"AM\" , matched_text , flags = re . I ) matched_text = re . sub ( r \"NN\" , r \"PM\" , matched_text , flags = re . I ) matched_text = re . sub ( r \"(\\d)[. ](\\d)\" , r \"\\1:\\2\" , matched_text ) matched_text = f \"1970-01-01 { matched_text } \" parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) elif \"dd\" in regex_label or \"YYYY\" in regex_label : matched_text = re . sub ( r \"[ \\\\ ]\" , \"/\" , m . group ()) parsed_date = dateutil . parser . parse ( matched_text , dayfirst = dayfirst ) except ValueError : pass matches . append ( { \"REGEX_LABEL\" : regex_label , \"MATCH\" : m . group (), \"START\" : m . start (), \"END\" : m . end (), \"MATCH_LEN\" : m . end () - m . start (), \"NORM_TEXT_LEN\" : len ( text ), \"CONTEXT\" : context_str , \"PARSED\" : parsed_date , } ) # narrow to longest match for match in matches : if not longest or all ( ( other [ \"START\" ] >= match [ \"START\" ] and other [ \"END\" ] <= match [ \"END\" ]) or other [ \"START\" ] > match [ \"END\" ] or other [ \"END\" ] < match [ \"START\" ] for other in matches ): # don't return emails or urls if match [ \"REGEX_LABEL\" ] not in { \"eml\" , \"url\" , \"dot\" }: yield match bl = [] file_lines = list_dates for line_num , line in enumerate ( file_lines ): bl_int = [] for match_info in regex_text ( line ): try : ye , mo , da , ho , mi , se = ( match_info [ \"PARSED\" ] . year , match_info [ \"PARSED\" ] . month , match_info [ \"PARSED\" ] . day , match_info [ \"PARSED\" ] . hour , match_info [ \"PARSED\" ] . minute , match_info [ \"PARSED\" ] . second , ) if len ( bl_int ) == 0 : bl_int = [ ye , mo , da , ho , mi , se ] else : if ye == 1970 and mo == 1 and da == 1 : pass if ho + mi + se == 0 : pass if ye > 1970 : bl_int [ 0 ] = ye if mo > 0 and ye != 1970 : bl_int [ 1 ] = mo if da > 0 and ye != 1970 : bl_int [ 2 ] = da if ho > 0 : bl_int [ 3 ] = ho if mi > 0 : bl_int [ 4 ] = mi if se > 0 : bl_int [ 5 ] = se else : pass except : pass bl . append ( [ match_info [ \"CONTEXT\" ], datetime . datetime ( bl_int [ 0 ], bl_int [ 1 ], bl_int [ 2 ], bl_int [ 3 ], bl_int [ 4 ], bl_int [ 5 ], ), ] ) if len ( bl ) >= 1 : columns = [ col , col + \"_ts\" ] # output_df = spark.createDataFrame(spark.parallelize(bl),columns) output_df = spark . createDataFrame ( pd . DataFrame ( bl , columns = columns )) else : return idf elif trans_cat in [ \"string_c\" , \"int_c\" ]: if int ( val_unique_cat ) == 4 : output_df = idf . select ( col ) . withColumn ( col + \"_ts\" , F . col ( col ) . cast ( \"string\" ) . cast ( \"date\" ) ) elif int ( val_unique_cat ) == 6 : output_df = ( idf . select ( col ) . withColumn ( col , F . concat ( col , F . lit ( \"01\" ))) . withColumn ( col + \"_ts\" , F . to_date ( col , \"yyyyMMdd\" )) ) elif int ( val_unique_cat ) == 8 : f = ( idf . select ( F . max ( F . substring ( col , 1 , 4 )), F . max ( F . substring ( col , 5 , 2 )), F . max ( F . substring ( col , 7 , 2 )), ) . rdd . flatMap ( lambda x : x ) . collect () ) if int ( f [ 1 ]) > 12 : frmt = \"yyyyddMM\" elif int ( f [ 2 ]) > 12 : frmt = \"yyyyMMdd\" elif ( ( int ( f [ 0 ]) > 1970 & int ( f [ 0 ]) < 2049 ) & ( int ( f [ 1 ]) > 0 & int ( f [ 1 ]) <= 12 ) & ( int ( f [ 2 ]) > 0 & int ( f [ 2 ]) <= 31 ) ): frmt = \"yyyyMMdd\" elif ( ( int ( f [ 0 ]) > 1970 & int ( f [ 0 ]) < 2049 ) & ( int ( f [ 1 ]) > 0 & int ( f [ 1 ]) <= 31 ) & ( int ( f [ 2 ]) > 0 & int ( f [ 2 ]) <= 12 ) ): frmt = \"yyyyddMM\" else : return idf output_df = idf . select ( F . col ( col ) . cast ( \"string\" )) . withColumn ( col + \"_ts\" , F . to_date ( col , frmt ) ) else : return idf else : return idf # if ((output_df.where(F.col(col + \"_ts\").isNull()).count()) / output_df.count()) > 0.9: # return idf # else: # pass if output_mode == \"replace\" : output_df = ( idf . join ( output_df , col , \"left_outer\" ) . drop ( col ) . withColumnRenamed ( col + \"_ts\" , col ) . orderBy ( id_col , col ) ) elif output_mode == \"append\" : output_df = idf . join ( output_df , col , \"left_outer\" ) . orderBy ( id_col , col + \"_ts\" ) else : return \"Incorrect Output Mode Selected\" if save_output : output_df . write . parquet ( save_output , mode = \"overwrite\" ) else : return output_df def ts_loop_cols_pre ( idf , id_col ): \"\"\" This function helps to analyze the potential columns which can be passed for the time series check. The columns are passed on to the auto-detection block. Parameters ---------- idf Input dataframe id_col ID Column Returns ------- Three lists \"\"\" lc1 , lc2 , lc3 = [], [], [] for i in idf . dtypes : try : col_len = ( idf . select ( F . max ( F . length ( i [ 0 ]))) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) except : col_len = 0 if idf . select ( i [ 0 ]) . dropna () . distinct () . count () == 0 : lc1 . append ( i [ 0 ]) lc2 . append ( \"NA\" ) lc3 . append ( col_len ) elif ( ( i [ 0 ] != id_col ) & ( idf . select ( F . length ( i [ 0 ])) . distinct () . count () == 1 ) & ( col_len in [ 4 , 6 , 8 , 10 , 13 ]) ): if i [ 1 ] == \"string\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"string_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"long\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"long_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"bigint\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"bigint_c\" ) lc3 . append ( col_len ) elif i [ 1 ] == \"int\" : lc1 . append ( i [ 0 ]) lc2 . append ( \"int_c\" ) lc3 . append ( col_len ) elif ( i [ 0 ] != id_col ) & ( i [ 1 ] in [ \"string\" , \"object\" ]): lc1 . append ( i [ 0 ]) lc2 . append ( \"string\" ) lc3 . append ( col_len ) elif ( i [ 0 ] != id_col ) & ( i [ 1 ] in [ \"timestamp\" , \"date\" ]): lc1 . append ( i [ 0 ]) lc2 . append ( \"dt\" ) lc3 . append ( col_len ) else : lc1 . append ( i [ 0 ]) lc2 . append ( \"NA\" ) lc3 . append ( col_len ) return lc1 , lc2 , lc3 def ts_preprocess ( spark , idf , id_col , output_path , tz_offset = \"local\" , run_type = \"local\" , mlflow_config = None , auth_key = \"NA\" , ): \"\"\" This function helps to read the input spark dataframe as source and do all the necessary processing. All the intermediate data created through this step foro the Time Series Analyzer. Parameters ---------- spark Spark session idf Input dataframe id_col ID Column output_path Output path where the data would be saved tz_offset Timezone offset (Option to chose between options like Local, GMT, UTC, etc.). Default option is set as \"Local\". run_type Option to choose between run type \"local\" or \"emr\" or \"databricks\" or \"ak8s\" basis the user flexibility. Default option is set as \"local\". mlflow_config MLflow configuration. If None, all MLflow features are disabled. auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for \"ak8s\" run_type. Returns ------- DataFrame,Output[CSV] \"\"\" if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) ts_loop_col_dtls = ts_loop_cols_pre ( idf , id_col ) l1 , l2 = [], [] for index , i in enumerate ( ts_loop_col_dtls [ 1 ]): if i != \"NA\" : l1 . append ( index ) if i == \"dt\" : l2 . append ( index ) ts_loop_cols = [ ts_loop_col_dtls [ 0 ][ i ] for i in l1 ] pre_exist_ts_cols = [ ts_loop_col_dtls [ 0 ][ i ] for i in l2 ] for i in ts_loop_cols : try : idx = ts_loop_col_dtls [ 0 ] . index ( i ) val_unique_cat = ts_loop_col_dtls [ 2 ][ idx ] trans_cat = ts_loop_col_dtls [ 1 ][ idx ] idf = regex_date_time_parser ( spark , idf , id_col , i , tz_offset , val_unique_cat , trans_cat , save_output = None , output_mode = \"replace\" , ) idf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) except : pass odf = idf . distinct () ts_loop_cols_post = [ x [ 0 ] for x in idf . dtypes if x [ 1 ] in [ \"timestamp\" , \"date\" ]] num_cols = [ x for x in num_cols if x not in [ id_col ] + ts_loop_cols_post ] cat_cols = [ x for x in cat_cols if x not in [ id_col ] + ts_loop_cols_post ] c1 = ts_loop_cols c2 = list ( set ( ts_loop_cols_post ) - set ( pre_exist_ts_cols )) c3 = pre_exist_ts_cols c4 = ts_loop_cols_post f = pd . DataFrame ( [ [ \",\" . join ( idf . columns )], [ \",\" . join ( c1 )], [ \",\" . join ( c2 )], [ \",\" . join ( c3 )], [ \",\" . join ( c4 )], [ \",\" . join ( num_cols )], [ \",\" . join ( cat_cols )], ], columns = [ \"cols\" ], ) f . to_csv ( ends_with ( local_path ) + \"ts_cols_stats.csv\" , index = False ) if mlflow_config is not None : mlflow . log_artifact ( ends_with ( local_path ) + \"ts_cols_stats.csv\" ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( output_path ) ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( output_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) return odf","title":"ts_auto_detection"},{"location":"api/data_ingest/ts_auto_detection.html#functions","text":"def regex_date_time_parser ( spark, idf, id_col, col, tz, val_unique_cat, trans_cat, save_output=None, output_mode='replace') This function helps to produce the transformed output in timestamp (if auto-detected) based on the input data.","title":"Functions"},{"location":"api/data_report/_index.html","text":"Overview Sub-modules anovos.data_report.basic_report_generation anovos.data_report.report_generation This module generates the final report output specific to the intermediate data generated across each of the modules. The final report, however, can \u2026 anovos.data_report.report_preprocessing","title":"Overview"},{"location":"api/data_report/_index.html#overview","text":"","title":"Overview"},{"location":"api/data_report/_index.html#sub-modules","text":"anovos.data_report.basic_report_generation anovos.data_report.report_generation This module generates the final report output specific to the intermediate data generated across each of the modules. The final report, however, can \u2026 anovos.data_report.report_preprocessing","title":"Sub-modules"},{"location":"api/data_report/basic_report_generation.html","text":"basic_report_generation Expand source code import subprocess from pathlib import Path import datapane as dp import mlflow import pandas as pd import plotly.express as px from anovos.data_analyzer.association_evaluator import ( IG_calculation , IV_calculation , correlation_matrix , variable_clustering , ) from anovos.data_analyzer.quality_checker import ( IDness_detection , biasedness_detection , duplicate_detection , invalidEntries_detection , nullColumns_detection , nullRows_detection , outlier_detection , ) from anovos.data_analyzer.stats_generator import ( global_summary , measures_of_cardinality , measures_of_centralTendency , measures_of_counts , measures_of_dispersion , measures_of_percentiles , measures_of_shape , ) from anovos.shared.utils import ends_with , output_to_local , path_ak8s_modify global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" default_template = ( dp . HTML ( \"\"\" <html> <img src=\"https://mobilewalla-anovos.s3.amazonaws.com/anovos.png\" style=\"height:100px;display:flex;margin:auto;float:right\"/> </html> \"\"\" ), dp . Text ( \"# ML-Anovos Report\" ), ) blank_df = dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ) def stats_args ( path , func ): \"\"\" Parameters ---------- path Path to pre-saved statistics func Quality Checker function Returns ------- Dictionary Each key/value is argument (related to pre-saved statistics) to be passed for the quality checker function. \"\"\" output = {} mainfunc_to_args = { \"biasedness_detection\" : [ \"stats_mode\" ], \"IDness_detection\" : [ \"stats_unique\" ], \"nullColumns_detection\" : [ \"stats_unique\" , \"stats_mode\" , \"stats_missing\" ], \"variable_clustering\" : [ \"stats_mode\" ], } args_to_statsfunc = { \"stats_unique\" : \"measures_of_cardinality\" , \"stats_mode\" : \"measures_of_centralTendency\" , \"stats_missing\" : \"measures_of_counts\" , } for arg in mainfunc_to_args . get ( func , []): output [ arg ] = { \"file_path\" : ( ends_with ( path ) + args_to_statsfunc [ arg ] + \".csv\" ), \"file_type\" : \"csv\" , \"file_configs\" : { \"header\" : True , \"inferSchema\" : True }, } return output def anovos_basic_report ( spark , idf , id_col = \"\" , label_col = \"\" , event_label = \"\" , skip_corr_matrix = True , output_path = \".\" , run_type = \"local\" , auth_key = \"NA\" , print_impact = True , mlflow_config = None , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe id_col ID column (Default value = \"\") label_col Label/Target column (Default value = \"\") event_label Value of (positive) event (i.e label 1) (Default value = \"\") skip_corr_matrix True, False. This argument is to skip correlation matrix generation in basic_report.(Default value = True) output_path File Path for saving metrics and basic report (Default value = \".\") run_type \"local\", \"emr\" or \"databricks\" or \"ak8s\" \"emr\" if the files are read from or written in AWS s3 \"databricks\" if the files are read from or written in dbfs in azure databricks \"ak8s\" if the files are read from or written to in wasbs:// container in azure environment (Default value = \"local\") auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. print_impact True, False. This argument is to print out the data analyzer statistics.(Default value = False) mlflow_config MLflow configuration. If None, all MLflow features are disabled. \"\"\" global num_cols global cat_cols SG_funcs = [ global_summary , measures_of_counts , measures_of_centralTendency , measures_of_cardinality , measures_of_dispersion , measures_of_percentiles , measures_of_shape , ] QC_rows_funcs = [ duplicate_detection , nullRows_detection ] QC_cols_funcs = [ nullColumns_detection , outlier_detection , IDness_detection , biasedness_detection , invalidEntries_detection , ] if mlflow_config is not None : output_path = output_path + \"/\" + mlflow_config . get ( \"run_id\" , \"\" ) if skip_corr_matrix : AA_funcs = [ variable_clustering ] else : AA_funcs = [ correlation_matrix , variable_clustering ] AT_funcs = [ IV_calculation , IG_calculation ] all_funcs = SG_funcs + QC_rows_funcs + QC_cols_funcs + AA_funcs + AT_funcs if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) for func in all_funcs : if func in SG_funcs : stats = func ( spark , idf ) elif func in ( QC_rows_funcs + QC_cols_funcs ): extra_args = stats_args ( output_path , func . __name__ ) if func . __name__ in [ \"outlier_detection\" , \"duplicate_detection\" ]: extra_args [ \"print_impact\" ] = True stats = func ( spark , idf , ** extra_args )[ 1 ] elif func in AA_funcs : extra_args = stats_args ( output_path , func . __name__ ) stats = func ( spark , idf , drop_cols = id_col , ** extra_args ) elif label_col : if func in AT_funcs : stats = func ( spark , idf , label_col = label_col , event_label = event_label ) else : continue stats . toPandas () . to_csv ( ends_with ( local_path ) + func . __name__ + \".csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + func . __name__ + \".csv \" + ends_with ( output_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) elif run_type == \"ak8s\" : local_file = ends_with ( local_path ) + func . __name__ + \".csv\" output_path_mod = path_ak8s_modify ( output_path ) bash_cmd = ( 'azcopy cp \"' + local_file + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true' ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if print_impact : print ( func . __name__ , \": \\n \" ) stats = spark . read . csv ( ends_with ( output_path ) + func . __name__ + \".csv\" , header = True , inferSchema = True , ) stats . show () def remove_u_score ( col ): col_ = col . split ( \"_\" ) bl = [] for i in col_ : if i == \"nullColumns\" or i == \"nullRows\" : bl . append ( \"Null\" ) else : bl . append ( i [ 0 ] . upper () + i [ 1 :]) return \" \" . join ( bl ) global_summary_df = pd . read_csv ( ends_with ( local_path ) + \"global_summary.csv\" ) rows_count = int ( global_summary_df [ global_summary_df . metric . values == \"rows_count\" ] . value . values [ 0 ] ) catcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"catcols_count\" ] . value . values [ 0 ] ) numcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"numcols_count\" ] . value . values [ 0 ] ) columns_count = int ( global_summary_df [ global_summary_df . metric . values == \"columns_count\" ] . value . values [ 0 ] ) catcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"catcols_name\" ] . value . values ) ) numcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"numcols_name\" ] . value . values ) ) l1 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the dataset with key statistical metrics.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Global Summary\" ), dp . Group ( dp . Text ( \" Total Number of Records: **\" + str ( f \" { rows_count : ,d } \" ) + \"**\" ), dp . Text ( \" Total Number of Attributes: **\" + str ( columns_count ) + \"**\" ), dp . Text ( \" Number of Numerical Attributes : **\" + str ( numcols_count ) + \"**\" ), dp . Text ( \" Numerical Attributes Name : **\" + str ( numcols_name ) + \"**\" ), dp . Text ( \" Number of Categorical Attributes : **\" + str ( catcols_count ) + \"**\" ), dp . Text ( \" Categorical Attributes Name : **\" + str ( catcols_name ) + \"**\" ), ), ) l2 = dp . Text ( \"### Statistics by Metric Type\" ) SG_content = [] for i in SG_funcs : if i . __name__ != \"global_summary\" : SG_content . append ( dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ), label = remove_u_score ( i . __name__ ), ) ) l3 = dp . Group ( dp . Select ( blocks = SG_content , type = dp . SelectType . TABS ), dp . Text ( \"# \" )) tab1 = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) QCcol_content = [] for i in QC_cols_funcs : QCcol_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) QCrow_content = [] for i in QC_rows_funcs : if i . __name__ == \"duplicate_detection\" : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) unique_rows_count = ( \" No. Of Unique Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"unique_rows_count\" ] . value . values ), \",\" , ) ) + \"**\" ) total_rows_count = ( \" No. of Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"rows_count\" ] . value . values ), \",\" ) ) + \"**\" ) duplicate_rows_count = ( \" No. of Duplicate Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"duplicate_rows\" ] . value . values ), \",\" , ) ) + \"**\" ) duplicate_rows_pct = ( \" Percentage of Duplicate Rows: **\" + str ( float ( stats [ stats [ \"metric\" ] == \"duplicate_pct\" ] . value . values * 100.0 ) ) + \" %\" + \"**\" ) QCrow_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . Group ( dp . Text ( total_rows_count ), dp . Text ( unique_rows_count ), dp . Text ( duplicate_rows_count ), dp . Text ( duplicate_rows_pct ), ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) else : QCrow_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) QCcol_content = [ item for sublist in QCcol_content for item in sublist ] QCrow_content = [ item for sublist in QCrow_content for item in sublist ] tab2 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Text ( \"# \" ), dp . Group ( * QCcol_content ), label = \"Column Level\" ), dp . Group ( dp . Text ( \"# \" ), dp . Group ( * QCrow_content ), label = \"Row Level\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) AA_content = [] for i in AA_funcs + AT_funcs : if i . __name__ == \"correlation_matrix\" : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) feats_order = list ( stats [ \"attribute\" ] . values ) stats = stats . round ( 3 ) fig = px . imshow ( stats [ feats_order ], y = feats_order , color_continuous_scale = global_theme , aspect = \"auto\" , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats [[ \"attribute\" ] + feats_order ]), dp . Plot ( fig ), label = remove_u_score ( i . __name__ ), ) ) elif i . __name__ == \"variable_clustering\" : stats = ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) . sort_values ( by = [ \"Cluster\" ], ascending = True ) ) fig = px . sunburst ( stats , path = [ \"Cluster\" , \"Attribute\" ], values = \"RS_Ratio\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . layout . autosize = True AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats ), dp . Plot ( fig ), label = remove_u_score ( i . __name__ ), ) ) else : if label_col : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) col_nm = [ x for x in list ( stats . columns ) if \"attribute\" not in x ] stats = stats . sort_values ( col_nm [ 0 ], ascending = True ) fig = px . bar ( stats , x = col_nm [ 0 ], y = \"attribute\" , orientation = \"h\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . layout . autosize = True AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats ), dp . Plot ( fig ), label = remove_u_score ( i . __name__ ), ) ) if len ( AA_content ) == 1 : AA_content . append ( blank_df ) else : AA_content # @TODO: is there better templating approach such as jinja tab3 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Association Matrix & Plot\" ), dp . Select ( blocks = AA_content , type = dp . SelectType . DROPDOWN ), dp . Text ( \"### \" ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = [ tab1 , tab2 , tab3 ], type = dp . SelectType . TABS ), ) . save ( ends_with ( local_path ) + \"basic_report.html\" , open = True ) if mlflow_config is not None : mlflow . log_artifacts ( local_dir = local_path , artifact_path = output_path ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + \"basic_report.html \" + ends_with ( output_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( output_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + 'basic_report.html\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\"' ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) Functions def anovos_basic_report ( spark, idf, id_col='', label_col='', event_label='', skip_corr_matrix=True, output_path='.', run_type='local', auth_key='NA', print_impact=True, mlflow_config=None) Parameters spark Spark Session idf Input Dataframe id_col ID column (Default value = \"\") label_col Label/Target column (Default value = \"\") event_label Value of (positive) event (i.e label 1) (Default value = \"\") skip_corr_matrix True, False. This argument is to skip correlation matrix generation in basic_report.(Default value = True) output_path File Path for saving metrics and basic report (Default value = \".\") run_type \"local\", \"emr\" or \"databricks\" or \"ak8s\" \"emr\" if the files are read from or written in AWS s3 \"databricks\" if the files are read from or written in dbfs in azure databricks \"ak8s\" if the files are read from or written to in wasbs:// container in azure environment (Default value = \"local\") auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. print_impact True, False. This argument is to print out the data analyzer statistics.(Default value = False) mlflow_config MLflow configuration. If None, all MLflow features are disabled. Expand source code def anovos_basic_report ( spark , idf , id_col = \"\" , label_col = \"\" , event_label = \"\" , skip_corr_matrix = True , output_path = \".\" , run_type = \"local\" , auth_key = \"NA\" , print_impact = True , mlflow_config = None , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe id_col ID column (Default value = \"\") label_col Label/Target column (Default value = \"\") event_label Value of (positive) event (i.e label 1) (Default value = \"\") skip_corr_matrix True, False. This argument is to skip correlation matrix generation in basic_report.(Default value = True) output_path File Path for saving metrics and basic report (Default value = \".\") run_type \"local\", \"emr\" or \"databricks\" or \"ak8s\" \"emr\" if the files are read from or written in AWS s3 \"databricks\" if the files are read from or written in dbfs in azure databricks \"ak8s\" if the files are read from or written to in wasbs:// container in azure environment (Default value = \"local\") auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. print_impact True, False. This argument is to print out the data analyzer statistics.(Default value = False) mlflow_config MLflow configuration. If None, all MLflow features are disabled. \"\"\" global num_cols global cat_cols SG_funcs = [ global_summary , measures_of_counts , measures_of_centralTendency , measures_of_cardinality , measures_of_dispersion , measures_of_percentiles , measures_of_shape , ] QC_rows_funcs = [ duplicate_detection , nullRows_detection ] QC_cols_funcs = [ nullColumns_detection , outlier_detection , IDness_detection , biasedness_detection , invalidEntries_detection , ] if mlflow_config is not None : output_path = output_path + \"/\" + mlflow_config . get ( \"run_id\" , \"\" ) if skip_corr_matrix : AA_funcs = [ variable_clustering ] else : AA_funcs = [ correlation_matrix , variable_clustering ] AT_funcs = [ IV_calculation , IG_calculation ] all_funcs = SG_funcs + QC_rows_funcs + QC_cols_funcs + AA_funcs + AT_funcs if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) for func in all_funcs : if func in SG_funcs : stats = func ( spark , idf ) elif func in ( QC_rows_funcs + QC_cols_funcs ): extra_args = stats_args ( output_path , func . __name__ ) if func . __name__ in [ \"outlier_detection\" , \"duplicate_detection\" ]: extra_args [ \"print_impact\" ] = True stats = func ( spark , idf , ** extra_args )[ 1 ] elif func in AA_funcs : extra_args = stats_args ( output_path , func . __name__ ) stats = func ( spark , idf , drop_cols = id_col , ** extra_args ) elif label_col : if func in AT_funcs : stats = func ( spark , idf , label_col = label_col , event_label = event_label ) else : continue stats . toPandas () . to_csv ( ends_with ( local_path ) + func . __name__ + \".csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + func . __name__ + \".csv \" + ends_with ( output_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) elif run_type == \"ak8s\" : local_file = ends_with ( local_path ) + func . __name__ + \".csv\" output_path_mod = path_ak8s_modify ( output_path ) bash_cmd = ( 'azcopy cp \"' + local_file + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true' ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if print_impact : print ( func . __name__ , \": \\n \" ) stats = spark . read . csv ( ends_with ( output_path ) + func . __name__ + \".csv\" , header = True , inferSchema = True , ) stats . show () def remove_u_score ( col ): col_ = col . split ( \"_\" ) bl = [] for i in col_ : if i == \"nullColumns\" or i == \"nullRows\" : bl . append ( \"Null\" ) else : bl . append ( i [ 0 ] . upper () + i [ 1 :]) return \" \" . join ( bl ) global_summary_df = pd . read_csv ( ends_with ( local_path ) + \"global_summary.csv\" ) rows_count = int ( global_summary_df [ global_summary_df . metric . values == \"rows_count\" ] . value . values [ 0 ] ) catcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"catcols_count\" ] . value . values [ 0 ] ) numcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"numcols_count\" ] . value . values [ 0 ] ) columns_count = int ( global_summary_df [ global_summary_df . metric . values == \"columns_count\" ] . value . values [ 0 ] ) catcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"catcols_name\" ] . value . values ) ) numcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"numcols_name\" ] . value . values ) ) l1 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the dataset with key statistical metrics.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Global Summary\" ), dp . Group ( dp . Text ( \" Total Number of Records: **\" + str ( f \" { rows_count : ,d } \" ) + \"**\" ), dp . Text ( \" Total Number of Attributes: **\" + str ( columns_count ) + \"**\" ), dp . Text ( \" Number of Numerical Attributes : **\" + str ( numcols_count ) + \"**\" ), dp . Text ( \" Numerical Attributes Name : **\" + str ( numcols_name ) + \"**\" ), dp . Text ( \" Number of Categorical Attributes : **\" + str ( catcols_count ) + \"**\" ), dp . Text ( \" Categorical Attributes Name : **\" + str ( catcols_name ) + \"**\" ), ), ) l2 = dp . Text ( \"### Statistics by Metric Type\" ) SG_content = [] for i in SG_funcs : if i . __name__ != \"global_summary\" : SG_content . append ( dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ), label = remove_u_score ( i . __name__ ), ) ) l3 = dp . Group ( dp . Select ( blocks = SG_content , type = dp . SelectType . TABS ), dp . Text ( \"# \" )) tab1 = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) QCcol_content = [] for i in QC_cols_funcs : QCcol_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) QCrow_content = [] for i in QC_rows_funcs : if i . __name__ == \"duplicate_detection\" : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) unique_rows_count = ( \" No. Of Unique Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"unique_rows_count\" ] . value . values ), \",\" , ) ) + \"**\" ) total_rows_count = ( \" No. of Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"rows_count\" ] . value . values ), \",\" ) ) + \"**\" ) duplicate_rows_count = ( \" No. of Duplicate Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"duplicate_rows\" ] . value . values ), \",\" , ) ) + \"**\" ) duplicate_rows_pct = ( \" Percentage of Duplicate Rows: **\" + str ( float ( stats [ stats [ \"metric\" ] == \"duplicate_pct\" ] . value . values * 100.0 ) ) + \" %\" + \"**\" ) QCrow_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . Group ( dp . Text ( total_rows_count ), dp . Text ( unique_rows_count ), dp . Text ( duplicate_rows_count ), dp . Text ( duplicate_rows_pct ), ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) else : QCrow_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) QCcol_content = [ item for sublist in QCcol_content for item in sublist ] QCrow_content = [ item for sublist in QCrow_content for item in sublist ] tab2 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Text ( \"# \" ), dp . Group ( * QCcol_content ), label = \"Column Level\" ), dp . Group ( dp . Text ( \"# \" ), dp . Group ( * QCrow_content ), label = \"Row Level\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) AA_content = [] for i in AA_funcs + AT_funcs : if i . __name__ == \"correlation_matrix\" : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) feats_order = list ( stats [ \"attribute\" ] . values ) stats = stats . round ( 3 ) fig = px . imshow ( stats [ feats_order ], y = feats_order , color_continuous_scale = global_theme , aspect = \"auto\" , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats [[ \"attribute\" ] + feats_order ]), dp . Plot ( fig ), label = remove_u_score ( i . __name__ ), ) ) elif i . __name__ == \"variable_clustering\" : stats = ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) . sort_values ( by = [ \"Cluster\" ], ascending = True ) ) fig = px . sunburst ( stats , path = [ \"Cluster\" , \"Attribute\" ], values = \"RS_Ratio\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . layout . autosize = True AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats ), dp . Plot ( fig ), label = remove_u_score ( i . __name__ ), ) ) else : if label_col : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) col_nm = [ x for x in list ( stats . columns ) if \"attribute\" not in x ] stats = stats . sort_values ( col_nm [ 0 ], ascending = True ) fig = px . bar ( stats , x = col_nm [ 0 ], y = \"attribute\" , orientation = \"h\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . layout . autosize = True AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats ), dp . Plot ( fig ), label = remove_u_score ( i . __name__ ), ) ) if len ( AA_content ) == 1 : AA_content . append ( blank_df ) else : AA_content # @TODO: is there better templating approach such as jinja tab3 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Association Matrix & Plot\" ), dp . Select ( blocks = AA_content , type = dp . SelectType . DROPDOWN ), dp . Text ( \"### \" ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = [ tab1 , tab2 , tab3 ], type = dp . SelectType . TABS ), ) . save ( ends_with ( local_path ) + \"basic_report.html\" , open = True ) if mlflow_config is not None : mlflow . log_artifacts ( local_dir = local_path , artifact_path = output_path ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + \"basic_report.html \" + ends_with ( output_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( output_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + 'basic_report.html\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\"' ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) def stats_args ( path, func) Parameters path Path to pre-saved statistics func Quality Checker function Returns Dictionary Each key/value is argument (related to pre-saved statistics) to be passed for the quality checker function. Expand source code def stats_args ( path , func ): \"\"\" Parameters ---------- path Path to pre-saved statistics func Quality Checker function Returns ------- Dictionary Each key/value is argument (related to pre-saved statistics) to be passed for the quality checker function. \"\"\" output = {} mainfunc_to_args = { \"biasedness_detection\" : [ \"stats_mode\" ], \"IDness_detection\" : [ \"stats_unique\" ], \"nullColumns_detection\" : [ \"stats_unique\" , \"stats_mode\" , \"stats_missing\" ], \"variable_clustering\" : [ \"stats_mode\" ], } args_to_statsfunc = { \"stats_unique\" : \"measures_of_cardinality\" , \"stats_mode\" : \"measures_of_centralTendency\" , \"stats_missing\" : \"measures_of_counts\" , } for arg in mainfunc_to_args . get ( func , []): output [ arg ] = { \"file_path\" : ( ends_with ( path ) + args_to_statsfunc [ arg ] + \".csv\" ), \"file_type\" : \"csv\" , \"file_configs\" : { \"header\" : True , \"inferSchema\" : True }, } return output","title":"<code>basic_report_generation</code>"},{"location":"api/data_report/basic_report_generation.html#basic_report_generation","text":"Expand source code import subprocess from pathlib import Path import datapane as dp import mlflow import pandas as pd import plotly.express as px from anovos.data_analyzer.association_evaluator import ( IG_calculation , IV_calculation , correlation_matrix , variable_clustering , ) from anovos.data_analyzer.quality_checker import ( IDness_detection , biasedness_detection , duplicate_detection , invalidEntries_detection , nullColumns_detection , nullRows_detection , outlier_detection , ) from anovos.data_analyzer.stats_generator import ( global_summary , measures_of_cardinality , measures_of_centralTendency , measures_of_counts , measures_of_dispersion , measures_of_percentiles , measures_of_shape , ) from anovos.shared.utils import ends_with , output_to_local , path_ak8s_modify global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" default_template = ( dp . HTML ( \"\"\" <html> <img src=\"https://mobilewalla-anovos.s3.amazonaws.com/anovos.png\" style=\"height:100px;display:flex;margin:auto;float:right\"/> </html> \"\"\" ), dp . Text ( \"# ML-Anovos Report\" ), ) blank_df = dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ) def stats_args ( path , func ): \"\"\" Parameters ---------- path Path to pre-saved statistics func Quality Checker function Returns ------- Dictionary Each key/value is argument (related to pre-saved statistics) to be passed for the quality checker function. \"\"\" output = {} mainfunc_to_args = { \"biasedness_detection\" : [ \"stats_mode\" ], \"IDness_detection\" : [ \"stats_unique\" ], \"nullColumns_detection\" : [ \"stats_unique\" , \"stats_mode\" , \"stats_missing\" ], \"variable_clustering\" : [ \"stats_mode\" ], } args_to_statsfunc = { \"stats_unique\" : \"measures_of_cardinality\" , \"stats_mode\" : \"measures_of_centralTendency\" , \"stats_missing\" : \"measures_of_counts\" , } for arg in mainfunc_to_args . get ( func , []): output [ arg ] = { \"file_path\" : ( ends_with ( path ) + args_to_statsfunc [ arg ] + \".csv\" ), \"file_type\" : \"csv\" , \"file_configs\" : { \"header\" : True , \"inferSchema\" : True }, } return output def anovos_basic_report ( spark , idf , id_col = \"\" , label_col = \"\" , event_label = \"\" , skip_corr_matrix = True , output_path = \".\" , run_type = \"local\" , auth_key = \"NA\" , print_impact = True , mlflow_config = None , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe id_col ID column (Default value = \"\") label_col Label/Target column (Default value = \"\") event_label Value of (positive) event (i.e label 1) (Default value = \"\") skip_corr_matrix True, False. This argument is to skip correlation matrix generation in basic_report.(Default value = True) output_path File Path for saving metrics and basic report (Default value = \".\") run_type \"local\", \"emr\" or \"databricks\" or \"ak8s\" \"emr\" if the files are read from or written in AWS s3 \"databricks\" if the files are read from or written in dbfs in azure databricks \"ak8s\" if the files are read from or written to in wasbs:// container in azure environment (Default value = \"local\") auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. print_impact True, False. This argument is to print out the data analyzer statistics.(Default value = False) mlflow_config MLflow configuration. If None, all MLflow features are disabled. \"\"\" global num_cols global cat_cols SG_funcs = [ global_summary , measures_of_counts , measures_of_centralTendency , measures_of_cardinality , measures_of_dispersion , measures_of_percentiles , measures_of_shape , ] QC_rows_funcs = [ duplicate_detection , nullRows_detection ] QC_cols_funcs = [ nullColumns_detection , outlier_detection , IDness_detection , biasedness_detection , invalidEntries_detection , ] if mlflow_config is not None : output_path = output_path + \"/\" + mlflow_config . get ( \"run_id\" , \"\" ) if skip_corr_matrix : AA_funcs = [ variable_clustering ] else : AA_funcs = [ correlation_matrix , variable_clustering ] AT_funcs = [ IV_calculation , IG_calculation ] all_funcs = SG_funcs + QC_rows_funcs + QC_cols_funcs + AA_funcs + AT_funcs if run_type == \"local\" : local_path = output_path elif run_type == \"databricks\" : local_path = output_to_local ( output_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) for func in all_funcs : if func in SG_funcs : stats = func ( spark , idf ) elif func in ( QC_rows_funcs + QC_cols_funcs ): extra_args = stats_args ( output_path , func . __name__ ) if func . __name__ in [ \"outlier_detection\" , \"duplicate_detection\" ]: extra_args [ \"print_impact\" ] = True stats = func ( spark , idf , ** extra_args )[ 1 ] elif func in AA_funcs : extra_args = stats_args ( output_path , func . __name__ ) stats = func ( spark , idf , drop_cols = id_col , ** extra_args ) elif label_col : if func in AT_funcs : stats = func ( spark , idf , label_col = label_col , event_label = event_label ) else : continue stats . toPandas () . to_csv ( ends_with ( local_path ) + func . __name__ + \".csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + func . __name__ + \".csv \" + ends_with ( output_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) elif run_type == \"ak8s\" : local_file = ends_with ( local_path ) + func . __name__ + \".csv\" output_path_mod = path_ak8s_modify ( output_path ) bash_cmd = ( 'azcopy cp \"' + local_file + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true' ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if print_impact : print ( func . __name__ , \": \\n \" ) stats = spark . read . csv ( ends_with ( output_path ) + func . __name__ + \".csv\" , header = True , inferSchema = True , ) stats . show () def remove_u_score ( col ): col_ = col . split ( \"_\" ) bl = [] for i in col_ : if i == \"nullColumns\" or i == \"nullRows\" : bl . append ( \"Null\" ) else : bl . append ( i [ 0 ] . upper () + i [ 1 :]) return \" \" . join ( bl ) global_summary_df = pd . read_csv ( ends_with ( local_path ) + \"global_summary.csv\" ) rows_count = int ( global_summary_df [ global_summary_df . metric . values == \"rows_count\" ] . value . values [ 0 ] ) catcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"catcols_count\" ] . value . values [ 0 ] ) numcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"numcols_count\" ] . value . values [ 0 ] ) columns_count = int ( global_summary_df [ global_summary_df . metric . values == \"columns_count\" ] . value . values [ 0 ] ) catcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"catcols_name\" ] . value . values ) ) numcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"numcols_name\" ] . value . values ) ) l1 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the dataset with key statistical metrics.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Global Summary\" ), dp . Group ( dp . Text ( \" Total Number of Records: **\" + str ( f \" { rows_count : ,d } \" ) + \"**\" ), dp . Text ( \" Total Number of Attributes: **\" + str ( columns_count ) + \"**\" ), dp . Text ( \" Number of Numerical Attributes : **\" + str ( numcols_count ) + \"**\" ), dp . Text ( \" Numerical Attributes Name : **\" + str ( numcols_name ) + \"**\" ), dp . Text ( \" Number of Categorical Attributes : **\" + str ( catcols_count ) + \"**\" ), dp . Text ( \" Categorical Attributes Name : **\" + str ( catcols_name ) + \"**\" ), ), ) l2 = dp . Text ( \"### Statistics by Metric Type\" ) SG_content = [] for i in SG_funcs : if i . __name__ != \"global_summary\" : SG_content . append ( dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ), label = remove_u_score ( i . __name__ ), ) ) l3 = dp . Group ( dp . Select ( blocks = SG_content , type = dp . SelectType . TABS ), dp . Text ( \"# \" )) tab1 = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) QCcol_content = [] for i in QC_cols_funcs : QCcol_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) QCrow_content = [] for i in QC_rows_funcs : if i . __name__ == \"duplicate_detection\" : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) unique_rows_count = ( \" No. Of Unique Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"unique_rows_count\" ] . value . values ), \",\" , ) ) + \"**\" ) total_rows_count = ( \" No. of Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"rows_count\" ] . value . values ), \",\" ) ) + \"**\" ) duplicate_rows_count = ( \" No. of Duplicate Rows: **\" + str ( format ( int ( stats [ stats [ \"metric\" ] == \"duplicate_rows\" ] . value . values ), \",\" , ) ) + \"**\" ) duplicate_rows_pct = ( \" Percentage of Duplicate Rows: **\" + str ( float ( stats [ stats [ \"metric\" ] == \"duplicate_pct\" ] . value . values * 100.0 ) ) + \" %\" + \"**\" ) QCrow_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . Group ( dp . Text ( total_rows_count ), dp . Text ( unique_rows_count ), dp . Text ( duplicate_rows_count ), dp . Text ( duplicate_rows_pct ), ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) else : QCrow_content . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i . __name__ ))), dp . DataTable ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) QCcol_content = [ item for sublist in QCcol_content for item in sublist ] QCrow_content = [ item for sublist in QCrow_content for item in sublist ] tab2 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Text ( \"# \" ), dp . Group ( * QCcol_content ), label = \"Column Level\" ), dp . Group ( dp . Text ( \"# \" ), dp . Group ( * QCrow_content ), label = \"Row Level\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) AA_content = [] for i in AA_funcs + AT_funcs : if i . __name__ == \"correlation_matrix\" : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) feats_order = list ( stats [ \"attribute\" ] . values ) stats = stats . round ( 3 ) fig = px . imshow ( stats [ feats_order ], y = feats_order , color_continuous_scale = global_theme , aspect = \"auto\" , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats [[ \"attribute\" ] + feats_order ]), dp . Plot ( fig ), label = remove_u_score ( i . __name__ ), ) ) elif i . __name__ == \"variable_clustering\" : stats = ( pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) . sort_values ( by = [ \"Cluster\" ], ascending = True ) ) fig = px . sunburst ( stats , path = [ \"Cluster\" , \"Attribute\" ], values = \"RS_Ratio\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . layout . autosize = True AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats ), dp . Plot ( fig ), label = remove_u_score ( i . __name__ ), ) ) else : if label_col : stats = pd . read_csv ( ends_with ( local_path ) + str ( i . __name__ ) + \".csv\" ) . round ( 3 ) col_nm = [ x for x in list ( stats . columns ) if \"attribute\" not in x ] stats = stats . sort_values ( col_nm [ 0 ], ascending = True ) fig = px . bar ( stats , x = col_nm [ 0 ], y = \"attribute\" , orientation = \"h\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . layout . autosize = True AA_content . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( stats ), dp . Plot ( fig ), label = remove_u_score ( i . __name__ ), ) ) if len ( AA_content ) == 1 : AA_content . append ( blank_df ) else : AA_content # @TODO: is there better templating approach such as jinja tab3 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Association Matrix & Plot\" ), dp . Select ( blocks = AA_content , type = dp . SelectType . DROPDOWN ), dp . Text ( \"### \" ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = [ tab1 , tab2 , tab3 ], type = dp . SelectType . TABS ), ) . save ( ends_with ( local_path ) + \"basic_report.html\" , open = True ) if mlflow_config is not None : mlflow . log_artifacts ( local_dir = local_path , artifact_path = output_path ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + \"basic_report.html \" + ends_with ( output_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( output_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + 'basic_report.html\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\"' ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ])","title":"basic_report_generation"},{"location":"api/data_report/basic_report_generation.html#functions","text":"def anovos_basic_report ( spark, idf, id_col='', label_col='', event_label='', skip_corr_matrix=True, output_path='.', run_type='local', auth_key='NA', print_impact=True, mlflow_config=None)","title":"Functions"},{"location":"api/data_report/report_generation.html","text":"report_generation This module generates the final report output specific to the intermediate data generated across each of the modules. The final report, however, can be proccessed through the config.yaml file or by generating it through the respective functions. Below are some of the functions used to process the final output. line_chart_gen_stability data_analyzer_output drift_stability_ind chart_gen_list executive_summary_gen wiki_generator descriptive_statistics quality_check attribute_associations data_drift_stability plotSeasonalDecompose gen_time_series_plots list_ts_remove_append ts_viz_1_1 \u2014 ts_viz_1_3 ts_viz_2_1 \u2014 ts_viz_2_3 ts_viz_3_1 \u2014 ts_viz_3_3 ts_landscape ts_stats ts_viz_generate overall_stats_gen loc_field_stats read_stats_ll_geo read_cluster_stats_ll_geo read_loc_charts loc_report_gen anovos_report However, each of the functions have been detailed in the respective sections across the parameters used. Expand source code # coding=utf-8 \"\"\"This module generates the final report output specific to the intermediate data generated across each of the modules. The final report, however, can be proccessed through the config.yaml file or by generating it through the respective functions. Below are some of the functions used to process the final output. - line_chart_gen_stability - data_analyzer_output - drift_stability_ind - chart_gen_list - executive_summary_gen - wiki_generator - descriptive_statistics - quality_check - attribute_associations - data_drift_stability - plotSeasonalDecompose - gen_time_series_plots - list_ts_remove_append - ts_viz_1_1 \u2014 ts_viz_1_3 - ts_viz_2_1 \u2014 ts_viz_2_3 - ts_viz_3_1 \u2014 ts_viz_3_3 - ts_landscape - ts_stats - ts_viz_generate - overall_stats_gen - loc_field_stats - read_stats_ll_geo - read_cluster_stats_ll_geo - read_loc_charts - loc_report_gen - anovos_report However, each of the functions have been detailed in the respective sections across the parameters used. \"\"\" import json import os import subprocess import warnings import datapane as dp import dateutil.parser import mlflow import numpy as np import pandas as pd import plotly.express as px import plotly.graph_objects as go import plotly.tools as tls from loguru import logger from plotly.subplots import make_subplots from sklearn.preprocessing import PowerTransformer from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.stattools import adfuller , kpss from anovos.shared.utils import ends_with , output_to_local , path_ak8s_modify warnings . filterwarnings ( \"ignore\" ) global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" default_template = ( dp . HTML ( \"\"\" <html> <img src=\"https://mobilewalla-anovos.s3.amazonaws.com/anovos.png\" style=\"height:100px;display:flex;margin:auto;float:right\" /> </html>\"\"\" ), dp . Text ( \"# ML-Anovos Report\" ), ) def remove_u_score ( col ): \"\"\" This functions help to remove the \"_\" present in a specific text Parameters ---------- col Analysis column containing \"_\" present gets replaced along with upper case conversion Returns ------- String \"\"\" col_ = col . split ( \"_\" ) bl = [] for i in col_ : if i == \"nullColumns\" or i == \"nullRows\" : bl . append ( \"Null\" ) else : bl . append ( i [ 0 ] . upper () + i [ 1 :]) return \" \" . join ( bl ) def line_chart_gen_stability ( df1 , df2 , col ): \"\"\" This function helps to produce charts which are specific to data stability index. It taken into account the stability input along with the analysis column to produce the desired output. Parameters ---------- df1 Analysis dataframe pertaining to summarized stability metrics df2 Analysis dataframe pertaining to historical data col Analysis column Returns ------- DatapaneObject \"\"\" def val_cat ( val ): \"\"\" Parameters ---------- val Returns ------- String \"\"\" if val >= 3.5 : return \"Very Stable\" elif val >= 3 and val < 3.5 : return \"Stable\" elif val >= 2 and val < 3 : return \"Marginally Stable\" elif val >= 1 and val < 2 : return \"Unstable\" elif val >= 0 and val < 1 : return \"Very Unstable\" else : return \"Out of Range\" val_si = list ( df2 [ df2 [ \"attribute\" ] == col ] . stability_index . values )[ 0 ] f1 = go . Figure () f1 . add_trace ( go . Indicator ( mode = \"gauge+number\" , value = val_si , gauge = { \"axis\" : { \"range\" : [ None , 4 ], \"tickwidth\" : 1 , \"tickcolor\" : \"black\" }, \"bgcolor\" : \"white\" , \"steps\" : [ { \"range\" : [ 0 , 1 ], \"color\" : px . colors . sequential . Reds [ 7 ]}, { \"range\" : [ 1 , 2 ], \"color\" : px . colors . sequential . Reds [ 6 ]}, { \"range\" : [ 2 , 3 ], \"color\" : px . colors . sequential . Oranges [ 4 ]}, { \"range\" : [ 3 , 3.5 ], \"color\" : px . colors . sequential . BuGn [ 7 ]}, { \"range\" : [ 3.5 , 4 ], \"color\" : px . colors . sequential . BuGn [ 8 ]}, ], \"threshold\" : { \"line\" : { \"color\" : \"black\" , \"width\" : 3 }, \"thickness\" : 1 , \"value\" : val_si , }, \"bar\" : { \"color\" : global_plot_bg_color }, }, title = { \"text\" : \"Order of Stability: \" + val_cat ( val_si )}, ) ) f1 . update_layout ( height = 400 , font = { \"color\" : \"black\" , \"family\" : \"Arial\" }) f5 = \"Stability Index for \" + str ( col . upper ()) if len ( df1 . columns ) > 0 : attr_type = df1 [ \"type\" ] . tolist ()[ 0 ] if attr_type == \"Numerical\" : f2 = px . line ( df1 , x = \"idx\" , y = \"mean\" , markers = True , title = \"CV of Mean is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . mean_cv . values )[ 0 ]), ) f2 . update_traces ( line_color = global_theme [ 2 ], marker = dict ( size = 14 )) f2 . layout . plot_bgcolor = global_plot_bg_color f2 . layout . paper_bgcolor = global_paper_bg_color f3 = px . line ( df1 , x = \"idx\" , y = \"stddev\" , markers = True , title = \"CV of Stddev is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . stddev_cv . values )[ 0 ]), ) f3 . update_traces ( line_color = global_theme [ 6 ], marker = dict ( size = 14 )) f3 . layout . plot_bgcolor = global_plot_bg_color f3 . layout . paper_bgcolor = global_paper_bg_color f4 = px . line ( df1 , x = \"idx\" , y = \"kurtosis\" , markers = True , title = \"CV of Kurtosis is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . kurtosis_cv . values )[ 0 ]), ) f4 . update_traces ( line_color = global_theme [ 4 ], marker = dict ( size = 14 )) f4 . layout . plot_bgcolor = global_plot_bg_color f4 . layout . paper_bgcolor = global_paper_bg_color return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), dp . Group ( dp . Plot ( f2 ), dp . Plot ( f3 ), dp . Plot ( f4 ), columns = 3 ), label = col , ) else : f2 = px . line ( df1 , x = \"idx\" , y = \"mean\" , markers = True , title = \"Standard deviation of Mean is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . mean_stddev . values )[ 0 ]), ) f2 . update_traces ( line_color = global_theme [ 2 ], marker = dict ( size = 14 )) f2 . layout . plot_bgcolor = global_plot_bg_color f2 . layout . paper_bgcolor = global_paper_bg_color return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), dp . Group ( dp . Plot ( f2 ), columns = 1 ), label = col , ) else : return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), label = col ) def data_analyzer_output ( master_path , avl_recs_tab , tab_name ): \"\"\" This section produces output in form of datapane objects which is specific to the different data analyzer modules. It is used by referring to the Master path along with the Available list of metrics & the Tab name. Parameters ---------- master_path Path containing all the output from analyzed data avl_recs_tab Available file names from the analysis tab tab_name Analysis tab from association_evaluator / quality_checker / stats_generator Returns ------- DatapaneObject \"\"\" df_list = [] df_plot_list = [] # @FIXME: unused variables plot_list = [] avl_recs_tab = [ x for x in avl_recs_tab if \"global_summary\" not in x ] for index , i in enumerate ( avl_recs_tab ): data = pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) if len ( data . index ) == 0 : continue if tab_name == \"quality_checker\" : if i == \"duplicate_detection\" : duplicate_recs = pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) _unique_rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"unique_rows_count\" ] . value . values ) _rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"rows_count\" ] . value . values ) _duplicate_rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"duplicate_rows\" ] . value . values ) _duplicate_pct = float ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"duplicate_pct\" ] . value . values * 100.0 ) unique_rows_count = f \" No. Of Unique Rows: ** { _unique_rows_count } **\" # @FIXME: variable names exists in outer scope rows_count = f \" No. of Rows: ** { _rows_count } **\" duplicate_rows = f \" No. of Duplicate Rows: ** { _duplicate_rows_count } **\" duplicate_pct = f \" Percentage of Duplicate Rows: ** { _duplicate_pct } %**\" df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . Group ( dp . Text ( rows_count ), dp . Text ( unique_rows_count ), dp . Text ( duplicate_rows ), dp . Text ( duplicate_pct ), ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) elif i == \"outlier_detection\" : df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) ), \"outlier_charts_placeholder\" , ] ) else : df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) elif tab_name == \"association_evaluator\" : for j in avl_recs_tab : if j == \"correlation_matrix\" : df_list_ = pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) feats_order = list ( df_list_ [ \"attribute\" ] . values ) df_list_ = df_list_ . round ( 3 ) fig = px . imshow ( df_list_ [ feats_order ], y = feats_order , color_continuous_scale = global_theme , aspect = \"auto\" , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Correlation Plot \")) df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ [[ \"attribute\" ] + feats_order ]), dp . Plot ( fig ), label = remove_u_score ( j ), ) ) elif j == \"variable_clustering\" : df_list_ = ( pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) . sort_values ( by = [ \"Cluster\" ], ascending = True ) ) fig = px . sunburst ( df_list_ , path = [ \"Cluster\" , \"Attribute\" ], values = \"RS_Ratio\" , color_discrete_sequence = global_theme , ) # fig.update_layout(title_text=str(\"Distribution of homogenous variable across Clusters\")) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Variable Clustering Plot \")) fig . layout . autosize = True df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ ), dp . Plot ( fig ), label = remove_u_score ( j ), ) ) else : try : df_list_ = pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) col_nm = [ x for x in list ( df_list_ . columns ) if \"attribute\" not in x ] df_list_ = df_list_ . sort_values ( col_nm [ 0 ], ascending = True ) fig = px . bar ( df_list_ , x = col_nm [ 0 ], y = \"attribute\" , orientation = \"h\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Representation of \" + str(remove_u_score(j)))) fig . layout . autosize = True df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ ), dp . Plot ( fig ), label = remove_u_score ( j ), ) ) except Exception as e : logger . error ( f \"processing failed, error { e } \" ) pass if len ( avl_recs_tab ) == 1 : df_plot_list . append ( dp . Group ( dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), dp . Plot ( blank_chart , label = \" \" ), label = \" \" , ) ) else : pass return df_plot_list else : df_list . append ( dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ), label = remove_u_score ( avl_recs_tab [ index ]), ) ) if tab_name == \"quality_checker\" and len ( avl_recs_tab ) == 1 : return df_list [ 0 ], [ dp . Text ( \"#\" ), dp . Plot ( blank_chart )] elif tab_name == \"stats_generator\" and len ( avl_recs_tab ) == 1 : return [ df_list [ 0 ], dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), ] else : return df_list def drift_stability_ind ( missing_recs_drift , drift_tab , missing_recs_stability , stability_tab ): \"\"\" This function helps to produce the drift & stability indicator for further processing. Ideally a data with both drift & stability should produce a list of [1,1] Parameters ---------- missing_recs_drift Missing files from the drift tab drift_tab \"drift_statistics\" missing_recs_stability Missing files from the stability tab stability_tab \"stability_index, stabilityIndex_metrics\" Returns ------- List \"\"\" if len ( missing_recs_drift ) == len ( drift_tab ): drift_ind = 0 else : drift_ind = 1 if len ( missing_recs_stability ) == len ( stability_tab ): stability_ind = 0 elif ( \"stabilityIndex_metrics\" in missing_recs_stability ) and ( \"stability_index\" not in missing_recs_stability ): stability_ind = 0.5 else : stability_ind = 1 return drift_ind , stability_ind def chart_gen_list ( master_path , chart_type , type_col = None ): \"\"\" This function helps to produce the charts in a list object form nested by a datapane object. Parameters ---------- master_path Path containing all the charts same as the other files from data analyzed output chart_type Files containing only the specific chart names for the specific chart category type_col None. Default value is kept as None Returns ------- DatapaneObject \"\"\" plot_list = [] for i in chart_type : col_name = i [ i . find ( \"_\" ) + 1 :] if type_col == \"numerical\" : if col_name in numcols_name . replace ( \" \" , \"\" ) . split ( \",\" ): plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) else : pass elif type_col == \"categorical\" : if col_name in catcols_name . replace ( \" \" , \"\" ) . split ( \",\" ): plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) else : pass else : plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) return plot_list def executive_summary_gen ( master_path , label_col , ds_ind , id_col , iv_threshold , corr_threshold , print_report = False , ): \"\"\" This function helps to produce output specific to the Executive Summary Tab. Parameters ---------- master_path Path containing the input files. label_col Label column. ds_ind Drift stability indicator in list form. id_col ID column. iv_threshold IV threshold beyond which attributes can be called as significant. corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" try : obj_dtls = json . load ( open ( ends_with ( master_path ) + \"freqDist_\" + str ( label_col )) ) # @FIXME: never used local variable text_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 8 ][ 1 ] x_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 10 ][ 1 ] y_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 12 ][ 1 ] label_fig_ = go . Figure ( data = [ go . Pie ( labels = x_val , values = y_val , textinfo = \"label+percent\" , insidetextorientation = \"radial\" , pull = [ 0 , 0.1 ], marker_colors = global_theme , ) ] ) label_fig_ . update_traces ( textposition = \"inside\" , textinfo = \"percent+label\" ) label_fig_ . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) label_fig_ . layout . plot_bgcolor = global_plot_bg_color label_fig_ . layout . paper_bgcolor = global_paper_bg_color except Exception as e : logger . error ( f \"processing failed, error { e } \" ) label_fig_ = None a1 = ( \"The dataset contains **\" + str ( f \" { rows_count : ,d } \" ) + \"** records and **\" + str ( numcols_count + catcols_count ) + \"** attributes (**\" + str ( numcols_count ) + \"** numerical + **\" + str ( catcols_count ) + \"** categorical).\" ) if label_col is None : a2 = dp . Group ( dp . Text ( \"- There is **no** target variable in the dataset\" ), dp . Text ( \"- Data Diagnosis:\" ), ) else : if label_fig_ is None : a2 = dp . Group ( dp . Text ( \"- Target variable is **\" + str ( label_col ) + \"** \" ), dp . Text ( \"- Data Diagnosis:\" ), ) else : a2 = dp . Group ( dp . Text ( \"- Target variable is **\" + str ( label_col ) + \"** \" ), dp . Plot ( label_fig_ ), dp . Text ( \"- Data Diagnosis:\" ), ) try : x1 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_dispersion.csv\" ) . query ( \"`cov`>1\" ) . attribute . values ) if len ( x1 ) > 0 : x1_1 = [ \"High Variance\" , x1 ] else : x1_1 = [ \"High Variance\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x1_1 = [ \"High Variance\" , None ] try : x2 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`skewness`>0\" ) . attribute . values ) if len ( x2 ) > 0 : x2_1 = [ \"Positive Skewness\" , x2 ] else : x2_1 = [ \"Positive Skewness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x2_1 = [ \"Positive Skewness\" , None ] try : x3 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`skewness`<0\" ) . attribute . values ) if len ( x3 ) > 0 : x3_1 = [ \"Negative Skewness\" , x3 ] else : x3_1 = [ \"Negative Skewness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x3_1 = [ \"Negative Skewness\" , None ] try : x4 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`kurtosis`>0\" ) . attribute . values ) if len ( x4 ) > 0 : x4_1 = [ \"High Kurtosis\" , x4 ] else : x4_1 = [ \"High Kurtosis\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x4_1 = [ \"High Kurtosis\" , None ] try : x5 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`kurtosis`<0\" ) . attribute . values ) if len ( x5 ) > 0 : x5_1 = [ \"Low Kurtosis\" , x5 ] else : x5_1 = [ \"Low Kurtosis\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x5_1 = [ \"Low Kurtosis\" , None ] try : x6 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_counts.csv\" ) . query ( \"`fill_pct`<0.7\" ) . attribute . values ) if len ( x6 ) > 0 : x6_1 = [ \"Low Fill Rates\" , x6 ] else : x6_1 = [ \"Low Fill Rates\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x6_1 = [ \"Low Fill Rates\" , None ] try : biasedness_df = pd . read_csv ( ends_with ( master_path ) + \"biasedness_detection.csv\" ) if \"treated\" in biasedness_df : x7 = list ( biasedness_df . query ( \"`treated`>0\" ) . attribute . values ) else : x7 = list ( biasedness_df . query ( \"`flagged`>0\" ) . attribute . values ) if len ( x7 ) > 0 : x7_1 = [ \"High Biasedness\" , x7 ] else : x7_1 = [ \"High Biasedness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x7_1 = [ \"High Biasedness\" , None ] try : x8 = list ( pd . read_csv ( ends_with ( master_path ) + \"outlier_detection.csv\" ) . attribute . values ) if len ( x8 ) > 0 : x8_1 = [ \"Outliers\" , x8 ] else : x8_1 = [ \"Outliers\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x8_1 = [ \"Outliers\" , None ] try : corr_matrx = pd . read_csv ( ends_with ( master_path ) + \"correlation_matrix.csv\" ) corr_matrx = corr_matrx [ list ( corr_matrx . attribute . values )] corr_matrx = corr_matrx . where ( np . triu ( np . ones ( corr_matrx . shape ), k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in corr_matrx . columns if any ( corr_matrx [ column ] > corr_threshold ) ] if len ( to_drop ) > 0 : x9_1 = [ \"High Correlation\" , to_drop ] else : x9_1 = [ \"High Correlation\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x9_1 = [ \"High Correlation\" , None ] try : x10 = list ( pd . read_csv ( ends_with ( master_path ) + \"IV_calculation.csv\" ) . query ( \"`iv`>\" + str ( iv_threshold )) . attribute . values ) if len ( x10 ) > 0 : x10_1 = [ \"Significant Attributes\" , x10 ] else : x10_1 = [ \"Significant Attributes\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x10_1 = [ \"Significant Attributes\" , None ] blank_list_df = [] for i in [ x1_1 , x2_1 , x3_1 , x4_1 , x5_1 , x6_1 , x7_1 , x8_1 , x9_1 , x10_1 ]: try : for j in i [ 1 ]: blank_list_df . append ([ i [ 0 ], j ]) except Exception as e : logger . error ( f \"processing failed, error { e } \" ) blank_list_df . append ([ i [ 0 ], \"NA\" ]) list_n = [] x1 = pd . DataFrame ( blank_list_df , columns = [ \"Metric\" , \"Attribute\" ]) x1 [ \"Value\" ] = \"\u2714\" all_cols = ( catcols_name . replace ( \" \" , \"\" ) + \",\" + numcols_name . replace ( \" \" , \"\" ) ) . split ( \",\" ) remainder_cols = list ( set ( all_cols ) - set ( x1 . Attribute . values )) total_metrics = set ( list ( x1 . Metric . values )) for i in remainder_cols : for j in total_metrics : list_n . append ([ j , i ]) x2 = pd . DataFrame ( list_n , columns = [ \"Metric\" , \"Attribute\" ]) x2 [ \"Value\" ] = \"\u2718\" x = x1 . append ( x2 , ignore_index = True ) x = ( x . drop_duplicates () . pivot ( index = \"Attribute\" , columns = \"Metric\" , values = \"Value\" ) . fillna ( \"\u2718\" ) . reset_index ()[ [ \"Attribute\" , \"Outliers\" , \"Significant Attributes\" , \"Positive Skewness\" , \"Negative Skewness\" , \"High Variance\" , \"High Correlation\" , \"High Kurtosis\" , \"Low Kurtosis\" , ] ] ) x = x [ ~ ( ( x [ \"Attribute\" ] . isnull ()) | ( x . Attribute . values == \"NA\" ) | ( x [ \"Attribute\" ] == \" \" ) ) ] if ds_ind [ 0 ] == 1 and ds_ind [ 1 ] >= 0.5 : a5 = \"Data Health based on Drift Metrics & Stability Index : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Drifted Attributes\" , value = str ( str ( drifted_feats ) + \" out of \" + str ( len_feats )), ), dp . BigNumber ( heading = \"% Drifted Attributes\" , value = str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%\" , ), dp . BigNumber ( heading = \"# Unstable Attributes\" , value = str ( len ( unstable_attr )) + \" out of \" + str ( len ( total_unstable_attr )), change = \"numerical\" , is_upward_change = True , ), dp . BigNumber ( heading = \"% Unstable Attributes\" , value = str ( np . round ( 100 * len ( unstable_attr ) / len ( total_unstable_attr ), 2 ) ) + \"%\" , ), columns = 4 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] >= 0.5 : a5 = \"Data Health based on Stability Index : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Unstable Attributes\" , value = str ( len ( unstable_attr )) + \" out of \" + str ( len ( total_unstable_attr )), change = \"numerical\" , is_upward_change = True , ), dp . BigNumber ( heading = \"% Unstable Attributes\" , value = str ( np . round ( 100 * len ( unstable_attr ) / len ( total_unstable_attr ), 2 ) ) + \"%\" , ), columns = 2 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 1 and ds_ind [ 1 ] == 0 : a5 = \"Data Health based on Drift Metrics : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Drifted Attributes\" , value = str ( str ( drifted_feats ) + \" out of \" + str ( len_feats )), ), dp . BigNumber ( heading = \"% Drifted Attributes\" , value = str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%\" , ), columns = 2 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] == 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"executive_summary.html\" , open = True ) return report # @FIXME: rename variables with their corresponding within the config files def wiki_generator ( master_path , dataDict_path = None , metricDict_path = None , print_report = False ): \"\"\" This function helps to produce output specific to the Wiki Tab. Parameters ---------- master_path Path containing the input files. dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" try : datatype_df = pd . read_csv ( ends_with ( master_path ) + \"data_type.csv\" ) except FileNotFoundError : logger . error ( f \"file { master_path } /data_type.csv doesn't exist, cannot read datatypes\" ) except Exception : logger . info ( \"generate an empty dataframe with columns attribute and data_type \" ) datatype_df = pd . DataFrame ( columns = [ \"attribute\" , \"data_type\" ], index = range ( 1 )) try : data_dict = pd . read_csv ( dataDict_path ) . merge ( datatype_df , how = \"outer\" , on = \"attribute\" ) except FileNotFoundError : logger . error ( f \"file { dataDict_path } doesn't exist, cannot read data dict\" ) except Exception : data_dict = datatype_df try : metric_dict = pd . read_csv ( metricDict_path ) except FileNotFoundError : logger . error ( f \"file { metricDict_path } doesn't exist, cannot read metrics dict\" ) except Exception : metric_dict = pd . DataFrame ( columns = [ \"Section Category\" , \"Section Name\" , \"Metric Name\" , \"Metric Definitions\" , ], index = range ( 1 ), ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *A quick reference to the attributes from the dataset (Data Dictionary) and the metrics computed in the report (Metric Dictionary).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Group ( dp . Text ( \"## \" ), dp . DataTable ( data_dict )), label = \"Data Dictionary\" , ), dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( metric_dict ), label = \"Metric Dictionary\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Wiki\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"wiki_generator.html\" , open = True ) return report def descriptive_statistics ( master_path , SG_tabs , avl_recs_SG , missing_recs_SG , all_charts_num_1_ , all_charts_cat_1_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Descriptive Stats Tab. Parameters ---------- master_path Path containing the input files. SG_tabs measures_of_counts','measures_of_centralTendency','measures_of_cardinality','measures_of_percentiles','measures_of_dispersion','measures_of_shape','global_summary' avl_recs_SG Available files from the SG_tabs (Stats Generator tabs) missing_recs_SG Missing files from the SG_tabs (Stats Generator tabs) all_charts_num_1_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_1_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" if \"global_summary\" in avl_recs_SG : cnt = 0 else : cnt = 1 if len ( missing_recs_SG ) + cnt == len ( SG_tabs ): return \"null_report\" else : if \"global_summary\" in avl_recs_SG : l1 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the dataset with key statistical metrics and distribution plots.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Global Summary\" ), dp . Group ( dp . Text ( \" Total Number of Records: **\" + f \" { rows_count : , } \" + \"**\" ), dp . Text ( \" Total Number of Attributes: **\" + str ( columns_count ) + \"**\" ), dp . Text ( \" Number of Numerical Attributes : **\" + str ( numcols_count ) + \"**\" ), dp . Text ( \" Numerical Attributes Name : **\" + str ( numcols_name ) + \"**\" ), dp . Text ( \" Number of Categorical Attributes : **\" + str ( catcols_count ) + \"**\" ), dp . Text ( \" Categorical Attributes Name : **\" + str ( catcols_name ) + \"**\" ), ), ) else : l1 = dp . Text ( \"# \" ) if len ( data_analyzer_output ( master_path , avl_recs_SG , \"stats_generator\" )) > 0 : l2 = dp . Text ( \"### Statistics by Metric Type\" ) l3 = dp . Group ( dp . Select ( blocks = data_analyzer_output ( master_path , avl_recs_SG , \"stats_generator\" ), type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), ) else : l2 = dp . Text ( \"# \" ) l3 = dp . Text ( \"# \" ) if len ( all_charts_num_1_ ) == 0 and len ( all_charts_cat_1_ ) == 0 : l4 = 1 elif len ( all_charts_num_1_ ) == 0 and len ( all_charts_cat_1_ ) > 0 : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Select ( blocks = all_charts_cat_1_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) elif len ( all_charts_num_1_ ) > 0 and len ( all_charts_cat_1_ ) == 0 : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Select ( blocks = all_charts_num_1_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) else : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Group ( dp . Select ( blocks = [ dp . Group ( dp . Select ( blocks = all_charts_num_1_ , type = dp . SelectType . DROPDOWN , ), label = \"Numerical\" , ), dp . Group ( dp . Select ( blocks = all_charts_cat_1_ , type = dp . SelectType . DROPDOWN , ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ) ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) if l4 == 1 : report = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) else : report = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , * l4 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"descriptive_statistics.html\" , open = True ) return report def quality_check ( master_path , QC_tabs , avl_recs_QC , missing_recs_QC , all_charts_num_3_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Quality Checker Tab. Parameters ---------- master_path Path containing the input files. QC_tabs nullColumns_detection','IDness_detection','biasedness_detection','invalidEntries_detection','duplicate_detection','nullRows_detection','outlier_detection' avl_recs_QC Available files from the QC_tabs (Quality Checker tabs) missing_recs_QC Missing files from the QC_tabs (Quality Checker tabs) all_charts_num_3_ Numerical charts (outlier charts) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" c_ = [] r_ = [] if len ( missing_recs_QC ) == len ( QC_tabs ): return \"null_report\" else : row_wise = [ \"duplicate_detection\" , \"nullRows_detection\" ] col_wise = [ \"nullColumns_detection\" , \"IDness_detection\" , \"biasedness_detection\" , \"invalidEntries_detection\" , \"outlier_detection\" , ] row_wise_ = [ p for p in row_wise if p in avl_recs_QC ] col_wise_ = [ p for p in col_wise if p in avl_recs_QC ] len_row_wise = len ([ p for p in row_wise if p in avl_recs_QC ]) len_col_wise = len ([ p for p in col_wise if p in avl_recs_QC ]) if len_row_wise == 0 : c = data_analyzer_output ( master_path , col_wise_ , \"quality_checker\" ) for i in c : for j in i : if j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) > 1 : c_ . append ( dp . Select ( blocks = all_charts_num_3_ , type = dp . SelectType . DROPDOWN ) ) elif ( j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) == 0 ): c_ . append ( dp . Plot ( blank_chart )) else : c_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Group ( * c_ ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) elif len_col_wise == 0 : r = data_analyzer_output ( master_path , row_wise_ , \"quality_checker\" ) for i in r : for j in i : r_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Group ( * r_ ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) else : c = data_analyzer_output ( master_path , col_wise_ , \"quality_checker\" ) for i in c : for j in i : if j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) > 1 : c_ . append ( dp . Select ( blocks = all_charts_num_3_ , type = dp . SelectType . DROPDOWN ) ) elif ( j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) == 0 ): c_ . append ( dp . Plot ( blank_chart )) else : c_ . append ( j ) r = data_analyzer_output ( master_path , row_wise_ , \"quality_checker\" ) for i in r : for j in i : r_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Text ( \"# \" ), dp . Group ( * c_ ), label = \"Column Level\" ), dp . Group ( dp . Text ( \"# \" ), dp . Group ( * r_ ), label = \"Row Level\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"quality_check.html\" , open = True ) return report def attribute_associations ( master_path , AE_tabs , avl_recs_AE , missing_recs_AE , label_col , all_charts_num_2_ , all_charts_cat_2_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Attribute Association Tab. Parameters ---------- master_path Path containing the input files. AE_tabs correlation_matrix','IV_calculation','IG_calculation','variable_clustering' avl_recs_AE Available files from the AE_tabs (Association Evaluator tabs) missing_recs_AE Missing files from the AE_tabs (Association Evaluator tabs) label_col label column all_charts_num_2_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_2_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" if ( len ( missing_recs_AE ) == len ( AE_tabs )) and ( ( len ( all_charts_num_2_ ) + len ( all_charts_cat_2_ )) == 0 ): return \"null_report\" else : if len ( all_charts_num_2_ ) == 0 and len ( all_charts_cat_2_ ) == 0 : target_association_rep = dp . Text ( \"##\" ) else : if len ( all_charts_num_2_ ) > 0 and len ( all_charts_cat_2_ ) == 0 : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Text ( \"\"\" *Bivariate Distribution considering the event captured across different attribute splits (or categories)* \"\"\" ), dp . Select ( blocks = all_charts_num_2_ , type = dp . SelectType . DROPDOWN ), label = \"Numerical\" , ) elif len ( all_charts_num_2_ ) == 0 and len ( all_charts_cat_2_ ) > 0 : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Text ( \"\"\" *Bivariate Distribution considering the event captured across different attribute splits (or categories)* \"\"\" ), dp . Select ( blocks = all_charts_cat_2_ , type = dp . SelectType . DROPDOWN ), label = \"Categorical\" , ) else : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Select ( blocks = [ dp . Group ( dp . Select ( blocks = all_charts_num_2_ , type = dp . SelectType . DROPDOWN , ), label = \"Numerical\" , ), dp . Group ( dp . Select ( blocks = all_charts_cat_2_ , type = dp . SelectType . DROPDOWN , ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), dp . Text ( \"\"\" *Event Rate is defined as % of event label (i.e. label 1) in a bin or a categorical value of an attribute.* \"\"\" ), dp . Text ( \"# \" ), ) if len ( missing_recs_AE ) == len ( AE_tabs ): report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"## \" ), target_association_rep , dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Association Matrix & Plot\" ), dp . Select ( blocks = data_analyzer_output ( master_path , avl_recs_AE , tab_name = \"association_evaluator\" ), type = dp . SelectType . DROPDOWN , ), dp . Text ( \"### \" ), dp . Text ( \"## \" ), target_association_rep , dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"attribute_associations.html\" , open = True ) return report def data_drift_stability ( master_path , ds_ind , id_col , drift_threshold_model , all_drift_charts_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Data Drift & Stability Tab. Parameters ---------- master_path Path containing the input files. ds_ind Drift stability indicator in list form. id_col ID column drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not all_drift_charts_ Charts (histogram/barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" line_chart_list = [] if ds_ind [ 0 ] > 0 : fig_metric_drift = go . Figure () fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 0 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 1 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 0 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 1 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 3 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 1 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 2 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 5 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 2 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 3 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 7 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 3 ], ) ) fig_metric_drift . add_vrect ( x0 = 0 , x1 = drift_threshold_model , fillcolor = global_theme [ 7 ], opacity = 0.1 , layer = \"below\" , line_width = 1 , ), fig_metric_drift . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) fig_metric_drift . layout . plot_bgcolor = global_plot_bg_color fig_metric_drift . layout . paper_bgcolor = global_paper_bg_color fig_metric_drift . update_xaxes ( showline = True , linewidth = 2 , gridcolor = px . colors . sequential . Greys [ 1 ] ) fig_metric_drift . update_yaxes ( showline = True , linewidth = 2 , gridcolor = px . colors . sequential . Greys [ 2 ] ) # Drift Chart - 2 fig_gauge_drift = go . Figure ( go . Indicator ( domain = { \"x\" : [ 0 , 1 ], \"y\" : [ 0 , 1 ]}, value = drifted_feats , mode = \"gauge+number\" , title = { \"text\" : \"\" }, gauge = { \"axis\" : { \"range\" : [ None , len_feats ]}, \"bar\" : { \"color\" : px . colors . sequential . Reds [ 7 ]}, \"steps\" : [ { \"range\" : [ 0 , drifted_feats ], \"color\" : px . colors . sequential . Reds [ 8 ], }, { \"range\" : [ drifted_feats , len_feats ], \"color\" : px . colors . sequential . Greens [ 8 ], }, ], \"threshold\" : { \"line\" : { \"color\" : \"black\" , \"width\" : 3 }, \"thickness\" : 1 , \"value\" : len_feats , }, }, ) ) fig_gauge_drift . update_layout ( font = { \"color\" : \"black\" , \"family\" : \"Arial\" }) def drift_text_gen ( drifted_feats , len_feats ): \"\"\" Parameters ---------- drifted_feats count of attributes drifted len_feats count of attributes passed for analysis Returns ------- String \"\"\" if drifted_feats == 0 : text = \"\"\" *Drift barometer does not indicate any drift in the underlying data. Please refer to the metric values as displayed in the above table & comparison plot for better understanding* \"\"\" elif drifted_feats == 1 : text = ( \"*Drift barometer indicates that \" + str ( drifted_feats ) + \" out of \" + str ( len_feats ) + \" (\" + str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%) attributes has been drifted from its source behaviour.*\" ) elif drifted_feats > 1 : text = ( \"*Drift barometer indicates that \" + str ( drifted_feats ) + \" out of \" + str ( len_feats ) + \" (\" + str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%) attributes have been drifted from its source behaviour.*\" ) else : text = \"\" return text else : pass if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] == 0 : return \"null_report\" elif ds_ind [ 0 ] == 0 and ds_ind [ 1 ] > 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 1 and ds_ind [ 1 ] == 0 : if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 1 and ds_ind [ 1 ] >= 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 0 and ds_ind [ 1 ] >= 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) else : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"data_drift_stability.html\" , open = True ) return report def plotSeasonalDecompose ( base_path , x_col , y_col , metric_col = \"median\" , title = \"Seasonal Decomposition\" ): \"\"\" This function helps to produce output specific to the Seasonal Decomposition of Time Series. Ideally it's expected to source a data containing atleast 2 cycles or 24 months as the most. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names metric_col Metric of aggregation. Options can be between \"Median\", \"Mean\", \"Min\", \"Max\" title \"Title Description\" Returns ------- Plot \"\"\" df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_daily.csv\" ) . dropna () df [ x_col ] = pd . to_datetime ( df [ x_col ], format = \"%Y-%m- %d %H:%M:%S. %f \" ) df = df . set_index ( x_col ) if len ([ x for x in df . columns if \"min\" in x ]) == 0 : # result = seasonal_decompose(df[metric_col],model=\"additive\") pass else : result = seasonal_decompose ( df [ metric_col ], model = \"additive\" , period = 12 ) fig = make_subplots ( rows = 2 , cols = 2 , subplot_titles = [ \"Observed\" , \"Trend\" , \"Seasonal\" , \"Residuals\" ], ) # fig = go.Figure() fig . add_trace ( go . Scatter ( x = df . index , y = result . observed , name = \"Observed\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 0 ]), ), row = 1 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . trend , name = \"Trend\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 2 ]), ), row = 1 , col = 2 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . seasonal , name = \"Seasonal\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 4 ]), ), row = 2 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . resid , name = \"Residuals\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 6 ]), ), row = 2 , col = 2 , ) # fig.add_trace(go.Scatter(x=df.index, y=result.observed, name =\"Observed\", mode='lines+markers',line=dict(color=global_theme[0]))) # fig.add_trace(go.Scatter(x=df.index, y=result.trend, name =\"Trend\", mode='lines+markers',line=dict(color=global_theme[2]))) # fig.add_trace(go.Scatter(x=df.index, y=result.seasonal, name =\"Seasonal\", mode='lines+markers',line=dict(color=global_theme[4]))) # fig.add_trace(go.Scatter(x=df.index, y=result.resid, name =\"Residuals\", mode='lines+markers',line=dict(color=global_theme[6]))) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( autosize = True , width = 2000 , height = 800 ) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def gen_time_series_plots ( base_path , x_col , y_col , time_cat ): \"\"\" This function helps to produce Time Series Plots by sourcing the aggregated data as Daily/Hourly/Weekly level. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names time_cat Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Plot \"\"\" df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_\" + time_cat + \".csv\" ) . dropna () if len ([ x for x in df . columns if \"min\" in x ]) == 0 : if time_cat == \"daily\" : # x_col = x_col + \"_ts\" fig = px . line ( df , x = x_col , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) fig . update_layout ( xaxis = dict ( rangeselector = dict ( buttons = list ( [ dict ( count = 1 , label = \"1m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 3 , label = \"3m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 6 , label = \"6m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 1 , label = \"YTD\" , step = \"year\" , stepmode = \"todate\" ), dict ( count = 1 , label = \"1y\" , step = \"year\" , stepmode = \"backward\" , ), dict ( step = \"all\" ), ] ) ), rangeslider = dict ( visible = True ), type = \"date\" , ) ) elif time_cat == \"weekly\" : fig = px . bar ( df , x = \"dow\" , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) # fig.update_layout(barmode='stack') elif time_cat == \"hourly\" : fig = px . bar ( df , x = \"daypart_cat\" , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) # fig.update_layout(barmode='stack') else : pass else : if time_cat == \"daily\" : # x_col = x_col + \"_ts\" f1 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"min\" ]), name = \"Min\" , line = dict ( color = global_theme [ 6 ]), ) f2 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"max\" ]), name = \"Max\" , line = dict ( color = global_theme [ 4 ]), ) f3 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"mean\" ]), name = \"Mean\" , line = dict ( color = global_theme [ 2 ]), ) f4 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"median\" ]), name = \"Median\" , line = dict ( color = global_theme [ 0 ]), ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( xaxis = dict ( rangeselector = dict ( buttons = list ( [ dict ( count = 1 , label = \"1m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 3 , label = \"3m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 6 , label = \"6m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 1 , label = \"YTD\" , step = \"year\" , stepmode = \"todate\" ), dict ( count = 1 , label = \"1y\" , step = \"year\" , stepmode = \"backward\" , ), dict ( step = \"all\" ), ] ) ), rangeslider = dict ( visible = True ), type = \"date\" , ) ) elif time_cat == \"weekly\" : f1 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"min\" ]), marker_color = global_theme [ 6 ], name = \"Min\" , ) f2 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"max\" ]), marker_color = global_theme [ 4 ], name = \"Max\" , ) f3 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"mean\" ]), marker_color = global_theme [ 2 ], name = \"Mean\" , ) f4 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"median\" ]), marker_color = global_theme [ 0 ], name = \"Median\" , ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( barmode = \"group\" ) elif time_cat == \"hourly\" : f1 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"min\" ]), marker_color = global_theme [ 6 ], name = \"Min\" , ) f2 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"max\" ]), marker_color = global_theme [ 4 ], name = \"Max\" , ) f3 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"mean\" ]), marker_color = global_theme [ 2 ], name = \"Mean\" , ) f4 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"median\" ]), marker_color = global_theme [ 0 ], name = \"Median\" , ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( barmode = \"group\" ) else : pass fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def list_ts_remove_append ( l , opt ): \"\"\" This function helps to remove or append \"_ts\" from any list. Parameters ---------- l List containing column name opt Option to choose between 1 & Others to enable the functionality of removing or appending \"_ts\" within the elements of a list Returns ------- List \"\"\" ll = [] if opt == 1 : for i in l : if i [ - 3 :] == \"_ts\" : ll . append ( i [ 0 : - 3 :]) else : ll . append ( i ) return ll else : for i in l : if i [ - 3 :] == \"_ts\" : ll . append ( i ) else : ll . append ( i + \"_ts\" ) return ll def ts_viz_1_1 ( base_path , x_col , y_col , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Plot \"\"\" ts_fig = gen_time_series_plots ( base_path , x_col , y_col , output_type ) return ts_fig def ts_viz_1_2 ( base_path , ts_col , col_list , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical / Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( col_list ) > 1 : bl . append ( dp . Group ( ts_viz_1_1 ( base_path , ts_col , i , output_type ), label = i )) else : bl . append ( dp . Group ( ts_viz_1_1 ( base_path , ts_col , i , output_type ), label = i )) bl . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_1_3 ( base_path , ts_col , num_cols , cat_cols , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names cat_cols Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject \"\"\" ts_v = [] # print(num_cols) # print(cat_cols) if len ( num_cols ) == 0 : for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = i ) ) else : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = i ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) elif len ( cat_cols ) == 0 : for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = i ) ) else : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = i ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) elif ( len ( num_cols ) >= 1 ) & ( len ( cat_cols ) >= 1 ): for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( dp . Select ( blocks = [ dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = \"Numerical\" , ), dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), label = i , ) ) else : ts_v . append ( dp . Group ( dp . Select ( blocks = [ dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = \"Numerical\" , ), dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_viz_2_1 ( base_path , x_col , y_col ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns ------- DatapaneObject \"\"\" ts_fig = [] for i in [ \"mean\" , \"median\" , \"min\" , \"max\" ]: ts_fig . append ( dp . Plot ( plotSeasonalDecompose ( base_path , x_col , y_col , metric_col = i ), label = i . title (), ) ) return dp . Select ( blocks = ts_fig , type = dp . SelectType . TABS ) def ts_viz_2_2 ( base_path , ts_col , col_list ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( col_list ) > 1 : bl . append ( dp . Group ( ts_viz_2_1 ( base_path , ts_col , i ), label = i )) else : bl . append ( dp . Group ( ts_viz_2_1 ( base_path , ts_col , i ), label = i )) bl . append ( dp . Group ( dp . Plot ( blank_chart , label = \" \" ), label = \" \" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_2_3 ( base_path , ts_col , num_cols ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns ------- DatapaneObject \"\"\" ts_v = [] if len ( ts_col ) > 1 : for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 24 : ts_v . append ( dp . Group ( ts_viz_2_2 ( base_path , i , num_cols ), label = i )) else : ts_v . append ( dp . Group ( dp . Text ( \"The plots couldn't be displayed as x must have 2 complete cycles requires 24 observations. x only has \" + str ( f ) + \" observation(s)\" ), label = i , ) ) else : for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 24 : ts_v . append ( dp . Group ( ts_viz_2_2 ( base_path , i , num_cols ), label = i )) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) else : ts_v . append ( dp . Group ( dp . Text ( \"The plots couldn't be displayed as x must have 2 complete cycles requires 24 observations. x only has \" + str ( f ) + \" observation(s)\" ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_landscape ( base_path , ts_cols , id_col ): \"\"\" This function helps to produce a basic landscaping view of the data by picking up the base path for reading the aggregated data and specified by the timestamp / date column & the ID column. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name id_col ID Column Returns ------- DatapaneObject \"\"\" if ts_cols is None : return dp . Text ( \"#\" ) else : df_stats_ts = [] for i in ts_cols : if len ( ts_cols ) > 1 : df_stats_ts . append ( dp . Group ( dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*ID considered here is : \" + str ( id_col ) + \"*\" ), dp . Text ( \"# \" ), dp . Text ( \"#### Consistency Analysis Of Dates\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_1.csv\" ) . set_index ( \"attribute\" ) . T , label = i , ), ), dp . Group ( dp . Text ( \"*The Percentile distribution across different bins of ID-Date / Date-ID combination should be in a considerable range to determine the regularity of Time series. In an ideal scenario the proportion of dates within each ID should be same. Also, the count of IDs across unique dates should be consistent for a balanced distribution*\" ), dp . Text ( \"# \" ), dp . Text ( \"#### Vital Statistics\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . T . rename ( columns = { 0 : \"\" }), label = i , ), ), label = i , ) ) else : df_stats_ts . append ( dp . Group ( dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*ID considered here is : \" + str ( id_col ) + \"*\" ), dp . Text ( \"#### Consistency Analysis Of Dates\" ), dp . Text ( \"# \" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_1.csv\" ) . set_index ( \"attribute\" ) . T , label = i , ), ), dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"#### Vital Statistics\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . T . rename ( columns = { 0 : \"\" }), label = i , ), ), label = i , ) ) df_stats_ts . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Group ( dp . Text ( \"### Time Stamp Data Diagnosis\" ), dp . Select ( blocks = df_stats_ts , type = dp . SelectType . DROPDOWN ), ) def lambda_cat ( val ): \"\"\" Parameters ---------- val Value of Box Cox Test which translates into the transformation to be applied. Returns ------- String \"\"\" if val < - 1 : return \"Reciprocal Square Transform\" elif val >= - 1 and val < - 0.5 : return \"Reciprocal Transform\" elif val >= - 0.5 and val < 0 : return \"Receiprocal Square Root Transform\" elif val >= 0 and val < 0.5 : return \"Log Transform\" elif val >= 0.5 and val < 1 : return \"Square Root Transform\" elif val >= 1 and val < 2 : return \"No Transform\" elif val >= 2 : return \"Square Transform\" else : return \"ValueOutOfRange\" def ts_viz_3_1 ( base_path , x_col , y_col ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns ------- DatapaneObject \"\"\" ts_fig = [] df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_daily.csv\" ) . dropna () df [ x_col ] = pd . to_datetime ( df [ x_col ], format = \"%Y-%m- %d %H:%M:%S. %f \" ) df = df . set_index ( x_col ) for metric_col in [ \"mean\" , \"median\" , \"min\" , \"max\" ]: try : adf_test = ( round ( adfuller ( df [ metric_col ])[ 0 ], 3 ), round ( adfuller ( df [ metric_col ])[ 1 ], 3 ), ) if adf_test [ 1 ] < 0.05 : adf_flag = True else : adf_flag = False except : adf_test = ( \"nan\" , \"nan\" ) adf_flag = False try : kpss_test = ( round ( kpss ( df [ metric_col ], regression = \"ct\" )[ 0 ], 3 ), round ( kpss ( df [ metric_col ], regression = \"ct\" )[ 1 ], 3 ), ) if kpss_test [ 1 ] < 0.05 : kpss_flag = True else : kpss_flag = False except : kpss_test = ( \"nan\" , \"nan\" ) kpss_flag = False # df[metric_col] = df[metric_col].apply(lambda x: boxcox1p(x,0.25)) # lambda_box_cox = round(boxcox(df[metric_col])[1],5) fit = PowerTransformer ( method = \"yeo-johnson\" ) try : lambda_box_cox = round ( fit . fit ( np . array ( df [ metric_col ]) . reshape ( - 1 , 1 )) . lambdas_ [ 0 ], 3 ) cnt = 0 except : cnt = 1 if cnt == 0 : # df[metric_col+\"_transformed\"] = boxcox(df[metric_col],lmbda=lambda_box_cox) df [ metric_col + \"_transformed\" ] = fit . transform ( np . array ( df [ metric_col ]) . reshape ( - 1 , 1 ) ) fig = make_subplots ( rows = 1 , cols = 2 , subplot_titles = [ \"Pre-Transformation\" , \"Post-Transformation\" ], ) fig . add_trace ( go . Scatter ( x = df . index , y = df [ metric_col ], mode = \"lines+markers\" , name = metric_col , line = dict ( color = global_theme [ 1 ]), ), row = 1 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = df [ metric_col + \"_transformed\" ], mode = \"lines+markers\" , name = metric_col + \"_transformed\" , line = dict ( color = global_theme [ 7 ]), ), row = 1 , col = 2 , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( autosize = True , width = 2000 , height = 400 ) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) ts_fig . append ( dp . Group ( dp . Group ( dp . BigNumber ( heading = \"ADF Test Statistic\" , value = adf_test [ 0 ], change = adf_test [ 1 ], is_upward_change = adf_flag , ), dp . BigNumber ( heading = \"KPSS Test Statistic\" , value = kpss_test [ 0 ], change = kpss_test [ 1 ], is_upward_change = kpss_flag , ), dp . BigNumber ( heading = \"Box-Cox Transformation\" , value = lambda_box_cox , change = str ( lambda_cat ( lambda_box_cox )), is_upward_change = True , ), columns = 3 , ), dp . Text ( \"#### Transformation View\" ), dp . Text ( \"Below Transformation is basis the inferencing from the Box Cox Transformation. The Lambda value of \" + str ( lambda_box_cox ) + \" indicates a \" + str ( lambda_cat ( lambda_box_cox )) + \". A Pre-Post Transformation Visualization is done for better clarity. \" ), dp . Plot ( fig ), dp . Text ( \"**Guidelines :** \" ), dp . Text ( \"**ADF** : *The more negative the statistic, the more likely we are to reject the null hypothesis. If the p-value is less than the significance level of 0.05, we can reject the null hypothesis and take that the series is stationary*\" ), dp . Text ( \"**KPSS** : *If the p-value is high, we cannot reject the null hypothesis. So the series is stationary.*\" ), label = metric_col . title (), ) ) else : ts_fig . append ( dp . Group ( dp . Group ( dp . BigNumber ( heading = \"ADF Test Statistic\" , value = adf_test [ 0 ], change = adf_test [ 1 ], is_upward_change = adf_flag , ), dp . BigNumber ( heading = \"KPSS Test Statistic\" , value = kpss_test [ 0 ], change = kpss_test [ 1 ], is_upward_change = kpss_flag , ), dp . BigNumber ( heading = \"Box-Cox Transformation\" , value = \"ValueOutOfRange\" , change = \"ValueOutOfRange\" , is_upward_change = True , ), columns = 3 , ), dp . Text ( \"**Guidelines :** \" ), dp . Text ( \"**ADF** : *The more negative the statistic, the more likely we are to reject the null hypothesis. If the p-value is less than the significance level of 0.05, we can reject the null hypothesis and take that the series is stationary*\" ), dp . Text ( \"**KPSS** : *If the p-value is high, we cannot reject the null hypothesis. So the series is stationary.*\" ), label = metric_col . title (), ) ) return dp . Select ( blocks = ts_fig , type = dp . SelectType . TABS ) def ts_viz_3_2 ( base_path , ts_col , col_list ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( num_cols ) > 1 : bl . append ( dp . Group ( ts_viz_3_1 ( base_path , ts_col , i ), label = i )) else : bl . append ( dp . Group ( ts_viz_3_1 ( base_path , ts_col , i ), label = i )) bl . append ( dp . Group ( dp . Plot ( blank_chart , label = \" \" ), label = \" \" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_3_3 ( base_path , ts_col , num_cols ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns ------- DatapaneObject \"\"\" # f = list(pd.read_csv(ends_with(base_path) + \"stats_\" + i + \"_2.csv\").count_unique_dates.values)[0] # if f >= 6: if len ( ts_col ) > 1 : ts_v = [] for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 6 : ts_v . append ( dp . Group ( ts_viz_3_2 ( base_path , i , num_cols ), label = i )) else : ts_v . append ( dp . Group ( dp . Text ( \"The data contains insufficient data points for the desired transformation analysis. Please ensure the number of unique dates is sufficient.\" ), label = i , ) ) else : ts_v = [] for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 6 : ts_v . append ( dp . Group ( ts_viz_3_2 ( base_path , i , num_cols ), label = i )) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) else : ts_v . append ( dp . Group ( dp . Text ( \"The data contains insufficient data points for the desired transformation analysis. Please ensure the number of unique dates is sufficient.\" ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_stats ( base_path ): \"\"\" This function helps to read the base data containing desired input and produces output specific to the `ts_cols_stats.csv` file Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. Returns ------- List \"\"\" df = pd . read_csv ( base_path + \"ts_cols_stats.csv\" ) all_stats = [] for i in range ( 0 , 7 ): try : all_stats . append ( df [ df . index . values == i ] . values [ 0 ][ 0 ] . split ( \",\" )) except : all_stats . append ([]) c0 = pd . DataFrame ( all_stats [ 0 ], columns = [ \"attributes\" ]) c1 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 1 ], 1 ), columns = [ \"attributes\" ]) c1 [ \"Analyzed Attributes\" ] = \"\u2714\" c2 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 2 ], 1 ), columns = [ \"attributes\" ]) c2 [ \"Attributes Identified\" ] = \"\u2714\" c3 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 3 ], 1 ), columns = [ \"attributes\" ]) c3 [ \"Attributes Pre-Existed\" ] = \"\u2714\" c4 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 4 ], 1 ), columns = [ \"attributes\" ]) c4 [ \"Overall TimeStamp Attributes\" ] = \"\u2714\" c5 = list_ts_remove_append ( all_stats [ 5 ], 1 ) c6 = list_ts_remove_append ( all_stats [ 6 ], 1 ) return c0 , c1 , c2 , c3 , c4 , c5 , c6 def ts_viz_generate ( master_path , id_col , print_report = False , output_type = None ): \"\"\" This function helps to produce the output in the nested / recursive function supported by datapane. Eventually this is populated at the final report. Parameters ---------- master_path Master path where the aggregated data resides. id_col ID Column print_report Option to specify whether the Report needs to be saved or not. True / False can be used to specify the needful. output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject / Output[HTML] \"\"\" master_path = ends_with ( master_path ) try : c0 , c1 , c2 , c3 , c4 , c5 , c6 = ts_stats ( master_path ) except : return \"null_report\" stats_df = ( c0 . merge ( c1 , on = \"attributes\" , how = \"left\" ) . merge ( c2 , on = \"attributes\" , how = \"left\" ) . merge ( c3 , on = \"attributes\" , how = \"left\" ) . merge ( c4 , on = \"attributes\" , how = \"left\" ) . fillna ( \"\u2718\" ) ) global num_cols global cat_cols num_cols , cat_cols = c5 , c6 final_ts_cols = list ( ts_stats ( master_path )[ 4 ] . attributes . values ) if output_type == \"daily\" : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Basic Landscaping\" ), dp . Text ( \"Out of **\" + str ( len ( list ( ts_stats ( master_path )[ 1 ] . attributes . values ))) + \"** potential attributes in the data, the module could locate **\" + str ( len ( final_ts_cols )) + \"** attributes as Timestamp\" ), dp . DataTable ( stats_df ), ts_landscape ( master_path , final_ts_cols , id_col ), dp . Text ( \"*Lower the **CoV** (Coefficient Of Variation), Higher the Consistency between the consecutive dates. Similarly the Mean & Variance should be consistent over time*\" ), dp . Text ( \"### Visualization across the Shortlisted Timestamp Attributes\" ), ts_viz_1_3 ( master_path , final_ts_cols , num_cols , cat_cols , output_type ), dp . Text ( \"### Decomposed View\" ), ts_viz_2_3 ( master_path , final_ts_cols , num_cols ), dp . Text ( \"### Stationarity & Transformations\" ), ts_viz_3_3 ( master_path , final_ts_cols , num_cols ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), label = \"Time Series Analyzer\" , ) elif output_type is None : report = \"null_report\" else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Basic Landscaping\" ), dp . Text ( \"Out of **\" + str ( len ( list ( ts_stats ( master_path )[ 1 ] . attributes . values ))) + \"** potential attributes in the data, the module could locate **\" + str ( len ( final_ts_cols )) + \"** attributes as Timestamp\" ), dp . DataTable ( stats_df ), ts_landscape ( master_path , final_ts_cols , id_col ), dp . Text ( \"*Lower the **CoV** (Coefficient Of Variation), Higher the Consistency between the consecutive dates. Similarly the Mean & Variance should be consistent over time*\" ), dp . Text ( \"### Visualization across the Shortlisted Timestamp Attributes\" ), ts_viz_1_3 ( master_path , final_ts_cols , num_cols , cat_cols , output_type ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), label = \"Time Series Analyzer\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"time_series_analyzer.html\" , open = True ) return report def overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list ): \"\"\" This function helps to produce a basic summary of all the geospatial fields auto-detected in a dictionary along with the length of lat-lon & geohash cols identified. Parameters ---------- lat_col_list List of latitude columns identified long_col_list List of longitude columns identified geohash_col_list List of geohash columns identified Returns ------- Dictionary,Integer,Integer \"\"\" d = {} ll = [] col_list = [ \"Latitude Col\" , \"Longitude Col\" , \"Geohash Col\" ] # for idx,i in enumerate([lat_col_list,long_col_list,geohash_col_list,polygon_col_list]): for idx , i in enumerate ([ lat_col_list , long_col_list , geohash_col_list ]): if i is None : ll = [] elif i is not None : ll = [] for j in i : ll . append ( j ) d [ col_list [ idx ]] = \",\" . join ( ll ) l1 = len ( lat_col_list ) l2 = len ( geohash_col_list ) return d , l1 , l2 def loc_field_stats ( lat_col_list , long_col_list , geohash_col_list , max_records ): \"\"\" This function helps to produce a basic summary of all the geospatial fields auto-detected Parameters ---------- lat_col_list List of latitude columns identified long_col_list List of longitude columns identified geohash_col_list List of geohash columns identified max_records Maximum geospatial points analyzed Returns ------- DatapaneObject \"\"\" loc_cnt = ( overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 1 ] * 2 ) + ( overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 2 ]) loc_var_stats = overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 0 ] x = \"#\" t0 = dp . Text ( x ) t1 = dp . Text ( \"There are **\" + str ( loc_cnt ) + \"** location fields captured in the data containing \" + str ( overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 1 ]) + \" pair(s) of **Lat,Long** & \" + str ( overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 2 ]) + \" **Geohash** field(s)\" ) t2 = dp . DataTable ( pd . DataFrame ( pd . Series ( loc_var_stats , index = loc_var_stats . keys ())) . rename ( columns = { 0 : \"\" } ) ) return dp . Group ( t0 , t1 , t2 ) def read_stats_ll_geo ( lat_col , long_col , geohash_col , master_path , top_geo_records ): \"\"\" This function helps to read all the basis stats output for the lat-lon & geohash field produced from the analyzer module Parameters ---------- lat_col Latitude column identified long_col Longitude column identified geohash_col Geohash column identified master_path Master path where the aggregated data resides top_geo_records Top geospatial records displayed Returns ------- DatapaneObject \"\"\" try : len_lat_col = len ( lat_col ) except : len_lat_col = 0 try : len_geohash_col = len ( geohash_col ) except : len_geohash_col = 0 ll_stats , geohash_stats = [], [] if len_lat_col > 0 : if len_lat_col == 1 : for idx , i in enumerate ( lat_col ): ll_stats . append ( dp . Group ( dp . Select ( blocks = [ dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Overall_Summary_1_\" + lat_col [ idx ] + \"_\" + long_col [ idx ] + \".csv\" ), label = \"Overall Summary\" , ), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Top_\" + str ( top_geo_records ) + \"_Lat_Long_1_\" + lat_col [ idx ] + \"_\" + long_col [ idx ] + \".csv\" ), label = \"Top \" + str ( top_geo_records ) + \" Lat Long\" , ), ], type = dp . SelectType . TABS , ), label = lat_col [ idx ] + \"_\" + long_col [ idx ], ) ) ll_stats . append ( dp . Group ( dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), label = \" \" , ) ) elif len_lat_col > 1 : for idx , i in enumerate ( lat_col ): ll_stats . append ( dp . Group ( dp . Select ( blocks = [ dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Overall_Summary_1_\" + lat_col [ idx ] + \"_\" + long_col [ idx ] + \".csv\" ), label = \"Overall Summary\" , ), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Top_\" + str ( top_geo_records ) + \"_Lat_Long_1_\" + lat_col [ idx ] + \"_\" + long_col [ idx ] + \".csv\" ), label = \"Top \" + str ( top_geo_records ) + \" Lat Long\" , ), ], type = dp . SelectType . TABS , ), label = lat_col [ idx ] + \"_\" + long_col [ idx ], ) ) ll_stats = dp . Select ( blocks = ll_stats , type = dp . SelectType . DROPDOWN ) if len_geohash_col > 0 : if len_geohash_col == 1 : for idx , i in enumerate ( geohash_col ): geohash_stats . append ( dp . Group ( dp . Select ( blocks = [ dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Overall_Summary_2_\" + geohash_col [ idx ] + \".csv\" ), label = \"Overall Summary\" , ), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Top_\" + str ( top_geo_records ) + \"_Geohash_Distribution_2_\" + geohash_col [ idx ] + \".csv\" ), label = \"Top \" + str ( top_geo_records ) + \" Geohash Distribution\" , ), ], type = dp . SelectType . TABS , ), label = geohash_col [ idx ], ) ) geohash_stats . append ( dp . Group ( dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), label = \" \" , ) ) elif len_geohash_col > 1 : for idx , i in enumerate ( geohash_col ): geohash_stats . append ( dp . Group ( dp . Select ( blocks = [ dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Overall_Summary_2_\" + geohash_col [ idx ] + \".csv\" ), label = \"Overall Summary\" , ), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Top_\" + str ( top_geo_records ) + \"_Geohash_Distribution_2_\" + geohash_col [ idx ] + \".csv\" ), label = \"Top \" + str ( top_geo_records ) + \" Geohash Distribution\" , ), ], type = dp . SelectType . TABS , ), label = geohash_col [ idx ], ) ) geohash_stats = dp . Select ( blocks = geohash_stats , type = dp . SelectType . DROPDOWN ) if ( len_lat_col + len_geohash_col ) == 1 : if len_lat_col == 0 : return geohash_stats else : return ll_stats elif ( len_lat_col + len_geohash_col ) > 1 : if ( len_lat_col > 1 ) and ( len_geohash_col == 0 ): return ll_stats elif ( len_lat_col == 0 ) and ( len_geohash_col > 1 ): return geohash_stats elif ( len_lat_col >= 1 ) and ( len_geohash_col >= 1 ): return dp . Select ( blocks = [ dp . Group ( ll_stats , label = \"Lat-Long-Stats\" ), dp . Group ( geohash_stats , label = \"Geohash-Stats\" ), ], type = dp . SelectType . TABS , ) def read_cluster_stats_ll_geo ( lat_col , long_col , geohash_col , master_path ): \"\"\" This function helps to read all the cluster analysis output for the lat-lon & geohash field produced from the analyzer module Parameters ---------- lat_col Latitude column identified long_col Longitude column identified geohash_col Geohash column identified master_path Master path where the aggregated data resides Returns ------- DatapaneObject \"\"\" ll_col , plot_ll , all_geo_cols = [], [], [] try : len_lat_col = len ( lat_col ) except : len_lat_col = 0 try : len_geohash_col = len ( geohash_col ) except : len_geohash_col = 0 if ( len_lat_col > 0 ) or ( len_geohash_col > 0 ): try : for idx , i in enumerate ( lat_col ): ll_col . append ( lat_col [ idx ] + \"_\" + long_col [ idx ]) except : pass all_geo_cols = ll_col + geohash_col if len ( all_geo_cols ) > 0 : for i in all_geo_cols : if len ( all_geo_cols ) == 1 : p1 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_1_elbow_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_1_silhoutte_\" + i ) ) ) ), label = \"Cluster Identification\" , ) p2 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_2_kmeans_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_2_dbscan_\" + i ) ) ) ), label = \"Cluster Distribution\" , ) p3 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_3_kmeans_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_3_dbscan_\" + i ) ) ) ), label = \"Visualization\" , ) p4 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_1_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_2_\" + i ) ) ) ), label = \"Outlier Points\" , ) plot_ll . append ( dp . Group ( dp . Select ( blocks = [ p1 , p2 , p3 , p4 ], type = dp . SelectType . TABS ), label = i , ) ) plot_ll . append ( dp . Plot ( blank_chart , label = \" \" )) elif len ( all_geo_cols ) > 1 : p1 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_1_elbow_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_1_silhoutte_\" + i ) ) ) ), label = \"Cluster Identification\" , ) p2 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_2_kmeans_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_2_dbscan_\" + i ) ) ) ), label = \"Cluster Distribution\" , ) p3 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_3_kmeans_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_3_dbscan_\" + i ) ) ) ), label = \"Visualization\" , ) p4 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_1_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_2_\" + i ) ) ) ), label = \"Outlier Points\" , ) plot_ll . append ( dp . Group ( dp . Select ( blocks = [ p1 , p2 , p3 , p4 ], type = dp . SelectType . TABS ), label = i , ) ) return dp . Select ( blocks = plot_ll , type = dp . SelectType . DROPDOWN ) def read_loc_charts ( master_path ): \"\"\" This function helps to read all the geospatial charts from the master path and populate in the report Parameters ---------- master_path Master path where the aggregated data resides Returns ------- DatapaneObject \"\"\" ll_charts_nm = [ x for x in os . listdir ( master_path ) if \"loc_charts_ll\" in x ] geo_charts_nm = [ x for x in os . listdir ( master_path ) if \"loc_charts_gh\" in x ] ll_col_charts , geo_col_charts = [], [] if len ( ll_charts_nm ) > 0 : if len ( ll_charts_nm ) == 1 : for i1 in ll_charts_nm : col_name = i1 . replace ( \"loc_charts_ll_\" , \"\" ) ll_col_charts . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i1 ))), label = col_name , ) ) ll_col_charts . append ( dp . Plot ( blank_chart , label = \" \" )) elif len ( ll_charts_nm ) > 1 : for i1 in ll_charts_nm : col_name = i1 . replace ( \"loc_charts_ll_\" , \"\" ) ll_col_charts . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i1 ))), label = col_name , ) ) ll_col_charts = dp . Select ( blocks = ll_col_charts , type = dp . SelectType . DROPDOWN ) if len ( geo_charts_nm ) > 0 : if len ( geo_charts_nm ) == 1 : for i2 in geo_charts_nm : col_name = i2 . replace ( \"loc_charts_gh_\" , \"\" ) geo_col_charts . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i2 ))), label = col_name , ) ) geo_col_charts . append ( dp . Plot ( blank_chart , label = \" \" )) elif len ( geo_charts_nm ) > 1 : for i2 in geo_charts_nm : col_name = i2 . replace ( \"loc_charts_gh_\" , \"\" ) geo_col_charts . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i2 ))), label = col_name , ) ) geo_col_charts = dp . Select ( blocks = geo_col_charts , type = dp . SelectType . DROPDOWN ) if ( len ( ll_charts_nm ) > 0 ) and ( len ( geo_charts_nm ) == 0 ): return ll_col_charts elif ( len ( ll_charts_nm ) == 0 ) and ( len ( geo_charts_nm ) > 0 ): return geo_col_charts elif ( len ( ll_charts_nm ) > 0 ) and ( len ( geo_charts_nm ) > 0 ): return dp . Select ( blocks = [ dp . Group ( ll_col_charts , label = \"Lat-Long-Plot\" ), dp . Group ( geo_col_charts , label = \"Geohash-Plot\" ), ], type = dp . SelectType . TABS , ) def loc_report_gen ( lat_cols , long_cols , geohash_cols , master_path , max_records , top_geo_records , print_report = False , ): \"\"\" This function helps to read all the lat,long & geohash columns as input alongside few input parameters to produce the geospatial analysis report tab Parameters ---------- lat_cols Latitude columns identified in the data long_cols Longitude columns identified in the data geohash_cols Geohash columns identified in the data master_path Master path where the aggregated data resides max_records Maximum geospatial points analyzed top_geo_records Top geospatial records displayed print_report Option to specify whether the Report needs to be saved or not. True / False can be used to specify the needful Returns ------- DatapaneObject \"\"\" _ = dp . Text ( \"#\" ) dp1 = dp . Group ( _ , dp . Text ( \"*This section summarizes the information about the geospatial features identified in the data and their landscaping view*\" ), loc_field_stats ( lat_cols , long_cols , geohash_cols , max_records ), ) if ( len ( lat_cols ) + len ( geohash_cols )) > 0 : dp2 = dp . Group ( _ , dp . Text ( \"## Descriptive Analysis by Location Attributes\" ), read_stats_ll_geo ( lat_cols , long_cols , geohash_cols , master_path , top_geo_records ), _ , ) dp3 = dp . Group ( _ , dp . Text ( \"## Clustering Geospatial Field\" ), read_cluster_stats_ll_geo ( lat_cols , long_cols , geohash_cols , master_path ), _ , ) dp4 = dp . Group ( _ , dp . Text ( \"## Visualization by Geospatial Fields\" ), read_loc_charts ( master_path ), _ , ) report = dp . Group ( dp1 , dp2 , dp3 , dp4 , label = \"Geospatial Analyzer\" ) elif ( len ( lat_cols ) + len ( geohash_cols )) == 0 : report = \"null_report\" if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"geospatial_analyzer.html\" , open = True ) return report def anovos_report ( master_path , id_col = None , label_col = None , corr_threshold = 0.4 , iv_threshold = 0.02 , drift_threshold_model = 0.1 , dataDict_path = \".\" , metricDict_path = \".\" , run_type = \"local\" , final_report_path = \".\" , output_type = None , mlflow_config = None , lat_cols = [], long_cols = [], gh_cols = [], max_records = 100000 , top_geo_records = 100 , auth_key = \"NA\" , ): \"\"\" This function actually helps to produce the final report by scanning through the output processed from the data analyzer module. Parameters ---------- master_path Path containing the input files. id_col ID column (Default value = \"\") label_col label column (Default value = \"\") corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. (Default value = 0.4) iv_threshold IV threshold beyond which attributes can be called as significant. (Default value = 0.02) drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not (Default value = 0.1) dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. run_type local or emr or databricks or ak8s option. Default is kept as local auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. final_report_path Path where the report will be saved. (Default value = \".\") output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" mlflow_config MLflow configuration. If None, all MLflow features are disabled. lat_cols Latitude columns identified in the data long_cols Longitude columns identified in the data gh_cols Geohash columns identified in the data max_records Maximum geospatial points analyzed top_geo_records Top geospatial records displayed Returns ------- Output[HTML] \"\"\" if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( master_path ) + \" \" + ends_with ( \"report_stats\" ) ) master_path = \"report_stats\" subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"databricks\" : master_path = output_to_local ( master_path ) dataDict_path = output_to_local ( dataDict_path ) metricDict_path = output_to_local ( metricDict_path ) final_report_path = output_to_local ( final_report_path ) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( master_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" \"' + ends_with ( \"report_stats\" ) + '\" --recursive=true' ) master_path = \"report_stats\" subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if \"global_summary.csv\" not in os . listdir ( master_path ): print ( \"Minimum supporting data is unavailable, hence the Report could not be generated.\" ) return None global global_summary_df global numcols_name global catcols_name global rows_count global columns_count global numcols_count global catcols_count global blank_chart global df_si_ global df_si global unstable_attr global total_unstable_attr global drift_df global metric_drift global drift_df global len_feats global drift_df_stats global drifted_feats global df_stability global n_df_stability global stability_interpretation_table global plot_index_stability SG_tabs = [ \"measures_of_counts\" , \"measures_of_centralTendency\" , \"measures_of_cardinality\" , \"measures_of_percentiles\" , \"measures_of_dispersion\" , \"measures_of_shape\" , \"global_summary\" , ] QC_tabs = [ \"nullColumns_detection\" , \"IDness_detection\" , \"biasedness_detection\" , \"invalidEntries_detection\" , \"duplicate_detection\" , \"nullRows_detection\" , \"outlier_detection\" , ] AE_tabs = [ \"correlation_matrix\" , \"IV_calculation\" , \"IG_calculation\" , \"variable_clustering\" , ] drift_tab = [ \"drift_statistics\" ] stability_tab = [ \"stability_index\" , \"stabilityIndex_metrics\" ] avl_SG , avl_QC , avl_AE = [], [], [] stability_interpretation_table = pd . DataFrame ( [ [ \"0-1\" , \"Very Unstable\" ], [ \"1-2\" , \"Unstable\" ], [ \"2-3\" , \"Marginally Stable\" ], [ \"3-3.5\" , \"Stable\" ], [ \"3.5-4\" , \"Very Stable\" ], ], columns = [ \"StabilityIndex\" , \"StabilityOrder\" ], ) plot_index_stability = go . Figure ( data = [ go . Table ( header = dict ( values = list ( stability_interpretation_table . columns ), fill_color = px . colors . sequential . Greys [ 2 ], align = \"center\" , font = dict ( size = 12 ), ), cells = dict ( values = [ stability_interpretation_table . StabilityIndex , stability_interpretation_table . StabilityOrder , ], line_color = px . colors . sequential . Greys [ 2 ], fill_color = \"white\" , align = \"center\" , height = 25 , ), columnwidth = [ 2 , 10 ], ) ] ) plot_index_stability . update_layout ( margin = dict ( l = 20 , r = 700 , t = 20 , b = 20 )) blank_chart = go . Figure () blank_chart . update_layout ( autosize = False , width = 10 , height = 10 ) blank_chart . layout . plot_bgcolor = global_plot_bg_color blank_chart . layout . paper_bgcolor = global_paper_bg_color blank_chart . update_xaxes ( visible = False ) blank_chart . update_yaxes ( visible = False ) global_summary_df = pd . read_csv ( ends_with ( master_path ) + \"global_summary.csv\" ) rows_count = int ( global_summary_df [ global_summary_df . metric . values == \"rows_count\" ] . value . values [ 0 ] ) catcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"catcols_count\" ] . value . values [ 0 ] ) numcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"numcols_count\" ] . value . values [ 0 ] ) columns_count = int ( global_summary_df [ global_summary_df . metric . values == \"columns_count\" ] . value . values [ 0 ] ) if catcols_count > 0 : catcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"catcols_name\" ] . value . values ) ) else : catcols_name = \"\" if numcols_count > 0 : numcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"numcols_name\" ] . value . values ) ) else : numcols_name = \"\" all_files = os . listdir ( master_path ) eventDist_charts = [ x for x in all_files if \"eventDist\" in x ] stats_files = [ x for x in all_files if \".csv\" in x ] freq_charts = [ x for x in all_files if \"freqDist\" in x ] outlier_charts = [ x for x in all_files if \"outlier\" in x ] drift_charts = [ x for x in all_files if \"drift\" in x and \".csv\" not in x ] all_charts_num_1_ = chart_gen_list ( master_path , chart_type = freq_charts , type_col = \"numerical\" ) all_charts_num_2_ = chart_gen_list ( master_path , chart_type = eventDist_charts , type_col = \"numerical\" ) all_charts_num_3_ = chart_gen_list ( master_path , chart_type = outlier_charts , type_col = \"numerical\" ) all_charts_cat_1_ = chart_gen_list ( master_path , chart_type = freq_charts , type_col = \"categorical\" ) all_charts_cat_2_ = chart_gen_list ( master_path , chart_type = eventDist_charts , type_col = \"categorical\" ) all_drift_charts_ = chart_gen_list ( master_path , chart_type = drift_charts ) for x in [ all_charts_num_1_ , all_charts_num_2_ , all_charts_num_3_ , all_charts_cat_1_ , all_charts_cat_2_ , all_drift_charts_ , ]: if len ( x ) == 1 : x . append ( dp . Plot ( blank_chart , label = \" \" )) else : x mapping_tab_list = [] for i in stats_files : if i . split ( \".csv\" )[ 0 ] in SG_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Descriptive Statistics\" ]) elif i . split ( \".csv\" )[ 0 ] in QC_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Quality Check\" ]) elif i . split ( \".csv\" )[ 0 ] in AE_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Attribute Associations\" ]) elif i . split ( \".csv\" )[ 0 ] in drift_tab or i . split ( \".csv\" )[ 0 ] in stability_tab : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Data Drift & Data Stability\" ]) else : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"null\" ]) xx = pd . DataFrame ( mapping_tab_list , columns = [ \"file_name\" , \"tab_name\" ]) xx_avl = list ( set ( xx . file_name . values )) for i in SG_tabs : if i in xx_avl : avl_SG . append ( i ) for j in QC_tabs : if j in xx_avl : avl_QC . append ( j ) for k in AE_tabs : if k in xx_avl : avl_AE . append ( k ) missing_SG = list ( set ( SG_tabs ) - set ( avl_SG )) missing_QC = list ( set ( QC_tabs ) - set ( avl_QC )) missing_AE = list ( set ( AE_tabs ) - set ( avl_AE )) missing_drift = list ( set ( drift_tab ) - set ( xx [ xx . tab_name . values == \"Data Drift & Data Stability\" ] . file_name . values ) ) missing_stability = list ( set ( stability_tab ) - set ( xx [ xx . tab_name . values == \"Data Drift & Data Stability\" ] . file_name . values ) ) ds_ind = drift_stability_ind ( missing_drift , drift_tab , missing_stability , stability_tab ) if ds_ind [ 0 ] > 0 : drift_df = pd . read_csv ( ends_with ( master_path ) + \"drift_statistics.csv\" ) . sort_values ( by = [ \"flagged\" ], ascending = False ) metric_drift = list ( drift_df . drop ([ \"attribute\" , \"flagged\" ], 1 ) . columns ) drift_df = drift_df [ drift_df . attribute . values != id_col ] len_feats = drift_df . shape [ 0 ] drift_df_stats = ( drift_df [ drift_df . flagged . values == 1 ] . melt ( id_vars = \"attribute\" , value_vars = metric_drift ) . sort_values ( by = [ \"variable\" , \"value\" ], ascending = False ) ) drifted_feats = drift_df [ drift_df . flagged . values == 1 ] . shape [ 0 ] if ds_ind [ 1 ] > 0.5 : df_stability = pd . read_csv ( ends_with ( master_path ) + \"stabilityIndex_metrics.csv\" ) df_stability [ \"idx\" ] = df_stability [ \"idx\" ] . astype ( str ) . apply ( lambda x : \"df\" + x ) n_df_stability = str ( df_stability [ \"idx\" ] . nunique ()) df_si_ = pd . read_csv ( ends_with ( master_path ) + \"stability_index.csv\" ) df_si = df_si_ [ [ \"attribute\" , \"stability_index\" , \"mean_si\" , \"stddev_si\" , \"kurtosis_si\" , \"flagged\" , ] ] unstable_attr = list ( df_si_ [ df_si_ . flagged . values == 1 ] . attribute . values ) total_unstable_attr = list ( df_si_ . attribute . values ) elif ds_ind [ 1 ] == 0.5 : df_si_ = pd . read_csv ( ends_with ( master_path ) + \"stability_index.csv\" ) df_si = df_si_ [ [ \"attribute\" , \"stability_index\" , \"mean_si\" , \"stddev_si\" , \"kurtosis_si\" , \"flagged\" , ] ] unstable_attr = list ( df_si_ [ df_si_ . flagged . values == 1 ] . attribute . values ) total_unstable_attr = list ( df_si_ . attribute . values ) df_stability = pd . DataFrame () n_df_stability = \"the\" else : pass tab1 = executive_summary_gen ( master_path , label_col , ds_ind , id_col , iv_threshold , corr_threshold ) tab2 = wiki_generator ( master_path , dataDict_path = dataDict_path , metricDict_path = metricDict_path ) tab3 = descriptive_statistics ( master_path , SG_tabs , avl_SG , missing_SG , all_charts_num_1_ , all_charts_cat_1_ ) tab4 = quality_check ( master_path , QC_tabs , avl_QC , missing_QC , all_charts_num_3_ ) tab5 = attribute_associations ( master_path , AE_tabs , avl_AE , missing_AE , label_col , all_charts_num_2_ , all_charts_cat_2_ , ) tab6 = data_drift_stability ( master_path , ds_ind , id_col , drift_threshold_model , all_drift_charts_ ) tab7 = ts_viz_generate ( master_path , id_col , False , output_type ) tab8 = loc_report_gen ( lat_cols , long_cols , gh_cols , master_path , max_records , top_geo_records , False ) final_tabs_list = [] for i in [ tab1 , tab2 , tab3 , tab4 , tab5 , tab6 , tab7 , tab8 ]: if i == \"null_report\" : pass else : final_tabs_list . append ( i ) if run_type in ( \"local\" , \"databricks\" ): run_id = ( mlflow_config [ \"run_id\" ] if mlflow_config is not None and mlflow_config [ \"track_reports\" ] else \"\" ) report_run_path = ends_with ( final_report_path ) + run_id + \"/\" dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( report_run_path + \"ml_anovos_report.html\" , open = True ) if mlflow_config is not None : mlflow . log_artifact ( report_run_path ) elif run_type == \"emr\" : dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( \"ml_anovos_report.html\" , open = True ) bash_cmd = \"aws s3 cp ml_anovos_report.html \" + ends_with ( final_report_path ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) elif run_type == \"ak8s\" : dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( \"ml_anovos_report.html\" , open = True ) bash_cmd = ( 'azcopy cp \"ml_anovos_report.html\" ' + ends_with ( path_ak8s_modify ( final_report_path )) + str ( auth_key ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) else : raise ValueError ( \"Invalid run_type\" ) print ( \"Report generated successfully at the specified location\" ) Functions def anovos_report ( master_path, id_col=None, label_col=None, corr_threshold=0.4, iv_threshold=0.02, drift_threshold_model=0.1, dataDict_path='.', metricDict_path='.', run_type='local', final_report_path='.', output_type=None, mlflow_config=None, lat_cols=[], long_cols=[], gh_cols=[], max_records=100000, top_geo_records=100, auth_key='NA') This function actually helps to produce the final report by scanning through the output processed from the data analyzer module. Parameters master_path Path containing the input files. id_col ID column (Default value = \"\") label_col label column (Default value = \"\") corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. (Default value = 0.4) iv_threshold IV threshold beyond which attributes can be called as significant. (Default value = 0.02) drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not (Default value = 0.1) dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. run_type local or emr or databricks or ak8s option. Default is kept as local auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. final_report_path Path where the report will be saved. (Default value = \".\") output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" mlflow_config MLflow configuration. If None, all MLflow features are disabled. lat_cols Latitude columns identified in the data long_cols Longitude columns identified in the data gh_cols Geohash columns identified in the data max_records Maximum geospatial points analyzed top_geo_records Top geospatial records displayed Returns Output[HTML] Expand source code def anovos_report ( master_path , id_col = None , label_col = None , corr_threshold = 0.4 , iv_threshold = 0.02 , drift_threshold_model = 0.1 , dataDict_path = \".\" , metricDict_path = \".\" , run_type = \"local\" , final_report_path = \".\" , output_type = None , mlflow_config = None , lat_cols = [], long_cols = [], gh_cols = [], max_records = 100000 , top_geo_records = 100 , auth_key = \"NA\" , ): \"\"\" This function actually helps to produce the final report by scanning through the output processed from the data analyzer module. Parameters ---------- master_path Path containing the input files. id_col ID column (Default value = \"\") label_col label column (Default value = \"\") corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. (Default value = 0.4) iv_threshold IV threshold beyond which attributes can be called as significant. (Default value = 0.02) drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not (Default value = 0.1) dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. run_type local or emr or databricks or ak8s option. Default is kept as local auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. final_report_path Path where the report will be saved. (Default value = \".\") output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" mlflow_config MLflow configuration. If None, all MLflow features are disabled. lat_cols Latitude columns identified in the data long_cols Longitude columns identified in the data gh_cols Geohash columns identified in the data max_records Maximum geospatial points analyzed top_geo_records Top geospatial records displayed Returns ------- Output[HTML] \"\"\" if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( master_path ) + \" \" + ends_with ( \"report_stats\" ) ) master_path = \"report_stats\" subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"databricks\" : master_path = output_to_local ( master_path ) dataDict_path = output_to_local ( dataDict_path ) metricDict_path = output_to_local ( metricDict_path ) final_report_path = output_to_local ( final_report_path ) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( master_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" \"' + ends_with ( \"report_stats\" ) + '\" --recursive=true' ) master_path = \"report_stats\" subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if \"global_summary.csv\" not in os . listdir ( master_path ): print ( \"Minimum supporting data is unavailable, hence the Report could not be generated.\" ) return None global global_summary_df global numcols_name global catcols_name global rows_count global columns_count global numcols_count global catcols_count global blank_chart global df_si_ global df_si global unstable_attr global total_unstable_attr global drift_df global metric_drift global drift_df global len_feats global drift_df_stats global drifted_feats global df_stability global n_df_stability global stability_interpretation_table global plot_index_stability SG_tabs = [ \"measures_of_counts\" , \"measures_of_centralTendency\" , \"measures_of_cardinality\" , \"measures_of_percentiles\" , \"measures_of_dispersion\" , \"measures_of_shape\" , \"global_summary\" , ] QC_tabs = [ \"nullColumns_detection\" , \"IDness_detection\" , \"biasedness_detection\" , \"invalidEntries_detection\" , \"duplicate_detection\" , \"nullRows_detection\" , \"outlier_detection\" , ] AE_tabs = [ \"correlation_matrix\" , \"IV_calculation\" , \"IG_calculation\" , \"variable_clustering\" , ] drift_tab = [ \"drift_statistics\" ] stability_tab = [ \"stability_index\" , \"stabilityIndex_metrics\" ] avl_SG , avl_QC , avl_AE = [], [], [] stability_interpretation_table = pd . DataFrame ( [ [ \"0-1\" , \"Very Unstable\" ], [ \"1-2\" , \"Unstable\" ], [ \"2-3\" , \"Marginally Stable\" ], [ \"3-3.5\" , \"Stable\" ], [ \"3.5-4\" , \"Very Stable\" ], ], columns = [ \"StabilityIndex\" , \"StabilityOrder\" ], ) plot_index_stability = go . Figure ( data = [ go . Table ( header = dict ( values = list ( stability_interpretation_table . columns ), fill_color = px . colors . sequential . Greys [ 2 ], align = \"center\" , font = dict ( size = 12 ), ), cells = dict ( values = [ stability_interpretation_table . StabilityIndex , stability_interpretation_table . StabilityOrder , ], line_color = px . colors . sequential . Greys [ 2 ], fill_color = \"white\" , align = \"center\" , height = 25 , ), columnwidth = [ 2 , 10 ], ) ] ) plot_index_stability . update_layout ( margin = dict ( l = 20 , r = 700 , t = 20 , b = 20 )) blank_chart = go . Figure () blank_chart . update_layout ( autosize = False , width = 10 , height = 10 ) blank_chart . layout . plot_bgcolor = global_plot_bg_color blank_chart . layout . paper_bgcolor = global_paper_bg_color blank_chart . update_xaxes ( visible = False ) blank_chart . update_yaxes ( visible = False ) global_summary_df = pd . read_csv ( ends_with ( master_path ) + \"global_summary.csv\" ) rows_count = int ( global_summary_df [ global_summary_df . metric . values == \"rows_count\" ] . value . values [ 0 ] ) catcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"catcols_count\" ] . value . values [ 0 ] ) numcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"numcols_count\" ] . value . values [ 0 ] ) columns_count = int ( global_summary_df [ global_summary_df . metric . values == \"columns_count\" ] . value . values [ 0 ] ) if catcols_count > 0 : catcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"catcols_name\" ] . value . values ) ) else : catcols_name = \"\" if numcols_count > 0 : numcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"numcols_name\" ] . value . values ) ) else : numcols_name = \"\" all_files = os . listdir ( master_path ) eventDist_charts = [ x for x in all_files if \"eventDist\" in x ] stats_files = [ x for x in all_files if \".csv\" in x ] freq_charts = [ x for x in all_files if \"freqDist\" in x ] outlier_charts = [ x for x in all_files if \"outlier\" in x ] drift_charts = [ x for x in all_files if \"drift\" in x and \".csv\" not in x ] all_charts_num_1_ = chart_gen_list ( master_path , chart_type = freq_charts , type_col = \"numerical\" ) all_charts_num_2_ = chart_gen_list ( master_path , chart_type = eventDist_charts , type_col = \"numerical\" ) all_charts_num_3_ = chart_gen_list ( master_path , chart_type = outlier_charts , type_col = \"numerical\" ) all_charts_cat_1_ = chart_gen_list ( master_path , chart_type = freq_charts , type_col = \"categorical\" ) all_charts_cat_2_ = chart_gen_list ( master_path , chart_type = eventDist_charts , type_col = \"categorical\" ) all_drift_charts_ = chart_gen_list ( master_path , chart_type = drift_charts ) for x in [ all_charts_num_1_ , all_charts_num_2_ , all_charts_num_3_ , all_charts_cat_1_ , all_charts_cat_2_ , all_drift_charts_ , ]: if len ( x ) == 1 : x . append ( dp . Plot ( blank_chart , label = \" \" )) else : x mapping_tab_list = [] for i in stats_files : if i . split ( \".csv\" )[ 0 ] in SG_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Descriptive Statistics\" ]) elif i . split ( \".csv\" )[ 0 ] in QC_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Quality Check\" ]) elif i . split ( \".csv\" )[ 0 ] in AE_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Attribute Associations\" ]) elif i . split ( \".csv\" )[ 0 ] in drift_tab or i . split ( \".csv\" )[ 0 ] in stability_tab : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Data Drift & Data Stability\" ]) else : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"null\" ]) xx = pd . DataFrame ( mapping_tab_list , columns = [ \"file_name\" , \"tab_name\" ]) xx_avl = list ( set ( xx . file_name . values )) for i in SG_tabs : if i in xx_avl : avl_SG . append ( i ) for j in QC_tabs : if j in xx_avl : avl_QC . append ( j ) for k in AE_tabs : if k in xx_avl : avl_AE . append ( k ) missing_SG = list ( set ( SG_tabs ) - set ( avl_SG )) missing_QC = list ( set ( QC_tabs ) - set ( avl_QC )) missing_AE = list ( set ( AE_tabs ) - set ( avl_AE )) missing_drift = list ( set ( drift_tab ) - set ( xx [ xx . tab_name . values == \"Data Drift & Data Stability\" ] . file_name . values ) ) missing_stability = list ( set ( stability_tab ) - set ( xx [ xx . tab_name . values == \"Data Drift & Data Stability\" ] . file_name . values ) ) ds_ind = drift_stability_ind ( missing_drift , drift_tab , missing_stability , stability_tab ) if ds_ind [ 0 ] > 0 : drift_df = pd . read_csv ( ends_with ( master_path ) + \"drift_statistics.csv\" ) . sort_values ( by = [ \"flagged\" ], ascending = False ) metric_drift = list ( drift_df . drop ([ \"attribute\" , \"flagged\" ], 1 ) . columns ) drift_df = drift_df [ drift_df . attribute . values != id_col ] len_feats = drift_df . shape [ 0 ] drift_df_stats = ( drift_df [ drift_df . flagged . values == 1 ] . melt ( id_vars = \"attribute\" , value_vars = metric_drift ) . sort_values ( by = [ \"variable\" , \"value\" ], ascending = False ) ) drifted_feats = drift_df [ drift_df . flagged . values == 1 ] . shape [ 0 ] if ds_ind [ 1 ] > 0.5 : df_stability = pd . read_csv ( ends_with ( master_path ) + \"stabilityIndex_metrics.csv\" ) df_stability [ \"idx\" ] = df_stability [ \"idx\" ] . astype ( str ) . apply ( lambda x : \"df\" + x ) n_df_stability = str ( df_stability [ \"idx\" ] . nunique ()) df_si_ = pd . read_csv ( ends_with ( master_path ) + \"stability_index.csv\" ) df_si = df_si_ [ [ \"attribute\" , \"stability_index\" , \"mean_si\" , \"stddev_si\" , \"kurtosis_si\" , \"flagged\" , ] ] unstable_attr = list ( df_si_ [ df_si_ . flagged . values == 1 ] . attribute . values ) total_unstable_attr = list ( df_si_ . attribute . values ) elif ds_ind [ 1 ] == 0.5 : df_si_ = pd . read_csv ( ends_with ( master_path ) + \"stability_index.csv\" ) df_si = df_si_ [ [ \"attribute\" , \"stability_index\" , \"mean_si\" , \"stddev_si\" , \"kurtosis_si\" , \"flagged\" , ] ] unstable_attr = list ( df_si_ [ df_si_ . flagged . values == 1 ] . attribute . values ) total_unstable_attr = list ( df_si_ . attribute . values ) df_stability = pd . DataFrame () n_df_stability = \"the\" else : pass tab1 = executive_summary_gen ( master_path , label_col , ds_ind , id_col , iv_threshold , corr_threshold ) tab2 = wiki_generator ( master_path , dataDict_path = dataDict_path , metricDict_path = metricDict_path ) tab3 = descriptive_statistics ( master_path , SG_tabs , avl_SG , missing_SG , all_charts_num_1_ , all_charts_cat_1_ ) tab4 = quality_check ( master_path , QC_tabs , avl_QC , missing_QC , all_charts_num_3_ ) tab5 = attribute_associations ( master_path , AE_tabs , avl_AE , missing_AE , label_col , all_charts_num_2_ , all_charts_cat_2_ , ) tab6 = data_drift_stability ( master_path , ds_ind , id_col , drift_threshold_model , all_drift_charts_ ) tab7 = ts_viz_generate ( master_path , id_col , False , output_type ) tab8 = loc_report_gen ( lat_cols , long_cols , gh_cols , master_path , max_records , top_geo_records , False ) final_tabs_list = [] for i in [ tab1 , tab2 , tab3 , tab4 , tab5 , tab6 , tab7 , tab8 ]: if i == \"null_report\" : pass else : final_tabs_list . append ( i ) if run_type in ( \"local\" , \"databricks\" ): run_id = ( mlflow_config [ \"run_id\" ] if mlflow_config is not None and mlflow_config [ \"track_reports\" ] else \"\" ) report_run_path = ends_with ( final_report_path ) + run_id + \"/\" dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( report_run_path + \"ml_anovos_report.html\" , open = True ) if mlflow_config is not None : mlflow . log_artifact ( report_run_path ) elif run_type == \"emr\" : dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( \"ml_anovos_report.html\" , open = True ) bash_cmd = \"aws s3 cp ml_anovos_report.html \" + ends_with ( final_report_path ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) elif run_type == \"ak8s\" : dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( \"ml_anovos_report.html\" , open = True ) bash_cmd = ( 'azcopy cp \"ml_anovos_report.html\" ' + ends_with ( path_ak8s_modify ( final_report_path )) + str ( auth_key ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) else : raise ValueError ( \"Invalid run_type\" ) print ( \"Report generated successfully at the specified location\" ) def attribute_associations ( master_path, AE_tabs, avl_recs_AE, missing_recs_AE, label_col, all_charts_num_2_, all_charts_cat_2_, print_report=False) This function helps to produce output specific to the Attribute Association Tab. Parameters master_path Path containing the input files. AE_tabs correlation_matrix','IV_calculation','IG_calculation','variable_clustering' avl_recs_AE Available files from the AE_tabs (Association Evaluator tabs) missing_recs_AE Missing files from the AE_tabs (Association Evaluator tabs) label_col label column all_charts_num_2_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_2_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns DatapaneObject / Output[HTML] Expand source code def attribute_associations ( master_path , AE_tabs , avl_recs_AE , missing_recs_AE , label_col , all_charts_num_2_ , all_charts_cat_2_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Attribute Association Tab. Parameters ---------- master_path Path containing the input files. AE_tabs correlation_matrix','IV_calculation','IG_calculation','variable_clustering' avl_recs_AE Available files from the AE_tabs (Association Evaluator tabs) missing_recs_AE Missing files from the AE_tabs (Association Evaluator tabs) label_col label column all_charts_num_2_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_2_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" if ( len ( missing_recs_AE ) == len ( AE_tabs )) and ( ( len ( all_charts_num_2_ ) + len ( all_charts_cat_2_ )) == 0 ): return \"null_report\" else : if len ( all_charts_num_2_ ) == 0 and len ( all_charts_cat_2_ ) == 0 : target_association_rep = dp . Text ( \"##\" ) else : if len ( all_charts_num_2_ ) > 0 and len ( all_charts_cat_2_ ) == 0 : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Text ( \"\"\" *Bivariate Distribution considering the event captured across different attribute splits (or categories)* \"\"\" ), dp . Select ( blocks = all_charts_num_2_ , type = dp . SelectType . DROPDOWN ), label = \"Numerical\" , ) elif len ( all_charts_num_2_ ) == 0 and len ( all_charts_cat_2_ ) > 0 : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Text ( \"\"\" *Bivariate Distribution considering the event captured across different attribute splits (or categories)* \"\"\" ), dp . Select ( blocks = all_charts_cat_2_ , type = dp . SelectType . DROPDOWN ), label = \"Categorical\" , ) else : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Select ( blocks = [ dp . Group ( dp . Select ( blocks = all_charts_num_2_ , type = dp . SelectType . DROPDOWN , ), label = \"Numerical\" , ), dp . Group ( dp . Select ( blocks = all_charts_cat_2_ , type = dp . SelectType . DROPDOWN , ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), dp . Text ( \"\"\" *Event Rate is defined as % of event label (i.e. label 1) in a bin or a categorical value of an attribute.* \"\"\" ), dp . Text ( \"# \" ), ) if len ( missing_recs_AE ) == len ( AE_tabs ): report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"## \" ), target_association_rep , dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Association Matrix & Plot\" ), dp . Select ( blocks = data_analyzer_output ( master_path , avl_recs_AE , tab_name = \"association_evaluator\" ), type = dp . SelectType . DROPDOWN , ), dp . Text ( \"### \" ), dp . Text ( \"## \" ), target_association_rep , dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"attribute_associations.html\" , open = True ) return report def chart_gen_list ( master_path, chart_type, type_col=None) This function helps to produce the charts in a list object form nested by a datapane object. Parameters master_path Path containing all the charts same as the other files from data analyzed output chart_type Files containing only the specific chart names for the specific chart category type_col None. Default value is kept as None Returns DatapaneObject Expand source code def chart_gen_list ( master_path , chart_type , type_col = None ): \"\"\" This function helps to produce the charts in a list object form nested by a datapane object. Parameters ---------- master_path Path containing all the charts same as the other files from data analyzed output chart_type Files containing only the specific chart names for the specific chart category type_col None. Default value is kept as None Returns ------- DatapaneObject \"\"\" plot_list = [] for i in chart_type : col_name = i [ i . find ( \"_\" ) + 1 :] if type_col == \"numerical\" : if col_name in numcols_name . replace ( \" \" , \"\" ) . split ( \",\" ): plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) else : pass elif type_col == \"categorical\" : if col_name in catcols_name . replace ( \" \" , \"\" ) . split ( \",\" ): plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) else : pass else : plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) return plot_list def data_analyzer_output ( master_path, avl_recs_tab, tab_name) This section produces output in form of datapane objects which is specific to the different data analyzer modules. It is used by referring to the Master path along with the Available list of metrics & the Tab name. Parameters master_path Path containing all the output from analyzed data avl_recs_tab Available file names from the analysis tab tab_name Analysis tab from association_evaluator / quality_checker / stats_generator Returns DatapaneObject Expand source code def data_analyzer_output ( master_path , avl_recs_tab , tab_name ): \"\"\" This section produces output in form of datapane objects which is specific to the different data analyzer modules. It is used by referring to the Master path along with the Available list of metrics & the Tab name. Parameters ---------- master_path Path containing all the output from analyzed data avl_recs_tab Available file names from the analysis tab tab_name Analysis tab from association_evaluator / quality_checker / stats_generator Returns ------- DatapaneObject \"\"\" df_list = [] df_plot_list = [] # @FIXME: unused variables plot_list = [] avl_recs_tab = [ x for x in avl_recs_tab if \"global_summary\" not in x ] for index , i in enumerate ( avl_recs_tab ): data = pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) if len ( data . index ) == 0 : continue if tab_name == \"quality_checker\" : if i == \"duplicate_detection\" : duplicate_recs = pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) _unique_rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"unique_rows_count\" ] . value . values ) _rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"rows_count\" ] . value . values ) _duplicate_rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"duplicate_rows\" ] . value . values ) _duplicate_pct = float ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"duplicate_pct\" ] . value . values * 100.0 ) unique_rows_count = f \" No. Of Unique Rows: ** { _unique_rows_count } **\" # @FIXME: variable names exists in outer scope rows_count = f \" No. of Rows: ** { _rows_count } **\" duplicate_rows = f \" No. of Duplicate Rows: ** { _duplicate_rows_count } **\" duplicate_pct = f \" Percentage of Duplicate Rows: ** { _duplicate_pct } %**\" df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . Group ( dp . Text ( rows_count ), dp . Text ( unique_rows_count ), dp . Text ( duplicate_rows ), dp . Text ( duplicate_pct ), ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) elif i == \"outlier_detection\" : df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) ), \"outlier_charts_placeholder\" , ] ) else : df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) elif tab_name == \"association_evaluator\" : for j in avl_recs_tab : if j == \"correlation_matrix\" : df_list_ = pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) feats_order = list ( df_list_ [ \"attribute\" ] . values ) df_list_ = df_list_ . round ( 3 ) fig = px . imshow ( df_list_ [ feats_order ], y = feats_order , color_continuous_scale = global_theme , aspect = \"auto\" , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Correlation Plot \")) df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ [[ \"attribute\" ] + feats_order ]), dp . Plot ( fig ), label = remove_u_score ( j ), ) ) elif j == \"variable_clustering\" : df_list_ = ( pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) . sort_values ( by = [ \"Cluster\" ], ascending = True ) ) fig = px . sunburst ( df_list_ , path = [ \"Cluster\" , \"Attribute\" ], values = \"RS_Ratio\" , color_discrete_sequence = global_theme , ) # fig.update_layout(title_text=str(\"Distribution of homogenous variable across Clusters\")) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Variable Clustering Plot \")) fig . layout . autosize = True df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ ), dp . Plot ( fig ), label = remove_u_score ( j ), ) ) else : try : df_list_ = pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) col_nm = [ x for x in list ( df_list_ . columns ) if \"attribute\" not in x ] df_list_ = df_list_ . sort_values ( col_nm [ 0 ], ascending = True ) fig = px . bar ( df_list_ , x = col_nm [ 0 ], y = \"attribute\" , orientation = \"h\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Representation of \" + str(remove_u_score(j)))) fig . layout . autosize = True df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ ), dp . Plot ( fig ), label = remove_u_score ( j ), ) ) except Exception as e : logger . error ( f \"processing failed, error { e } \" ) pass if len ( avl_recs_tab ) == 1 : df_plot_list . append ( dp . Group ( dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), dp . Plot ( blank_chart , label = \" \" ), label = \" \" , ) ) else : pass return df_plot_list else : df_list . append ( dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ), label = remove_u_score ( avl_recs_tab [ index ]), ) ) if tab_name == \"quality_checker\" and len ( avl_recs_tab ) == 1 : return df_list [ 0 ], [ dp . Text ( \"#\" ), dp . Plot ( blank_chart )] elif tab_name == \"stats_generator\" and len ( avl_recs_tab ) == 1 : return [ df_list [ 0 ], dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), ] else : return df_list def data_drift_stability ( master_path, ds_ind, id_col, drift_threshold_model, all_drift_charts_, print_report=False) This function helps to produce output specific to the Data Drift & Stability Tab. Parameters master_path Path containing the input files. ds_ind Drift stability indicator in list form. id_col ID column drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not all_drift_charts_ Charts (histogram/barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns DatapaneObject / Output[HTML] Expand source code def data_drift_stability ( master_path , ds_ind , id_col , drift_threshold_model , all_drift_charts_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Data Drift & Stability Tab. Parameters ---------- master_path Path containing the input files. ds_ind Drift stability indicator in list form. id_col ID column drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not all_drift_charts_ Charts (histogram/barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" line_chart_list = [] if ds_ind [ 0 ] > 0 : fig_metric_drift = go . Figure () fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 0 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 1 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 0 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 1 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 3 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 1 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 2 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 5 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 2 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 3 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 7 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 3 ], ) ) fig_metric_drift . add_vrect ( x0 = 0 , x1 = drift_threshold_model , fillcolor = global_theme [ 7 ], opacity = 0.1 , layer = \"below\" , line_width = 1 , ), fig_metric_drift . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) fig_metric_drift . layout . plot_bgcolor = global_plot_bg_color fig_metric_drift . layout . paper_bgcolor = global_paper_bg_color fig_metric_drift . update_xaxes ( showline = True , linewidth = 2 , gridcolor = px . colors . sequential . Greys [ 1 ] ) fig_metric_drift . update_yaxes ( showline = True , linewidth = 2 , gridcolor = px . colors . sequential . Greys [ 2 ] ) # Drift Chart - 2 fig_gauge_drift = go . Figure ( go . Indicator ( domain = { \"x\" : [ 0 , 1 ], \"y\" : [ 0 , 1 ]}, value = drifted_feats , mode = \"gauge+number\" , title = { \"text\" : \"\" }, gauge = { \"axis\" : { \"range\" : [ None , len_feats ]}, \"bar\" : { \"color\" : px . colors . sequential . Reds [ 7 ]}, \"steps\" : [ { \"range\" : [ 0 , drifted_feats ], \"color\" : px . colors . sequential . Reds [ 8 ], }, { \"range\" : [ drifted_feats , len_feats ], \"color\" : px . colors . sequential . Greens [ 8 ], }, ], \"threshold\" : { \"line\" : { \"color\" : \"black\" , \"width\" : 3 }, \"thickness\" : 1 , \"value\" : len_feats , }, }, ) ) fig_gauge_drift . update_layout ( font = { \"color\" : \"black\" , \"family\" : \"Arial\" }) def drift_text_gen ( drifted_feats , len_feats ): \"\"\" Parameters ---------- drifted_feats count of attributes drifted len_feats count of attributes passed for analysis Returns ------- String \"\"\" if drifted_feats == 0 : text = \"\"\" *Drift barometer does not indicate any drift in the underlying data. Please refer to the metric values as displayed in the above table & comparison plot for better understanding* \"\"\" elif drifted_feats == 1 : text = ( \"*Drift barometer indicates that \" + str ( drifted_feats ) + \" out of \" + str ( len_feats ) + \" (\" + str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%) attributes has been drifted from its source behaviour.*\" ) elif drifted_feats > 1 : text = ( \"*Drift barometer indicates that \" + str ( drifted_feats ) + \" out of \" + str ( len_feats ) + \" (\" + str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%) attributes have been drifted from its source behaviour.*\" ) else : text = \"\" return text else : pass if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] == 0 : return \"null_report\" elif ds_ind [ 0 ] == 0 and ds_ind [ 1 ] > 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 1 and ds_ind [ 1 ] == 0 : if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 1 and ds_ind [ 1 ] >= 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 0 and ds_ind [ 1 ] >= 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) else : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"data_drift_stability.html\" , open = True ) return report def descriptive_statistics ( master_path, SG_tabs, avl_recs_SG, missing_recs_SG, all_charts_num_1_, all_charts_cat_1_, print_report=False) This function helps to produce output specific to the Descriptive Stats Tab. Parameters master_path Path containing the input files. SG_tabs measures_of_counts','measures_of_centralTendency','measures_of_cardinality','measures_of_percentiles','measures_of_dispersion','measures_of_shape','global_summary' avl_recs_SG Available files from the SG_tabs (Stats Generator tabs) missing_recs_SG Missing files from the SG_tabs (Stats Generator tabs) all_charts_num_1_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_1_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns DatapaneObject / Output[HTML] Expand source code def descriptive_statistics ( master_path , SG_tabs , avl_recs_SG , missing_recs_SG , all_charts_num_1_ , all_charts_cat_1_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Descriptive Stats Tab. Parameters ---------- master_path Path containing the input files. SG_tabs measures_of_counts','measures_of_centralTendency','measures_of_cardinality','measures_of_percentiles','measures_of_dispersion','measures_of_shape','global_summary' avl_recs_SG Available files from the SG_tabs (Stats Generator tabs) missing_recs_SG Missing files from the SG_tabs (Stats Generator tabs) all_charts_num_1_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_1_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" if \"global_summary\" in avl_recs_SG : cnt = 0 else : cnt = 1 if len ( missing_recs_SG ) + cnt == len ( SG_tabs ): return \"null_report\" else : if \"global_summary\" in avl_recs_SG : l1 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the dataset with key statistical metrics and distribution plots.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Global Summary\" ), dp . Group ( dp . Text ( \" Total Number of Records: **\" + f \" { rows_count : , } \" + \"**\" ), dp . Text ( \" Total Number of Attributes: **\" + str ( columns_count ) + \"**\" ), dp . Text ( \" Number of Numerical Attributes : **\" + str ( numcols_count ) + \"**\" ), dp . Text ( \" Numerical Attributes Name : **\" + str ( numcols_name ) + \"**\" ), dp . Text ( \" Number of Categorical Attributes : **\" + str ( catcols_count ) + \"**\" ), dp . Text ( \" Categorical Attributes Name : **\" + str ( catcols_name ) + \"**\" ), ), ) else : l1 = dp . Text ( \"# \" ) if len ( data_analyzer_output ( master_path , avl_recs_SG , \"stats_generator\" )) > 0 : l2 = dp . Text ( \"### Statistics by Metric Type\" ) l3 = dp . Group ( dp . Select ( blocks = data_analyzer_output ( master_path , avl_recs_SG , \"stats_generator\" ), type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), ) else : l2 = dp . Text ( \"# \" ) l3 = dp . Text ( \"# \" ) if len ( all_charts_num_1_ ) == 0 and len ( all_charts_cat_1_ ) == 0 : l4 = 1 elif len ( all_charts_num_1_ ) == 0 and len ( all_charts_cat_1_ ) > 0 : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Select ( blocks = all_charts_cat_1_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) elif len ( all_charts_num_1_ ) > 0 and len ( all_charts_cat_1_ ) == 0 : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Select ( blocks = all_charts_num_1_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) else : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Group ( dp . Select ( blocks = [ dp . Group ( dp . Select ( blocks = all_charts_num_1_ , type = dp . SelectType . DROPDOWN , ), label = \"Numerical\" , ), dp . Group ( dp . Select ( blocks = all_charts_cat_1_ , type = dp . SelectType . DROPDOWN , ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ) ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) if l4 == 1 : report = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) else : report = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , * l4 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"descriptive_statistics.html\" , open = True ) return report def drift_stability_ind ( missing_recs_drift, drift_tab, missing_recs_stability, stability_tab) This function helps to produce the drift & stability indicator for further processing. Ideally a data with both drift & stability should produce a list of [1,1] Parameters missing_recs_drift Missing files from the drift tab drift_tab \"drift_statistics\" missing_recs_stability Missing files from the stability tab stability_tab \"stability_index, stabilityIndex_metrics\" Returns List Expand source code def drift_stability_ind ( missing_recs_drift , drift_tab , missing_recs_stability , stability_tab ): \"\"\" This function helps to produce the drift & stability indicator for further processing. Ideally a data with both drift & stability should produce a list of [1,1] Parameters ---------- missing_recs_drift Missing files from the drift tab drift_tab \"drift_statistics\" missing_recs_stability Missing files from the stability tab stability_tab \"stability_index, stabilityIndex_metrics\" Returns ------- List \"\"\" if len ( missing_recs_drift ) == len ( drift_tab ): drift_ind = 0 else : drift_ind = 1 if len ( missing_recs_stability ) == len ( stability_tab ): stability_ind = 0 elif ( \"stabilityIndex_metrics\" in missing_recs_stability ) and ( \"stability_index\" not in missing_recs_stability ): stability_ind = 0.5 else : stability_ind = 1 return drift_ind , stability_ind def executive_summary_gen ( master_path, label_col, ds_ind, id_col, iv_threshold, corr_threshold, print_report=False) This function helps to produce output specific to the Executive Summary Tab. Parameters master_path Path containing the input files. label_col Label column. ds_ind Drift stability indicator in list form. id_col ID column. iv_threshold IV threshold beyond which attributes can be called as significant. corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. print_report Printing option flexibility. Default value is kept as False. Returns DatapaneObject / Output[HTML] Expand source code def executive_summary_gen ( master_path , label_col , ds_ind , id_col , iv_threshold , corr_threshold , print_report = False , ): \"\"\" This function helps to produce output specific to the Executive Summary Tab. Parameters ---------- master_path Path containing the input files. label_col Label column. ds_ind Drift stability indicator in list form. id_col ID column. iv_threshold IV threshold beyond which attributes can be called as significant. corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" try : obj_dtls = json . load ( open ( ends_with ( master_path ) + \"freqDist_\" + str ( label_col )) ) # @FIXME: never used local variable text_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 8 ][ 1 ] x_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 10 ][ 1 ] y_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 12 ][ 1 ] label_fig_ = go . Figure ( data = [ go . Pie ( labels = x_val , values = y_val , textinfo = \"label+percent\" , insidetextorientation = \"radial\" , pull = [ 0 , 0.1 ], marker_colors = global_theme , ) ] ) label_fig_ . update_traces ( textposition = \"inside\" , textinfo = \"percent+label\" ) label_fig_ . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) label_fig_ . layout . plot_bgcolor = global_plot_bg_color label_fig_ . layout . paper_bgcolor = global_paper_bg_color except Exception as e : logger . error ( f \"processing failed, error { e } \" ) label_fig_ = None a1 = ( \"The dataset contains **\" + str ( f \" { rows_count : ,d } \" ) + \"** records and **\" + str ( numcols_count + catcols_count ) + \"** attributes (**\" + str ( numcols_count ) + \"** numerical + **\" + str ( catcols_count ) + \"** categorical).\" ) if label_col is None : a2 = dp . Group ( dp . Text ( \"- There is **no** target variable in the dataset\" ), dp . Text ( \"- Data Diagnosis:\" ), ) else : if label_fig_ is None : a2 = dp . Group ( dp . Text ( \"- Target variable is **\" + str ( label_col ) + \"** \" ), dp . Text ( \"- Data Diagnosis:\" ), ) else : a2 = dp . Group ( dp . Text ( \"- Target variable is **\" + str ( label_col ) + \"** \" ), dp . Plot ( label_fig_ ), dp . Text ( \"- Data Diagnosis:\" ), ) try : x1 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_dispersion.csv\" ) . query ( \"`cov`>1\" ) . attribute . values ) if len ( x1 ) > 0 : x1_1 = [ \"High Variance\" , x1 ] else : x1_1 = [ \"High Variance\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x1_1 = [ \"High Variance\" , None ] try : x2 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`skewness`>0\" ) . attribute . values ) if len ( x2 ) > 0 : x2_1 = [ \"Positive Skewness\" , x2 ] else : x2_1 = [ \"Positive Skewness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x2_1 = [ \"Positive Skewness\" , None ] try : x3 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`skewness`<0\" ) . attribute . values ) if len ( x3 ) > 0 : x3_1 = [ \"Negative Skewness\" , x3 ] else : x3_1 = [ \"Negative Skewness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x3_1 = [ \"Negative Skewness\" , None ] try : x4 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`kurtosis`>0\" ) . attribute . values ) if len ( x4 ) > 0 : x4_1 = [ \"High Kurtosis\" , x4 ] else : x4_1 = [ \"High Kurtosis\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x4_1 = [ \"High Kurtosis\" , None ] try : x5 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`kurtosis`<0\" ) . attribute . values ) if len ( x5 ) > 0 : x5_1 = [ \"Low Kurtosis\" , x5 ] else : x5_1 = [ \"Low Kurtosis\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x5_1 = [ \"Low Kurtosis\" , None ] try : x6 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_counts.csv\" ) . query ( \"`fill_pct`<0.7\" ) . attribute . values ) if len ( x6 ) > 0 : x6_1 = [ \"Low Fill Rates\" , x6 ] else : x6_1 = [ \"Low Fill Rates\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x6_1 = [ \"Low Fill Rates\" , None ] try : biasedness_df = pd . read_csv ( ends_with ( master_path ) + \"biasedness_detection.csv\" ) if \"treated\" in biasedness_df : x7 = list ( biasedness_df . query ( \"`treated`>0\" ) . attribute . values ) else : x7 = list ( biasedness_df . query ( \"`flagged`>0\" ) . attribute . values ) if len ( x7 ) > 0 : x7_1 = [ \"High Biasedness\" , x7 ] else : x7_1 = [ \"High Biasedness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x7_1 = [ \"High Biasedness\" , None ] try : x8 = list ( pd . read_csv ( ends_with ( master_path ) + \"outlier_detection.csv\" ) . attribute . values ) if len ( x8 ) > 0 : x8_1 = [ \"Outliers\" , x8 ] else : x8_1 = [ \"Outliers\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x8_1 = [ \"Outliers\" , None ] try : corr_matrx = pd . read_csv ( ends_with ( master_path ) + \"correlation_matrix.csv\" ) corr_matrx = corr_matrx [ list ( corr_matrx . attribute . values )] corr_matrx = corr_matrx . where ( np . triu ( np . ones ( corr_matrx . shape ), k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in corr_matrx . columns if any ( corr_matrx [ column ] > corr_threshold ) ] if len ( to_drop ) > 0 : x9_1 = [ \"High Correlation\" , to_drop ] else : x9_1 = [ \"High Correlation\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x9_1 = [ \"High Correlation\" , None ] try : x10 = list ( pd . read_csv ( ends_with ( master_path ) + \"IV_calculation.csv\" ) . query ( \"`iv`>\" + str ( iv_threshold )) . attribute . values ) if len ( x10 ) > 0 : x10_1 = [ \"Significant Attributes\" , x10 ] else : x10_1 = [ \"Significant Attributes\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x10_1 = [ \"Significant Attributes\" , None ] blank_list_df = [] for i in [ x1_1 , x2_1 , x3_1 , x4_1 , x5_1 , x6_1 , x7_1 , x8_1 , x9_1 , x10_1 ]: try : for j in i [ 1 ]: blank_list_df . append ([ i [ 0 ], j ]) except Exception as e : logger . error ( f \"processing failed, error { e } \" ) blank_list_df . append ([ i [ 0 ], \"NA\" ]) list_n = [] x1 = pd . DataFrame ( blank_list_df , columns = [ \"Metric\" , \"Attribute\" ]) x1 [ \"Value\" ] = \"\u2714\" all_cols = ( catcols_name . replace ( \" \" , \"\" ) + \",\" + numcols_name . replace ( \" \" , \"\" ) ) . split ( \",\" ) remainder_cols = list ( set ( all_cols ) - set ( x1 . Attribute . values )) total_metrics = set ( list ( x1 . Metric . values )) for i in remainder_cols : for j in total_metrics : list_n . append ([ j , i ]) x2 = pd . DataFrame ( list_n , columns = [ \"Metric\" , \"Attribute\" ]) x2 [ \"Value\" ] = \"\u2718\" x = x1 . append ( x2 , ignore_index = True ) x = ( x . drop_duplicates () . pivot ( index = \"Attribute\" , columns = \"Metric\" , values = \"Value\" ) . fillna ( \"\u2718\" ) . reset_index ()[ [ \"Attribute\" , \"Outliers\" , \"Significant Attributes\" , \"Positive Skewness\" , \"Negative Skewness\" , \"High Variance\" , \"High Correlation\" , \"High Kurtosis\" , \"Low Kurtosis\" , ] ] ) x = x [ ~ ( ( x [ \"Attribute\" ] . isnull ()) | ( x . Attribute . values == \"NA\" ) | ( x [ \"Attribute\" ] == \" \" ) ) ] if ds_ind [ 0 ] == 1 and ds_ind [ 1 ] >= 0.5 : a5 = \"Data Health based on Drift Metrics & Stability Index : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Drifted Attributes\" , value = str ( str ( drifted_feats ) + \" out of \" + str ( len_feats )), ), dp . BigNumber ( heading = \"% Drifted Attributes\" , value = str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%\" , ), dp . BigNumber ( heading = \"# Unstable Attributes\" , value = str ( len ( unstable_attr )) + \" out of \" + str ( len ( total_unstable_attr )), change = \"numerical\" , is_upward_change = True , ), dp . BigNumber ( heading = \"% Unstable Attributes\" , value = str ( np . round ( 100 * len ( unstable_attr ) / len ( total_unstable_attr ), 2 ) ) + \"%\" , ), columns = 4 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] >= 0.5 : a5 = \"Data Health based on Stability Index : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Unstable Attributes\" , value = str ( len ( unstable_attr )) + \" out of \" + str ( len ( total_unstable_attr )), change = \"numerical\" , is_upward_change = True , ), dp . BigNumber ( heading = \"% Unstable Attributes\" , value = str ( np . round ( 100 * len ( unstable_attr ) / len ( total_unstable_attr ), 2 ) ) + \"%\" , ), columns = 2 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 1 and ds_ind [ 1 ] == 0 : a5 = \"Data Health based on Drift Metrics : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Drifted Attributes\" , value = str ( str ( drifted_feats ) + \" out of \" + str ( len_feats )), ), dp . BigNumber ( heading = \"% Drifted Attributes\" , value = str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%\" , ), columns = 2 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] == 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"executive_summary.html\" , open = True ) return report def gen_time_series_plots ( base_path, x_col, y_col, time_cat) This function helps to produce Time Series Plots by sourcing the aggregated data as Daily/Hourly/Weekly level. Parameters base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names time_cat Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns Plot Expand source code def gen_time_series_plots ( base_path , x_col , y_col , time_cat ): \"\"\" This function helps to produce Time Series Plots by sourcing the aggregated data as Daily/Hourly/Weekly level. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names time_cat Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Plot \"\"\" df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_\" + time_cat + \".csv\" ) . dropna () if len ([ x for x in df . columns if \"min\" in x ]) == 0 : if time_cat == \"daily\" : # x_col = x_col + \"_ts\" fig = px . line ( df , x = x_col , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) fig . update_layout ( xaxis = dict ( rangeselector = dict ( buttons = list ( [ dict ( count = 1 , label = \"1m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 3 , label = \"3m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 6 , label = \"6m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 1 , label = \"YTD\" , step = \"year\" , stepmode = \"todate\" ), dict ( count = 1 , label = \"1y\" , step = \"year\" , stepmode = \"backward\" , ), dict ( step = \"all\" ), ] ) ), rangeslider = dict ( visible = True ), type = \"date\" , ) ) elif time_cat == \"weekly\" : fig = px . bar ( df , x = \"dow\" , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) # fig.update_layout(barmode='stack') elif time_cat == \"hourly\" : fig = px . bar ( df , x = \"daypart_cat\" , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) # fig.update_layout(barmode='stack') else : pass else : if time_cat == \"daily\" : # x_col = x_col + \"_ts\" f1 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"min\" ]), name = \"Min\" , line = dict ( color = global_theme [ 6 ]), ) f2 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"max\" ]), name = \"Max\" , line = dict ( color = global_theme [ 4 ]), ) f3 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"mean\" ]), name = \"Mean\" , line = dict ( color = global_theme [ 2 ]), ) f4 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"median\" ]), name = \"Median\" , line = dict ( color = global_theme [ 0 ]), ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( xaxis = dict ( rangeselector = dict ( buttons = list ( [ dict ( count = 1 , label = \"1m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 3 , label = \"3m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 6 , label = \"6m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 1 , label = \"YTD\" , step = \"year\" , stepmode = \"todate\" ), dict ( count = 1 , label = \"1y\" , step = \"year\" , stepmode = \"backward\" , ), dict ( step = \"all\" ), ] ) ), rangeslider = dict ( visible = True ), type = \"date\" , ) ) elif time_cat == \"weekly\" : f1 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"min\" ]), marker_color = global_theme [ 6 ], name = \"Min\" , ) f2 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"max\" ]), marker_color = global_theme [ 4 ], name = \"Max\" , ) f3 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"mean\" ]), marker_color = global_theme [ 2 ], name = \"Mean\" , ) f4 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"median\" ]), marker_color = global_theme [ 0 ], name = \"Median\" , ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( barmode = \"group\" ) elif time_cat == \"hourly\" : f1 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"min\" ]), marker_color = global_theme [ 6 ], name = \"Min\" , ) f2 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"max\" ]), marker_color = global_theme [ 4 ], name = \"Max\" , ) f3 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"mean\" ]), marker_color = global_theme [ 2 ], name = \"Mean\" , ) f4 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"median\" ]), marker_color = global_theme [ 0 ], name = \"Median\" , ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( barmode = \"group\" ) else : pass fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def lambda_cat ( val) Parameters val Value of Box Cox Test which translates into the transformation to be applied. Returns String Expand source code def lambda_cat ( val ): \"\"\" Parameters ---------- val Value of Box Cox Test which translates into the transformation to be applied. Returns ------- String \"\"\" if val < - 1 : return \"Reciprocal Square Transform\" elif val >= - 1 and val < - 0.5 : return \"Reciprocal Transform\" elif val >= - 0.5 and val < 0 : return \"Receiprocal Square Root Transform\" elif val >= 0 and val < 0.5 : return \"Log Transform\" elif val >= 0.5 and val < 1 : return \"Square Root Transform\" elif val >= 1 and val < 2 : return \"No Transform\" elif val >= 2 : return \"Square Transform\" else : return \"ValueOutOfRange\" def line_chart_gen_stability ( df1, df2, col) This function helps to produce charts which are specific to data stability index. It taken into account the stability input along with the analysis column to produce the desired output. Parameters df1 Analysis dataframe pertaining to summarized stability metrics df2 Analysis dataframe pertaining to historical data col Analysis column Returns DatapaneObject Expand source code def line_chart_gen_stability ( df1 , df2 , col ): \"\"\" This function helps to produce charts which are specific to data stability index. It taken into account the stability input along with the analysis column to produce the desired output. Parameters ---------- df1 Analysis dataframe pertaining to summarized stability metrics df2 Analysis dataframe pertaining to historical data col Analysis column Returns ------- DatapaneObject \"\"\" def val_cat ( val ): \"\"\" Parameters ---------- val Returns ------- String \"\"\" if val >= 3.5 : return \"Very Stable\" elif val >= 3 and val < 3.5 : return \"Stable\" elif val >= 2 and val < 3 : return \"Marginally Stable\" elif val >= 1 and val < 2 : return \"Unstable\" elif val >= 0 and val < 1 : return \"Very Unstable\" else : return \"Out of Range\" val_si = list ( df2 [ df2 [ \"attribute\" ] == col ] . stability_index . values )[ 0 ] f1 = go . Figure () f1 . add_trace ( go . Indicator ( mode = \"gauge+number\" , value = val_si , gauge = { \"axis\" : { \"range\" : [ None , 4 ], \"tickwidth\" : 1 , \"tickcolor\" : \"black\" }, \"bgcolor\" : \"white\" , \"steps\" : [ { \"range\" : [ 0 , 1 ], \"color\" : px . colors . sequential . Reds [ 7 ]}, { \"range\" : [ 1 , 2 ], \"color\" : px . colors . sequential . Reds [ 6 ]}, { \"range\" : [ 2 , 3 ], \"color\" : px . colors . sequential . Oranges [ 4 ]}, { \"range\" : [ 3 , 3.5 ], \"color\" : px . colors . sequential . BuGn [ 7 ]}, { \"range\" : [ 3.5 , 4 ], \"color\" : px . colors . sequential . BuGn [ 8 ]}, ], \"threshold\" : { \"line\" : { \"color\" : \"black\" , \"width\" : 3 }, \"thickness\" : 1 , \"value\" : val_si , }, \"bar\" : { \"color\" : global_plot_bg_color }, }, title = { \"text\" : \"Order of Stability: \" + val_cat ( val_si )}, ) ) f1 . update_layout ( height = 400 , font = { \"color\" : \"black\" , \"family\" : \"Arial\" }) f5 = \"Stability Index for \" + str ( col . upper ()) if len ( df1 . columns ) > 0 : attr_type = df1 [ \"type\" ] . tolist ()[ 0 ] if attr_type == \"Numerical\" : f2 = px . line ( df1 , x = \"idx\" , y = \"mean\" , markers = True , title = \"CV of Mean is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . mean_cv . values )[ 0 ]), ) f2 . update_traces ( line_color = global_theme [ 2 ], marker = dict ( size = 14 )) f2 . layout . plot_bgcolor = global_plot_bg_color f2 . layout . paper_bgcolor = global_paper_bg_color f3 = px . line ( df1 , x = \"idx\" , y = \"stddev\" , markers = True , title = \"CV of Stddev is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . stddev_cv . values )[ 0 ]), ) f3 . update_traces ( line_color = global_theme [ 6 ], marker = dict ( size = 14 )) f3 . layout . plot_bgcolor = global_plot_bg_color f3 . layout . paper_bgcolor = global_paper_bg_color f4 = px . line ( df1 , x = \"idx\" , y = \"kurtosis\" , markers = True , title = \"CV of Kurtosis is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . kurtosis_cv . values )[ 0 ]), ) f4 . update_traces ( line_color = global_theme [ 4 ], marker = dict ( size = 14 )) f4 . layout . plot_bgcolor = global_plot_bg_color f4 . layout . paper_bgcolor = global_paper_bg_color return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), dp . Group ( dp . Plot ( f2 ), dp . Plot ( f3 ), dp . Plot ( f4 ), columns = 3 ), label = col , ) else : f2 = px . line ( df1 , x = \"idx\" , y = \"mean\" , markers = True , title = \"Standard deviation of Mean is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . mean_stddev . values )[ 0 ]), ) f2 . update_traces ( line_color = global_theme [ 2 ], marker = dict ( size = 14 )) f2 . layout . plot_bgcolor = global_plot_bg_color f2 . layout . paper_bgcolor = global_paper_bg_color return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), dp . Group ( dp . Plot ( f2 ), columns = 1 ), label = col , ) else : return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), label = col ) def list_ts_remove_append ( l, opt) This function helps to remove or append \"_ts\" from any list. Parameters l List containing column name opt Option to choose between 1 & Others to enable the functionality of removing or appending \"_ts\" within the elements of a list Returns List Expand source code def list_ts_remove_append ( l , opt ): \"\"\" This function helps to remove or append \"_ts\" from any list. Parameters ---------- l List containing column name opt Option to choose between 1 & Others to enable the functionality of removing or appending \"_ts\" within the elements of a list Returns ------- List \"\"\" ll = [] if opt == 1 : for i in l : if i [ - 3 :] == \"_ts\" : ll . append ( i [ 0 : - 3 :]) else : ll . append ( i ) return ll else : for i in l : if i [ - 3 :] == \"_ts\" : ll . append ( i ) else : ll . append ( i + \"_ts\" ) return ll def loc_field_stats ( lat_col_list, long_col_list, geohash_col_list, max_records) This function helps to produce a basic summary of all the geospatial fields auto-detected Parameters lat_col_list List of latitude columns identified long_col_list List of longitude columns identified geohash_col_list List of geohash columns identified max_records Maximum geospatial points analyzed Returns DatapaneObject Expand source code def loc_field_stats ( lat_col_list , long_col_list , geohash_col_list , max_records ): \"\"\" This function helps to produce a basic summary of all the geospatial fields auto-detected Parameters ---------- lat_col_list List of latitude columns identified long_col_list List of longitude columns identified geohash_col_list List of geohash columns identified max_records Maximum geospatial points analyzed Returns ------- DatapaneObject \"\"\" loc_cnt = ( overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 1 ] * 2 ) + ( overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 2 ]) loc_var_stats = overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 0 ] x = \"#\" t0 = dp . Text ( x ) t1 = dp . Text ( \"There are **\" + str ( loc_cnt ) + \"** location fields captured in the data containing \" + str ( overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 1 ]) + \" pair(s) of **Lat,Long** & \" + str ( overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 2 ]) + \" **Geohash** field(s)\" ) t2 = dp . DataTable ( pd . DataFrame ( pd . Series ( loc_var_stats , index = loc_var_stats . keys ())) . rename ( columns = { 0 : \"\" } ) ) return dp . Group ( t0 , t1 , t2 ) def loc_report_gen ( lat_cols, long_cols, geohash_cols, master_path, max_records, top_geo_records, print_report=False) This function helps to read all the lat,long & geohash columns as input alongside few input parameters to produce the geospatial analysis report tab Parameters lat_cols Latitude columns identified in the data long_cols Longitude columns identified in the data geohash_cols Geohash columns identified in the data master_path Master path where the aggregated data resides max_records Maximum geospatial points analyzed top_geo_records Top geospatial records displayed print_report Option to specify whether the Report needs to be saved or not. True / False can be used to specify the needful Returns DatapaneObject Expand source code def loc_report_gen ( lat_cols , long_cols , geohash_cols , master_path , max_records , top_geo_records , print_report = False , ): \"\"\" This function helps to read all the lat,long & geohash columns as input alongside few input parameters to produce the geospatial analysis report tab Parameters ---------- lat_cols Latitude columns identified in the data long_cols Longitude columns identified in the data geohash_cols Geohash columns identified in the data master_path Master path where the aggregated data resides max_records Maximum geospatial points analyzed top_geo_records Top geospatial records displayed print_report Option to specify whether the Report needs to be saved or not. True / False can be used to specify the needful Returns ------- DatapaneObject \"\"\" _ = dp . Text ( \"#\" ) dp1 = dp . Group ( _ , dp . Text ( \"*This section summarizes the information about the geospatial features identified in the data and their landscaping view*\" ), loc_field_stats ( lat_cols , long_cols , geohash_cols , max_records ), ) if ( len ( lat_cols ) + len ( geohash_cols )) > 0 : dp2 = dp . Group ( _ , dp . Text ( \"## Descriptive Analysis by Location Attributes\" ), read_stats_ll_geo ( lat_cols , long_cols , geohash_cols , master_path , top_geo_records ), _ , ) dp3 = dp . Group ( _ , dp . Text ( \"## Clustering Geospatial Field\" ), read_cluster_stats_ll_geo ( lat_cols , long_cols , geohash_cols , master_path ), _ , ) dp4 = dp . Group ( _ , dp . Text ( \"## Visualization by Geospatial Fields\" ), read_loc_charts ( master_path ), _ , ) report = dp . Group ( dp1 , dp2 , dp3 , dp4 , label = \"Geospatial Analyzer\" ) elif ( len ( lat_cols ) + len ( geohash_cols )) == 0 : report = \"null_report\" if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"geospatial_analyzer.html\" , open = True ) return report def overall_stats_gen ( lat_col_list, long_col_list, geohash_col_list) This function helps to produce a basic summary of all the geospatial fields auto-detected in a dictionary along with the length of lat-lon & geohash cols identified. Parameters lat_col_list List of latitude columns identified long_col_list List of longitude columns identified geohash_col_list List of geohash columns identified Returns Dictionary,Integer,Integer Expand source code def overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list ): \"\"\" This function helps to produce a basic summary of all the geospatial fields auto-detected in a dictionary along with the length of lat-lon & geohash cols identified. Parameters ---------- lat_col_list List of latitude columns identified long_col_list List of longitude columns identified geohash_col_list List of geohash columns identified Returns ------- Dictionary,Integer,Integer \"\"\" d = {} ll = [] col_list = [ \"Latitude Col\" , \"Longitude Col\" , \"Geohash Col\" ] # for idx,i in enumerate([lat_col_list,long_col_list,geohash_col_list,polygon_col_list]): for idx , i in enumerate ([ lat_col_list , long_col_list , geohash_col_list ]): if i is None : ll = [] elif i is not None : ll = [] for j in i : ll . append ( j ) d [ col_list [ idx ]] = \",\" . join ( ll ) l1 = len ( lat_col_list ) l2 = len ( geohash_col_list ) return d , l1 , l2 def plotSeasonalDecompose ( base_path, x_col, y_col, metric_col='median', title='Seasonal Decomposition') This function helps to produce output specific to the Seasonal Decomposition of Time Series. Ideally it's expected to source a data containing atleast 2 cycles or 24 months as the most. Parameters base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names metric_col Metric of aggregation. Options can be between \"Median\", \"Mean\", \"Min\", \"Max\" title \"Title Description\" Returns Plot Expand source code def plotSeasonalDecompose ( base_path , x_col , y_col , metric_col = \"median\" , title = \"Seasonal Decomposition\" ): \"\"\" This function helps to produce output specific to the Seasonal Decomposition of Time Series. Ideally it's expected to source a data containing atleast 2 cycles or 24 months as the most. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names metric_col Metric of aggregation. Options can be between \"Median\", \"Mean\", \"Min\", \"Max\" title \"Title Description\" Returns ------- Plot \"\"\" df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_daily.csv\" ) . dropna () df [ x_col ] = pd . to_datetime ( df [ x_col ], format = \"%Y-%m- %d %H:%M:%S. %f \" ) df = df . set_index ( x_col ) if len ([ x for x in df . columns if \"min\" in x ]) == 0 : # result = seasonal_decompose(df[metric_col],model=\"additive\") pass else : result = seasonal_decompose ( df [ metric_col ], model = \"additive\" , period = 12 ) fig = make_subplots ( rows = 2 , cols = 2 , subplot_titles = [ \"Observed\" , \"Trend\" , \"Seasonal\" , \"Residuals\" ], ) # fig = go.Figure() fig . add_trace ( go . Scatter ( x = df . index , y = result . observed , name = \"Observed\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 0 ]), ), row = 1 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . trend , name = \"Trend\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 2 ]), ), row = 1 , col = 2 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . seasonal , name = \"Seasonal\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 4 ]), ), row = 2 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . resid , name = \"Residuals\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 6 ]), ), row = 2 , col = 2 , ) # fig.add_trace(go.Scatter(x=df.index, y=result.observed, name =\"Observed\", mode='lines+markers',line=dict(color=global_theme[0]))) # fig.add_trace(go.Scatter(x=df.index, y=result.trend, name =\"Trend\", mode='lines+markers',line=dict(color=global_theme[2]))) # fig.add_trace(go.Scatter(x=df.index, y=result.seasonal, name =\"Seasonal\", mode='lines+markers',line=dict(color=global_theme[4]))) # fig.add_trace(go.Scatter(x=df.index, y=result.resid, name =\"Residuals\", mode='lines+markers',line=dict(color=global_theme[6]))) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( autosize = True , width = 2000 , height = 800 ) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def quality_check ( master_path, QC_tabs, avl_recs_QC, missing_recs_QC, all_charts_num_3_, print_report=False) This function helps to produce output specific to the Quality Checker Tab. Parameters master_path Path containing the input files. QC_tabs nullColumns_detection','IDness_detection','biasedness_detection','invalidEntries_detection','duplicate_detection','nullRows_detection','outlier_detection' avl_recs_QC Available files from the QC_tabs (Quality Checker tabs) missing_recs_QC Missing files from the QC_tabs (Quality Checker tabs) all_charts_num_3_ Numerical charts (outlier charts) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns DatapaneObject / Output[HTML] Expand source code def quality_check ( master_path , QC_tabs , avl_recs_QC , missing_recs_QC , all_charts_num_3_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Quality Checker Tab. Parameters ---------- master_path Path containing the input files. QC_tabs nullColumns_detection','IDness_detection','biasedness_detection','invalidEntries_detection','duplicate_detection','nullRows_detection','outlier_detection' avl_recs_QC Available files from the QC_tabs (Quality Checker tabs) missing_recs_QC Missing files from the QC_tabs (Quality Checker tabs) all_charts_num_3_ Numerical charts (outlier charts) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" c_ = [] r_ = [] if len ( missing_recs_QC ) == len ( QC_tabs ): return \"null_report\" else : row_wise = [ \"duplicate_detection\" , \"nullRows_detection\" ] col_wise = [ \"nullColumns_detection\" , \"IDness_detection\" , \"biasedness_detection\" , \"invalidEntries_detection\" , \"outlier_detection\" , ] row_wise_ = [ p for p in row_wise if p in avl_recs_QC ] col_wise_ = [ p for p in col_wise if p in avl_recs_QC ] len_row_wise = len ([ p for p in row_wise if p in avl_recs_QC ]) len_col_wise = len ([ p for p in col_wise if p in avl_recs_QC ]) if len_row_wise == 0 : c = data_analyzer_output ( master_path , col_wise_ , \"quality_checker\" ) for i in c : for j in i : if j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) > 1 : c_ . append ( dp . Select ( blocks = all_charts_num_3_ , type = dp . SelectType . DROPDOWN ) ) elif ( j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) == 0 ): c_ . append ( dp . Plot ( blank_chart )) else : c_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Group ( * c_ ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) elif len_col_wise == 0 : r = data_analyzer_output ( master_path , row_wise_ , \"quality_checker\" ) for i in r : for j in i : r_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Group ( * r_ ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) else : c = data_analyzer_output ( master_path , col_wise_ , \"quality_checker\" ) for i in c : for j in i : if j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) > 1 : c_ . append ( dp . Select ( blocks = all_charts_num_3_ , type = dp . SelectType . DROPDOWN ) ) elif ( j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) == 0 ): c_ . append ( dp . Plot ( blank_chart )) else : c_ . append ( j ) r = data_analyzer_output ( master_path , row_wise_ , \"quality_checker\" ) for i in r : for j in i : r_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Text ( \"# \" ), dp . Group ( * c_ ), label = \"Column Level\" ), dp . Group ( dp . Text ( \"# \" ), dp . Group ( * r_ ), label = \"Row Level\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"quality_check.html\" , open = True ) return report def read_cluster_stats_ll_geo ( lat_col, long_col, geohash_col, master_path) This function helps to read all the cluster analysis output for the lat-lon & geohash field produced from the analyzer module Parameters lat_col Latitude column identified long_col Longitude column identified geohash_col Geohash column identified master_path Master path where the aggregated data resides Returns DatapaneObject Expand source code def read_cluster_stats_ll_geo ( lat_col , long_col , geohash_col , master_path ): \"\"\" This function helps to read all the cluster analysis output for the lat-lon & geohash field produced from the analyzer module Parameters ---------- lat_col Latitude column identified long_col Longitude column identified geohash_col Geohash column identified master_path Master path where the aggregated data resides Returns ------- DatapaneObject \"\"\" ll_col , plot_ll , all_geo_cols = [], [], [] try : len_lat_col = len ( lat_col ) except : len_lat_col = 0 try : len_geohash_col = len ( geohash_col ) except : len_geohash_col = 0 if ( len_lat_col > 0 ) or ( len_geohash_col > 0 ): try : for idx , i in enumerate ( lat_col ): ll_col . append ( lat_col [ idx ] + \"_\" + long_col [ idx ]) except : pass all_geo_cols = ll_col + geohash_col if len ( all_geo_cols ) > 0 : for i in all_geo_cols : if len ( all_geo_cols ) == 1 : p1 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_1_elbow_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_1_silhoutte_\" + i ) ) ) ), label = \"Cluster Identification\" , ) p2 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_2_kmeans_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_2_dbscan_\" + i ) ) ) ), label = \"Cluster Distribution\" , ) p3 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_3_kmeans_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_3_dbscan_\" + i ) ) ) ), label = \"Visualization\" , ) p4 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_1_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_2_\" + i ) ) ) ), label = \"Outlier Points\" , ) plot_ll . append ( dp . Group ( dp . Select ( blocks = [ p1 , p2 , p3 , p4 ], type = dp . SelectType . TABS ), label = i , ) ) plot_ll . append ( dp . Plot ( blank_chart , label = \" \" )) elif len ( all_geo_cols ) > 1 : p1 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_1_elbow_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_1_silhoutte_\" + i ) ) ) ), label = \"Cluster Identification\" , ) p2 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_2_kmeans_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_2_dbscan_\" + i ) ) ) ), label = \"Cluster Distribution\" , ) p3 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_3_kmeans_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_3_dbscan_\" + i ) ) ) ), label = \"Visualization\" , ) p4 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_1_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_2_\" + i ) ) ) ), label = \"Outlier Points\" , ) plot_ll . append ( dp . Group ( dp . Select ( blocks = [ p1 , p2 , p3 , p4 ], type = dp . SelectType . TABS ), label = i , ) ) return dp . Select ( blocks = plot_ll , type = dp . SelectType . DROPDOWN ) def read_loc_charts ( master_path) This function helps to read all the geospatial charts from the master path and populate in the report Parameters master_path Master path where the aggregated data resides Returns DatapaneObject Expand source code def read_loc_charts ( master_path ): \"\"\" This function helps to read all the geospatial charts from the master path and populate in the report Parameters ---------- master_path Master path where the aggregated data resides Returns ------- DatapaneObject \"\"\" ll_charts_nm = [ x for x in os . listdir ( master_path ) if \"loc_charts_ll\" in x ] geo_charts_nm = [ x for x in os . listdir ( master_path ) if \"loc_charts_gh\" in x ] ll_col_charts , geo_col_charts = [], [] if len ( ll_charts_nm ) > 0 : if len ( ll_charts_nm ) == 1 : for i1 in ll_charts_nm : col_name = i1 . replace ( \"loc_charts_ll_\" , \"\" ) ll_col_charts . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i1 ))), label = col_name , ) ) ll_col_charts . append ( dp . Plot ( blank_chart , label = \" \" )) elif len ( ll_charts_nm ) > 1 : for i1 in ll_charts_nm : col_name = i1 . replace ( \"loc_charts_ll_\" , \"\" ) ll_col_charts . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i1 ))), label = col_name , ) ) ll_col_charts = dp . Select ( blocks = ll_col_charts , type = dp . SelectType . DROPDOWN ) if len ( geo_charts_nm ) > 0 : if len ( geo_charts_nm ) == 1 : for i2 in geo_charts_nm : col_name = i2 . replace ( \"loc_charts_gh_\" , \"\" ) geo_col_charts . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i2 ))), label = col_name , ) ) geo_col_charts . append ( dp . Plot ( blank_chart , label = \" \" )) elif len ( geo_charts_nm ) > 1 : for i2 in geo_charts_nm : col_name = i2 . replace ( \"loc_charts_gh_\" , \"\" ) geo_col_charts . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i2 ))), label = col_name , ) ) geo_col_charts = dp . Select ( blocks = geo_col_charts , type = dp . SelectType . DROPDOWN ) if ( len ( ll_charts_nm ) > 0 ) and ( len ( geo_charts_nm ) == 0 ): return ll_col_charts elif ( len ( ll_charts_nm ) == 0 ) and ( len ( geo_charts_nm ) > 0 ): return geo_col_charts elif ( len ( ll_charts_nm ) > 0 ) and ( len ( geo_charts_nm ) > 0 ): return dp . Select ( blocks = [ dp . Group ( ll_col_charts , label = \"Lat-Long-Plot\" ), dp . Group ( geo_col_charts , label = \"Geohash-Plot\" ), ], type = dp . SelectType . TABS , ) def read_stats_ll_geo ( lat_col, long_col, geohash_col, master_path, top_geo_records) This function helps to read all the basis stats output for the lat-lon & geohash field produced from the analyzer module Parameters lat_col Latitude column identified long_col Longitude column identified geohash_col Geohash column identified master_path Master path where the aggregated data resides top_geo_records Top geospatial records displayed Returns DatapaneObject Expand source code def read_stats_ll_geo ( lat_col , long_col , geohash_col , master_path , top_geo_records ): \"\"\" This function helps to read all the basis stats output for the lat-lon & geohash field produced from the analyzer module Parameters ---------- lat_col Latitude column identified long_col Longitude column identified geohash_col Geohash column identified master_path Master path where the aggregated data resides top_geo_records Top geospatial records displayed Returns ------- DatapaneObject \"\"\" try : len_lat_col = len ( lat_col ) except : len_lat_col = 0 try : len_geohash_col = len ( geohash_col ) except : len_geohash_col = 0 ll_stats , geohash_stats = [], [] if len_lat_col > 0 : if len_lat_col == 1 : for idx , i in enumerate ( lat_col ): ll_stats . append ( dp . Group ( dp . Select ( blocks = [ dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Overall_Summary_1_\" + lat_col [ idx ] + \"_\" + long_col [ idx ] + \".csv\" ), label = \"Overall Summary\" , ), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Top_\" + str ( top_geo_records ) + \"_Lat_Long_1_\" + lat_col [ idx ] + \"_\" + long_col [ idx ] + \".csv\" ), label = \"Top \" + str ( top_geo_records ) + \" Lat Long\" , ), ], type = dp . SelectType . TABS , ), label = lat_col [ idx ] + \"_\" + long_col [ idx ], ) ) ll_stats . append ( dp . Group ( dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), label = \" \" , ) ) elif len_lat_col > 1 : for idx , i in enumerate ( lat_col ): ll_stats . append ( dp . Group ( dp . Select ( blocks = [ dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Overall_Summary_1_\" + lat_col [ idx ] + \"_\" + long_col [ idx ] + \".csv\" ), label = \"Overall Summary\" , ), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Top_\" + str ( top_geo_records ) + \"_Lat_Long_1_\" + lat_col [ idx ] + \"_\" + long_col [ idx ] + \".csv\" ), label = \"Top \" + str ( top_geo_records ) + \" Lat Long\" , ), ], type = dp . SelectType . TABS , ), label = lat_col [ idx ] + \"_\" + long_col [ idx ], ) ) ll_stats = dp . Select ( blocks = ll_stats , type = dp . SelectType . DROPDOWN ) if len_geohash_col > 0 : if len_geohash_col == 1 : for idx , i in enumerate ( geohash_col ): geohash_stats . append ( dp . Group ( dp . Select ( blocks = [ dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Overall_Summary_2_\" + geohash_col [ idx ] + \".csv\" ), label = \"Overall Summary\" , ), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Top_\" + str ( top_geo_records ) + \"_Geohash_Distribution_2_\" + geohash_col [ idx ] + \".csv\" ), label = \"Top \" + str ( top_geo_records ) + \" Geohash Distribution\" , ), ], type = dp . SelectType . TABS , ), label = geohash_col [ idx ], ) ) geohash_stats . append ( dp . Group ( dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), label = \" \" , ) ) elif len_geohash_col > 1 : for idx , i in enumerate ( geohash_col ): geohash_stats . append ( dp . Group ( dp . Select ( blocks = [ dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Overall_Summary_2_\" + geohash_col [ idx ] + \".csv\" ), label = \"Overall Summary\" , ), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Top_\" + str ( top_geo_records ) + \"_Geohash_Distribution_2_\" + geohash_col [ idx ] + \".csv\" ), label = \"Top \" + str ( top_geo_records ) + \" Geohash Distribution\" , ), ], type = dp . SelectType . TABS , ), label = geohash_col [ idx ], ) ) geohash_stats = dp . Select ( blocks = geohash_stats , type = dp . SelectType . DROPDOWN ) if ( len_lat_col + len_geohash_col ) == 1 : if len_lat_col == 0 : return geohash_stats else : return ll_stats elif ( len_lat_col + len_geohash_col ) > 1 : if ( len_lat_col > 1 ) and ( len_geohash_col == 0 ): return ll_stats elif ( len_lat_col == 0 ) and ( len_geohash_col > 1 ): return geohash_stats elif ( len_lat_col >= 1 ) and ( len_geohash_col >= 1 ): return dp . Select ( blocks = [ dp . Group ( ll_stats , label = \"Lat-Long-Stats\" ), dp . Group ( geohash_stats , label = \"Geohash-Stats\" ), ], type = dp . SelectType . TABS , ) def remove_u_score ( col) This functions help to remove the \"_\" present in a specific text Parameters col Analysis column containing \"_\" present gets replaced along with upper case conversion Returns String Expand source code def remove_u_score ( col ): \"\"\" This functions help to remove the \"_\" present in a specific text Parameters ---------- col Analysis column containing \"_\" present gets replaced along with upper case conversion Returns ------- String \"\"\" col_ = col . split ( \"_\" ) bl = [] for i in col_ : if i == \"nullColumns\" or i == \"nullRows\" : bl . append ( \"Null\" ) else : bl . append ( i [ 0 ] . upper () + i [ 1 :]) return \" \" . join ( bl ) def ts_landscape ( base_path, ts_cols, id_col) This function helps to produce a basic landscaping view of the data by picking up the base path for reading the aggregated data and specified by the timestamp / date column & the ID column. Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name id_col ID Column Returns DatapaneObject Expand source code def ts_landscape ( base_path , ts_cols , id_col ): \"\"\" This function helps to produce a basic landscaping view of the data by picking up the base path for reading the aggregated data and specified by the timestamp / date column & the ID column. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name id_col ID Column Returns ------- DatapaneObject \"\"\" if ts_cols is None : return dp . Text ( \"#\" ) else : df_stats_ts = [] for i in ts_cols : if len ( ts_cols ) > 1 : df_stats_ts . append ( dp . Group ( dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*ID considered here is : \" + str ( id_col ) + \"*\" ), dp . Text ( \"# \" ), dp . Text ( \"#### Consistency Analysis Of Dates\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_1.csv\" ) . set_index ( \"attribute\" ) . T , label = i , ), ), dp . Group ( dp . Text ( \"*The Percentile distribution across different bins of ID-Date / Date-ID combination should be in a considerable range to determine the regularity of Time series. In an ideal scenario the proportion of dates within each ID should be same. Also, the count of IDs across unique dates should be consistent for a balanced distribution*\" ), dp . Text ( \"# \" ), dp . Text ( \"#### Vital Statistics\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . T . rename ( columns = { 0 : \"\" }), label = i , ), ), label = i , ) ) else : df_stats_ts . append ( dp . Group ( dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*ID considered here is : \" + str ( id_col ) + \"*\" ), dp . Text ( \"#### Consistency Analysis Of Dates\" ), dp . Text ( \"# \" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_1.csv\" ) . set_index ( \"attribute\" ) . T , label = i , ), ), dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"#### Vital Statistics\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . T . rename ( columns = { 0 : \"\" }), label = i , ), ), label = i , ) ) df_stats_ts . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Group ( dp . Text ( \"### Time Stamp Data Diagnosis\" ), dp . Select ( blocks = df_stats_ts , type = dp . SelectType . DROPDOWN ), ) def ts_stats ( base_path) This function helps to read the base data containing desired input and produces output specific to the ts_cols_stats.csv file Parameters base_path Base path which is the same as Master path where the aggregated data resides. Returns List Expand source code def ts_stats ( base_path ): \"\"\" This function helps to read the base data containing desired input and produces output specific to the `ts_cols_stats.csv` file Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. Returns ------- List \"\"\" df = pd . read_csv ( base_path + \"ts_cols_stats.csv\" ) all_stats = [] for i in range ( 0 , 7 ): try : all_stats . append ( df [ df . index . values == i ] . values [ 0 ][ 0 ] . split ( \",\" )) except : all_stats . append ([]) c0 = pd . DataFrame ( all_stats [ 0 ], columns = [ \"attributes\" ]) c1 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 1 ], 1 ), columns = [ \"attributes\" ]) c1 [ \"Analyzed Attributes\" ] = \"\u2714\" c2 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 2 ], 1 ), columns = [ \"attributes\" ]) c2 [ \"Attributes Identified\" ] = \"\u2714\" c3 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 3 ], 1 ), columns = [ \"attributes\" ]) c3 [ \"Attributes Pre-Existed\" ] = \"\u2714\" c4 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 4 ], 1 ), columns = [ \"attributes\" ]) c4 [ \"Overall TimeStamp Attributes\" ] = \"\u2714\" c5 = list_ts_remove_append ( all_stats [ 5 ], 1 ) c6 = list_ts_remove_append ( all_stats [ 6 ], 1 ) return c0 , c1 , c2 , c3 , c4 , c5 , c6 def ts_viz_1_1 ( base_path, x_col, y_col, output_type) Parameters base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns Plot Expand source code def ts_viz_1_1 ( base_path , x_col , y_col , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Plot \"\"\" ts_fig = gen_time_series_plots ( base_path , x_col , y_col , output_type ) return ts_fig def ts_viz_1_2 ( base_path, ts_col, col_list, output_type) Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical / Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns DatapaneObject Expand source code def ts_viz_1_2 ( base_path , ts_col , col_list , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical / Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( col_list ) > 1 : bl . append ( dp . Group ( ts_viz_1_1 ( base_path , ts_col , i , output_type ), label = i )) else : bl . append ( dp . Group ( ts_viz_1_1 ( base_path , ts_col , i , output_type ), label = i )) bl . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_1_3 ( base_path, ts_col, num_cols, cat_cols, output_type) Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names cat_cols Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns DatapaneObject Expand source code def ts_viz_1_3 ( base_path , ts_col , num_cols , cat_cols , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names cat_cols Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject \"\"\" ts_v = [] # print(num_cols) # print(cat_cols) if len ( num_cols ) == 0 : for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = i ) ) else : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = i ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) elif len ( cat_cols ) == 0 : for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = i ) ) else : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = i ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) elif ( len ( num_cols ) >= 1 ) & ( len ( cat_cols ) >= 1 ): for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( dp . Select ( blocks = [ dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = \"Numerical\" , ), dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), label = i , ) ) else : ts_v . append ( dp . Group ( dp . Select ( blocks = [ dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = \"Numerical\" , ), dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_viz_2_1 ( base_path, x_col, y_col) Parameters base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns DatapaneObject Expand source code def ts_viz_2_1 ( base_path , x_col , y_col ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns ------- DatapaneObject \"\"\" ts_fig = [] for i in [ \"mean\" , \"median\" , \"min\" , \"max\" ]: ts_fig . append ( dp . Plot ( plotSeasonalDecompose ( base_path , x_col , y_col , metric_col = i ), label = i . title (), ) ) return dp . Select ( blocks = ts_fig , type = dp . SelectType . TABS ) def ts_viz_2_2 ( base_path, ts_col, col_list) Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns DatapaneObject Expand source code def ts_viz_2_2 ( base_path , ts_col , col_list ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( col_list ) > 1 : bl . append ( dp . Group ( ts_viz_2_1 ( base_path , ts_col , i ), label = i )) else : bl . append ( dp . Group ( ts_viz_2_1 ( base_path , ts_col , i ), label = i )) bl . append ( dp . Group ( dp . Plot ( blank_chart , label = \" \" ), label = \" \" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_2_3 ( base_path, ts_col, num_cols) Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns DatapaneObject Expand source code def ts_viz_2_3 ( base_path , ts_col , num_cols ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns ------- DatapaneObject \"\"\" ts_v = [] if len ( ts_col ) > 1 : for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 24 : ts_v . append ( dp . Group ( ts_viz_2_2 ( base_path , i , num_cols ), label = i )) else : ts_v . append ( dp . Group ( dp . Text ( \"The plots couldn't be displayed as x must have 2 complete cycles requires 24 observations. x only has \" + str ( f ) + \" observation(s)\" ), label = i , ) ) else : for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 24 : ts_v . append ( dp . Group ( ts_viz_2_2 ( base_path , i , num_cols ), label = i )) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) else : ts_v . append ( dp . Group ( dp . Text ( \"The plots couldn't be displayed as x must have 2 complete cycles requires 24 observations. x only has \" + str ( f ) + \" observation(s)\" ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_viz_3_1 ( base_path, x_col, y_col) Parameters base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns DatapaneObject Expand source code def ts_viz_3_1 ( base_path , x_col , y_col ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns ------- DatapaneObject \"\"\" ts_fig = [] df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_daily.csv\" ) . dropna () df [ x_col ] = pd . to_datetime ( df [ x_col ], format = \"%Y-%m- %d %H:%M:%S. %f \" ) df = df . set_index ( x_col ) for metric_col in [ \"mean\" , \"median\" , \"min\" , \"max\" ]: try : adf_test = ( round ( adfuller ( df [ metric_col ])[ 0 ], 3 ), round ( adfuller ( df [ metric_col ])[ 1 ], 3 ), ) if adf_test [ 1 ] < 0.05 : adf_flag = True else : adf_flag = False except : adf_test = ( \"nan\" , \"nan\" ) adf_flag = False try : kpss_test = ( round ( kpss ( df [ metric_col ], regression = \"ct\" )[ 0 ], 3 ), round ( kpss ( df [ metric_col ], regression = \"ct\" )[ 1 ], 3 ), ) if kpss_test [ 1 ] < 0.05 : kpss_flag = True else : kpss_flag = False except : kpss_test = ( \"nan\" , \"nan\" ) kpss_flag = False # df[metric_col] = df[metric_col].apply(lambda x: boxcox1p(x,0.25)) # lambda_box_cox = round(boxcox(df[metric_col])[1],5) fit = PowerTransformer ( method = \"yeo-johnson\" ) try : lambda_box_cox = round ( fit . fit ( np . array ( df [ metric_col ]) . reshape ( - 1 , 1 )) . lambdas_ [ 0 ], 3 ) cnt = 0 except : cnt = 1 if cnt == 0 : # df[metric_col+\"_transformed\"] = boxcox(df[metric_col],lmbda=lambda_box_cox) df [ metric_col + \"_transformed\" ] = fit . transform ( np . array ( df [ metric_col ]) . reshape ( - 1 , 1 ) ) fig = make_subplots ( rows = 1 , cols = 2 , subplot_titles = [ \"Pre-Transformation\" , \"Post-Transformation\" ], ) fig . add_trace ( go . Scatter ( x = df . index , y = df [ metric_col ], mode = \"lines+markers\" , name = metric_col , line = dict ( color = global_theme [ 1 ]), ), row = 1 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = df [ metric_col + \"_transformed\" ], mode = \"lines+markers\" , name = metric_col + \"_transformed\" , line = dict ( color = global_theme [ 7 ]), ), row = 1 , col = 2 , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( autosize = True , width = 2000 , height = 400 ) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) ts_fig . append ( dp . Group ( dp . Group ( dp . BigNumber ( heading = \"ADF Test Statistic\" , value = adf_test [ 0 ], change = adf_test [ 1 ], is_upward_change = adf_flag , ), dp . BigNumber ( heading = \"KPSS Test Statistic\" , value = kpss_test [ 0 ], change = kpss_test [ 1 ], is_upward_change = kpss_flag , ), dp . BigNumber ( heading = \"Box-Cox Transformation\" , value = lambda_box_cox , change = str ( lambda_cat ( lambda_box_cox )), is_upward_change = True , ), columns = 3 , ), dp . Text ( \"#### Transformation View\" ), dp . Text ( \"Below Transformation is basis the inferencing from the Box Cox Transformation. The Lambda value of \" + str ( lambda_box_cox ) + \" indicates a \" + str ( lambda_cat ( lambda_box_cox )) + \". A Pre-Post Transformation Visualization is done for better clarity. \" ), dp . Plot ( fig ), dp . Text ( \"**Guidelines :** \" ), dp . Text ( \"**ADF** : *The more negative the statistic, the more likely we are to reject the null hypothesis. If the p-value is less than the significance level of 0.05, we can reject the null hypothesis and take that the series is stationary*\" ), dp . Text ( \"**KPSS** : *If the p-value is high, we cannot reject the null hypothesis. So the series is stationary.*\" ), label = metric_col . title (), ) ) else : ts_fig . append ( dp . Group ( dp . Group ( dp . BigNumber ( heading = \"ADF Test Statistic\" , value = adf_test [ 0 ], change = adf_test [ 1 ], is_upward_change = adf_flag , ), dp . BigNumber ( heading = \"KPSS Test Statistic\" , value = kpss_test [ 0 ], change = kpss_test [ 1 ], is_upward_change = kpss_flag , ), dp . BigNumber ( heading = \"Box-Cox Transformation\" , value = \"ValueOutOfRange\" , change = \"ValueOutOfRange\" , is_upward_change = True , ), columns = 3 , ), dp . Text ( \"**Guidelines :** \" ), dp . Text ( \"**ADF** : *The more negative the statistic, the more likely we are to reject the null hypothesis. If the p-value is less than the significance level of 0.05, we can reject the null hypothesis and take that the series is stationary*\" ), dp . Text ( \"**KPSS** : *If the p-value is high, we cannot reject the null hypothesis. So the series is stationary.*\" ), label = metric_col . title (), ) ) return dp . Select ( blocks = ts_fig , type = dp . SelectType . TABS ) def ts_viz_3_2 ( base_path, ts_col, col_list) Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns DatapaneObject Expand source code def ts_viz_3_2 ( base_path , ts_col , col_list ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( num_cols ) > 1 : bl . append ( dp . Group ( ts_viz_3_1 ( base_path , ts_col , i ), label = i )) else : bl . append ( dp . Group ( ts_viz_3_1 ( base_path , ts_col , i ), label = i )) bl . append ( dp . Group ( dp . Plot ( blank_chart , label = \" \" ), label = \" \" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_3_3 ( base_path, ts_col, num_cols) Parameters base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns DatapaneObject Expand source code def ts_viz_3_3 ( base_path , ts_col , num_cols ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns ------- DatapaneObject \"\"\" # f = list(pd.read_csv(ends_with(base_path) + \"stats_\" + i + \"_2.csv\").count_unique_dates.values)[0] # if f >= 6: if len ( ts_col ) > 1 : ts_v = [] for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 6 : ts_v . append ( dp . Group ( ts_viz_3_2 ( base_path , i , num_cols ), label = i )) else : ts_v . append ( dp . Group ( dp . Text ( \"The data contains insufficient data points for the desired transformation analysis. Please ensure the number of unique dates is sufficient.\" ), label = i , ) ) else : ts_v = [] for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 6 : ts_v . append ( dp . Group ( ts_viz_3_2 ( base_path , i , num_cols ), label = i )) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) else : ts_v . append ( dp . Group ( dp . Text ( \"The data contains insufficient data points for the desired transformation analysis. Please ensure the number of unique dates is sufficient.\" ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_viz_generate ( master_path, id_col, print_report=False, output_type=None) This function helps to produce the output in the nested / recursive function supported by datapane. Eventually this is populated at the final report. Parameters master_path Master path where the aggregated data resides. id_col ID Column print_report Option to specify whether the Report needs to be saved or not. True / False can be used to specify the needful. output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns DatapaneObject / Output[HTML] Expand source code def ts_viz_generate ( master_path , id_col , print_report = False , output_type = None ): \"\"\" This function helps to produce the output in the nested / recursive function supported by datapane. Eventually this is populated at the final report. Parameters ---------- master_path Master path where the aggregated data resides. id_col ID Column print_report Option to specify whether the Report needs to be saved or not. True / False can be used to specify the needful. output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject / Output[HTML] \"\"\" master_path = ends_with ( master_path ) try : c0 , c1 , c2 , c3 , c4 , c5 , c6 = ts_stats ( master_path ) except : return \"null_report\" stats_df = ( c0 . merge ( c1 , on = \"attributes\" , how = \"left\" ) . merge ( c2 , on = \"attributes\" , how = \"left\" ) . merge ( c3 , on = \"attributes\" , how = \"left\" ) . merge ( c4 , on = \"attributes\" , how = \"left\" ) . fillna ( \"\u2718\" ) ) global num_cols global cat_cols num_cols , cat_cols = c5 , c6 final_ts_cols = list ( ts_stats ( master_path )[ 4 ] . attributes . values ) if output_type == \"daily\" : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Basic Landscaping\" ), dp . Text ( \"Out of **\" + str ( len ( list ( ts_stats ( master_path )[ 1 ] . attributes . values ))) + \"** potential attributes in the data, the module could locate **\" + str ( len ( final_ts_cols )) + \"** attributes as Timestamp\" ), dp . DataTable ( stats_df ), ts_landscape ( master_path , final_ts_cols , id_col ), dp . Text ( \"*Lower the **CoV** (Coefficient Of Variation), Higher the Consistency between the consecutive dates. Similarly the Mean & Variance should be consistent over time*\" ), dp . Text ( \"### Visualization across the Shortlisted Timestamp Attributes\" ), ts_viz_1_3 ( master_path , final_ts_cols , num_cols , cat_cols , output_type ), dp . Text ( \"### Decomposed View\" ), ts_viz_2_3 ( master_path , final_ts_cols , num_cols ), dp . Text ( \"### Stationarity & Transformations\" ), ts_viz_3_3 ( master_path , final_ts_cols , num_cols ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), label = \"Time Series Analyzer\" , ) elif output_type is None : report = \"null_report\" else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Basic Landscaping\" ), dp . Text ( \"Out of **\" + str ( len ( list ( ts_stats ( master_path )[ 1 ] . attributes . values ))) + \"** potential attributes in the data, the module could locate **\" + str ( len ( final_ts_cols )) + \"** attributes as Timestamp\" ), dp . DataTable ( stats_df ), ts_landscape ( master_path , final_ts_cols , id_col ), dp . Text ( \"*Lower the **CoV** (Coefficient Of Variation), Higher the Consistency between the consecutive dates. Similarly the Mean & Variance should be consistent over time*\" ), dp . Text ( \"### Visualization across the Shortlisted Timestamp Attributes\" ), ts_viz_1_3 ( master_path , final_ts_cols , num_cols , cat_cols , output_type ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), label = \"Time Series Analyzer\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"time_series_analyzer.html\" , open = True ) return report def wiki_generator ( master_path, dataDict_path=None, metricDict_path=None, print_report=False) This function helps to produce output specific to the Wiki Tab. Parameters master_path Path containing the input files. dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. print_report Printing option flexibility. Default value is kept as False. Returns DatapaneObject / Output[HTML] Expand source code def wiki_generator ( master_path , dataDict_path = None , metricDict_path = None , print_report = False ): \"\"\" This function helps to produce output specific to the Wiki Tab. Parameters ---------- master_path Path containing the input files. dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" try : datatype_df = pd . read_csv ( ends_with ( master_path ) + \"data_type.csv\" ) except FileNotFoundError : logger . error ( f \"file { master_path } /data_type.csv doesn't exist, cannot read datatypes\" ) except Exception : logger . info ( \"generate an empty dataframe with columns attribute and data_type \" ) datatype_df = pd . DataFrame ( columns = [ \"attribute\" , \"data_type\" ], index = range ( 1 )) try : data_dict = pd . read_csv ( dataDict_path ) . merge ( datatype_df , how = \"outer\" , on = \"attribute\" ) except FileNotFoundError : logger . error ( f \"file { dataDict_path } doesn't exist, cannot read data dict\" ) except Exception : data_dict = datatype_df try : metric_dict = pd . read_csv ( metricDict_path ) except FileNotFoundError : logger . error ( f \"file { metricDict_path } doesn't exist, cannot read metrics dict\" ) except Exception : metric_dict = pd . DataFrame ( columns = [ \"Section Category\" , \"Section Name\" , \"Metric Name\" , \"Metric Definitions\" , ], index = range ( 1 ), ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *A quick reference to the attributes from the dataset (Data Dictionary) and the metrics computed in the report (Metric Dictionary).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Group ( dp . Text ( \"## \" ), dp . DataTable ( data_dict )), label = \"Data Dictionary\" , ), dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( metric_dict ), label = \"Metric Dictionary\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Wiki\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"wiki_generator.html\" , open = True ) return report","title":"<code>report_generation</code>"},{"location":"api/data_report/report_generation.html#report_generation","text":"This module generates the final report output specific to the intermediate data generated across each of the modules. The final report, however, can be proccessed through the config.yaml file or by generating it through the respective functions. Below are some of the functions used to process the final output. line_chart_gen_stability data_analyzer_output drift_stability_ind chart_gen_list executive_summary_gen wiki_generator descriptive_statistics quality_check attribute_associations data_drift_stability plotSeasonalDecompose gen_time_series_plots list_ts_remove_append ts_viz_1_1 \u2014 ts_viz_1_3 ts_viz_2_1 \u2014 ts_viz_2_3 ts_viz_3_1 \u2014 ts_viz_3_3 ts_landscape ts_stats ts_viz_generate overall_stats_gen loc_field_stats read_stats_ll_geo read_cluster_stats_ll_geo read_loc_charts loc_report_gen anovos_report However, each of the functions have been detailed in the respective sections across the parameters used. Expand source code # coding=utf-8 \"\"\"This module generates the final report output specific to the intermediate data generated across each of the modules. The final report, however, can be proccessed through the config.yaml file or by generating it through the respective functions. Below are some of the functions used to process the final output. - line_chart_gen_stability - data_analyzer_output - drift_stability_ind - chart_gen_list - executive_summary_gen - wiki_generator - descriptive_statistics - quality_check - attribute_associations - data_drift_stability - plotSeasonalDecompose - gen_time_series_plots - list_ts_remove_append - ts_viz_1_1 \u2014 ts_viz_1_3 - ts_viz_2_1 \u2014 ts_viz_2_3 - ts_viz_3_1 \u2014 ts_viz_3_3 - ts_landscape - ts_stats - ts_viz_generate - overall_stats_gen - loc_field_stats - read_stats_ll_geo - read_cluster_stats_ll_geo - read_loc_charts - loc_report_gen - anovos_report However, each of the functions have been detailed in the respective sections across the parameters used. \"\"\" import json import os import subprocess import warnings import datapane as dp import dateutil.parser import mlflow import numpy as np import pandas as pd import plotly.express as px import plotly.graph_objects as go import plotly.tools as tls from loguru import logger from plotly.subplots import make_subplots from sklearn.preprocessing import PowerTransformer from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.stattools import adfuller , kpss from anovos.shared.utils import ends_with , output_to_local , path_ak8s_modify warnings . filterwarnings ( \"ignore\" ) global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" default_template = ( dp . HTML ( \"\"\" <html> <img src=\"https://mobilewalla-anovos.s3.amazonaws.com/anovos.png\" style=\"height:100px;display:flex;margin:auto;float:right\" /> </html>\"\"\" ), dp . Text ( \"# ML-Anovos Report\" ), ) def remove_u_score ( col ): \"\"\" This functions help to remove the \"_\" present in a specific text Parameters ---------- col Analysis column containing \"_\" present gets replaced along with upper case conversion Returns ------- String \"\"\" col_ = col . split ( \"_\" ) bl = [] for i in col_ : if i == \"nullColumns\" or i == \"nullRows\" : bl . append ( \"Null\" ) else : bl . append ( i [ 0 ] . upper () + i [ 1 :]) return \" \" . join ( bl ) def line_chart_gen_stability ( df1 , df2 , col ): \"\"\" This function helps to produce charts which are specific to data stability index. It taken into account the stability input along with the analysis column to produce the desired output. Parameters ---------- df1 Analysis dataframe pertaining to summarized stability metrics df2 Analysis dataframe pertaining to historical data col Analysis column Returns ------- DatapaneObject \"\"\" def val_cat ( val ): \"\"\" Parameters ---------- val Returns ------- String \"\"\" if val >= 3.5 : return \"Very Stable\" elif val >= 3 and val < 3.5 : return \"Stable\" elif val >= 2 and val < 3 : return \"Marginally Stable\" elif val >= 1 and val < 2 : return \"Unstable\" elif val >= 0 and val < 1 : return \"Very Unstable\" else : return \"Out of Range\" val_si = list ( df2 [ df2 [ \"attribute\" ] == col ] . stability_index . values )[ 0 ] f1 = go . Figure () f1 . add_trace ( go . Indicator ( mode = \"gauge+number\" , value = val_si , gauge = { \"axis\" : { \"range\" : [ None , 4 ], \"tickwidth\" : 1 , \"tickcolor\" : \"black\" }, \"bgcolor\" : \"white\" , \"steps\" : [ { \"range\" : [ 0 , 1 ], \"color\" : px . colors . sequential . Reds [ 7 ]}, { \"range\" : [ 1 , 2 ], \"color\" : px . colors . sequential . Reds [ 6 ]}, { \"range\" : [ 2 , 3 ], \"color\" : px . colors . sequential . Oranges [ 4 ]}, { \"range\" : [ 3 , 3.5 ], \"color\" : px . colors . sequential . BuGn [ 7 ]}, { \"range\" : [ 3.5 , 4 ], \"color\" : px . colors . sequential . BuGn [ 8 ]}, ], \"threshold\" : { \"line\" : { \"color\" : \"black\" , \"width\" : 3 }, \"thickness\" : 1 , \"value\" : val_si , }, \"bar\" : { \"color\" : global_plot_bg_color }, }, title = { \"text\" : \"Order of Stability: \" + val_cat ( val_si )}, ) ) f1 . update_layout ( height = 400 , font = { \"color\" : \"black\" , \"family\" : \"Arial\" }) f5 = \"Stability Index for \" + str ( col . upper ()) if len ( df1 . columns ) > 0 : attr_type = df1 [ \"type\" ] . tolist ()[ 0 ] if attr_type == \"Numerical\" : f2 = px . line ( df1 , x = \"idx\" , y = \"mean\" , markers = True , title = \"CV of Mean is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . mean_cv . values )[ 0 ]), ) f2 . update_traces ( line_color = global_theme [ 2 ], marker = dict ( size = 14 )) f2 . layout . plot_bgcolor = global_plot_bg_color f2 . layout . paper_bgcolor = global_paper_bg_color f3 = px . line ( df1 , x = \"idx\" , y = \"stddev\" , markers = True , title = \"CV of Stddev is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . stddev_cv . values )[ 0 ]), ) f3 . update_traces ( line_color = global_theme [ 6 ], marker = dict ( size = 14 )) f3 . layout . plot_bgcolor = global_plot_bg_color f3 . layout . paper_bgcolor = global_paper_bg_color f4 = px . line ( df1 , x = \"idx\" , y = \"kurtosis\" , markers = True , title = \"CV of Kurtosis is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . kurtosis_cv . values )[ 0 ]), ) f4 . update_traces ( line_color = global_theme [ 4 ], marker = dict ( size = 14 )) f4 . layout . plot_bgcolor = global_plot_bg_color f4 . layout . paper_bgcolor = global_paper_bg_color return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), dp . Group ( dp . Plot ( f2 ), dp . Plot ( f3 ), dp . Plot ( f4 ), columns = 3 ), label = col , ) else : f2 = px . line ( df1 , x = \"idx\" , y = \"mean\" , markers = True , title = \"Standard deviation of Mean is \" + str ( list ( df2 [ df2 [ \"attribute\" ] == col ] . mean_stddev . values )[ 0 ]), ) f2 . update_traces ( line_color = global_theme [ 2 ], marker = dict ( size = 14 )) f2 . layout . plot_bgcolor = global_plot_bg_color f2 . layout . paper_bgcolor = global_paper_bg_color return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), dp . Group ( dp . Plot ( f2 ), columns = 1 ), label = col , ) else : return dp . Group ( dp . Text ( \"#\" ), dp . Text ( f5 ), dp . Plot ( f1 ), label = col ) def data_analyzer_output ( master_path , avl_recs_tab , tab_name ): \"\"\" This section produces output in form of datapane objects which is specific to the different data analyzer modules. It is used by referring to the Master path along with the Available list of metrics & the Tab name. Parameters ---------- master_path Path containing all the output from analyzed data avl_recs_tab Available file names from the analysis tab tab_name Analysis tab from association_evaluator / quality_checker / stats_generator Returns ------- DatapaneObject \"\"\" df_list = [] df_plot_list = [] # @FIXME: unused variables plot_list = [] avl_recs_tab = [ x for x in avl_recs_tab if \"global_summary\" not in x ] for index , i in enumerate ( avl_recs_tab ): data = pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) if len ( data . index ) == 0 : continue if tab_name == \"quality_checker\" : if i == \"duplicate_detection\" : duplicate_recs = pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) _unique_rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"unique_rows_count\" ] . value . values ) _rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"rows_count\" ] . value . values ) _duplicate_rows_count = int ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"duplicate_rows\" ] . value . values ) _duplicate_pct = float ( duplicate_recs [ duplicate_recs [ \"metric\" ] == \"duplicate_pct\" ] . value . values * 100.0 ) unique_rows_count = f \" No. Of Unique Rows: ** { _unique_rows_count } **\" # @FIXME: variable names exists in outer scope rows_count = f \" No. of Rows: ** { _rows_count } **\" duplicate_rows = f \" No. of Duplicate Rows: ** { _duplicate_rows_count } **\" duplicate_pct = f \" Percentage of Duplicate Rows: ** { _duplicate_pct } %**\" df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . Group ( dp . Text ( rows_count ), dp . Text ( unique_rows_count ), dp . Text ( duplicate_rows ), dp . Text ( duplicate_pct ), ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) elif i == \"outlier_detection\" : df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) ), \"outlier_charts_placeholder\" , ] ) else : df_list . append ( [ dp . Text ( \"### \" + str ( remove_u_score ( i ))), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ) ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), ] ) elif tab_name == \"association_evaluator\" : for j in avl_recs_tab : if j == \"correlation_matrix\" : df_list_ = pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) feats_order = list ( df_list_ [ \"attribute\" ] . values ) df_list_ = df_list_ . round ( 3 ) fig = px . imshow ( df_list_ [ feats_order ], y = feats_order , color_continuous_scale = global_theme , aspect = \"auto\" , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Correlation Plot \")) df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ [[ \"attribute\" ] + feats_order ]), dp . Plot ( fig ), label = remove_u_score ( j ), ) ) elif j == \"variable_clustering\" : df_list_ = ( pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) . sort_values ( by = [ \"Cluster\" ], ascending = True ) ) fig = px . sunburst ( df_list_ , path = [ \"Cluster\" , \"Attribute\" ], values = \"RS_Ratio\" , color_discrete_sequence = global_theme , ) # fig.update_layout(title_text=str(\"Distribution of homogenous variable across Clusters\")) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Variable Clustering Plot \")) fig . layout . autosize = True df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ ), dp . Plot ( fig ), label = remove_u_score ( j ), ) ) else : try : df_list_ = pd . read_csv ( ends_with ( master_path ) + str ( j ) + \".csv\" ) . round ( 3 ) col_nm = [ x for x in list ( df_list_ . columns ) if \"attribute\" not in x ] df_list_ = df_list_ . sort_values ( col_nm [ 0 ], ascending = True ) fig = px . bar ( df_list_ , x = col_nm [ 0 ], y = \"attribute\" , orientation = \"h\" , color_discrete_sequence = global_theme , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # fig.update_layout(title_text=str(\"Representation of \" + str(remove_u_score(j)))) fig . layout . autosize = True df_plot_list . append ( dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( df_list_ ), dp . Plot ( fig ), label = remove_u_score ( j ), ) ) except Exception as e : logger . error ( f \"processing failed, error { e } \" ) pass if len ( avl_recs_tab ) == 1 : df_plot_list . append ( dp . Group ( dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), dp . Plot ( blank_chart , label = \" \" ), label = \" \" , ) ) else : pass return df_plot_list else : df_list . append ( dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + str ( i ) + \".csv\" ) . round ( 3 ), label = remove_u_score ( avl_recs_tab [ index ]), ) ) if tab_name == \"quality_checker\" and len ( avl_recs_tab ) == 1 : return df_list [ 0 ], [ dp . Text ( \"#\" ), dp . Plot ( blank_chart )] elif tab_name == \"stats_generator\" and len ( avl_recs_tab ) == 1 : return [ df_list [ 0 ], dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), ] else : return df_list def drift_stability_ind ( missing_recs_drift , drift_tab , missing_recs_stability , stability_tab ): \"\"\" This function helps to produce the drift & stability indicator for further processing. Ideally a data with both drift & stability should produce a list of [1,1] Parameters ---------- missing_recs_drift Missing files from the drift tab drift_tab \"drift_statistics\" missing_recs_stability Missing files from the stability tab stability_tab \"stability_index, stabilityIndex_metrics\" Returns ------- List \"\"\" if len ( missing_recs_drift ) == len ( drift_tab ): drift_ind = 0 else : drift_ind = 1 if len ( missing_recs_stability ) == len ( stability_tab ): stability_ind = 0 elif ( \"stabilityIndex_metrics\" in missing_recs_stability ) and ( \"stability_index\" not in missing_recs_stability ): stability_ind = 0.5 else : stability_ind = 1 return drift_ind , stability_ind def chart_gen_list ( master_path , chart_type , type_col = None ): \"\"\" This function helps to produce the charts in a list object form nested by a datapane object. Parameters ---------- master_path Path containing all the charts same as the other files from data analyzed output chart_type Files containing only the specific chart names for the specific chart category type_col None. Default value is kept as None Returns ------- DatapaneObject \"\"\" plot_list = [] for i in chart_type : col_name = i [ i . find ( \"_\" ) + 1 :] if type_col == \"numerical\" : if col_name in numcols_name . replace ( \" \" , \"\" ) . split ( \",\" ): plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) else : pass elif type_col == \"categorical\" : if col_name in catcols_name . replace ( \" \" , \"\" ) . split ( \",\" ): plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) else : pass else : plot_list . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i ))), label = col_name , ) ) return plot_list def executive_summary_gen ( master_path , label_col , ds_ind , id_col , iv_threshold , corr_threshold , print_report = False , ): \"\"\" This function helps to produce output specific to the Executive Summary Tab. Parameters ---------- master_path Path containing the input files. label_col Label column. ds_ind Drift stability indicator in list form. id_col ID column. iv_threshold IV threshold beyond which attributes can be called as significant. corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" try : obj_dtls = json . load ( open ( ends_with ( master_path ) + \"freqDist_\" + str ( label_col )) ) # @FIXME: never used local variable text_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 8 ][ 1 ] x_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 10 ][ 1 ] y_val = list ( list ( obj_dtls . values ())[ 0 ][ 0 ] . items ())[ 12 ][ 1 ] label_fig_ = go . Figure ( data = [ go . Pie ( labels = x_val , values = y_val , textinfo = \"label+percent\" , insidetextorientation = \"radial\" , pull = [ 0 , 0.1 ], marker_colors = global_theme , ) ] ) label_fig_ . update_traces ( textposition = \"inside\" , textinfo = \"percent+label\" ) label_fig_ . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) label_fig_ . layout . plot_bgcolor = global_plot_bg_color label_fig_ . layout . paper_bgcolor = global_paper_bg_color except Exception as e : logger . error ( f \"processing failed, error { e } \" ) label_fig_ = None a1 = ( \"The dataset contains **\" + str ( f \" { rows_count : ,d } \" ) + \"** records and **\" + str ( numcols_count + catcols_count ) + \"** attributes (**\" + str ( numcols_count ) + \"** numerical + **\" + str ( catcols_count ) + \"** categorical).\" ) if label_col is None : a2 = dp . Group ( dp . Text ( \"- There is **no** target variable in the dataset\" ), dp . Text ( \"- Data Diagnosis:\" ), ) else : if label_fig_ is None : a2 = dp . Group ( dp . Text ( \"- Target variable is **\" + str ( label_col ) + \"** \" ), dp . Text ( \"- Data Diagnosis:\" ), ) else : a2 = dp . Group ( dp . Text ( \"- Target variable is **\" + str ( label_col ) + \"** \" ), dp . Plot ( label_fig_ ), dp . Text ( \"- Data Diagnosis:\" ), ) try : x1 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_dispersion.csv\" ) . query ( \"`cov`>1\" ) . attribute . values ) if len ( x1 ) > 0 : x1_1 = [ \"High Variance\" , x1 ] else : x1_1 = [ \"High Variance\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x1_1 = [ \"High Variance\" , None ] try : x2 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`skewness`>0\" ) . attribute . values ) if len ( x2 ) > 0 : x2_1 = [ \"Positive Skewness\" , x2 ] else : x2_1 = [ \"Positive Skewness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x2_1 = [ \"Positive Skewness\" , None ] try : x3 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`skewness`<0\" ) . attribute . values ) if len ( x3 ) > 0 : x3_1 = [ \"Negative Skewness\" , x3 ] else : x3_1 = [ \"Negative Skewness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x3_1 = [ \"Negative Skewness\" , None ] try : x4 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`kurtosis`>0\" ) . attribute . values ) if len ( x4 ) > 0 : x4_1 = [ \"High Kurtosis\" , x4 ] else : x4_1 = [ \"High Kurtosis\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x4_1 = [ \"High Kurtosis\" , None ] try : x5 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_shape.csv\" ) . query ( \"`kurtosis`<0\" ) . attribute . values ) if len ( x5 ) > 0 : x5_1 = [ \"Low Kurtosis\" , x5 ] else : x5_1 = [ \"Low Kurtosis\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x5_1 = [ \"Low Kurtosis\" , None ] try : x6 = list ( pd . read_csv ( ends_with ( master_path ) + \"measures_of_counts.csv\" ) . query ( \"`fill_pct`<0.7\" ) . attribute . values ) if len ( x6 ) > 0 : x6_1 = [ \"Low Fill Rates\" , x6 ] else : x6_1 = [ \"Low Fill Rates\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x6_1 = [ \"Low Fill Rates\" , None ] try : biasedness_df = pd . read_csv ( ends_with ( master_path ) + \"biasedness_detection.csv\" ) if \"treated\" in biasedness_df : x7 = list ( biasedness_df . query ( \"`treated`>0\" ) . attribute . values ) else : x7 = list ( biasedness_df . query ( \"`flagged`>0\" ) . attribute . values ) if len ( x7 ) > 0 : x7_1 = [ \"High Biasedness\" , x7 ] else : x7_1 = [ \"High Biasedness\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x7_1 = [ \"High Biasedness\" , None ] try : x8 = list ( pd . read_csv ( ends_with ( master_path ) + \"outlier_detection.csv\" ) . attribute . values ) if len ( x8 ) > 0 : x8_1 = [ \"Outliers\" , x8 ] else : x8_1 = [ \"Outliers\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x8_1 = [ \"Outliers\" , None ] try : corr_matrx = pd . read_csv ( ends_with ( master_path ) + \"correlation_matrix.csv\" ) corr_matrx = corr_matrx [ list ( corr_matrx . attribute . values )] corr_matrx = corr_matrx . where ( np . triu ( np . ones ( corr_matrx . shape ), k = 1 ) . astype ( np . bool ) ) to_drop = [ column for column in corr_matrx . columns if any ( corr_matrx [ column ] > corr_threshold ) ] if len ( to_drop ) > 0 : x9_1 = [ \"High Correlation\" , to_drop ] else : x9_1 = [ \"High Correlation\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x9_1 = [ \"High Correlation\" , None ] try : x10 = list ( pd . read_csv ( ends_with ( master_path ) + \"IV_calculation.csv\" ) . query ( \"`iv`>\" + str ( iv_threshold )) . attribute . values ) if len ( x10 ) > 0 : x10_1 = [ \"Significant Attributes\" , x10 ] else : x10_1 = [ \"Significant Attributes\" , None ] except Exception as e : logger . error ( f \"processing failed, error { e } \" ) x10_1 = [ \"Significant Attributes\" , None ] blank_list_df = [] for i in [ x1_1 , x2_1 , x3_1 , x4_1 , x5_1 , x6_1 , x7_1 , x8_1 , x9_1 , x10_1 ]: try : for j in i [ 1 ]: blank_list_df . append ([ i [ 0 ], j ]) except Exception as e : logger . error ( f \"processing failed, error { e } \" ) blank_list_df . append ([ i [ 0 ], \"NA\" ]) list_n = [] x1 = pd . DataFrame ( blank_list_df , columns = [ \"Metric\" , \"Attribute\" ]) x1 [ \"Value\" ] = \"\u2714\" all_cols = ( catcols_name . replace ( \" \" , \"\" ) + \",\" + numcols_name . replace ( \" \" , \"\" ) ) . split ( \",\" ) remainder_cols = list ( set ( all_cols ) - set ( x1 . Attribute . values )) total_metrics = set ( list ( x1 . Metric . values )) for i in remainder_cols : for j in total_metrics : list_n . append ([ j , i ]) x2 = pd . DataFrame ( list_n , columns = [ \"Metric\" , \"Attribute\" ]) x2 [ \"Value\" ] = \"\u2718\" x = x1 . append ( x2 , ignore_index = True ) x = ( x . drop_duplicates () . pivot ( index = \"Attribute\" , columns = \"Metric\" , values = \"Value\" ) . fillna ( \"\u2718\" ) . reset_index ()[ [ \"Attribute\" , \"Outliers\" , \"Significant Attributes\" , \"Positive Skewness\" , \"Negative Skewness\" , \"High Variance\" , \"High Correlation\" , \"High Kurtosis\" , \"Low Kurtosis\" , ] ] ) x = x [ ~ ( ( x [ \"Attribute\" ] . isnull ()) | ( x . Attribute . values == \"NA\" ) | ( x [ \"Attribute\" ] == \" \" ) ) ] if ds_ind [ 0 ] == 1 and ds_ind [ 1 ] >= 0.5 : a5 = \"Data Health based on Drift Metrics & Stability Index : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Drifted Attributes\" , value = str ( str ( drifted_feats ) + \" out of \" + str ( len_feats )), ), dp . BigNumber ( heading = \"% Drifted Attributes\" , value = str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%\" , ), dp . BigNumber ( heading = \"# Unstable Attributes\" , value = str ( len ( unstable_attr )) + \" out of \" + str ( len ( total_unstable_attr )), change = \"numerical\" , is_upward_change = True , ), dp . BigNumber ( heading = \"% Unstable Attributes\" , value = str ( np . round ( 100 * len ( unstable_attr ) / len ( total_unstable_attr ), 2 ) ) + \"%\" , ), columns = 4 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] >= 0.5 : a5 = \"Data Health based on Stability Index : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Unstable Attributes\" , value = str ( len ( unstable_attr )) + \" out of \" + str ( len ( total_unstable_attr )), change = \"numerical\" , is_upward_change = True , ), dp . BigNumber ( heading = \"% Unstable Attributes\" , value = str ( np . round ( 100 * len ( unstable_attr ) / len ( total_unstable_attr ), 2 ) ) + \"%\" , ), columns = 2 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 1 and ds_ind [ 1 ] == 0 : a5 = \"Data Health based on Drift Metrics : \" report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"- \" + a5 ), dp . Group ( dp . BigNumber ( heading = \"# Drifted Attributes\" , value = str ( str ( drifted_feats ) + \" out of \" + str ( len_feats )), ), dp . BigNumber ( heading = \"% Drifted Attributes\" , value = str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%\" , ), columns = 2 , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] == 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"**Key Report Highlights**\" ), dp . Text ( \"# \" ), dp . Text ( \"- \" + a1 ), a2 , dp . DataTable ( x ), dp . Text ( \"# \" ), label = \"Executive Summary\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"executive_summary.html\" , open = True ) return report # @FIXME: rename variables with their corresponding within the config files def wiki_generator ( master_path , dataDict_path = None , metricDict_path = None , print_report = False ): \"\"\" This function helps to produce output specific to the Wiki Tab. Parameters ---------- master_path Path containing the input files. dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" try : datatype_df = pd . read_csv ( ends_with ( master_path ) + \"data_type.csv\" ) except FileNotFoundError : logger . error ( f \"file { master_path } /data_type.csv doesn't exist, cannot read datatypes\" ) except Exception : logger . info ( \"generate an empty dataframe with columns attribute and data_type \" ) datatype_df = pd . DataFrame ( columns = [ \"attribute\" , \"data_type\" ], index = range ( 1 )) try : data_dict = pd . read_csv ( dataDict_path ) . merge ( datatype_df , how = \"outer\" , on = \"attribute\" ) except FileNotFoundError : logger . error ( f \"file { dataDict_path } doesn't exist, cannot read data dict\" ) except Exception : data_dict = datatype_df try : metric_dict = pd . read_csv ( metricDict_path ) except FileNotFoundError : logger . error ( f \"file { metricDict_path } doesn't exist, cannot read metrics dict\" ) except Exception : metric_dict = pd . DataFrame ( columns = [ \"Section Category\" , \"Section Name\" , \"Metric Name\" , \"Metric Definitions\" , ], index = range ( 1 ), ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *A quick reference to the attributes from the dataset (Data Dictionary) and the metrics computed in the report (Metric Dictionary).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Group ( dp . Text ( \"## \" ), dp . DataTable ( data_dict )), label = \"Data Dictionary\" , ), dp . Group ( dp . Text ( \"##\" ), dp . DataTable ( metric_dict ), label = \"Metric Dictionary\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Wiki\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"wiki_generator.html\" , open = True ) return report def descriptive_statistics ( master_path , SG_tabs , avl_recs_SG , missing_recs_SG , all_charts_num_1_ , all_charts_cat_1_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Descriptive Stats Tab. Parameters ---------- master_path Path containing the input files. SG_tabs measures_of_counts','measures_of_centralTendency','measures_of_cardinality','measures_of_percentiles','measures_of_dispersion','measures_of_shape','global_summary' avl_recs_SG Available files from the SG_tabs (Stats Generator tabs) missing_recs_SG Missing files from the SG_tabs (Stats Generator tabs) all_charts_num_1_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_1_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" if \"global_summary\" in avl_recs_SG : cnt = 0 else : cnt = 1 if len ( missing_recs_SG ) + cnt == len ( SG_tabs ): return \"null_report\" else : if \"global_summary\" in avl_recs_SG : l1 = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the dataset with key statistical metrics and distribution plots.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Global Summary\" ), dp . Group ( dp . Text ( \" Total Number of Records: **\" + f \" { rows_count : , } \" + \"**\" ), dp . Text ( \" Total Number of Attributes: **\" + str ( columns_count ) + \"**\" ), dp . Text ( \" Number of Numerical Attributes : **\" + str ( numcols_count ) + \"**\" ), dp . Text ( \" Numerical Attributes Name : **\" + str ( numcols_name ) + \"**\" ), dp . Text ( \" Number of Categorical Attributes : **\" + str ( catcols_count ) + \"**\" ), dp . Text ( \" Categorical Attributes Name : **\" + str ( catcols_name ) + \"**\" ), ), ) else : l1 = dp . Text ( \"# \" ) if len ( data_analyzer_output ( master_path , avl_recs_SG , \"stats_generator\" )) > 0 : l2 = dp . Text ( \"### Statistics by Metric Type\" ) l3 = dp . Group ( dp . Select ( blocks = data_analyzer_output ( master_path , avl_recs_SG , \"stats_generator\" ), type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), ) else : l2 = dp . Text ( \"# \" ) l3 = dp . Text ( \"# \" ) if len ( all_charts_num_1_ ) == 0 and len ( all_charts_cat_1_ ) == 0 : l4 = 1 elif len ( all_charts_num_1_ ) == 0 and len ( all_charts_cat_1_ ) > 0 : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Select ( blocks = all_charts_cat_1_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) elif len ( all_charts_num_1_ ) > 0 and len ( all_charts_cat_1_ ) == 0 : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Select ( blocks = all_charts_num_1_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) else : l4 = ( dp . Text ( \"# \" ), dp . Text ( \"### Attribute Visualization\" ), dp . Group ( dp . Select ( blocks = [ dp . Group ( dp . Select ( blocks = all_charts_num_1_ , type = dp . SelectType . DROPDOWN , ), label = \"Numerical\" , ), dp . Group ( dp . Select ( blocks = all_charts_cat_1_ , type = dp . SelectType . DROPDOWN , ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ) ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), ) if l4 == 1 : report = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) else : report = dp . Group ( l1 , dp . Text ( \"# \" ), l2 , l3 , * l4 , dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Descriptive Statistics\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"descriptive_statistics.html\" , open = True ) return report def quality_check ( master_path , QC_tabs , avl_recs_QC , missing_recs_QC , all_charts_num_3_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Quality Checker Tab. Parameters ---------- master_path Path containing the input files. QC_tabs nullColumns_detection','IDness_detection','biasedness_detection','invalidEntries_detection','duplicate_detection','nullRows_detection','outlier_detection' avl_recs_QC Available files from the QC_tabs (Quality Checker tabs) missing_recs_QC Missing files from the QC_tabs (Quality Checker tabs) all_charts_num_3_ Numerical charts (outlier charts) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" c_ = [] r_ = [] if len ( missing_recs_QC ) == len ( QC_tabs ): return \"null_report\" else : row_wise = [ \"duplicate_detection\" , \"nullRows_detection\" ] col_wise = [ \"nullColumns_detection\" , \"IDness_detection\" , \"biasedness_detection\" , \"invalidEntries_detection\" , \"outlier_detection\" , ] row_wise_ = [ p for p in row_wise if p in avl_recs_QC ] col_wise_ = [ p for p in col_wise if p in avl_recs_QC ] len_row_wise = len ([ p for p in row_wise if p in avl_recs_QC ]) len_col_wise = len ([ p for p in col_wise if p in avl_recs_QC ]) if len_row_wise == 0 : c = data_analyzer_output ( master_path , col_wise_ , \"quality_checker\" ) for i in c : for j in i : if j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) > 1 : c_ . append ( dp . Select ( blocks = all_charts_num_3_ , type = dp . SelectType . DROPDOWN ) ) elif ( j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) == 0 ): c_ . append ( dp . Plot ( blank_chart )) else : c_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Group ( * c_ ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) elif len_col_wise == 0 : r = data_analyzer_output ( master_path , row_wise_ , \"quality_checker\" ) for i in r : for j in i : r_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Group ( * r_ ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) else : c = data_analyzer_output ( master_path , col_wise_ , \"quality_checker\" ) for i in c : for j in i : if j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) > 1 : c_ . append ( dp . Select ( blocks = all_charts_num_3_ , type = dp . SelectType . DROPDOWN ) ) elif ( j == \"outlier_charts_placeholder\" and len ( all_charts_num_3_ ) == 0 ): c_ . append ( dp . Plot ( blank_chart )) else : c_ . append ( j ) r = data_analyzer_output ( master_path , row_wise_ , \"quality_checker\" ) for i in r : for j in i : r_ . append ( j ) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section identifies the data quality issues at both row and column level.*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Select ( blocks = [ dp . Group ( dp . Text ( \"# \" ), dp . Group ( * c_ ), label = \"Column Level\" ), dp . Group ( dp . Text ( \"# \" ), dp . Group ( * r_ ), label = \"Row Level\" ), ], type = dp . SelectType . TABS , ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), label = \"Quality Check\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"quality_check.html\" , open = True ) return report def attribute_associations ( master_path , AE_tabs , avl_recs_AE , missing_recs_AE , label_col , all_charts_num_2_ , all_charts_cat_2_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Attribute Association Tab. Parameters ---------- master_path Path containing the input files. AE_tabs correlation_matrix','IV_calculation','IG_calculation','variable_clustering' avl_recs_AE Available files from the AE_tabs (Association Evaluator tabs) missing_recs_AE Missing files from the AE_tabs (Association Evaluator tabs) label_col label column all_charts_num_2_ Numerical charts (histogram) all collated in a list format supported as per datapane objects all_charts_cat_2_ Categorical charts (barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" if ( len ( missing_recs_AE ) == len ( AE_tabs )) and ( ( len ( all_charts_num_2_ ) + len ( all_charts_cat_2_ )) == 0 ): return \"null_report\" else : if len ( all_charts_num_2_ ) == 0 and len ( all_charts_cat_2_ ) == 0 : target_association_rep = dp . Text ( \"##\" ) else : if len ( all_charts_num_2_ ) > 0 and len ( all_charts_cat_2_ ) == 0 : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Text ( \"\"\" *Bivariate Distribution considering the event captured across different attribute splits (or categories)* \"\"\" ), dp . Select ( blocks = all_charts_num_2_ , type = dp . SelectType . DROPDOWN ), label = \"Numerical\" , ) elif len ( all_charts_num_2_ ) == 0 and len ( all_charts_cat_2_ ) > 0 : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Text ( \"\"\" *Bivariate Distribution considering the event captured across different attribute splits (or categories)* \"\"\" ), dp . Select ( blocks = all_charts_cat_2_ , type = dp . SelectType . DROPDOWN ), label = \"Categorical\" , ) else : target_association_rep = dp . Group ( dp . Text ( \"### Attribute to Target Association\" ), dp . Select ( blocks = [ dp . Group ( dp . Select ( blocks = all_charts_num_2_ , type = dp . SelectType . DROPDOWN , ), label = \"Numerical\" , ), dp . Group ( dp . Select ( blocks = all_charts_cat_2_ , type = dp . SelectType . DROPDOWN , ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), dp . Text ( \"\"\" *Event Rate is defined as % of event label (i.e. label 1) in a bin or a categorical value of an attribute.* \"\"\" ), dp . Text ( \"# \" ), ) if len ( missing_recs_AE ) == len ( AE_tabs ): report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"## \" ), target_association_rep , dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section analyzes the interaction between different attributes and/or the relationship between an attribute & the binary target variable.* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Association Matrix & Plot\" ), dp . Select ( blocks = data_analyzer_output ( master_path , avl_recs_AE , tab_name = \"association_evaluator\" ), type = dp . SelectType . DROPDOWN , ), dp . Text ( \"### \" ), dp . Text ( \"## \" ), target_association_rep , dp . Text ( \"## \" ), dp . Text ( \"## \" ), label = \"Attribute Associations\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"attribute_associations.html\" , open = True ) return report def data_drift_stability ( master_path , ds_ind , id_col , drift_threshold_model , all_drift_charts_ , print_report = False , ): \"\"\" This function helps to produce output specific to the Data Drift & Stability Tab. Parameters ---------- master_path Path containing the input files. ds_ind Drift stability indicator in list form. id_col ID column drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not all_drift_charts_ Charts (histogram/barplot) all collated in a list format supported as per datapane objects print_report Printing option flexibility. Default value is kept as False. Returns ------- DatapaneObject / Output[HTML] \"\"\" line_chart_list = [] if ds_ind [ 0 ] > 0 : fig_metric_drift = go . Figure () fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 0 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 1 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 0 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 1 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 3 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 1 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 2 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 5 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 2 ], ) ) fig_metric_drift . add_trace ( go . Scatter ( x = list ( drift_df [ drift_df . flagged . values == 1 ][ metric_drift [ 3 ]] . values ), y = list ( drift_df [ drift_df . flagged . values == 1 ] . attribute . values ), marker = dict ( color = global_theme [ 7 ], size = 14 ), mode = \"markers\" , name = metric_drift [ 3 ], ) ) fig_metric_drift . add_vrect ( x0 = 0 , x1 = drift_threshold_model , fillcolor = global_theme [ 7 ], opacity = 0.1 , layer = \"below\" , line_width = 1 , ), fig_metric_drift . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) fig_metric_drift . layout . plot_bgcolor = global_plot_bg_color fig_metric_drift . layout . paper_bgcolor = global_paper_bg_color fig_metric_drift . update_xaxes ( showline = True , linewidth = 2 , gridcolor = px . colors . sequential . Greys [ 1 ] ) fig_metric_drift . update_yaxes ( showline = True , linewidth = 2 , gridcolor = px . colors . sequential . Greys [ 2 ] ) # Drift Chart - 2 fig_gauge_drift = go . Figure ( go . Indicator ( domain = { \"x\" : [ 0 , 1 ], \"y\" : [ 0 , 1 ]}, value = drifted_feats , mode = \"gauge+number\" , title = { \"text\" : \"\" }, gauge = { \"axis\" : { \"range\" : [ None , len_feats ]}, \"bar\" : { \"color\" : px . colors . sequential . Reds [ 7 ]}, \"steps\" : [ { \"range\" : [ 0 , drifted_feats ], \"color\" : px . colors . sequential . Reds [ 8 ], }, { \"range\" : [ drifted_feats , len_feats ], \"color\" : px . colors . sequential . Greens [ 8 ], }, ], \"threshold\" : { \"line\" : { \"color\" : \"black\" , \"width\" : 3 }, \"thickness\" : 1 , \"value\" : len_feats , }, }, ) ) fig_gauge_drift . update_layout ( font = { \"color\" : \"black\" , \"family\" : \"Arial\" }) def drift_text_gen ( drifted_feats , len_feats ): \"\"\" Parameters ---------- drifted_feats count of attributes drifted len_feats count of attributes passed for analysis Returns ------- String \"\"\" if drifted_feats == 0 : text = \"\"\" *Drift barometer does not indicate any drift in the underlying data. Please refer to the metric values as displayed in the above table & comparison plot for better understanding* \"\"\" elif drifted_feats == 1 : text = ( \"*Drift barometer indicates that \" + str ( drifted_feats ) + \" out of \" + str ( len_feats ) + \" (\" + str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%) attributes has been drifted from its source behaviour.*\" ) elif drifted_feats > 1 : text = ( \"*Drift barometer indicates that \" + str ( drifted_feats ) + \" out of \" + str ( len_feats ) + \" (\" + str ( np . round (( 100 * drifted_feats / len_feats ), 2 )) + \"%) attributes have been drifted from its source behaviour.*\" ) else : text = \"\" return text else : pass if ds_ind [ 0 ] == 0 and ds_ind [ 1 ] == 0 : return \"null_report\" elif ds_ind [ 0 ] == 0 and ds_ind [ 1 ] > 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 1 and ds_ind [ 1 ] == 0 : if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 1 and ds_ind [ 1 ] >= 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) elif ds_ind [ 0 ] == 0 and ds_ind [ 1 ] >= 0.5 : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) else : for i in total_unstable_attr : if len ( total_unstable_attr ) > 1 : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) else : line_chart_list . append ( line_chart_gen_stability ( df1 = df_stability , df2 = df_si_ , col = i ) ) line_chart_list . append ( dp . Plot ( blank_chart , label = \" \" )) if len ( all_drift_charts_ ) > 0 : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Select ( blocks = all_drift_charts_ , type = dp . SelectType . DROPDOWN ), dp . Text ( \"\"\" *Source & Target datasets were compared to see the % deviation at decile level for numerical attributes and at individual category level for categorical attributes* \"\"\" ), dp . Text ( \"### \" ), dp . Text ( \"### \" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"\"\" *This section examines the dataset stability wrt the baseline dataset (via computing drift statistics) and/or wrt the historical datasets (via computing stability index).* \"\"\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Data Drift Analysis\" ), dp . DataTable ( drift_df ), dp . Text ( \"*An attribute is flagged as drifted if any drift metric is found to be above the threshold of \" + str ( drift_threshold_model ) + \".*\" ), dp . Text ( \"##\" ), dp . Text ( \"### Data Health\" ), dp . Group ( dp . Plot ( fig_metric_drift ), dp . Plot ( fig_gauge_drift ), columns = 2 ), dp . Group ( dp . Text ( \"*Representation of attributes across different computed Drift Metrics*\" ), dp . Text ( drift_text_gen ( drifted_feats , len_feats )), columns = 2 , ), dp . Text ( \"## \" ), dp . Text ( \"## \" ), dp . Text ( \"### Data Stability Analysis\" ), dp . DataTable ( df_si ), dp . Select ( blocks = line_chart_list , type = dp . SelectType . DROPDOWN ), dp . Group ( dp . Text ( \"**Stability Index Interpretation:**\" ), dp . Plot ( plot_index_stability ), ), label = \"Drift & Stability\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"data_drift_stability.html\" , open = True ) return report def plotSeasonalDecompose ( base_path , x_col , y_col , metric_col = \"median\" , title = \"Seasonal Decomposition\" ): \"\"\" This function helps to produce output specific to the Seasonal Decomposition of Time Series. Ideally it's expected to source a data containing atleast 2 cycles or 24 months as the most. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names metric_col Metric of aggregation. Options can be between \"Median\", \"Mean\", \"Min\", \"Max\" title \"Title Description\" Returns ------- Plot \"\"\" df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_daily.csv\" ) . dropna () df [ x_col ] = pd . to_datetime ( df [ x_col ], format = \"%Y-%m- %d %H:%M:%S. %f \" ) df = df . set_index ( x_col ) if len ([ x for x in df . columns if \"min\" in x ]) == 0 : # result = seasonal_decompose(df[metric_col],model=\"additive\") pass else : result = seasonal_decompose ( df [ metric_col ], model = \"additive\" , period = 12 ) fig = make_subplots ( rows = 2 , cols = 2 , subplot_titles = [ \"Observed\" , \"Trend\" , \"Seasonal\" , \"Residuals\" ], ) # fig = go.Figure() fig . add_trace ( go . Scatter ( x = df . index , y = result . observed , name = \"Observed\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 0 ]), ), row = 1 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . trend , name = \"Trend\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 2 ]), ), row = 1 , col = 2 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . seasonal , name = \"Seasonal\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 4 ]), ), row = 2 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = result . resid , name = \"Residuals\" , mode = \"lines+markers\" , line = dict ( color = global_theme [ 6 ]), ), row = 2 , col = 2 , ) # fig.add_trace(go.Scatter(x=df.index, y=result.observed, name =\"Observed\", mode='lines+markers',line=dict(color=global_theme[0]))) # fig.add_trace(go.Scatter(x=df.index, y=result.trend, name =\"Trend\", mode='lines+markers',line=dict(color=global_theme[2]))) # fig.add_trace(go.Scatter(x=df.index, y=result.seasonal, name =\"Seasonal\", mode='lines+markers',line=dict(color=global_theme[4]))) # fig.add_trace(go.Scatter(x=df.index, y=result.resid, name =\"Residuals\", mode='lines+markers',line=dict(color=global_theme[6]))) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( autosize = True , width = 2000 , height = 800 ) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def gen_time_series_plots ( base_path , x_col , y_col , time_cat ): \"\"\" This function helps to produce Time Series Plots by sourcing the aggregated data as Daily/Hourly/Weekly level. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names time_cat Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Plot \"\"\" df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_\" + time_cat + \".csv\" ) . dropna () if len ([ x for x in df . columns if \"min\" in x ]) == 0 : if time_cat == \"daily\" : # x_col = x_col + \"_ts\" fig = px . line ( df , x = x_col , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) fig . update_layout ( xaxis = dict ( rangeselector = dict ( buttons = list ( [ dict ( count = 1 , label = \"1m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 3 , label = \"3m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 6 , label = \"6m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 1 , label = \"YTD\" , step = \"year\" , stepmode = \"todate\" ), dict ( count = 1 , label = \"1y\" , step = \"year\" , stepmode = \"backward\" , ), dict ( step = \"all\" ), ] ) ), rangeslider = dict ( visible = True ), type = \"date\" , ) ) elif time_cat == \"weekly\" : fig = px . bar ( df , x = \"dow\" , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) # fig.update_layout(barmode='stack') elif time_cat == \"hourly\" : fig = px . bar ( df , x = \"daypart_cat\" , y = \"count\" , color = y_col , color_discrete_sequence = global_theme , ) # fig.update_layout(barmode='stack') else : pass else : if time_cat == \"daily\" : # x_col = x_col + \"_ts\" f1 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"min\" ]), name = \"Min\" , line = dict ( color = global_theme [ 6 ]), ) f2 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"max\" ]), name = \"Max\" , line = dict ( color = global_theme [ 4 ]), ) f3 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"mean\" ]), name = \"Mean\" , line = dict ( color = global_theme [ 2 ]), ) f4 = go . Scatter ( x = list ( df [ x_col ]), y = list ( df [ \"median\" ]), name = \"Median\" , line = dict ( color = global_theme [ 0 ]), ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( xaxis = dict ( rangeselector = dict ( buttons = list ( [ dict ( count = 1 , label = \"1m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 3 , label = \"3m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 6 , label = \"6m\" , step = \"month\" , stepmode = \"backward\" , ), dict ( count = 1 , label = \"YTD\" , step = \"year\" , stepmode = \"todate\" ), dict ( count = 1 , label = \"1y\" , step = \"year\" , stepmode = \"backward\" , ), dict ( step = \"all\" ), ] ) ), rangeslider = dict ( visible = True ), type = \"date\" , ) ) elif time_cat == \"weekly\" : f1 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"min\" ]), marker_color = global_theme [ 6 ], name = \"Min\" , ) f2 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"max\" ]), marker_color = global_theme [ 4 ], name = \"Max\" , ) f3 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"mean\" ]), marker_color = global_theme [ 2 ], name = \"Mean\" , ) f4 = go . Bar ( x = list ( df [ \"dow\" ]), y = list ( df [ \"median\" ]), marker_color = global_theme [ 0 ], name = \"Median\" , ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( barmode = \"group\" ) elif time_cat == \"hourly\" : f1 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"min\" ]), marker_color = global_theme [ 6 ], name = \"Min\" , ) f2 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"max\" ]), marker_color = global_theme [ 4 ], name = \"Max\" , ) f3 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"mean\" ]), marker_color = global_theme [ 2 ], name = \"Mean\" , ) f4 = go . Bar ( x = list ( df [ \"daypart_cat\" ]), y = list ( df [ \"median\" ]), marker_color = global_theme [ 0 ], name = \"Median\" , ) fig = go . Figure ( data = [ f1 , f2 , f3 , f4 ]) fig . update_layout ( barmode = \"group\" ) else : pass fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def list_ts_remove_append ( l , opt ): \"\"\" This function helps to remove or append \"_ts\" from any list. Parameters ---------- l List containing column name opt Option to choose between 1 & Others to enable the functionality of removing or appending \"_ts\" within the elements of a list Returns ------- List \"\"\" ll = [] if opt == 1 : for i in l : if i [ - 3 :] == \"_ts\" : ll . append ( i [ 0 : - 3 :]) else : ll . append ( i ) return ll else : for i in l : if i [ - 3 :] == \"_ts\" : ll . append ( i ) else : ll . append ( i + \"_ts\" ) return ll def ts_viz_1_1 ( base_path , x_col , y_col , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- Plot \"\"\" ts_fig = gen_time_series_plots ( base_path , x_col , y_col , output_type ) return ts_fig def ts_viz_1_2 ( base_path , ts_col , col_list , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical / Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( col_list ) > 1 : bl . append ( dp . Group ( ts_viz_1_1 ( base_path , ts_col , i , output_type ), label = i )) else : bl . append ( dp . Group ( ts_viz_1_1 ( base_path , ts_col , i , output_type ), label = i )) bl . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_1_3 ( base_path , ts_col , num_cols , cat_cols , output_type ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names cat_cols Categorical column names output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject \"\"\" ts_v = [] # print(num_cols) # print(cat_cols) if len ( num_cols ) == 0 : for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = i ) ) else : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = i ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) elif len ( cat_cols ) == 0 : for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = i ) ) else : ts_v . append ( dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = i ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) elif ( len ( num_cols ) >= 1 ) & ( len ( cat_cols ) >= 1 ): for i in ts_col : if len ( ts_col ) > 1 : ts_v . append ( dp . Group ( dp . Select ( blocks = [ dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = \"Numerical\" , ), dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), label = i , ) ) else : ts_v . append ( dp . Group ( dp . Select ( blocks = [ dp . Group ( ts_viz_1_2 ( base_path , i , num_cols , output_type ), label = \"Numerical\" , ), dp . Group ( ts_viz_1_2 ( base_path , i , cat_cols , output_type ), label = \"Categorical\" , ), ], type = dp . SelectType . TABS , ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_viz_2_1 ( base_path , x_col , y_col ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns ------- DatapaneObject \"\"\" ts_fig = [] for i in [ \"mean\" , \"median\" , \"min\" , \"max\" ]: ts_fig . append ( dp . Plot ( plotSeasonalDecompose ( base_path , x_col , y_col , metric_col = i ), label = i . title (), ) ) return dp . Select ( blocks = ts_fig , type = dp . SelectType . TABS ) def ts_viz_2_2 ( base_path , ts_col , col_list ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( col_list ) > 1 : bl . append ( dp . Group ( ts_viz_2_1 ( base_path , ts_col , i ), label = i )) else : bl . append ( dp . Group ( ts_viz_2_1 ( base_path , ts_col , i ), label = i )) bl . append ( dp . Group ( dp . Plot ( blank_chart , label = \" \" ), label = \" \" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_2_3 ( base_path , ts_col , num_cols ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns ------- DatapaneObject \"\"\" ts_v = [] if len ( ts_col ) > 1 : for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 24 : ts_v . append ( dp . Group ( ts_viz_2_2 ( base_path , i , num_cols ), label = i )) else : ts_v . append ( dp . Group ( dp . Text ( \"The plots couldn't be displayed as x must have 2 complete cycles requires 24 observations. x only has \" + str ( f ) + \" observation(s)\" ), label = i , ) ) else : for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 24 : ts_v . append ( dp . Group ( ts_viz_2_2 ( base_path , i , num_cols ), label = i )) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) else : ts_v . append ( dp . Group ( dp . Text ( \"The plots couldn't be displayed as x must have 2 complete cycles requires 24 observations. x only has \" + str ( f ) + \" observation(s)\" ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_landscape ( base_path , ts_cols , id_col ): \"\"\" This function helps to produce a basic landscaping view of the data by picking up the base path for reading the aggregated data and specified by the timestamp / date column & the ID column. Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name id_col ID Column Returns ------- DatapaneObject \"\"\" if ts_cols is None : return dp . Text ( \"#\" ) else : df_stats_ts = [] for i in ts_cols : if len ( ts_cols ) > 1 : df_stats_ts . append ( dp . Group ( dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*ID considered here is : \" + str ( id_col ) + \"*\" ), dp . Text ( \"# \" ), dp . Text ( \"#### Consistency Analysis Of Dates\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_1.csv\" ) . set_index ( \"attribute\" ) . T , label = i , ), ), dp . Group ( dp . Text ( \"*The Percentile distribution across different bins of ID-Date / Date-ID combination should be in a considerable range to determine the regularity of Time series. In an ideal scenario the proportion of dates within each ID should be same. Also, the count of IDs across unique dates should be consistent for a balanced distribution*\" ), dp . Text ( \"# \" ), dp . Text ( \"#### Vital Statistics\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . T . rename ( columns = { 0 : \"\" }), label = i , ), ), label = i , ) ) else : df_stats_ts . append ( dp . Group ( dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*ID considered here is : \" + str ( id_col ) + \"*\" ), dp . Text ( \"#### Consistency Analysis Of Dates\" ), dp . Text ( \"# \" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_1.csv\" ) . set_index ( \"attribute\" ) . T , label = i , ), ), dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"#### Vital Statistics\" ), dp . DataTable ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . T . rename ( columns = { 0 : \"\" }), label = i , ), ), label = i , ) ) df_stats_ts . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Group ( dp . Text ( \"### Time Stamp Data Diagnosis\" ), dp . Select ( blocks = df_stats_ts , type = dp . SelectType . DROPDOWN ), ) def lambda_cat ( val ): \"\"\" Parameters ---------- val Value of Box Cox Test which translates into the transformation to be applied. Returns ------- String \"\"\" if val < - 1 : return \"Reciprocal Square Transform\" elif val >= - 1 and val < - 0.5 : return \"Reciprocal Transform\" elif val >= - 0.5 and val < 0 : return \"Receiprocal Square Root Transform\" elif val >= 0 and val < 0.5 : return \"Log Transform\" elif val >= 0.5 and val < 1 : return \"Square Root Transform\" elif val >= 1 and val < 2 : return \"No Transform\" elif val >= 2 : return \"Square Transform\" else : return \"ValueOutOfRange\" def ts_viz_3_1 ( base_path , x_col , y_col ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. x_col Timestamp / date column name y_col Numerical column names Returns ------- DatapaneObject \"\"\" ts_fig = [] df = pd . read_csv ( ends_with ( base_path ) + x_col + \"_\" + y_col + \"_daily.csv\" ) . dropna () df [ x_col ] = pd . to_datetime ( df [ x_col ], format = \"%Y-%m- %d %H:%M:%S. %f \" ) df = df . set_index ( x_col ) for metric_col in [ \"mean\" , \"median\" , \"min\" , \"max\" ]: try : adf_test = ( round ( adfuller ( df [ metric_col ])[ 0 ], 3 ), round ( adfuller ( df [ metric_col ])[ 1 ], 3 ), ) if adf_test [ 1 ] < 0.05 : adf_flag = True else : adf_flag = False except : adf_test = ( \"nan\" , \"nan\" ) adf_flag = False try : kpss_test = ( round ( kpss ( df [ metric_col ], regression = \"ct\" )[ 0 ], 3 ), round ( kpss ( df [ metric_col ], regression = \"ct\" )[ 1 ], 3 ), ) if kpss_test [ 1 ] < 0.05 : kpss_flag = True else : kpss_flag = False except : kpss_test = ( \"nan\" , \"nan\" ) kpss_flag = False # df[metric_col] = df[metric_col].apply(lambda x: boxcox1p(x,0.25)) # lambda_box_cox = round(boxcox(df[metric_col])[1],5) fit = PowerTransformer ( method = \"yeo-johnson\" ) try : lambda_box_cox = round ( fit . fit ( np . array ( df [ metric_col ]) . reshape ( - 1 , 1 )) . lambdas_ [ 0 ], 3 ) cnt = 0 except : cnt = 1 if cnt == 0 : # df[metric_col+\"_transformed\"] = boxcox(df[metric_col],lmbda=lambda_box_cox) df [ metric_col + \"_transformed\" ] = fit . transform ( np . array ( df [ metric_col ]) . reshape ( - 1 , 1 ) ) fig = make_subplots ( rows = 1 , cols = 2 , subplot_titles = [ \"Pre-Transformation\" , \"Post-Transformation\" ], ) fig . add_trace ( go . Scatter ( x = df . index , y = df [ metric_col ], mode = \"lines+markers\" , name = metric_col , line = dict ( color = global_theme [ 1 ]), ), row = 1 , col = 1 , ) fig . add_trace ( go . Scatter ( x = df . index , y = df [ metric_col + \"_transformed\" ], mode = \"lines+markers\" , name = metric_col + \"_transformed\" , line = dict ( color = global_theme [ 7 ]), ), row = 1 , col = 2 , ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_xaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_yaxes ( gridcolor = px . colors . sequential . Greys [ 1 ]) fig . update_layout ( autosize = True , width = 2000 , height = 400 ) fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) ts_fig . append ( dp . Group ( dp . Group ( dp . BigNumber ( heading = \"ADF Test Statistic\" , value = adf_test [ 0 ], change = adf_test [ 1 ], is_upward_change = adf_flag , ), dp . BigNumber ( heading = \"KPSS Test Statistic\" , value = kpss_test [ 0 ], change = kpss_test [ 1 ], is_upward_change = kpss_flag , ), dp . BigNumber ( heading = \"Box-Cox Transformation\" , value = lambda_box_cox , change = str ( lambda_cat ( lambda_box_cox )), is_upward_change = True , ), columns = 3 , ), dp . Text ( \"#### Transformation View\" ), dp . Text ( \"Below Transformation is basis the inferencing from the Box Cox Transformation. The Lambda value of \" + str ( lambda_box_cox ) + \" indicates a \" + str ( lambda_cat ( lambda_box_cox )) + \". A Pre-Post Transformation Visualization is done for better clarity. \" ), dp . Plot ( fig ), dp . Text ( \"**Guidelines :** \" ), dp . Text ( \"**ADF** : *The more negative the statistic, the more likely we are to reject the null hypothesis. If the p-value is less than the significance level of 0.05, we can reject the null hypothesis and take that the series is stationary*\" ), dp . Text ( \"**KPSS** : *If the p-value is high, we cannot reject the null hypothesis. So the series is stationary.*\" ), label = metric_col . title (), ) ) else : ts_fig . append ( dp . Group ( dp . Group ( dp . BigNumber ( heading = \"ADF Test Statistic\" , value = adf_test [ 0 ], change = adf_test [ 1 ], is_upward_change = adf_flag , ), dp . BigNumber ( heading = \"KPSS Test Statistic\" , value = kpss_test [ 0 ], change = kpss_test [ 1 ], is_upward_change = kpss_flag , ), dp . BigNumber ( heading = \"Box-Cox Transformation\" , value = \"ValueOutOfRange\" , change = \"ValueOutOfRange\" , is_upward_change = True , ), columns = 3 , ), dp . Text ( \"**Guidelines :** \" ), dp . Text ( \"**ADF** : *The more negative the statistic, the more likely we are to reject the null hypothesis. If the p-value is less than the significance level of 0.05, we can reject the null hypothesis and take that the series is stationary*\" ), dp . Text ( \"**KPSS** : *If the p-value is high, we cannot reject the null hypothesis. So the series is stationary.*\" ), label = metric_col . title (), ) ) return dp . Select ( blocks = ts_fig , type = dp . SelectType . TABS ) def ts_viz_3_2 ( base_path , ts_col , col_list ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name col_list Numerical column names Returns ------- DatapaneObject \"\"\" bl = [] for i in col_list : if len ( num_cols ) > 1 : bl . append ( dp . Group ( ts_viz_3_1 ( base_path , ts_col , i ), label = i )) else : bl . append ( dp . Group ( ts_viz_3_1 ( base_path , ts_col , i ), label = i )) bl . append ( dp . Group ( dp . Plot ( blank_chart , label = \" \" ), label = \" \" )) return dp . Select ( blocks = bl , type = dp . SelectType . DROPDOWN ) def ts_viz_3_3 ( base_path , ts_col , num_cols ): \"\"\" Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. ts_col Timestamp / date column name num_cols Numerical column names Returns ------- DatapaneObject \"\"\" # f = list(pd.read_csv(ends_with(base_path) + \"stats_\" + i + \"_2.csv\").count_unique_dates.values)[0] # if f >= 6: if len ( ts_col ) > 1 : ts_v = [] for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 6 : ts_v . append ( dp . Group ( ts_viz_3_2 ( base_path , i , num_cols ), label = i )) else : ts_v . append ( dp . Group ( dp . Text ( \"The data contains insufficient data points for the desired transformation analysis. Please ensure the number of unique dates is sufficient.\" ), label = i , ) ) else : ts_v = [] for i in ts_col : f = list ( pd . read_csv ( ends_with ( base_path ) + \"stats_\" + i + \"_2.csv\" ) . count_unique_dates . values )[ 0 ] if f >= 6 : ts_v . append ( dp . Group ( ts_viz_3_2 ( base_path , i , num_cols ), label = i )) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) else : ts_v . append ( dp . Group ( dp . Text ( \"The data contains insufficient data points for the desired transformation analysis. Please ensure the number of unique dates is sufficient.\" ), label = i , ) ) ts_v . append ( dp . Plot ( blank_chart , label = \"_\" )) return dp . Select ( blocks = ts_v , type = dp . SelectType . DROPDOWN ) def ts_stats ( base_path ): \"\"\" This function helps to read the base data containing desired input and produces output specific to the `ts_cols_stats.csv` file Parameters ---------- base_path Base path which is the same as Master path where the aggregated data resides. Returns ------- List \"\"\" df = pd . read_csv ( base_path + \"ts_cols_stats.csv\" ) all_stats = [] for i in range ( 0 , 7 ): try : all_stats . append ( df [ df . index . values == i ] . values [ 0 ][ 0 ] . split ( \",\" )) except : all_stats . append ([]) c0 = pd . DataFrame ( all_stats [ 0 ], columns = [ \"attributes\" ]) c1 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 1 ], 1 ), columns = [ \"attributes\" ]) c1 [ \"Analyzed Attributes\" ] = \"\u2714\" c2 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 2 ], 1 ), columns = [ \"attributes\" ]) c2 [ \"Attributes Identified\" ] = \"\u2714\" c3 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 3 ], 1 ), columns = [ \"attributes\" ]) c3 [ \"Attributes Pre-Existed\" ] = \"\u2714\" c4 = pd . DataFrame ( list_ts_remove_append ( all_stats [ 4 ], 1 ), columns = [ \"attributes\" ]) c4 [ \"Overall TimeStamp Attributes\" ] = \"\u2714\" c5 = list_ts_remove_append ( all_stats [ 5 ], 1 ) c6 = list_ts_remove_append ( all_stats [ 6 ], 1 ) return c0 , c1 , c2 , c3 , c4 , c5 , c6 def ts_viz_generate ( master_path , id_col , print_report = False , output_type = None ): \"\"\" This function helps to produce the output in the nested / recursive function supported by datapane. Eventually this is populated at the final report. Parameters ---------- master_path Master path where the aggregated data resides. id_col ID Column print_report Option to specify whether the Report needs to be saved or not. True / False can be used to specify the needful. output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" Returns ------- DatapaneObject / Output[HTML] \"\"\" master_path = ends_with ( master_path ) try : c0 , c1 , c2 , c3 , c4 , c5 , c6 = ts_stats ( master_path ) except : return \"null_report\" stats_df = ( c0 . merge ( c1 , on = \"attributes\" , how = \"left\" ) . merge ( c2 , on = \"attributes\" , how = \"left\" ) . merge ( c3 , on = \"attributes\" , how = \"left\" ) . merge ( c4 , on = \"attributes\" , how = \"left\" ) . fillna ( \"\u2718\" ) ) global num_cols global cat_cols num_cols , cat_cols = c5 , c6 final_ts_cols = list ( ts_stats ( master_path )[ 4 ] . attributes . values ) if output_type == \"daily\" : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Basic Landscaping\" ), dp . Text ( \"Out of **\" + str ( len ( list ( ts_stats ( master_path )[ 1 ] . attributes . values ))) + \"** potential attributes in the data, the module could locate **\" + str ( len ( final_ts_cols )) + \"** attributes as Timestamp\" ), dp . DataTable ( stats_df ), ts_landscape ( master_path , final_ts_cols , id_col ), dp . Text ( \"*Lower the **CoV** (Coefficient Of Variation), Higher the Consistency between the consecutive dates. Similarly the Mean & Variance should be consistent over time*\" ), dp . Text ( \"### Visualization across the Shortlisted Timestamp Attributes\" ), ts_viz_1_3 ( master_path , final_ts_cols , num_cols , cat_cols , output_type ), dp . Text ( \"### Decomposed View\" ), ts_viz_2_3 ( master_path , final_ts_cols , num_cols ), dp . Text ( \"### Stationarity & Transformations\" ), ts_viz_3_3 ( master_path , final_ts_cols , num_cols ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), label = \"Time Series Analyzer\" , ) elif output_type is None : report = \"null_report\" else : report = dp . Group ( dp . Text ( \"# \" ), dp . Text ( \"*This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications*\" ), dp . Text ( \"# \" ), dp . Text ( \"# \" ), dp . Text ( \"### Basic Landscaping\" ), dp . Text ( \"Out of **\" + str ( len ( list ( ts_stats ( master_path )[ 1 ] . attributes . values ))) + \"** potential attributes in the data, the module could locate **\" + str ( len ( final_ts_cols )) + \"** attributes as Timestamp\" ), dp . DataTable ( stats_df ), ts_landscape ( master_path , final_ts_cols , id_col ), dp . Text ( \"*Lower the **CoV** (Coefficient Of Variation), Higher the Consistency between the consecutive dates. Similarly the Mean & Variance should be consistent over time*\" ), dp . Text ( \"### Visualization across the Shortlisted Timestamp Attributes\" ), ts_viz_1_3 ( master_path , final_ts_cols , num_cols , cat_cols , output_type ), dp . Text ( \"#\" ), dp . Text ( \"#\" ), label = \"Time Series Analyzer\" , ) if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"time_series_analyzer.html\" , open = True ) return report def overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list ): \"\"\" This function helps to produce a basic summary of all the geospatial fields auto-detected in a dictionary along with the length of lat-lon & geohash cols identified. Parameters ---------- lat_col_list List of latitude columns identified long_col_list List of longitude columns identified geohash_col_list List of geohash columns identified Returns ------- Dictionary,Integer,Integer \"\"\" d = {} ll = [] col_list = [ \"Latitude Col\" , \"Longitude Col\" , \"Geohash Col\" ] # for idx,i in enumerate([lat_col_list,long_col_list,geohash_col_list,polygon_col_list]): for idx , i in enumerate ([ lat_col_list , long_col_list , geohash_col_list ]): if i is None : ll = [] elif i is not None : ll = [] for j in i : ll . append ( j ) d [ col_list [ idx ]] = \",\" . join ( ll ) l1 = len ( lat_col_list ) l2 = len ( geohash_col_list ) return d , l1 , l2 def loc_field_stats ( lat_col_list , long_col_list , geohash_col_list , max_records ): \"\"\" This function helps to produce a basic summary of all the geospatial fields auto-detected Parameters ---------- lat_col_list List of latitude columns identified long_col_list List of longitude columns identified geohash_col_list List of geohash columns identified max_records Maximum geospatial points analyzed Returns ------- DatapaneObject \"\"\" loc_cnt = ( overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 1 ] * 2 ) + ( overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 2 ]) loc_var_stats = overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 0 ] x = \"#\" t0 = dp . Text ( x ) t1 = dp . Text ( \"There are **\" + str ( loc_cnt ) + \"** location fields captured in the data containing \" + str ( overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 1 ]) + \" pair(s) of **Lat,Long** & \" + str ( overall_stats_gen ( lat_col_list , long_col_list , geohash_col_list )[ 2 ]) + \" **Geohash** field(s)\" ) t2 = dp . DataTable ( pd . DataFrame ( pd . Series ( loc_var_stats , index = loc_var_stats . keys ())) . rename ( columns = { 0 : \"\" } ) ) return dp . Group ( t0 , t1 , t2 ) def read_stats_ll_geo ( lat_col , long_col , geohash_col , master_path , top_geo_records ): \"\"\" This function helps to read all the basis stats output for the lat-lon & geohash field produced from the analyzer module Parameters ---------- lat_col Latitude column identified long_col Longitude column identified geohash_col Geohash column identified master_path Master path where the aggregated data resides top_geo_records Top geospatial records displayed Returns ------- DatapaneObject \"\"\" try : len_lat_col = len ( lat_col ) except : len_lat_col = 0 try : len_geohash_col = len ( geohash_col ) except : len_geohash_col = 0 ll_stats , geohash_stats = [], [] if len_lat_col > 0 : if len_lat_col == 1 : for idx , i in enumerate ( lat_col ): ll_stats . append ( dp . Group ( dp . Select ( blocks = [ dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Overall_Summary_1_\" + lat_col [ idx ] + \"_\" + long_col [ idx ] + \".csv\" ), label = \"Overall Summary\" , ), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Top_\" + str ( top_geo_records ) + \"_Lat_Long_1_\" + lat_col [ idx ] + \"_\" + long_col [ idx ] + \".csv\" ), label = \"Top \" + str ( top_geo_records ) + \" Lat Long\" , ), ], type = dp . SelectType . TABS , ), label = lat_col [ idx ] + \"_\" + long_col [ idx ], ) ) ll_stats . append ( dp . Group ( dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), label = \" \" , ) ) elif len_lat_col > 1 : for idx , i in enumerate ( lat_col ): ll_stats . append ( dp . Group ( dp . Select ( blocks = [ dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Overall_Summary_1_\" + lat_col [ idx ] + \"_\" + long_col [ idx ] + \".csv\" ), label = \"Overall Summary\" , ), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Top_\" + str ( top_geo_records ) + \"_Lat_Long_1_\" + lat_col [ idx ] + \"_\" + long_col [ idx ] + \".csv\" ), label = \"Top \" + str ( top_geo_records ) + \" Lat Long\" , ), ], type = dp . SelectType . TABS , ), label = lat_col [ idx ] + \"_\" + long_col [ idx ], ) ) ll_stats = dp . Select ( blocks = ll_stats , type = dp . SelectType . DROPDOWN ) if len_geohash_col > 0 : if len_geohash_col == 1 : for idx , i in enumerate ( geohash_col ): geohash_stats . append ( dp . Group ( dp . Select ( blocks = [ dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Overall_Summary_2_\" + geohash_col [ idx ] + \".csv\" ), label = \"Overall Summary\" , ), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Top_\" + str ( top_geo_records ) + \"_Geohash_Distribution_2_\" + geohash_col [ idx ] + \".csv\" ), label = \"Top \" + str ( top_geo_records ) + \" Geohash Distribution\" , ), ], type = dp . SelectType . TABS , ), label = geohash_col [ idx ], ) ) geohash_stats . append ( dp . Group ( dp . DataTable ( pd . DataFrame ( columns = [ \" \" ], index = range ( 1 )), label = \" \" ), label = \" \" , ) ) elif len_geohash_col > 1 : for idx , i in enumerate ( geohash_col ): geohash_stats . append ( dp . Group ( dp . Select ( blocks = [ dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Overall_Summary_2_\" + geohash_col [ idx ] + \".csv\" ), label = \"Overall Summary\" , ), dp . DataTable ( pd . read_csv ( ends_with ( master_path ) + \"Top_\" + str ( top_geo_records ) + \"_Geohash_Distribution_2_\" + geohash_col [ idx ] + \".csv\" ), label = \"Top \" + str ( top_geo_records ) + \" Geohash Distribution\" , ), ], type = dp . SelectType . TABS , ), label = geohash_col [ idx ], ) ) geohash_stats = dp . Select ( blocks = geohash_stats , type = dp . SelectType . DROPDOWN ) if ( len_lat_col + len_geohash_col ) == 1 : if len_lat_col == 0 : return geohash_stats else : return ll_stats elif ( len_lat_col + len_geohash_col ) > 1 : if ( len_lat_col > 1 ) and ( len_geohash_col == 0 ): return ll_stats elif ( len_lat_col == 0 ) and ( len_geohash_col > 1 ): return geohash_stats elif ( len_lat_col >= 1 ) and ( len_geohash_col >= 1 ): return dp . Select ( blocks = [ dp . Group ( ll_stats , label = \"Lat-Long-Stats\" ), dp . Group ( geohash_stats , label = \"Geohash-Stats\" ), ], type = dp . SelectType . TABS , ) def read_cluster_stats_ll_geo ( lat_col , long_col , geohash_col , master_path ): \"\"\" This function helps to read all the cluster analysis output for the lat-lon & geohash field produced from the analyzer module Parameters ---------- lat_col Latitude column identified long_col Longitude column identified geohash_col Geohash column identified master_path Master path where the aggregated data resides Returns ------- DatapaneObject \"\"\" ll_col , plot_ll , all_geo_cols = [], [], [] try : len_lat_col = len ( lat_col ) except : len_lat_col = 0 try : len_geohash_col = len ( geohash_col ) except : len_geohash_col = 0 if ( len_lat_col > 0 ) or ( len_geohash_col > 0 ): try : for idx , i in enumerate ( lat_col ): ll_col . append ( lat_col [ idx ] + \"_\" + long_col [ idx ]) except : pass all_geo_cols = ll_col + geohash_col if len ( all_geo_cols ) > 0 : for i in all_geo_cols : if len ( all_geo_cols ) == 1 : p1 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_1_elbow_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_1_silhoutte_\" + i ) ) ) ), label = \"Cluster Identification\" , ) p2 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_2_kmeans_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_2_dbscan_\" + i ) ) ) ), label = \"Cluster Distribution\" , ) p3 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_3_kmeans_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_3_dbscan_\" + i ) ) ) ), label = \"Visualization\" , ) p4 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_1_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_2_\" + i ) ) ) ), label = \"Outlier Points\" , ) plot_ll . append ( dp . Group ( dp . Select ( blocks = [ p1 , p2 , p3 , p4 ], type = dp . SelectType . TABS ), label = i , ) ) plot_ll . append ( dp . Plot ( blank_chart , label = \" \" )) elif len ( all_geo_cols ) > 1 : p1 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_1_elbow_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_1_silhoutte_\" + i ) ) ) ), label = \"Cluster Identification\" , ) p2 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_2_kmeans_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_2_dbscan_\" + i ) ) ) ), label = \"Cluster Distribution\" , ) p3 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_3_kmeans_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_3_dbscan_\" + i ) ) ) ), label = \"Visualization\" , ) p4 = dp . Group ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_1_\" + i ) ) ) ), dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + \"cluster_plot_4_dbscan_2_\" + i ) ) ) ), label = \"Outlier Points\" , ) plot_ll . append ( dp . Group ( dp . Select ( blocks = [ p1 , p2 , p3 , p4 ], type = dp . SelectType . TABS ), label = i , ) ) return dp . Select ( blocks = plot_ll , type = dp . SelectType . DROPDOWN ) def read_loc_charts ( master_path ): \"\"\" This function helps to read all the geospatial charts from the master path and populate in the report Parameters ---------- master_path Master path where the aggregated data resides Returns ------- DatapaneObject \"\"\" ll_charts_nm = [ x for x in os . listdir ( master_path ) if \"loc_charts_ll\" in x ] geo_charts_nm = [ x for x in os . listdir ( master_path ) if \"loc_charts_gh\" in x ] ll_col_charts , geo_col_charts = [], [] if len ( ll_charts_nm ) > 0 : if len ( ll_charts_nm ) == 1 : for i1 in ll_charts_nm : col_name = i1 . replace ( \"loc_charts_ll_\" , \"\" ) ll_col_charts . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i1 ))), label = col_name , ) ) ll_col_charts . append ( dp . Plot ( blank_chart , label = \" \" )) elif len ( ll_charts_nm ) > 1 : for i1 in ll_charts_nm : col_name = i1 . replace ( \"loc_charts_ll_\" , \"\" ) ll_col_charts . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i1 ))), label = col_name , ) ) ll_col_charts = dp . Select ( blocks = ll_col_charts , type = dp . SelectType . DROPDOWN ) if len ( geo_charts_nm ) > 0 : if len ( geo_charts_nm ) == 1 : for i2 in geo_charts_nm : col_name = i2 . replace ( \"loc_charts_gh_\" , \"\" ) geo_col_charts . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i2 ))), label = col_name , ) ) geo_col_charts . append ( dp . Plot ( blank_chart , label = \" \" )) elif len ( geo_charts_nm ) > 1 : for i2 in geo_charts_nm : col_name = i2 . replace ( \"loc_charts_gh_\" , \"\" ) geo_col_charts . append ( dp . Plot ( go . Figure ( json . load ( open ( ends_with ( master_path ) + i2 ))), label = col_name , ) ) geo_col_charts = dp . Select ( blocks = geo_col_charts , type = dp . SelectType . DROPDOWN ) if ( len ( ll_charts_nm ) > 0 ) and ( len ( geo_charts_nm ) == 0 ): return ll_col_charts elif ( len ( ll_charts_nm ) == 0 ) and ( len ( geo_charts_nm ) > 0 ): return geo_col_charts elif ( len ( ll_charts_nm ) > 0 ) and ( len ( geo_charts_nm ) > 0 ): return dp . Select ( blocks = [ dp . Group ( ll_col_charts , label = \"Lat-Long-Plot\" ), dp . Group ( geo_col_charts , label = \"Geohash-Plot\" ), ], type = dp . SelectType . TABS , ) def loc_report_gen ( lat_cols , long_cols , geohash_cols , master_path , max_records , top_geo_records , print_report = False , ): \"\"\" This function helps to read all the lat,long & geohash columns as input alongside few input parameters to produce the geospatial analysis report tab Parameters ---------- lat_cols Latitude columns identified in the data long_cols Longitude columns identified in the data geohash_cols Geohash columns identified in the data master_path Master path where the aggregated data resides max_records Maximum geospatial points analyzed top_geo_records Top geospatial records displayed print_report Option to specify whether the Report needs to be saved or not. True / False can be used to specify the needful Returns ------- DatapaneObject \"\"\" _ = dp . Text ( \"#\" ) dp1 = dp . Group ( _ , dp . Text ( \"*This section summarizes the information about the geospatial features identified in the data and their landscaping view*\" ), loc_field_stats ( lat_cols , long_cols , geohash_cols , max_records ), ) if ( len ( lat_cols ) + len ( geohash_cols )) > 0 : dp2 = dp . Group ( _ , dp . Text ( \"## Descriptive Analysis by Location Attributes\" ), read_stats_ll_geo ( lat_cols , long_cols , geohash_cols , master_path , top_geo_records ), _ , ) dp3 = dp . Group ( _ , dp . Text ( \"## Clustering Geospatial Field\" ), read_cluster_stats_ll_geo ( lat_cols , long_cols , geohash_cols , master_path ), _ , ) dp4 = dp . Group ( _ , dp . Text ( \"## Visualization by Geospatial Fields\" ), read_loc_charts ( master_path ), _ , ) report = dp . Group ( dp1 , dp2 , dp3 , dp4 , label = \"Geospatial Analyzer\" ) elif ( len ( lat_cols ) + len ( geohash_cols )) == 0 : report = \"null_report\" if print_report : dp . Report ( default_template [ 0 ], default_template [ 1 ], report ) . save ( ends_with ( master_path ) + \"geospatial_analyzer.html\" , open = True ) return report def anovos_report ( master_path , id_col = None , label_col = None , corr_threshold = 0.4 , iv_threshold = 0.02 , drift_threshold_model = 0.1 , dataDict_path = \".\" , metricDict_path = \".\" , run_type = \"local\" , final_report_path = \".\" , output_type = None , mlflow_config = None , lat_cols = [], long_cols = [], gh_cols = [], max_records = 100000 , top_geo_records = 100 , auth_key = \"NA\" , ): \"\"\" This function actually helps to produce the final report by scanning through the output processed from the data analyzer module. Parameters ---------- master_path Path containing the input files. id_col ID column (Default value = \"\") label_col label column (Default value = \"\") corr_threshold Correlation threshold beyond which attributes can be categorized under correlated. (Default value = 0.4) iv_threshold IV threshold beyond which attributes can be called as significant. (Default value = 0.02) drift_threshold_model threshold which the user is specifying for tagging an attribute to be drifted or not (Default value = 0.1) dataDict_path Data dictionary path. Default value is kept as None. metricDict_path Metric dictionary path. Default value is kept as None. run_type local or emr or databricks or ak8s option. Default is kept as local auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. final_report_path Path where the report will be saved. (Default value = \".\") output_type Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\" mlflow_config MLflow configuration. If None, all MLflow features are disabled. lat_cols Latitude columns identified in the data long_cols Longitude columns identified in the data gh_cols Geohash columns identified in the data max_records Maximum geospatial points analyzed top_geo_records Top geospatial records displayed Returns ------- Output[HTML] \"\"\" if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( master_path ) + \" \" + ends_with ( \"report_stats\" ) ) master_path = \"report_stats\" subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"databricks\" : master_path = output_to_local ( master_path ) dataDict_path = output_to_local ( dataDict_path ) metricDict_path = output_to_local ( metricDict_path ) final_report_path = output_to_local ( final_report_path ) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( master_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" \"' + ends_with ( \"report_stats\" ) + '\" --recursive=true' ) master_path = \"report_stats\" subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if \"global_summary.csv\" not in os . listdir ( master_path ): print ( \"Minimum supporting data is unavailable, hence the Report could not be generated.\" ) return None global global_summary_df global numcols_name global catcols_name global rows_count global columns_count global numcols_count global catcols_count global blank_chart global df_si_ global df_si global unstable_attr global total_unstable_attr global drift_df global metric_drift global drift_df global len_feats global drift_df_stats global drifted_feats global df_stability global n_df_stability global stability_interpretation_table global plot_index_stability SG_tabs = [ \"measures_of_counts\" , \"measures_of_centralTendency\" , \"measures_of_cardinality\" , \"measures_of_percentiles\" , \"measures_of_dispersion\" , \"measures_of_shape\" , \"global_summary\" , ] QC_tabs = [ \"nullColumns_detection\" , \"IDness_detection\" , \"biasedness_detection\" , \"invalidEntries_detection\" , \"duplicate_detection\" , \"nullRows_detection\" , \"outlier_detection\" , ] AE_tabs = [ \"correlation_matrix\" , \"IV_calculation\" , \"IG_calculation\" , \"variable_clustering\" , ] drift_tab = [ \"drift_statistics\" ] stability_tab = [ \"stability_index\" , \"stabilityIndex_metrics\" ] avl_SG , avl_QC , avl_AE = [], [], [] stability_interpretation_table = pd . DataFrame ( [ [ \"0-1\" , \"Very Unstable\" ], [ \"1-2\" , \"Unstable\" ], [ \"2-3\" , \"Marginally Stable\" ], [ \"3-3.5\" , \"Stable\" ], [ \"3.5-4\" , \"Very Stable\" ], ], columns = [ \"StabilityIndex\" , \"StabilityOrder\" ], ) plot_index_stability = go . Figure ( data = [ go . Table ( header = dict ( values = list ( stability_interpretation_table . columns ), fill_color = px . colors . sequential . Greys [ 2 ], align = \"center\" , font = dict ( size = 12 ), ), cells = dict ( values = [ stability_interpretation_table . StabilityIndex , stability_interpretation_table . StabilityOrder , ], line_color = px . colors . sequential . Greys [ 2 ], fill_color = \"white\" , align = \"center\" , height = 25 , ), columnwidth = [ 2 , 10 ], ) ] ) plot_index_stability . update_layout ( margin = dict ( l = 20 , r = 700 , t = 20 , b = 20 )) blank_chart = go . Figure () blank_chart . update_layout ( autosize = False , width = 10 , height = 10 ) blank_chart . layout . plot_bgcolor = global_plot_bg_color blank_chart . layout . paper_bgcolor = global_paper_bg_color blank_chart . update_xaxes ( visible = False ) blank_chart . update_yaxes ( visible = False ) global_summary_df = pd . read_csv ( ends_with ( master_path ) + \"global_summary.csv\" ) rows_count = int ( global_summary_df [ global_summary_df . metric . values == \"rows_count\" ] . value . values [ 0 ] ) catcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"catcols_count\" ] . value . values [ 0 ] ) numcols_count = int ( global_summary_df [ global_summary_df . metric . values == \"numcols_count\" ] . value . values [ 0 ] ) columns_count = int ( global_summary_df [ global_summary_df . metric . values == \"columns_count\" ] . value . values [ 0 ] ) if catcols_count > 0 : catcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"catcols_name\" ] . value . values ) ) else : catcols_name = \"\" if numcols_count > 0 : numcols_name = \",\" . join ( list ( global_summary_df [ global_summary_df . metric . values == \"numcols_name\" ] . value . values ) ) else : numcols_name = \"\" all_files = os . listdir ( master_path ) eventDist_charts = [ x for x in all_files if \"eventDist\" in x ] stats_files = [ x for x in all_files if \".csv\" in x ] freq_charts = [ x for x in all_files if \"freqDist\" in x ] outlier_charts = [ x for x in all_files if \"outlier\" in x ] drift_charts = [ x for x in all_files if \"drift\" in x and \".csv\" not in x ] all_charts_num_1_ = chart_gen_list ( master_path , chart_type = freq_charts , type_col = \"numerical\" ) all_charts_num_2_ = chart_gen_list ( master_path , chart_type = eventDist_charts , type_col = \"numerical\" ) all_charts_num_3_ = chart_gen_list ( master_path , chart_type = outlier_charts , type_col = \"numerical\" ) all_charts_cat_1_ = chart_gen_list ( master_path , chart_type = freq_charts , type_col = \"categorical\" ) all_charts_cat_2_ = chart_gen_list ( master_path , chart_type = eventDist_charts , type_col = \"categorical\" ) all_drift_charts_ = chart_gen_list ( master_path , chart_type = drift_charts ) for x in [ all_charts_num_1_ , all_charts_num_2_ , all_charts_num_3_ , all_charts_cat_1_ , all_charts_cat_2_ , all_drift_charts_ , ]: if len ( x ) == 1 : x . append ( dp . Plot ( blank_chart , label = \" \" )) else : x mapping_tab_list = [] for i in stats_files : if i . split ( \".csv\" )[ 0 ] in SG_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Descriptive Statistics\" ]) elif i . split ( \".csv\" )[ 0 ] in QC_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Quality Check\" ]) elif i . split ( \".csv\" )[ 0 ] in AE_tabs : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Attribute Associations\" ]) elif i . split ( \".csv\" )[ 0 ] in drift_tab or i . split ( \".csv\" )[ 0 ] in stability_tab : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"Data Drift & Data Stability\" ]) else : mapping_tab_list . append ([ i . split ( \".csv\" )[ 0 ], \"null\" ]) xx = pd . DataFrame ( mapping_tab_list , columns = [ \"file_name\" , \"tab_name\" ]) xx_avl = list ( set ( xx . file_name . values )) for i in SG_tabs : if i in xx_avl : avl_SG . append ( i ) for j in QC_tabs : if j in xx_avl : avl_QC . append ( j ) for k in AE_tabs : if k in xx_avl : avl_AE . append ( k ) missing_SG = list ( set ( SG_tabs ) - set ( avl_SG )) missing_QC = list ( set ( QC_tabs ) - set ( avl_QC )) missing_AE = list ( set ( AE_tabs ) - set ( avl_AE )) missing_drift = list ( set ( drift_tab ) - set ( xx [ xx . tab_name . values == \"Data Drift & Data Stability\" ] . file_name . values ) ) missing_stability = list ( set ( stability_tab ) - set ( xx [ xx . tab_name . values == \"Data Drift & Data Stability\" ] . file_name . values ) ) ds_ind = drift_stability_ind ( missing_drift , drift_tab , missing_stability , stability_tab ) if ds_ind [ 0 ] > 0 : drift_df = pd . read_csv ( ends_with ( master_path ) + \"drift_statistics.csv\" ) . sort_values ( by = [ \"flagged\" ], ascending = False ) metric_drift = list ( drift_df . drop ([ \"attribute\" , \"flagged\" ], 1 ) . columns ) drift_df = drift_df [ drift_df . attribute . values != id_col ] len_feats = drift_df . shape [ 0 ] drift_df_stats = ( drift_df [ drift_df . flagged . values == 1 ] . melt ( id_vars = \"attribute\" , value_vars = metric_drift ) . sort_values ( by = [ \"variable\" , \"value\" ], ascending = False ) ) drifted_feats = drift_df [ drift_df . flagged . values == 1 ] . shape [ 0 ] if ds_ind [ 1 ] > 0.5 : df_stability = pd . read_csv ( ends_with ( master_path ) + \"stabilityIndex_metrics.csv\" ) df_stability [ \"idx\" ] = df_stability [ \"idx\" ] . astype ( str ) . apply ( lambda x : \"df\" + x ) n_df_stability = str ( df_stability [ \"idx\" ] . nunique ()) df_si_ = pd . read_csv ( ends_with ( master_path ) + \"stability_index.csv\" ) df_si = df_si_ [ [ \"attribute\" , \"stability_index\" , \"mean_si\" , \"stddev_si\" , \"kurtosis_si\" , \"flagged\" , ] ] unstable_attr = list ( df_si_ [ df_si_ . flagged . values == 1 ] . attribute . values ) total_unstable_attr = list ( df_si_ . attribute . values ) elif ds_ind [ 1 ] == 0.5 : df_si_ = pd . read_csv ( ends_with ( master_path ) + \"stability_index.csv\" ) df_si = df_si_ [ [ \"attribute\" , \"stability_index\" , \"mean_si\" , \"stddev_si\" , \"kurtosis_si\" , \"flagged\" , ] ] unstable_attr = list ( df_si_ [ df_si_ . flagged . values == 1 ] . attribute . values ) total_unstable_attr = list ( df_si_ . attribute . values ) df_stability = pd . DataFrame () n_df_stability = \"the\" else : pass tab1 = executive_summary_gen ( master_path , label_col , ds_ind , id_col , iv_threshold , corr_threshold ) tab2 = wiki_generator ( master_path , dataDict_path = dataDict_path , metricDict_path = metricDict_path ) tab3 = descriptive_statistics ( master_path , SG_tabs , avl_SG , missing_SG , all_charts_num_1_ , all_charts_cat_1_ ) tab4 = quality_check ( master_path , QC_tabs , avl_QC , missing_QC , all_charts_num_3_ ) tab5 = attribute_associations ( master_path , AE_tabs , avl_AE , missing_AE , label_col , all_charts_num_2_ , all_charts_cat_2_ , ) tab6 = data_drift_stability ( master_path , ds_ind , id_col , drift_threshold_model , all_drift_charts_ ) tab7 = ts_viz_generate ( master_path , id_col , False , output_type ) tab8 = loc_report_gen ( lat_cols , long_cols , gh_cols , master_path , max_records , top_geo_records , False ) final_tabs_list = [] for i in [ tab1 , tab2 , tab3 , tab4 , tab5 , tab6 , tab7 , tab8 ]: if i == \"null_report\" : pass else : final_tabs_list . append ( i ) if run_type in ( \"local\" , \"databricks\" ): run_id = ( mlflow_config [ \"run_id\" ] if mlflow_config is not None and mlflow_config [ \"track_reports\" ] else \"\" ) report_run_path = ends_with ( final_report_path ) + run_id + \"/\" dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( report_run_path + \"ml_anovos_report.html\" , open = True ) if mlflow_config is not None : mlflow . log_artifact ( report_run_path ) elif run_type == \"emr\" : dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( \"ml_anovos_report.html\" , open = True ) bash_cmd = \"aws s3 cp ml_anovos_report.html \" + ends_with ( final_report_path ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) elif run_type == \"ak8s\" : dp . Report ( default_template [ 0 ], default_template [ 1 ], dp . Select ( blocks = final_tabs_list , type = dp . SelectType . TABS ), ) . save ( \"ml_anovos_report.html\" , open = True ) bash_cmd = ( 'azcopy cp \"ml_anovos_report.html\" ' + ends_with ( path_ak8s_modify ( final_report_path )) + str ( auth_key ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) else : raise ValueError ( \"Invalid run_type\" ) print ( \"Report generated successfully at the specified location\" )","title":"report_generation"},{"location":"api/data_report/report_generation.html#functions","text":"def anovos_report ( master_path, id_col=None, label_col=None, corr_threshold=0.4, iv_threshold=0.02, drift_threshold_model=0.1, dataDict_path='.', metricDict_path='.', run_type='local', final_report_path='.', output_type=None, mlflow_config=None, lat_cols=[], long_cols=[], gh_cols=[], max_records=100000, top_geo_records=100, auth_key='NA') This function actually helps to produce the final report by scanning through the output processed from the data analyzer module.","title":"Functions"},{"location":"api/data_report/report_preprocessing.html","text":"report_preprocessing Expand source code import subprocess import warnings from pathlib import Path import mlflow import numpy as np import pandas as pd import plotly.express as px import plotly.graph_objects as go import pyspark from loguru import logger from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql.window import Window from anovos.data_analyzer.stats_generator import uniqueCount_computation from anovos.data_ingest.data_ingest import read_dataset from anovos.data_transformer.transformers import ( attribute_binning , imputation_MMM , outlier_categories , ) from anovos.shared.utils import ( attributeType_segregation , ends_with , output_to_local , path_ak8s_modify , ) warnings . filterwarnings ( \"ignore\" ) global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" num_cols = [] cat_cols = [] def save_stats ( spark , idf , master_path , function_name , reread = False , run_type = \"local\" , mlflow_config = None , auth_key = \"NA\" , ): \"\"\" Parameters ---------- spark Spark Session idf input dataframe master_path Path to master folder under which all statistics will be saved in a csv file format. function_name Function Name for which statistics need to be saved. file name will be saved as csv reread option to reread. Default value is kept as False run_type local or emr or databricks or ak8s based on the mode of execution. Default value is kept as local mlflow_config MLflow configuration. If None, all MLflow features are disabled. auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns ------- \"\"\" if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = output_to_local ( master_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) local_path = ( local_path + \"/\" + mlflow_config [ \"run_id\" ] if mlflow_config is not None and mlflow_config . get ( \"track_reports\" , False ) else local_path ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) idf . toPandas () . to_csv ( ends_with ( local_path ) + function_name + \".csv\" , index = False ) if mlflow_config is not None : mlflow . log_artifact ( local_path ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + function_name + \".csv \" + ends_with ( master_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( master_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + function_name + '.csv\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\"' ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if reread : odf = spark . read . csv ( ends_with ( master_path ) + function_name + \".csv\" , header = True , inferSchema = True , ) return odf def edit_binRange ( col ): \"\"\" Parameters ---------- col The column which is passed as input and needs to be treated. The generated output will not contain any range whose value at either side is the same. Returns ------- \"\"\" try : list_col = col . split ( \"-\" ) deduped_col = list ( set ( list_col )) if len ( list_col ) != len ( deduped_col ): return deduped_col [ 0 ] else : return col except Exception as e : logger . error ( f \"processing failed during edit_binRange, error { e } \" ) pass f_edit_binRange = F . udf ( edit_binRange , T . StringType ()) def binRange_to_binIdx ( spark , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session col The input column which is needed to by mapped with respective index cutoffs_path paths containing the range cutoffs applicable for each index Returns ------- \"\"\" bin_cutoffs = ( spark . read . parquet ( cutoffs_path ) . where ( F . col ( \"attribute\" ) == col ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) bin_ranges = [] max_cat = len ( bin_cutoffs ) + 1 for idx in range ( 0 , max_cat ): if idx == 0 : bin_ranges . append ( \"<= \" + str ( round ( bin_cutoffs [ idx ], 4 ))) elif idx < ( max_cat - 1 ): bin_ranges . append ( str ( round ( bin_cutoffs [ idx - 1 ], 4 )) + \"-\" + str ( round ( bin_cutoffs [ idx ], 4 )) ) else : bin_ranges . append ( \"> \" + str ( round ( bin_cutoffs [ idx - 1 ], 4 ))) mapping = spark . createDataFrame ( zip ( range ( 1 , max_cat + 1 ), bin_ranges ), schema = [ \"bin_idx\" , col ] ) return mapping def plot_frequency ( spark , idf , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histograms col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . groupBy ( col ) . count () . withColumn ( \"count_%\" , 100 * ( F . col ( \"count\" ) / F . sum ( \"count\" ) . over ( Window . partitionBy ())), ) . withColumn ( col , f_edit_binRange ( col )) ) if col in cat_cols : odf_pd = odf . orderBy ( \"count\" , ascending = False ) . toPandas () . fillna ( \"Missing\" ) odf_pd . loc [ odf_pd [ col ] == \"others\" , col ] = \"others*\" if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = ( odf . join ( mapping , col , \"left_outer\" ) . orderBy ( \"bin_idx\" ) . toPandas () . fillna ( \"Missing\" ) ) fig = px . bar ( odf_pd , x = col , y = \"count\" , text = odf_pd [ \"count_%\" ] . apply ( lambda x : \" {0:1.2f} %\" . format ( x )), color_discrete_sequence = global_theme , ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( title_text = str ( \"Frequency Distribution for \" + str ( col . upper ()))) fig . update_xaxes ( type = \"category\" ) # fig.update_layout(barmode='stack', xaxis={'categoryorder':'total descending'}) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f\"{base_loc}/{file_name_}bar_graph.html\") return fig def plot_outlier ( spark , idf , col , split_var = None , sample_size = 500000 ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for capturing the outliers in form of violin charts col Analysis column split_var Column which is needed. Default value is kept as None sample_size Maximum Sample size. Default value is kept as 500000 Returns ------- \"\"\" idf_sample = idf . select ( col ) . sample ( False , min ( 1.0 , float ( sample_size ) / idf . count ()), 0 ) idf_sample . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () idf_imputed = imputation_MMM ( spark , idf_sample ) idf_pd = idf_imputed . toPandas () fig = px . violin ( idf_pd , y = col , color = split_var , box = True , points = \"outliers\" , color_discrete_sequence = [ global_theme_r [ 8 ], global_theme_r [ 4 ]], ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def plot_eventRate ( spark , idf , col , label_col , event_label , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histogram col Analysis column label_col Label column event_label Event label cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . pivot ( label_col ) . count () . fillna ( 0 , subset = [ \"0\" , \"1\" ]) . withColumn ( \"event_rate\" , 100 * ( F . col ( \"1\" ) / ( F . col ( \"0\" ) + F . col ( \"1\" )))) . withColumn ( \"attribute_name\" , F . lit ( col )) . withColumn ( col , f_edit_binRange ( col )) ) if col in cat_cols : odf_pd = odf . orderBy ( \"event_rate\" , ascending = False ) . toPandas () odf_pd . loc [ odf_pd [ col ] == \"others\" , col ] = \"others*\" if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = odf . join ( mapping , col , \"left_outer\" ) . orderBy ( \"bin_idx\" ) . toPandas () fig = px . bar ( odf_pd , x = col , y = \"event_rate\" , text = odf_pd [ \"event_rate\" ] . apply ( lambda x : \" {0:1.2f} %\" . format ( x )), color_discrete_sequence = global_theme , ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( title_text = str ( \"Event Rate Distribution for \" + str ( col . upper ()) + str ( \" [Target Variable : \" + str ( event_label ) + str ( \"]\" )) ) ) fig . update_xaxes ( type = \"category\" ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f\"{base_loc}/{file_name_}feat_analysis_label.html\") return fig def plot_comparative_drift ( spark , idf , source , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Target dataframe which would be referred for producing the frequency charts in form of bar plots / histogram source Source dataframe of comparison col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . groupBy ( col ) . agg (( F . count ( col ) / idf . count ()) . alias ( \"countpct_target\" )) . fillna ( np . nan , subset = [ col ]) ) if col in cat_cols : odf_pd = ( odf . join ( source . withColumnRenamed ( \"p\" , \"countpct_source\" ) . fillna ( np . nan , subset = [ col ] ), col , \"full_outer\" , ) . orderBy ( \"countpct_target\" , ascending = False ) . toPandas () ) if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = ( odf . join ( mapping , col , \"left_outer\" ) . fillna ( np . nan , subset = [ \"bin_idx\" ]) . join ( source . fillna ( np . nan , subset = [ col ]) . select ( F . col ( col ) . alias ( \"bin_idx\" ), F . col ( \"p\" ) . alias ( \"countpct_source\" ) ), \"bin_idx\" , \"full_outer\" , ) . orderBy ( \"bin_idx\" ) . toPandas () ) odf_pd . fillna ( { col : \"Missing\" , \"countpct_source\" : 0 , \"countpct_target\" : 0 }, inplace = True ) odf_pd [ \"%_diff\" ] = ( ( odf_pd [ \"countpct_target\" ] / odf_pd [ \"countpct_source\" ]) - 1 ) * 100 fig = go . Figure () fig . add_bar ( y = list ( odf_pd . countpct_source . values ), x = odf_pd [ col ], name = \"source\" , marker = dict ( color = global_theme ), ) fig . update_traces ( overwrite = True , marker = { \"opacity\" : 0.7 }) fig . add_bar ( y = list ( odf_pd . countpct_target . values ), x = odf_pd [ col ], name = \"target\" , text = odf_pd [ \"%_diff\" ] . apply ( lambda x : \" {0:0.2f} %\" . format ( x )), marker = dict ( color = global_theme ), ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( paper_bgcolor = global_paper_bg_color , plot_bgcolor = global_plot_bg_color , showlegend = False , ) fig . update_layout ( title_text = str ( \"Drift Comparison for \" + col + \"<br><sup>(L->R : Source->Target)</sup>\" ) ) fig . update_traces ( marker = dict ( color = global_theme )) fig . update_xaxes ( type = \"category\" ) # fig.add_trace(go.Scatter(x=odf_pd[col], y=odf_pd.countpct_target.values, mode='lines+markers', # line=dict(color=px.colors.qualitative.Antique[10], width=3, dash='dot'))) fig . update_layout ( xaxis_tickfont_size = 14 , yaxis = dict ( title = \"frequency\" , titlefont_size = 16 , tickfont_size = 14 ), ) return fig def charts_to_objects ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = None , event_label = 1 , bin_method = \"equal_range\" , bin_size = 10 , coverage = 1.0 , drift_detector = False , outlier_charts = False , source_path = \"NA\" , master_path = \".\" , stats_unique = {}, run_type = \"local\" , auth_key = \"NA\" , ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe list_of_cols List of columns passed for analysis (Default value = \"all\") drop_cols List of columns dropped from analysis (Default value = []) label_col Label column (Default value = None) event_label Event label (Default value = 1) bin_method Binning method equal_range or equal_frequency (Default value = \"equal_range\") bin_size Maximum bin size categories. Default value is kept as 10 coverage Maximum coverage of categories. Default value is kept as 1.0 (which is 100%) drift_detector True or False as per the availability. Default value is kept as False source_path Source data path. Default value is kept as \"NA\" to save intermediate data in \"intermediate_data/\" folder. master_path Path where the output needs to be saved, ideally the same path where the analyzed data output is also saved (Default value = \".\") stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type local or emr or databricks or ak8s run type. Default value is kept as local auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns ------- \"\"\" global num_cols global cat_cols if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + remove_cols )]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if cat_cols : idf_cleaned = outlier_categories ( spark , idf , list_of_cols = cat_cols , coverage = coverage , max_category = bin_size ) else : idf_cleaned = idf if source_path == \"NA\" : source_path = \"intermediate_data\" if drift_detector : encoding_model_exists = True binned_cols = ( spark . read . parquet ( source_path + \"/drift_statistics/attribute_binning\" ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) to_be_binned = [ e for e in num_cols if e not in binned_cols ] else : encoding_model_exists = False binned_cols = [] to_be_binned = num_cols if to_be_binned : idf_encoded = attribute_binning ( spark , idf_cleaned , list_of_cols = to_be_binned , method_type = bin_method , bin_size = bin_size , bin_dtype = \"categorical\" , pre_existing_model = False , model_path = source_path + \"/charts_to_objects\" , output_mode = \"append\" , ) else : idf_encoded = idf_cleaned if binned_cols : idf_encoded = attribute_binning ( spark , idf_encoded , list_of_cols = binned_cols , method_type = bin_method , bin_size = bin_size , bin_dtype = \"categorical\" , pre_existing_model = True , model_path = source_path + \"/drift_statistics\" , output_mode = \"append\" , ) cutoffs_path1 = source_path + \"/charts_to_objects/attribute_binning\" cutoffs_path2 = source_path + \"/drift_statistics/attribute_binning\" idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = output_to_local ( master_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) for idx , col in enumerate ( list_of_cols ): if col in binned_cols : cutoffs_path = cutoffs_path2 else : cutoffs_path = cutoffs_path1 if col in cat_cols : f = plot_frequency ( spark , idf_encoded , col , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"freqDist_\" + col ) if label_col : if col != label_col : f = plot_eventRate ( spark , idf_encoded , col , label_col , event_label , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"eventDist_\" + col ) if drift_detector : try : frequency_path = ( source_path + \"/drift_statistics/frequency_counts/\" + col ) idf_source = spark . read . csv ( frequency_path , header = True , inferSchema = True ) f = plot_comparative_drift ( spark , idf_encoded , idf_source , col , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"drift_\" + col ) except Exception as e : logger . error ( f \"processing failed during drift detection, error { e } \" ) pass if col in num_cols : if outlier_charts : f = plot_outlier ( spark , idf , col , split_var = None ) f . write_json ( ends_with ( local_path ) + \"outlier_\" + col ) f = plot_frequency ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), col , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"freqDist_\" + col ) if label_col : if col != label_col : f = plot_eventRate ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), col , label_col , event_label , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"eventDist_\" + col ) if drift_detector : try : frequency_path = ( source_path + \"/drift_statistics/frequency_counts/\" + col ) idf_source = spark . read . csv ( frequency_path , header = True , inferSchema = True ) f = plot_comparative_drift ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), idf_source , col , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"drift_\" + col ) except Exception as e : logger . error ( f \"processing failed during drift detection, error { e } \" ) pass pd . DataFrame ( idf . dtypes , columns = [ \"attribute\" , \"data_type\" ]) . to_csv ( ends_with ( local_path ) + \"data_type.csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( master_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( master_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true' ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) Functions def binRange_to_binIdx ( spark, col, cutoffs_path) Parameters spark Spark Session col The input column which is needed to by mapped with respective index cutoffs_path paths containing the range cutoffs applicable for each index Returns Expand source code def binRange_to_binIdx ( spark , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session col The input column which is needed to by mapped with respective index cutoffs_path paths containing the range cutoffs applicable for each index Returns ------- \"\"\" bin_cutoffs = ( spark . read . parquet ( cutoffs_path ) . where ( F . col ( \"attribute\" ) == col ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) bin_ranges = [] max_cat = len ( bin_cutoffs ) + 1 for idx in range ( 0 , max_cat ): if idx == 0 : bin_ranges . append ( \"<= \" + str ( round ( bin_cutoffs [ idx ], 4 ))) elif idx < ( max_cat - 1 ): bin_ranges . append ( str ( round ( bin_cutoffs [ idx - 1 ], 4 )) + \"-\" + str ( round ( bin_cutoffs [ idx ], 4 )) ) else : bin_ranges . append ( \"> \" + str ( round ( bin_cutoffs [ idx - 1 ], 4 ))) mapping = spark . createDataFrame ( zip ( range ( 1 , max_cat + 1 ), bin_ranges ), schema = [ \"bin_idx\" , col ] ) return mapping def charts_to_objects ( spark, idf, list_of_cols='all', drop_cols=[], label_col=None, event_label=1, bin_method='equal_range', bin_size=10, coverage=1.0, drift_detector=False, outlier_charts=False, source_path='NA', master_path='.', stats_unique={}, run_type='local', auth_key='NA') Parameters spark Spark Session idf Input dataframe list_of_cols List of columns passed for analysis (Default value = \"all\") drop_cols List of columns dropped from analysis (Default value = []) label_col Label column (Default value = None) event_label Event label (Default value = 1) bin_method Binning method equal_range or equal_frequency (Default value = \"equal_range\") bin_size Maximum bin size categories. Default value is kept as 10 coverage Maximum coverage of categories. Default value is kept as 1.0 (which is 100%) drift_detector True or False as per the availability. Default value is kept as False source_path Source data path. Default value is kept as \"NA\" to save intermediate data in \"intermediate_data/\" folder. master_path Path where the output needs to be saved, ideally the same path where the analyzed data output is also saved (Default value = \".\") stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type local or emr or databricks or ak8s run type. Default value is kept as local auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns Expand source code def charts_to_objects ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = None , event_label = 1 , bin_method = \"equal_range\" , bin_size = 10 , coverage = 1.0 , drift_detector = False , outlier_charts = False , source_path = \"NA\" , master_path = \".\" , stats_unique = {}, run_type = \"local\" , auth_key = \"NA\" , ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe list_of_cols List of columns passed for analysis (Default value = \"all\") drop_cols List of columns dropped from analysis (Default value = []) label_col Label column (Default value = None) event_label Event label (Default value = 1) bin_method Binning method equal_range or equal_frequency (Default value = \"equal_range\") bin_size Maximum bin size categories. Default value is kept as 10 coverage Maximum coverage of categories. Default value is kept as 1.0 (which is 100%) drift_detector True or False as per the availability. Default value is kept as False source_path Source data path. Default value is kept as \"NA\" to save intermediate data in \"intermediate_data/\" folder. master_path Path where the output needs to be saved, ideally the same path where the analyzed data output is also saved (Default value = \".\") stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type local or emr or databricks or ak8s run type. Default value is kept as local auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns ------- \"\"\" global num_cols global cat_cols if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + remove_cols )]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if cat_cols : idf_cleaned = outlier_categories ( spark , idf , list_of_cols = cat_cols , coverage = coverage , max_category = bin_size ) else : idf_cleaned = idf if source_path == \"NA\" : source_path = \"intermediate_data\" if drift_detector : encoding_model_exists = True binned_cols = ( spark . read . parquet ( source_path + \"/drift_statistics/attribute_binning\" ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) to_be_binned = [ e for e in num_cols if e not in binned_cols ] else : encoding_model_exists = False binned_cols = [] to_be_binned = num_cols if to_be_binned : idf_encoded = attribute_binning ( spark , idf_cleaned , list_of_cols = to_be_binned , method_type = bin_method , bin_size = bin_size , bin_dtype = \"categorical\" , pre_existing_model = False , model_path = source_path + \"/charts_to_objects\" , output_mode = \"append\" , ) else : idf_encoded = idf_cleaned if binned_cols : idf_encoded = attribute_binning ( spark , idf_encoded , list_of_cols = binned_cols , method_type = bin_method , bin_size = bin_size , bin_dtype = \"categorical\" , pre_existing_model = True , model_path = source_path + \"/drift_statistics\" , output_mode = \"append\" , ) cutoffs_path1 = source_path + \"/charts_to_objects/attribute_binning\" cutoffs_path2 = source_path + \"/drift_statistics/attribute_binning\" idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = output_to_local ( master_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) for idx , col in enumerate ( list_of_cols ): if col in binned_cols : cutoffs_path = cutoffs_path2 else : cutoffs_path = cutoffs_path1 if col in cat_cols : f = plot_frequency ( spark , idf_encoded , col , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"freqDist_\" + col ) if label_col : if col != label_col : f = plot_eventRate ( spark , idf_encoded , col , label_col , event_label , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"eventDist_\" + col ) if drift_detector : try : frequency_path = ( source_path + \"/drift_statistics/frequency_counts/\" + col ) idf_source = spark . read . csv ( frequency_path , header = True , inferSchema = True ) f = plot_comparative_drift ( spark , idf_encoded , idf_source , col , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"drift_\" + col ) except Exception as e : logger . error ( f \"processing failed during drift detection, error { e } \" ) pass if col in num_cols : if outlier_charts : f = plot_outlier ( spark , idf , col , split_var = None ) f . write_json ( ends_with ( local_path ) + \"outlier_\" + col ) f = plot_frequency ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), col , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"freqDist_\" + col ) if label_col : if col != label_col : f = plot_eventRate ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), col , label_col , event_label , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"eventDist_\" + col ) if drift_detector : try : frequency_path = ( source_path + \"/drift_statistics/frequency_counts/\" + col ) idf_source = spark . read . csv ( frequency_path , header = True , inferSchema = True ) f = plot_comparative_drift ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), idf_source , col , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"drift_\" + col ) except Exception as e : logger . error ( f \"processing failed during drift detection, error { e } \" ) pass pd . DataFrame ( idf . dtypes , columns = [ \"attribute\" , \"data_type\" ]) . to_csv ( ends_with ( local_path ) + \"data_type.csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( master_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( master_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true' ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) def edit_binRange ( col) Parameters col The column which is passed as input and needs to be treated. The generated output will not contain any range whose value at either side is the same. Returns Expand source code def edit_binRange ( col ): \"\"\" Parameters ---------- col The column which is passed as input and needs to be treated. The generated output will not contain any range whose value at either side is the same. Returns ------- \"\"\" try : list_col = col . split ( \"-\" ) deduped_col = list ( set ( list_col )) if len ( list_col ) != len ( deduped_col ): return deduped_col [ 0 ] else : return col except Exception as e : logger . error ( f \"processing failed during edit_binRange, error { e } \" ) pass def f_edit_binRange ( col) Parameters col The column which is passed as input and needs to be treated. The generated output will not contain any range whose value at either side is the same. Returns Expand source code def edit_binRange ( col ): \"\"\" Parameters ---------- col The column which is passed as input and needs to be treated. The generated output will not contain any range whose value at either side is the same. Returns ------- \"\"\" try : list_col = col . split ( \"-\" ) deduped_col = list ( set ( list_col )) if len ( list_col ) != len ( deduped_col ): return deduped_col [ 0 ] else : return col except Exception as e : logger . error ( f \"processing failed during edit_binRange, error { e } \" ) pass def plot_comparative_drift ( spark, idf, source, col, cutoffs_path) Parameters spark Spark Session idf Target dataframe which would be referred for producing the frequency charts in form of bar plots / histogram source Source dataframe of comparison col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns Expand source code def plot_comparative_drift ( spark , idf , source , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Target dataframe which would be referred for producing the frequency charts in form of bar plots / histogram source Source dataframe of comparison col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . groupBy ( col ) . agg (( F . count ( col ) / idf . count ()) . alias ( \"countpct_target\" )) . fillna ( np . nan , subset = [ col ]) ) if col in cat_cols : odf_pd = ( odf . join ( source . withColumnRenamed ( \"p\" , \"countpct_source\" ) . fillna ( np . nan , subset = [ col ] ), col , \"full_outer\" , ) . orderBy ( \"countpct_target\" , ascending = False ) . toPandas () ) if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = ( odf . join ( mapping , col , \"left_outer\" ) . fillna ( np . nan , subset = [ \"bin_idx\" ]) . join ( source . fillna ( np . nan , subset = [ col ]) . select ( F . col ( col ) . alias ( \"bin_idx\" ), F . col ( \"p\" ) . alias ( \"countpct_source\" ) ), \"bin_idx\" , \"full_outer\" , ) . orderBy ( \"bin_idx\" ) . toPandas () ) odf_pd . fillna ( { col : \"Missing\" , \"countpct_source\" : 0 , \"countpct_target\" : 0 }, inplace = True ) odf_pd [ \"%_diff\" ] = ( ( odf_pd [ \"countpct_target\" ] / odf_pd [ \"countpct_source\" ]) - 1 ) * 100 fig = go . Figure () fig . add_bar ( y = list ( odf_pd . countpct_source . values ), x = odf_pd [ col ], name = \"source\" , marker = dict ( color = global_theme ), ) fig . update_traces ( overwrite = True , marker = { \"opacity\" : 0.7 }) fig . add_bar ( y = list ( odf_pd . countpct_target . values ), x = odf_pd [ col ], name = \"target\" , text = odf_pd [ \"%_diff\" ] . apply ( lambda x : \" {0:0.2f} %\" . format ( x )), marker = dict ( color = global_theme ), ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( paper_bgcolor = global_paper_bg_color , plot_bgcolor = global_plot_bg_color , showlegend = False , ) fig . update_layout ( title_text = str ( \"Drift Comparison for \" + col + \"<br><sup>(L->R : Source->Target)</sup>\" ) ) fig . update_traces ( marker = dict ( color = global_theme )) fig . update_xaxes ( type = \"category\" ) # fig.add_trace(go.Scatter(x=odf_pd[col], y=odf_pd.countpct_target.values, mode='lines+markers', # line=dict(color=px.colors.qualitative.Antique[10], width=3, dash='dot'))) fig . update_layout ( xaxis_tickfont_size = 14 , yaxis = dict ( title = \"frequency\" , titlefont_size = 16 , tickfont_size = 14 ), ) return fig def plot_eventRate ( spark, idf, col, label_col, event_label, cutoffs_path) Parameters spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histogram col Analysis column label_col Label column event_label Event label cutoffs_path Path containing the range cut offs details for the analysis column Returns Expand source code def plot_eventRate ( spark , idf , col , label_col , event_label , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histogram col Analysis column label_col Label column event_label Event label cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . pivot ( label_col ) . count () . fillna ( 0 , subset = [ \"0\" , \"1\" ]) . withColumn ( \"event_rate\" , 100 * ( F . col ( \"1\" ) / ( F . col ( \"0\" ) + F . col ( \"1\" )))) . withColumn ( \"attribute_name\" , F . lit ( col )) . withColumn ( col , f_edit_binRange ( col )) ) if col in cat_cols : odf_pd = odf . orderBy ( \"event_rate\" , ascending = False ) . toPandas () odf_pd . loc [ odf_pd [ col ] == \"others\" , col ] = \"others*\" if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = odf . join ( mapping , col , \"left_outer\" ) . orderBy ( \"bin_idx\" ) . toPandas () fig = px . bar ( odf_pd , x = col , y = \"event_rate\" , text = odf_pd [ \"event_rate\" ] . apply ( lambda x : \" {0:1.2f} %\" . format ( x )), color_discrete_sequence = global_theme , ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( title_text = str ( \"Event Rate Distribution for \" + str ( col . upper ()) + str ( \" [Target Variable : \" + str ( event_label ) + str ( \"]\" )) ) ) fig . update_xaxes ( type = \"category\" ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f\"{base_loc}/{file_name_}feat_analysis_label.html\") return fig def plot_frequency ( spark, idf, col, cutoffs_path) Parameters spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histograms col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns Expand source code def plot_frequency ( spark , idf , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histograms col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . groupBy ( col ) . count () . withColumn ( \"count_%\" , 100 * ( F . col ( \"count\" ) / F . sum ( \"count\" ) . over ( Window . partitionBy ())), ) . withColumn ( col , f_edit_binRange ( col )) ) if col in cat_cols : odf_pd = odf . orderBy ( \"count\" , ascending = False ) . toPandas () . fillna ( \"Missing\" ) odf_pd . loc [ odf_pd [ col ] == \"others\" , col ] = \"others*\" if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = ( odf . join ( mapping , col , \"left_outer\" ) . orderBy ( \"bin_idx\" ) . toPandas () . fillna ( \"Missing\" ) ) fig = px . bar ( odf_pd , x = col , y = \"count\" , text = odf_pd [ \"count_%\" ] . apply ( lambda x : \" {0:1.2f} %\" . format ( x )), color_discrete_sequence = global_theme , ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( title_text = str ( \"Frequency Distribution for \" + str ( col . upper ()))) fig . update_xaxes ( type = \"category\" ) # fig.update_layout(barmode='stack', xaxis={'categoryorder':'total descending'}) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f\"{base_loc}/{file_name_}bar_graph.html\") return fig def plot_outlier ( spark, idf, col, split_var=None, sample_size=500000) Parameters spark Spark Session idf Input dataframe which would be referred for capturing the outliers in form of violin charts col Analysis column split_var Column which is needed. Default value is kept as None sample_size Maximum Sample size. Default value is kept as 500000 Returns Expand source code def plot_outlier ( spark , idf , col , split_var = None , sample_size = 500000 ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for capturing the outliers in form of violin charts col Analysis column split_var Column which is needed. Default value is kept as None sample_size Maximum Sample size. Default value is kept as 500000 Returns ------- \"\"\" idf_sample = idf . select ( col ) . sample ( False , min ( 1.0 , float ( sample_size ) / idf . count ()), 0 ) idf_sample . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () idf_imputed = imputation_MMM ( spark , idf_sample ) idf_pd = idf_imputed . toPandas () fig = px . violin ( idf_pd , y = col , color = split_var , box = True , points = \"outliers\" , color_discrete_sequence = [ global_theme_r [ 8 ], global_theme_r [ 4 ]], ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def save_stats ( spark, idf, master_path, function_name, reread=False, run_type='local', mlflow_config=None, auth_key='NA') Parameters spark Spark Session idf input dataframe master_path Path to master folder under which all statistics will be saved in a csv file format. function_name Function Name for which statistics need to be saved. file name will be saved as csv reread option to reread. Default value is kept as False run_type local or emr or databricks or ak8s based on the mode of execution. Default value is kept as local mlflow_config MLflow configuration. If None, all MLflow features are disabled. auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns Expand source code def save_stats ( spark , idf , master_path , function_name , reread = False , run_type = \"local\" , mlflow_config = None , auth_key = \"NA\" , ): \"\"\" Parameters ---------- spark Spark Session idf input dataframe master_path Path to master folder under which all statistics will be saved in a csv file format. function_name Function Name for which statistics need to be saved. file name will be saved as csv reread option to reread. Default value is kept as False run_type local or emr or databricks or ak8s based on the mode of execution. Default value is kept as local mlflow_config MLflow configuration. If None, all MLflow features are disabled. auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns ------- \"\"\" if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = output_to_local ( master_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) local_path = ( local_path + \"/\" + mlflow_config [ \"run_id\" ] if mlflow_config is not None and mlflow_config . get ( \"track_reports\" , False ) else local_path ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) idf . toPandas () . to_csv ( ends_with ( local_path ) + function_name + \".csv\" , index = False ) if mlflow_config is not None : mlflow . log_artifact ( local_path ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + function_name + \".csv \" + ends_with ( master_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( master_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + function_name + '.csv\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\"' ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if reread : odf = spark . read . csv ( ends_with ( master_path ) + function_name + \".csv\" , header = True , inferSchema = True , ) return odf","title":"<code>report_preprocessing</code>"},{"location":"api/data_report/report_preprocessing.html#report_preprocessing","text":"Expand source code import subprocess import warnings from pathlib import Path import mlflow import numpy as np import pandas as pd import plotly.express as px import plotly.graph_objects as go import pyspark from loguru import logger from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql.window import Window from anovos.data_analyzer.stats_generator import uniqueCount_computation from anovos.data_ingest.data_ingest import read_dataset from anovos.data_transformer.transformers import ( attribute_binning , imputation_MMM , outlier_categories , ) from anovos.shared.utils import ( attributeType_segregation , ends_with , output_to_local , path_ak8s_modify , ) warnings . filterwarnings ( \"ignore\" ) global_theme = px . colors . sequential . Plasma global_theme_r = px . colors . sequential . Plasma_r global_plot_bg_color = \"rgba(0,0,0,0)\" global_paper_bg_color = \"rgba(0,0,0,0)\" num_cols = [] cat_cols = [] def save_stats ( spark , idf , master_path , function_name , reread = False , run_type = \"local\" , mlflow_config = None , auth_key = \"NA\" , ): \"\"\" Parameters ---------- spark Spark Session idf input dataframe master_path Path to master folder under which all statistics will be saved in a csv file format. function_name Function Name for which statistics need to be saved. file name will be saved as csv reread option to reread. Default value is kept as False run_type local or emr or databricks or ak8s based on the mode of execution. Default value is kept as local mlflow_config MLflow configuration. If None, all MLflow features are disabled. auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns ------- \"\"\" if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = output_to_local ( master_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) local_path = ( local_path + \"/\" + mlflow_config [ \"run_id\" ] if mlflow_config is not None and mlflow_config . get ( \"track_reports\" , False ) else local_path ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) idf . toPandas () . to_csv ( ends_with ( local_path ) + function_name + \".csv\" , index = False ) if mlflow_config is not None : mlflow . log_artifact ( local_path ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + ends_with ( local_path ) + function_name + \".csv \" + ends_with ( master_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( master_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + function_name + '.csv\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\"' ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if reread : odf = spark . read . csv ( ends_with ( master_path ) + function_name + \".csv\" , header = True , inferSchema = True , ) return odf def edit_binRange ( col ): \"\"\" Parameters ---------- col The column which is passed as input and needs to be treated. The generated output will not contain any range whose value at either side is the same. Returns ------- \"\"\" try : list_col = col . split ( \"-\" ) deduped_col = list ( set ( list_col )) if len ( list_col ) != len ( deduped_col ): return deduped_col [ 0 ] else : return col except Exception as e : logger . error ( f \"processing failed during edit_binRange, error { e } \" ) pass f_edit_binRange = F . udf ( edit_binRange , T . StringType ()) def binRange_to_binIdx ( spark , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session col The input column which is needed to by mapped with respective index cutoffs_path paths containing the range cutoffs applicable for each index Returns ------- \"\"\" bin_cutoffs = ( spark . read . parquet ( cutoffs_path ) . where ( F . col ( \"attribute\" ) == col ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) bin_ranges = [] max_cat = len ( bin_cutoffs ) + 1 for idx in range ( 0 , max_cat ): if idx == 0 : bin_ranges . append ( \"<= \" + str ( round ( bin_cutoffs [ idx ], 4 ))) elif idx < ( max_cat - 1 ): bin_ranges . append ( str ( round ( bin_cutoffs [ idx - 1 ], 4 )) + \"-\" + str ( round ( bin_cutoffs [ idx ], 4 )) ) else : bin_ranges . append ( \"> \" + str ( round ( bin_cutoffs [ idx - 1 ], 4 ))) mapping = spark . createDataFrame ( zip ( range ( 1 , max_cat + 1 ), bin_ranges ), schema = [ \"bin_idx\" , col ] ) return mapping def plot_frequency ( spark , idf , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histograms col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . groupBy ( col ) . count () . withColumn ( \"count_%\" , 100 * ( F . col ( \"count\" ) / F . sum ( \"count\" ) . over ( Window . partitionBy ())), ) . withColumn ( col , f_edit_binRange ( col )) ) if col in cat_cols : odf_pd = odf . orderBy ( \"count\" , ascending = False ) . toPandas () . fillna ( \"Missing\" ) odf_pd . loc [ odf_pd [ col ] == \"others\" , col ] = \"others*\" if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = ( odf . join ( mapping , col , \"left_outer\" ) . orderBy ( \"bin_idx\" ) . toPandas () . fillna ( \"Missing\" ) ) fig = px . bar ( odf_pd , x = col , y = \"count\" , text = odf_pd [ \"count_%\" ] . apply ( lambda x : \" {0:1.2f} %\" . format ( x )), color_discrete_sequence = global_theme , ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( title_text = str ( \"Frequency Distribution for \" + str ( col . upper ()))) fig . update_xaxes ( type = \"category\" ) # fig.update_layout(barmode='stack', xaxis={'categoryorder':'total descending'}) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f\"{base_loc}/{file_name_}bar_graph.html\") return fig def plot_outlier ( spark , idf , col , split_var = None , sample_size = 500000 ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for capturing the outliers in form of violin charts col Analysis column split_var Column which is needed. Default value is kept as None sample_size Maximum Sample size. Default value is kept as 500000 Returns ------- \"\"\" idf_sample = idf . select ( col ) . sample ( False , min ( 1.0 , float ( sample_size ) / idf . count ()), 0 ) idf_sample . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () idf_imputed = imputation_MMM ( spark , idf_sample ) idf_pd = idf_imputed . toPandas () fig = px . violin ( idf_pd , y = col , color = split_var , box = True , points = \"outliers\" , color_discrete_sequence = [ global_theme_r [ 8 ], global_theme_r [ 4 ]], ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color fig . update_layout ( legend = dict ( orientation = \"h\" , x = 0.5 , yanchor = \"bottom\" , xanchor = \"center\" ) ) return fig def plot_eventRate ( spark , idf , col , label_col , event_label , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe which would be referred for producing the frequency charts in form of bar plots / histogram col Analysis column label_col Label column event_label Event label cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col ) . pivot ( label_col ) . count () . fillna ( 0 , subset = [ \"0\" , \"1\" ]) . withColumn ( \"event_rate\" , 100 * ( F . col ( \"1\" ) / ( F . col ( \"0\" ) + F . col ( \"1\" )))) . withColumn ( \"attribute_name\" , F . lit ( col )) . withColumn ( col , f_edit_binRange ( col )) ) if col in cat_cols : odf_pd = odf . orderBy ( \"event_rate\" , ascending = False ) . toPandas () odf_pd . loc [ odf_pd [ col ] == \"others\" , col ] = \"others*\" if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = odf . join ( mapping , col , \"left_outer\" ) . orderBy ( \"bin_idx\" ) . toPandas () fig = px . bar ( odf_pd , x = col , y = \"event_rate\" , text = odf_pd [ \"event_rate\" ] . apply ( lambda x : \" {0:1.2f} %\" . format ( x )), color_discrete_sequence = global_theme , ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( title_text = str ( \"Event Rate Distribution for \" + str ( col . upper ()) + str ( \" [Target Variable : \" + str ( event_label ) + str ( \"]\" )) ) ) fig . update_xaxes ( type = \"category\" ) fig . layout . plot_bgcolor = global_plot_bg_color fig . layout . paper_bgcolor = global_paper_bg_color # plotly.offline.plot(fig, auto_open=False, validate=False, filename=f\"{base_loc}/{file_name_}feat_analysis_label.html\") return fig def plot_comparative_drift ( spark , idf , source , col , cutoffs_path ): \"\"\" Parameters ---------- spark Spark Session idf Target dataframe which would be referred for producing the frequency charts in form of bar plots / histogram source Source dataframe of comparison col Analysis column cutoffs_path Path containing the range cut offs details for the analysis column Returns ------- \"\"\" odf = ( idf . groupBy ( col ) . agg (( F . count ( col ) / idf . count ()) . alias ( \"countpct_target\" )) . fillna ( np . nan , subset = [ col ]) ) if col in cat_cols : odf_pd = ( odf . join ( source . withColumnRenamed ( \"p\" , \"countpct_source\" ) . fillna ( np . nan , subset = [ col ] ), col , \"full_outer\" , ) . orderBy ( \"countpct_target\" , ascending = False ) . toPandas () ) if col in num_cols : mapping = binRange_to_binIdx ( spark , col , cutoffs_path ) odf_pd = ( odf . join ( mapping , col , \"left_outer\" ) . fillna ( np . nan , subset = [ \"bin_idx\" ]) . join ( source . fillna ( np . nan , subset = [ col ]) . select ( F . col ( col ) . alias ( \"bin_idx\" ), F . col ( \"p\" ) . alias ( \"countpct_source\" ) ), \"bin_idx\" , \"full_outer\" , ) . orderBy ( \"bin_idx\" ) . toPandas () ) odf_pd . fillna ( { col : \"Missing\" , \"countpct_source\" : 0 , \"countpct_target\" : 0 }, inplace = True ) odf_pd [ \"%_diff\" ] = ( ( odf_pd [ \"countpct_target\" ] / odf_pd [ \"countpct_source\" ]) - 1 ) * 100 fig = go . Figure () fig . add_bar ( y = list ( odf_pd . countpct_source . values ), x = odf_pd [ col ], name = \"source\" , marker = dict ( color = global_theme ), ) fig . update_traces ( overwrite = True , marker = { \"opacity\" : 0.7 }) fig . add_bar ( y = list ( odf_pd . countpct_target . values ), x = odf_pd [ col ], name = \"target\" , text = odf_pd [ \"%_diff\" ] . apply ( lambda x : \" {0:0.2f} %\" . format ( x )), marker = dict ( color = global_theme ), ) fig . update_traces ( textposition = \"outside\" ) fig . update_layout ( paper_bgcolor = global_paper_bg_color , plot_bgcolor = global_plot_bg_color , showlegend = False , ) fig . update_layout ( title_text = str ( \"Drift Comparison for \" + col + \"<br><sup>(L->R : Source->Target)</sup>\" ) ) fig . update_traces ( marker = dict ( color = global_theme )) fig . update_xaxes ( type = \"category\" ) # fig.add_trace(go.Scatter(x=odf_pd[col], y=odf_pd.countpct_target.values, mode='lines+markers', # line=dict(color=px.colors.qualitative.Antique[10], width=3, dash='dot'))) fig . update_layout ( xaxis_tickfont_size = 14 , yaxis = dict ( title = \"frequency\" , titlefont_size = 16 , tickfont_size = 14 ), ) return fig def charts_to_objects ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = None , event_label = 1 , bin_method = \"equal_range\" , bin_size = 10 , coverage = 1.0 , drift_detector = False , outlier_charts = False , source_path = \"NA\" , master_path = \".\" , stats_unique = {}, run_type = \"local\" , auth_key = \"NA\" , ): \"\"\" Parameters ---------- spark Spark Session idf Input dataframe list_of_cols List of columns passed for analysis (Default value = \"all\") drop_cols List of columns dropped from analysis (Default value = []) label_col Label column (Default value = None) event_label Event label (Default value = 1) bin_method Binning method equal_range or equal_frequency (Default value = \"equal_range\") bin_size Maximum bin size categories. Default value is kept as 10 coverage Maximum coverage of categories. Default value is kept as 1.0 (which is 100%) drift_detector True or False as per the availability. Default value is kept as False source_path Source data path. Default value is kept as \"NA\" to save intermediate data in \"intermediate_data/\" folder. master_path Path where the output needs to be saved, ideally the same path where the analyzed data output is also saved (Default value = \".\") stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type local or emr or databricks or ak8s run type. Default value is kept as local auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" Returns ------- \"\"\" global num_cols global cat_cols if list_of_cols == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) list_of_cols = num_cols + cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if stats_unique == {}: remove_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : remove_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) < 2 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + remove_cols )]) ) if any ( x not in idf . columns for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) if cat_cols : idf_cleaned = outlier_categories ( spark , idf , list_of_cols = cat_cols , coverage = coverage , max_category = bin_size ) else : idf_cleaned = idf if source_path == \"NA\" : source_path = \"intermediate_data\" if drift_detector : encoding_model_exists = True binned_cols = ( spark . read . parquet ( source_path + \"/drift_statistics/attribute_binning\" ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) to_be_binned = [ e for e in num_cols if e not in binned_cols ] else : encoding_model_exists = False binned_cols = [] to_be_binned = num_cols if to_be_binned : idf_encoded = attribute_binning ( spark , idf_cleaned , list_of_cols = to_be_binned , method_type = bin_method , bin_size = bin_size , bin_dtype = \"categorical\" , pre_existing_model = False , model_path = source_path + \"/charts_to_objects\" , output_mode = \"append\" , ) else : idf_encoded = idf_cleaned if binned_cols : idf_encoded = attribute_binning ( spark , idf_encoded , list_of_cols = binned_cols , method_type = bin_method , bin_size = bin_size , bin_dtype = \"categorical\" , pre_existing_model = True , model_path = source_path + \"/drift_statistics\" , output_mode = \"append\" , ) cutoffs_path1 = source_path + \"/charts_to_objects/attribute_binning\" cutoffs_path2 = source_path + \"/drift_statistics/attribute_binning\" idf_encoded . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) if run_type == \"local\" : local_path = master_path elif run_type == \"databricks\" : local_path = output_to_local ( master_path ) elif run_type in ( \"emr\" , \"ak8s\" ): local_path = \"report_stats\" else : raise ValueError ( \"Invalid run_type\" ) Path ( local_path ) . mkdir ( parents = True , exist_ok = True ) for idx , col in enumerate ( list_of_cols ): if col in binned_cols : cutoffs_path = cutoffs_path2 else : cutoffs_path = cutoffs_path1 if col in cat_cols : f = plot_frequency ( spark , idf_encoded , col , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"freqDist_\" + col ) if label_col : if col != label_col : f = plot_eventRate ( spark , idf_encoded , col , label_col , event_label , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"eventDist_\" + col ) if drift_detector : try : frequency_path = ( source_path + \"/drift_statistics/frequency_counts/\" + col ) idf_source = spark . read . csv ( frequency_path , header = True , inferSchema = True ) f = plot_comparative_drift ( spark , idf_encoded , idf_source , col , cutoffs_path ) f . write_json ( ends_with ( local_path ) + \"drift_\" + col ) except Exception as e : logger . error ( f \"processing failed during drift detection, error { e } \" ) pass if col in num_cols : if outlier_charts : f = plot_outlier ( spark , idf , col , split_var = None ) f . write_json ( ends_with ( local_path ) + \"outlier_\" + col ) f = plot_frequency ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), col , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"freqDist_\" + col ) if label_col : if col != label_col : f = plot_eventRate ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), col , label_col , event_label , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"eventDist_\" + col ) if drift_detector : try : frequency_path = ( source_path + \"/drift_statistics/frequency_counts/\" + col ) idf_source = spark . read . csv ( frequency_path , header = True , inferSchema = True ) f = plot_comparative_drift ( spark , idf_encoded . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ), idf_source , col , cutoffs_path , ) f . write_json ( ends_with ( local_path ) + \"drift_\" + col ) except Exception as e : logger . error ( f \"processing failed during drift detection, error { e } \" ) pass pd . DataFrame ( idf . dtypes , columns = [ \"attribute\" , \"data_type\" ]) . to_csv ( ends_with ( local_path ) + \"data_type.csv\" , index = False ) if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp --recursive \" + ends_with ( local_path ) + \" \" + ends_with ( master_path ) ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) if run_type == \"ak8s\" : output_path_mod = path_ak8s_modify ( master_path ) bash_cmd = ( 'azcopy cp \"' + ends_with ( local_path ) + '\" \"' + ends_with ( output_path_mod ) + str ( auth_key ) + '\" --recursive=true' ) subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ])","title":"report_preprocessing"},{"location":"api/data_report/report_preprocessing.html#functions","text":"def binRange_to_binIdx ( spark, col, cutoffs_path)","title":"Functions"},{"location":"api/data_transformer/_index.html","text":"Overview Sub-modules anovos.data_transformer.datetime Datetime module supports various transformations related to date and timestamp datatype columns. All available functions in this release can be \u2026 anovos.data_transformer.geo_utils anovos.data_transformer.geospatial The geospatial module supports transformation & calculation functions for geospatial fields, such as transforming between different formats, \u2026 anovos.data_transformer.transformers The data transformer module supports selected pre-processing & transformation functions, such as binning, encoding, scaling, imputation, to name a \u2026","title":"Overview"},{"location":"api/data_transformer/_index.html#overview","text":"","title":"Overview"},{"location":"api/data_transformer/_index.html#sub-modules","text":"anovos.data_transformer.datetime Datetime module supports various transformations related to date and timestamp datatype columns. All available functions in this release can be \u2026 anovos.data_transformer.geo_utils anovos.data_transformer.geospatial The geospatial module supports transformation & calculation functions for geospatial fields, such as transforming between different formats, \u2026 anovos.data_transformer.transformers The data transformer module supports selected pre-processing & transformation functions, such as binning, encoding, scaling, imputation, to name a \u2026","title":"Sub-modules"},{"location":"api/data_transformer/datetime.html","text":"datetime Datetime module supports various transformations related to date and timestamp datatype columns. All available functions in this release can be classified into the following 4 categories: Conversion: - Between Timestamp and Epoch (timestamp_to_unix and unix_to_timestamp) - Between Timestamp and String (timestamp_to_string and string_to_timestamp) - Between Date Formats (dateformat_conversion) - Between Time Zones (timezone_conversion) Calculation: - Time difference - [Timestamp 1 - Timestamp 2] (time_diff) - Time elapsed - [Current - Given Timestamp] (time_elapsed) - Adding/subtracting time units (adding_timeUnits) - Aggregate features at X granularity level (aggregator) - Aggregate features with window frame (window_aggregator) - Lagged features - lagged date and time diff from the lagged date (lagged_ts) Extraction: - Time component extraction (timeUnits_extraction) - Start/end of month/year/quarter (start_of_month, end_of_month, start_of_year, end_of_year, start_of_quarter and end_of_quarter) Binary features: - Timestamp comparison (timestamp_comparison) - Is start/end of month/year/quarter nor not (is_monthStart, is_monthEnd, is_yearStart, is_yearEnd, is_quarterStart, is_quarterEnd) - Is first half of the year/selected hours/leap year/weekend or not (is_yearFirstHalf, is_selectedHour, is_leapYear and is_weekend) Expand source code \"\"\" Datetime module supports various transformations related to date and timestamp datatype columns. All available functions in this release can be classified into the following 4 categories: Conversion: - Between Timestamp and Epoch (timestamp_to_unix and unix_to_timestamp) - Between Timestamp and String (timestamp_to_string and string_to_timestamp) - Between Date Formats (dateformat_conversion) - Between Time Zones (timezone_conversion) Calculation: - Time difference - [Timestamp 1 - Timestamp 2] (time_diff) - Time elapsed - [Current - Given Timestamp] (time_elapsed) - Adding/subtracting time units (adding_timeUnits) - Aggregate features at X granularity level (aggregator) - Aggregate features with window frame (window_aggregator) - Lagged features - lagged date and time diff from the lagged date (lagged_ts) Extraction: - Time component extraction (timeUnits_extraction) - Start/end of month/year/quarter (start_of_month, end_of_month, start_of_year, end_of_year, start_of_quarter and end_of_quarter) Binary features: - Timestamp comparison (timestamp_comparison) - Is start/end of month/year/quarter nor not (is_monthStart, is_monthEnd, is_yearStart, is_yearEnd, is_quarterStart, is_quarterEnd) - Is first half of the year/selected hours/leap year/weekend or not (is_yearFirstHalf, is_selectedHour, is_leapYear and is_weekend) \"\"\" import calendar import warnings from datetime import datetime as dt import pytz from pyspark.sql import Window from pyspark.sql import functions as F from pyspark.sql import types as T def argument_checker ( func_name , args ): \"\"\" Parameters ---------- func_name function name for which argument needs to be check args arguments to check in dictionary format Returns ------- List list of columns to analyze \"\"\" list_of_cols = args [ \"list_of_cols\" ] all_columns = args [ \"all_columns\" ] if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if any ( x not in all_columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No timestamp conversion - No column(s) to convert\" ) return [] if func_name not in [ \"aggregator\" ]: if args [ \"output_mode\" ] not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if func_name in [ \"timestamp_to_unix\" , \"unix_to_timestamp\" ]: if args [ \"precision\" ] not in ( \"ms\" , \"s\" ): raise TypeError ( \"Invalid input for precision\" ) if args [ \"tz\" ] not in ( \"local\" , \"gmt\" , \"utc\" ): raise TypeError ( \"Invalid input for timezone\" ) if func_name in [ \"string_to_timestamp\" ]: if args [ \"output_type\" ] not in ( \"ts\" , \"dt\" ): raise TypeError ( \"Invalid input for output_type\" ) if func_name in [ \"timeUnits_extraction\" ]: if any ( x not in args [ \"all_units\" ] for x in args [ \"units\" ]): raise TypeError ( \"Invalid input for Unit(s)\" ) if func_name in [ \"adding_timeUnits\" ]: if args [ \"unit\" ] not in ( args [ \"all_units\" ] + [( e + \"s\" ) for e in args [ \"all_units\" ]] ): raise TypeError ( \"Invalid input for Unit\" ) if func_name in [ \"timestamp_comparison\" ]: if args [ \"comparison_type\" ] not in args [ \"all_types\" ]: raise TypeError ( \"Invalid input for comparison_type\" ) if func_name in [ \"is_selectedHour\" ]: hours = list ( range ( 0 , 24 )) if args [ \"start_hour\" ] not in hours : raise TypeError ( \"Invalid input for start_hour\" ) if args [ \"end_hour\" ] not in hours : raise TypeError ( \"Invalid input for end_hour\" ) if func_name in [ \"window_aggregator\" ]: if any ( x not in args [ \"all_aggs\" ] for x in args [ \"list_of_aggs\" ]): raise TypeError ( \"Invalid input for Aggregate Function(s)\" ) if args [ \"window_type\" ] not in ( \"expanding\" , \"rolling\" ): raise TypeError ( \"Invalid input for Window Type\" ) if ( args [ \"window_type\" ] == \"rolling\" ) & ( not str ( args [ \"window_size\" ]) . isnumeric () ): raise TypeError ( \"Invalid input for Window Size\" ) if func_name in [ \"aggregator\" ]: if any ( x not in args [ \"all_aggs\" ] for x in args [ \"list_of_aggs\" ]): raise TypeError ( \"Invalid input for Aggregate Function(s)\" ) if args [ \"time_col\" ] not in all_columns : raise TypeError ( \"Invalid input for time_col\" ) if func_name in [ \"lagged_ts\" ]: if not str ( args [ \"lag\" ]) . isnumeric (): raise TypeError ( \"Invalid input for Lag\" ) if args [ \"output_type\" ] not in ( \"ts\" , \"ts_diff\" ): raise TypeError ( \"Invalid input for output_type\" ) return list_of_cols def timestamp_to_unix ( spark , idf , list_of_cols , precision = \"s\" , tz = \"local\" , output_mode = \"replace\" ): \"\"\" Convert timestamp columns in a specified time zone to Unix time stamp in seconds or milliseconds. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" option returns the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" option returns the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". Timezone of the input column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_unix\" e.g. column X is appended as X_unix. (Default value = \"replace\") Returns ------- DataFrame \"\"\" tz = tz . lower () list_of_cols = argument_checker ( \"timestamp_to_unix\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"precision\" : precision , \"tz\" : tz , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) factor = { \"ms\" : 1000 , \"s\" : 1 } odf = idf for i in list_of_cols : if ( tz in ( \"gmt\" , \"utc\" )) & ( localtz . lower () not in ( \"gmt\" , \"utc\" )): odf = odf . withColumn ( i + \"_local\" , F . from_utc_timestamp ( i , localtz )) else : odf = odf . withColumn ( i + \"_local\" , F . col ( i )) modify_col = { \"replace\" : i , \"append\" : i + \"_unix\" } odf = odf . withColumn ( modify_col [ output_mode ], ( F . col ( i + \"_local\" ) . cast ( T . TimestampType ()) . cast ( \"double\" ) * factor [ precision ] ) . cast ( \"long\" ), ) . drop ( i + \"_local\" ) return odf def unix_to_timestamp ( spark , idf , list_of_cols , precision = \"s\" , tz = \"local\" , output_mode = \"replace\" ): \"\"\" Convert the number of seconds or milliseconds from unix epoch (1970-01-01 00:00:00 UTC) to a timestamp column in the specified time zone. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" treats the input columns as the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" treats the input columns as the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". timezone of the output column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" tz = tz . lower () list_of_cols = argument_checker ( \"unix_to_timestamp\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"precision\" : precision , \"tz\" : tz , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) factor = { \"ms\" : 1000 , \"s\" : 1 } odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_ts\" } odf = odf . withColumn ( modify_col [ output_mode ], F . to_timestamp ( F . col ( i ) / factor [ precision ]) ) if ( tz in ( \"gmt\" , \"utc\" )) & ( localtz . lower () not in ( \"gmt\" , \"utc\" )): odf = odf . withColumn ( modify_col [ output_mode ], F . to_utc_timestamp ( modify_col [ output_mode ], localtz ), ) return odf def timezone_conversion ( spark , idf , list_of_cols , given_tz , output_tz , output_mode = \"replace\" ): \"\"\" Convert timestamp columns from the given timezone (given_tz) to the output timezone (output_tz). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". given_tz Timezone of the input column(s). If \"local\", the timezone of the spark session will be used. output_tz Timezone of the output column(s). If \"local\", the timezone of the spark session will be used. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_tzconverted\" e.g. column X is appended as X_tzconverted. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"timezone_conversion\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf given_tz = given_tz . upper () output_tz = output_tz . upper () localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) if given_tz == \"LOCAL\" : given_tz = localtz if output_tz == \"LOCAL\" : output_tz = localtz odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_tzconverted\" } odf = odf . withColumn ( modify_col [ output_mode ], F . from_utc_timestamp ( F . to_utc_timestamp ( i , given_tz ), output_tz ), ) return odf def string_to_timestamp ( spark , idf , list_of_cols , input_format = \"%Y-%m- %d %H:%M:%S\" , output_type = \"ts\" , output_mode = \"replace\" , ): \"\"\" Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to TimestampType or DateType columns. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_type \"ts\", \"dt\" \"ts\" option returns result in T.TimestampType() \"dt\" option returns result in T.DateType() (Default value = \"ts\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"string_to_timestamp\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"output_type\" : output_type , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) def conversion ( col , form ): if col is None : return None output = pytz . timezone ( localtz ) . localize ( dt . strptime ( str ( col ), form )) return output data_type = { \"ts\" : T . TimestampType (), \"dt\" : T . DateType ()} f_conversion = F . udf ( conversion , data_type [ output_type ]) odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_ts\" } odf = odf . withColumn ( modify_col [ output_mode ], f_conversion ( F . col ( i ), F . lit ( input_format )) ) return odf def timestamp_to_string ( spark , idf , list_of_cols , output_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"replace\" ): \"\"\" Convert timestamp/date columns to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default) Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". Columns must be of Datetime type or String type in \"%Y-%m-%d %H:%M:%S\" format. output_format Format of the output column(s) (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_str\" e.g. column X is appended as X_str. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"timestamp_to_string\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) def conversion ( col , form ): if col is None : return None output = col . astimezone ( pytz . timezone ( localtz )) . strftime ( form ) return output f_conversion = F . udf ( conversion , T . StringType ()) odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_str\" } odf = odf . withColumn ( modify_col [ output_mode ], f_conversion ( F . col ( i ) . cast ( T . TimestampType ()), F . lit ( output_format )), ) return odf def dateformat_conversion ( spark , idf , list_of_cols , input_format = \"%Y-%m- %d %H:%M:%S\" , output_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"replace\" , ): \"\"\" Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_format Format of the output column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"dateformat_conversion\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf_tmp = string_to_timestamp ( spark , idf , list_of_cols , input_format = input_format , output_type = \"ts\" , output_mode = output_mode , ) appended_cols = { \"append\" : [ col + \"_ts\" for col in list_of_cols ], \"replace\" : list_of_cols , } odf = timestamp_to_string ( spark , odf_tmp , appended_cols [ output_mode ], output_format = output_format , output_mode = \"replace\" , ) return odf def timeUnits_extraction ( idf , list_of_cols , units , output_mode = \"append\" ): \"\"\" Extract the unit(s) of given timestamp columns as integer. Currently the following units are supported: hour, minute, second, dayofmonth, dayofweek, dayofyear, weekofyear, month, quarter, year. Multiple units can be calculated at the same time by inputting a list of units or a string of units separated by pipe delimiter \u201c|\u201d. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". units List of unit(s) to extract. Alternatively, unit(s) can be specified in a string format, where different units are separated by pipe delimiter \u201c|\u201d e.g., \"hour|minute\". Supported units to extract: \"hour\", \"minute\", \"second\",\"dayofmonth\",\"dayofweek\", \"dayofyear\",\"weekofyear\",\"month\",\"quarter\",\"year\". \"all\" can be passed to compute all supported metrics. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_<unit>\", e.g. column X is replaced with X_second for units=\"second\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_<unit>\", e.g. column X is appended as X_second for units=\"second\". (Default value = \"append\") Returns ------- DataFrame \"\"\" all_units = [ \"hour\" , \"minute\" , \"second\" , \"dayofmonth\" , \"dayofweek\" , \"dayofyear\" , \"weekofyear\" , \"month\" , \"quarter\" , \"year\" , ] if units == \"all\" : units = all_units if isinstance ( units , str ): units = [ x . strip () for x in units . split ( \"|\" )] list_of_cols = argument_checker ( \"timeUnits_extraction\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"units\" : units , \"all_units\" : all_units , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : for e in units : func = getattr ( F , e ) odf = odf . withColumn ( i + \"_\" + e , func ( i )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def time_diff ( idf , ts1 , ts2 , unit , output_mode = \"append\" ): \"\"\" Calculate the time difference between 2 timestamp columns (Timestamp 1 - Timestamp 2) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe ts1 First column to calculate the difference ts2 Second column to calculate the difference. unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column <ts1>_<ts2>_<unit>diff, e.g. Given ts1=X, ts2=Y , X and Y are replaced with X_Y_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with name = <ts1>_<ts2>_<unit>diff, e.g. Given ts1=X, ts2=Y, X_Y_daydiff is appended for unit=\"day\". (Default value = \"append\") Returns ------- DataFrame \"\"\" argument_checker ( \"time_diff\" , { \"list_of_cols\" : [ ts1 , ts2 ], \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) factor_mapping = { \"second\" : 1 , \"minute\" : 60 , \"hour\" : 3600 , \"day\" : 86400 , \"week\" : 604800 , \"month\" : 2628000 , \"year\" : 31536000 , } if unit in factor_mapping . keys (): factor = factor_mapping [ unit ] elif unit in [( e + \"s\" ) for e in factor_mapping . keys ()]: unit = unit [: - 1 ] factor = factor_mapping [ unit ] else : raise TypeError ( \"Invalid input of unit\" ) odf = idf . withColumn ( ts1 + \"_\" + ts2 + \"_\" + unit + \"diff\" , F . abs ( ( F . col ( ts1 ) . cast ( T . TimestampType ()) . cast ( \"double\" ) - F . col ( ts2 ) . cast ( T . TimestampType ()) . cast ( \"double\" ) ) ) / factor , ) if output_mode == \"replace\" : odf = odf . drop ( ts1 , ts2 ) return odf def time_elapsed ( idf , list_of_cols , unit , output_mode = \"append\" ): \"\"\" Calculate time difference between the current and the given timestamp (Current - Given Timestamp) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_<unit>diff\", e.g. column X is replaced with X_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_<unit>diff\", e.g. column X is appended as X_daydiff for unit=\"day\". (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"time_elapsed\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf factor_mapping = { \"second\" : 1 , \"minute\" : 60 , \"hour\" : 3600 , \"day\" : 86400 , \"week\" : 604800 , \"month\" : 2628000 , \"year\" : 31536000 , } if unit in factor_mapping . keys (): factor = factor_mapping [ unit ] elif unit in [( e + \"s\" ) for e in factor_mapping . keys ()]: unit = unit [: - 1 ] factor = factor_mapping [ unit ] else : raise TypeError ( \"Invalid input of unit\" ) odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_\" + unit + \"diff\" , F . abs ( ( F . lit ( F . current_timestamp ()) . cast ( \"double\" ) - F . col ( i ) . cast ( T . TimestampType ()) . cast ( \"double\" ) ) ) / factor , ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def adding_timeUnits ( idf , list_of_cols , unit , unit_value , output_mode = \"append\" ): \"\"\" Add or subtract given time units to/from timestamp columns. Currently the following units are supported: second, minute, hour, day, week, month, year. Subtraction can be performed by setting a negative unit_value. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the added value. unit_value The value to be added to input column(s). output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_adjusted\", e.g. column X is replaced with X_adjusted. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_adjusted\", e.g. column X is appended as X_adjusted. (Default value = \"append\") Returns ------- DataFrame \"\"\" all_units = [ \"hour\" , \"minute\" , \"second\" , \"day\" , \"week\" , \"month\" , \"year\" ] list_of_cols = argument_checker ( \"adding_timeUnits\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"unit\" : unit , \"all_units\" : all_units , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_adjusted\" , F . col ( i ) . cast ( T . TimestampType ()) + F . expr ( \"Interval \" + str ( unit_value ) + \" \" + unit ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def timestamp_comparison ( spark , idf , list_of_cols , comparison_type , comparison_value , comparison_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"append\" , ): \"\"\" Compare timestamp columns with a given timestamp/date value (comparison_value) of given format ( comparison_format). Supported comparison types include greater_than, less_than, greaterThan_equalTo and lessThan_equalTo. The derived values are 1 if True and 0 if False. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". comparison_type greater_than\", \"less_than\", \"greaterThan_equalTo\", \"lessThan_equalTo\" The comparison type of the transformation. comparison_value The timestamp / date value to compare with in string. comparison_format The format of comparison_value in string. (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_compared\", e.g. column X is replaced with X_compared. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_compared\", e.g. column X is appended as X_compared. (Default value = \"append\") Returns ------- DataFrame \"\"\" all_types = [ \"greater_than\" , \"less_than\" , \"greaterThan_equalTo\" , \"lessThan_equalTo\" ] list_of_cols = argument_checker ( \"timestamp_comparison\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"comparison_type\" : comparison_type , \"all_types\" : all_types , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) base_ts = pytz . timezone ( localtz ) . localize ( dt . strptime ( comparison_value , comparison_format ) ) odf = idf for i in list_of_cols : if comparison_type == \"greater_than\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) > F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) elif comparison_type == \"less_than\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) < F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) elif comparison_type == \"greaterThan_equalTo\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) >= F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) else : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) <= F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_month ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the month of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthStart\", e.g. column X is appended as X_monthStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_month\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_monthStart\" , F . trunc ( i , \"month\" )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_monthStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a month. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthStart\", e.g. column X is appended as X_ismonthStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_monthStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_month ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_ismonthStart\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_monthStart\" ), 1 ) . otherwise ( F . when ( F . col ( i ) . isNull (), None ) . otherwise ( 0 ) ) ), ) . drop ( i + \"_monthStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_month ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the month of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthEnd\", e.g. column X is appended as X_monthEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_month\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_monthEnd\" , F . last_day ( i )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_monthEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a month. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthEnd\", e.g. column X is appended as X_ismonthEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_monthEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_month ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_ismonthEnd\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_monthEnd\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_monthEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_year ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the year of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearStart\", e.g. column X is appended as X_yearStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_year\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_yearStart\" , F . trunc ( i , \"year\" )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearStart\", e.g. column X is appended as X_isyearStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_year ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isyearStart\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_yearStart\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_yearStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_year ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the year of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearEnd\", e.g. column X is appended as X_yearEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_year\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_yearEnd\" , F . concat_ws ( \"-\" , F . year ( i ), F . lit ( 12 ), F . lit ( 31 )) . cast ( \"date\" ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearEnd\", e.g. column X is appended as X_isyearEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_year ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isyearEnd\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_yearEnd\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_yearEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_quarter ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the quarter of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterStart. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterStart\", e.g. column X is appended as X_quarterStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_quarter\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_quarterStart\" , F . to_date ( F . date_trunc ( \"quarter\" , i ))) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_quarterStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a quarter. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterStart\", e.g. column X is appended as X_isquarterStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_quarterStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_quarter ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isquarterStart\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_quarterStart\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_quarterStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_quarter ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the quarter of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterEnd\", e.g. column X is appended as X_quarterEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_quarter\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_quarterEnd\" , F . to_date ( F . date_trunc ( \"quarter\" , i )) + F . expr ( \"Interval 3 months\" ) + F . expr ( \"Interval -1 day\" ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_quarterEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a quarter. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterEnd\", e.g. column X is appended as X_isquarterEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_quarterEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_quarter ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isquarterEnd\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_quarterEnd\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_quarterEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearFirstHalf ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are in the first half of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isFirstHalf\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isFirstHalf\", e.g. column X is appended as X_isFirstHalf. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearFirstHalf\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isFirstHalf\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . month ( F . col ( i )) . isin ( * range ( 1 , 7 )), 1 ) . otherwise ( 0 ) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_selectedHour ( idf , list_of_cols , start_hour , end_hour , output_mode = \"append\" ): \"\"\" Check if the hour component of given timestamp columns are between start hour (inclusive) and end hour ( inclusive). The derived values are 1 if True and 0 if False. Start hour can be larger than end hour, for example, start_hour=22 and end_hour=3 can be used to check whether the hour component is in [22, 23, 0, 1, 2, 3]. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". start_hour The starting hour of the hour range (inclusive) end_hour The ending hour of the hour range (inclusive) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isselectedHour\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isselectedHour\", e.g. column X is appended as X_isselectedHour. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_selectedHour\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"start_hour\" : start_hour , \"end_hour\" : end_hour , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf if start_hour < end_hour : list_of_hrs = range ( start_hour , end_hour + 1 ) elif start_hour > end_hour : list_of_hrs = list ( range ( start_hour , 24 )) + list ( range ( 0 , end_hour + 1 )) else : list_of_hrs = [ start_hour ] for i in list_of_cols : odf = odf . withColumn ( i + \"_isselectedHour\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . hour ( F . col ( i )) . isin ( * list_of_hrs ), 1 ) . otherwise ( 0 ) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_leapYear ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are in a leap year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isleapYear\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isleapYear\", e.g. column X is appended as X_isleapYear. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_leapYear\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf def check ( year ): if year is None : return None if calendar . isleap ( year ): return 1 else : return 0 f_check = F . udf ( check , T . IntegerType ()) odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isleapYear\" , f_check ( F . year ( i ))) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_weekend ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are on weekends. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isweekend\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isweekend\", e.g. column X is appended as X_isweekend. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_weekend\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isweekend\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . dayofweek ( F . col ( i )) . isin ([ 1 , 7 ]), 1 ) . otherwise ( 0 ) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def aggregator ( spark , idf , list_of_cols , list_of_aggs , time_col , granularity_format = \"%Y-%m- %d \" ): \"\"\" aggregator performs groupBy over the timestamp/date column and calcuates a list of aggregate metrics over all input columns. The timestamp column is firstly converted to the given granularity format (\"%Y-%m-%d\", by default) before applying groupBy and the conversion step can be skipped by setting granularity format to be an empty string. The following aggregate metrics are supported: count, min, max, sum, mean, median, stddev, countDistinct, sumDistinct, collect_list, collect_set. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\",\"mean\",\"median\",\"stddev\", \"countDistinct\",\"sumDistinct\",\"collect_list\",\"collect_set\". time_col Timestamp) Column to group by. granularity_format Format to be applied to time_col before groupBy. The default value is '%Y-%m-%d', which means grouping by the date component of time_col. Alternatively, '' can be used if no formatting is necessary. Returns ------- DataFrame \"\"\" all_aggs = [ \"count\" , \"min\" , \"max\" , \"sum\" , \"mean\" , \"median\" , \"stddev\" , \"countDistinct\" , \"sumDistinct\" , \"collect_list\" , \"collect_set\" , ] if isinstance ( list_of_aggs , str ): list_of_aggs = [ x . strip () for x in list_of_aggs . split ( \"|\" )] list_of_cols = argument_checker ( \"aggregator\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"list_of_aggs\" : list_of_aggs , \"all_aggs\" : all_aggs , \"time_col\" : time_col , }, ) if not list_of_cols : return idf if granularity_format != \"\" : idf = timestamp_to_string ( spark , idf , time_col , output_format = granularity_format , output_mode = \"replace\" , ) def agg_funcs ( col , agg ): mapping = { \"count\" : F . count ( col ) . alias ( col + \"_count\" ), \"min\" : F . min ( col ) . alias ( col + \"_min\" ), \"max\" : F . max ( col ) . alias ( col + \"_max\" ), \"sum\" : F . sum ( col ) . alias ( col + \"_sum\" ), \"mean\" : F . mean ( col ) . alias ( col + \"_mean\" ), \"median\" : F . expr ( \"percentile_approx(\" + col + \", 0.5)\" ) . alias ( col + \"_median\" ), \"stddev\" : F . stddev ( col ) . alias ( col + \"_stddev\" ), \"countDistinct\" : F . countDistinct ( col ) . alias ( col + \"_countDistinct\" ), \"sumDistinct\" : F . sumDistinct ( col ) . alias ( col + \"_sumDistinct\" ), \"collect_list\" : F . collect_list ( col ) . alias ( col + \"_collect_list\" ), \"collect_set\" : F . collect_set ( col ) . alias ( col + \"_collect_set\" ), } return mapping [ agg ] derived_cols = [] for i in list_of_cols : for j in list_of_aggs : derived_cols . append ( agg_funcs ( i , j )) odf = idf . groupBy ( time_col ) . agg ( * derived_cols ) return odf def window_aggregator ( idf , list_of_cols , list_of_aggs , order_col , window_type = \"expanding\" , window_size = \"unbounded\" , partition_col = \"\" , output_mode = \"append\" , ): \"\"\" window_aggregator calcuates a list of aggregate metrics for all input columns over a window frame (expanding by default, or rolling type) ordered by the given timestamp column and partitioned by partition_col (\"\" by default, to indicate no partition). Window size needs to be provided as an integer for rolling window type. The following aggregate metrics are supported: count, min, max, sum, mean, median. Parameters ---------- idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\", \"mean\", \"median\" order_col Timestamp Column to order window window_type \"expanding\", \"rolling\" \"expanding\" option has a fixed lower bound (first row in the partition) \"rolling\" option has a fixed window size defined by window_size param (Default value = \"expanding\") window_size window size for rolling window type. Integer value with value >= 1. (Default value = \"unbounded\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column(s) with metric name as postfix. \u201cappend\u201d option appends derived column(s) to the input dataset with metric name as postfix, e.g. \"_count\", \"_mean\". (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_aggs , str ): list_of_aggs = [ x . strip () for x in list_of_aggs . split ( \"|\" )] all_aggs = [ \"count\" , \"min\" , \"max\" , \"sum\" , \"mean\" , \"median\" ] list_of_cols = argument_checker ( \"window_aggregator\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"list_of_aggs\" : list_of_aggs , \"all_aggs\" : all_aggs , \"output_mode\" : output_mode , \"window_type\" : window_type , \"window_size\" : window_size , }, ) if not list_of_cols : return idf odf = idf window_upper = ( Window . unboundedPreceding if window_type == \"expanding\" else - int ( window_size ) ) if partition_col : window = ( Window . partitionBy ( partition_col ) . orderBy ( order_col ) . rowsBetween ( window_upper , 0 ) ) else : window = Window . partitionBy () . orderBy ( order_col ) . rowsBetween ( window_upper , 0 ) def agg_funcs ( col ): mapping = { \"count\" : F . count ( col ) . over ( window ) . alias ( col + \"_count\" ), \"min\" : F . min ( col ) . over ( window ) . alias ( col + \"_min\" ), \"max\" : F . max ( col ) . over ( window ) . alias ( col + \"_max\" ), \"sum\" : F . sum ( col ) . over ( window ) . alias ( col + \"_sum\" ), \"mean\" : F . mean ( col ) . over ( window ) . alias ( col + \"_mean\" ), \"median\" : F . expr ( \"percentile_approx(\" + col + \", 0.5)\" ) . over ( window ) . alias ( col + \"_median\" ), } derived_cols = [] for agg in list_of_aggs : derived_cols . append ( mapping [ agg ]) return derived_cols for i in list_of_cols : derived_cols = agg_funcs ( i ) odf = odf . select ( odf . columns + derived_cols ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def lagged_ts ( idf , list_of_cols , lag , output_type = \"ts\" , tsdiff_unit = \"days\" , partition_col = \"\" , output_mode = \"append\" , ): \"\"\" lagged_ts returns the values that are *lag* rows before the current rows, and None if there is less than *lag* rows before the current rows. If output_type is \"ts_diff\", an additional column is generated with values being the time difference between the original timestamp and the lagged timestamp in given unit *tsdiff_unit*. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". lag Number of row(s) to extend. output_type \"ts\", \"ts_diff\". \"ts\" option generats a lag column for each input column having the value that is <lag> rows before the current row, and None if there is less than <lag> rows before the current row. \"ts_diff\" option generates the lag column in the same way as the \"ts\" option. On top of that, it appends a column which represents the time_diff between the original and the lag column. (Default value = \"ts\") tsdiff_unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the time_diff if output_type=\"ts_diff\". (Default value = \"days\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column: <col>_lag<lag> for \"ts\" output_type, <col>_<col>_lag<lag>_<tsdiff_unit>diff for \"ts_diff\" output_type. \u201cappend\u201d option appends derived column to the input dataset, e.g. given output_type=\"ts_diff\", lag=5, tsdiff_unit=\"days\", column X is appended as X_X_lag5_daydiff. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"lagged_ts\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"lag\" : lag , \"output_type\" : output_type , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : if partition_col : window = Window . partitionBy ( partition_col ) . orderBy ( i ) else : window = Window . partitionBy () . orderBy ( i ) lag = int ( lag ) odf = odf . withColumn ( i + \"_lag\" + str ( lag ), F . lag ( F . col ( i ), lag ) . over ( window )) if output_type == \"ts_diff\" : odf = time_diff ( odf , i , i + \"_lag\" + str ( lag ), unit = tsdiff_unit , output_mode = \"append\" ) . drop ( i + \"_lag\" + str ( lag )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf Functions def adding_timeUnits ( idf, list_of_cols, unit, unit_value, output_mode='append') Add or subtract given time units to/from timestamp columns. Currently the following units are supported: second, minute, hour, day, week, month, year. Subtraction can be performed by setting a negative unit_value. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the added value. unit_value The value to be added to input column(s). output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_adjusted\", e.g. column X is replaced with X_adjusted. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_adjusted\", e.g. column X is appended as X_adjusted. (Default value = \"append\") Returns DataFrame Expand source code def adding_timeUnits ( idf , list_of_cols , unit , unit_value , output_mode = \"append\" ): \"\"\" Add or subtract given time units to/from timestamp columns. Currently the following units are supported: second, minute, hour, day, week, month, year. Subtraction can be performed by setting a negative unit_value. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the added value. unit_value The value to be added to input column(s). output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_adjusted\", e.g. column X is replaced with X_adjusted. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_adjusted\", e.g. column X is appended as X_adjusted. (Default value = \"append\") Returns ------- DataFrame \"\"\" all_units = [ \"hour\" , \"minute\" , \"second\" , \"day\" , \"week\" , \"month\" , \"year\" ] list_of_cols = argument_checker ( \"adding_timeUnits\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"unit\" : unit , \"all_units\" : all_units , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_adjusted\" , F . col ( i ) . cast ( T . TimestampType ()) + F . expr ( \"Interval \" + str ( unit_value ) + \" \" + unit ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def aggregator ( spark, idf, list_of_cols, list_of_aggs, time_col, granularity_format='%Y-%m-%d') aggregator performs groupBy over the timestamp/date column and calcuates a list of aggregate metrics over all input columns. The timestamp column is firstly converted to the given granularity format (\"%Y-%m-%d\", by default) before applying groupBy and the conversion step can be skipped by setting granularity format to be an empty string. The following aggregate metrics are supported: count, min, max, sum, mean, median, stddev, countDistinct, sumDistinct, collect_list, collect_set. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\",\"mean\",\"median\",\"stddev\", \"countDistinct\",\"sumDistinct\",\"collect_list\",\"collect_set\". time_col Timestamp) Column to group by. granularity_format Format to be applied to time_col before groupBy. The default value is '%Y-%m-%d', which means grouping by the date component of time_col. Alternatively, '' can be used if no formatting is necessary. Returns DataFrame Expand source code def aggregator ( spark , idf , list_of_cols , list_of_aggs , time_col , granularity_format = \"%Y-%m- %d \" ): \"\"\" aggregator performs groupBy over the timestamp/date column and calcuates a list of aggregate metrics over all input columns. The timestamp column is firstly converted to the given granularity format (\"%Y-%m-%d\", by default) before applying groupBy and the conversion step can be skipped by setting granularity format to be an empty string. The following aggregate metrics are supported: count, min, max, sum, mean, median, stddev, countDistinct, sumDistinct, collect_list, collect_set. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\",\"mean\",\"median\",\"stddev\", \"countDistinct\",\"sumDistinct\",\"collect_list\",\"collect_set\". time_col Timestamp) Column to group by. granularity_format Format to be applied to time_col before groupBy. The default value is '%Y-%m-%d', which means grouping by the date component of time_col. Alternatively, '' can be used if no formatting is necessary. Returns ------- DataFrame \"\"\" all_aggs = [ \"count\" , \"min\" , \"max\" , \"sum\" , \"mean\" , \"median\" , \"stddev\" , \"countDistinct\" , \"sumDistinct\" , \"collect_list\" , \"collect_set\" , ] if isinstance ( list_of_aggs , str ): list_of_aggs = [ x . strip () for x in list_of_aggs . split ( \"|\" )] list_of_cols = argument_checker ( \"aggregator\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"list_of_aggs\" : list_of_aggs , \"all_aggs\" : all_aggs , \"time_col\" : time_col , }, ) if not list_of_cols : return idf if granularity_format != \"\" : idf = timestamp_to_string ( spark , idf , time_col , output_format = granularity_format , output_mode = \"replace\" , ) def agg_funcs ( col , agg ): mapping = { \"count\" : F . count ( col ) . alias ( col + \"_count\" ), \"min\" : F . min ( col ) . alias ( col + \"_min\" ), \"max\" : F . max ( col ) . alias ( col + \"_max\" ), \"sum\" : F . sum ( col ) . alias ( col + \"_sum\" ), \"mean\" : F . mean ( col ) . alias ( col + \"_mean\" ), \"median\" : F . expr ( \"percentile_approx(\" + col + \", 0.5)\" ) . alias ( col + \"_median\" ), \"stddev\" : F . stddev ( col ) . alias ( col + \"_stddev\" ), \"countDistinct\" : F . countDistinct ( col ) . alias ( col + \"_countDistinct\" ), \"sumDistinct\" : F . sumDistinct ( col ) . alias ( col + \"_sumDistinct\" ), \"collect_list\" : F . collect_list ( col ) . alias ( col + \"_collect_list\" ), \"collect_set\" : F . collect_set ( col ) . alias ( col + \"_collect_set\" ), } return mapping [ agg ] derived_cols = [] for i in list_of_cols : for j in list_of_aggs : derived_cols . append ( agg_funcs ( i , j )) odf = idf . groupBy ( time_col ) . agg ( * derived_cols ) return odf def argument_checker ( func_name, args) Parameters func_name function name for which argument needs to be check args arguments to check in dictionary format Returns List list of columns to analyze Expand source code def argument_checker ( func_name , args ): \"\"\" Parameters ---------- func_name function name for which argument needs to be check args arguments to check in dictionary format Returns ------- List list of columns to analyze \"\"\" list_of_cols = args [ \"list_of_cols\" ] all_columns = args [ \"all_columns\" ] if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if any ( x not in all_columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No timestamp conversion - No column(s) to convert\" ) return [] if func_name not in [ \"aggregator\" ]: if args [ \"output_mode\" ] not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if func_name in [ \"timestamp_to_unix\" , \"unix_to_timestamp\" ]: if args [ \"precision\" ] not in ( \"ms\" , \"s\" ): raise TypeError ( \"Invalid input for precision\" ) if args [ \"tz\" ] not in ( \"local\" , \"gmt\" , \"utc\" ): raise TypeError ( \"Invalid input for timezone\" ) if func_name in [ \"string_to_timestamp\" ]: if args [ \"output_type\" ] not in ( \"ts\" , \"dt\" ): raise TypeError ( \"Invalid input for output_type\" ) if func_name in [ \"timeUnits_extraction\" ]: if any ( x not in args [ \"all_units\" ] for x in args [ \"units\" ]): raise TypeError ( \"Invalid input for Unit(s)\" ) if func_name in [ \"adding_timeUnits\" ]: if args [ \"unit\" ] not in ( args [ \"all_units\" ] + [( e + \"s\" ) for e in args [ \"all_units\" ]] ): raise TypeError ( \"Invalid input for Unit\" ) if func_name in [ \"timestamp_comparison\" ]: if args [ \"comparison_type\" ] not in args [ \"all_types\" ]: raise TypeError ( \"Invalid input for comparison_type\" ) if func_name in [ \"is_selectedHour\" ]: hours = list ( range ( 0 , 24 )) if args [ \"start_hour\" ] not in hours : raise TypeError ( \"Invalid input for start_hour\" ) if args [ \"end_hour\" ] not in hours : raise TypeError ( \"Invalid input for end_hour\" ) if func_name in [ \"window_aggregator\" ]: if any ( x not in args [ \"all_aggs\" ] for x in args [ \"list_of_aggs\" ]): raise TypeError ( \"Invalid input for Aggregate Function(s)\" ) if args [ \"window_type\" ] not in ( \"expanding\" , \"rolling\" ): raise TypeError ( \"Invalid input for Window Type\" ) if ( args [ \"window_type\" ] == \"rolling\" ) & ( not str ( args [ \"window_size\" ]) . isnumeric () ): raise TypeError ( \"Invalid input for Window Size\" ) if func_name in [ \"aggregator\" ]: if any ( x not in args [ \"all_aggs\" ] for x in args [ \"list_of_aggs\" ]): raise TypeError ( \"Invalid input for Aggregate Function(s)\" ) if args [ \"time_col\" ] not in all_columns : raise TypeError ( \"Invalid input for time_col\" ) if func_name in [ \"lagged_ts\" ]: if not str ( args [ \"lag\" ]) . isnumeric (): raise TypeError ( \"Invalid input for Lag\" ) if args [ \"output_type\" ] not in ( \"ts\" , \"ts_diff\" ): raise TypeError ( \"Invalid input for output_type\" ) return list_of_cols def dateformat_conversion ( spark, idf, list_of_cols, input_format='%Y-%m-%d %H:%M:%S', output_format='%Y-%m-%d %H:%M:%S', output_mode='replace') Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default). Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_format Format of the output column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns DataFrame Expand source code def dateformat_conversion ( spark , idf , list_of_cols , input_format = \"%Y-%m- %d %H:%M:%S\" , output_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"replace\" , ): \"\"\" Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_format Format of the output column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"dateformat_conversion\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf_tmp = string_to_timestamp ( spark , idf , list_of_cols , input_format = input_format , output_type = \"ts\" , output_mode = output_mode , ) appended_cols = { \"append\" : [ col + \"_ts\" for col in list_of_cols ], \"replace\" : list_of_cols , } odf = timestamp_to_string ( spark , odf_tmp , appended_cols [ output_mode ], output_format = output_format , output_mode = \"replace\" , ) return odf def end_of_month ( idf, list_of_cols, output_mode='append') Extract the last day of the month of given timestamp/date columns. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthEnd\", e.g. column X is appended as X_monthEnd. (Default value = \"append\") Returns DataFrame Expand source code def end_of_month ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the month of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthEnd\", e.g. column X is appended as X_monthEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_month\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_monthEnd\" , F . last_day ( i )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_quarter ( idf, list_of_cols, output_mode='append') Extract the last day of the quarter of given timestamp/date columns. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterEnd\", e.g. column X is appended as X_quarterEnd. (Default value = \"append\") Returns DataFrame Expand source code def end_of_quarter ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the quarter of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterEnd\", e.g. column X is appended as X_quarterEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_quarter\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_quarterEnd\" , F . to_date ( F . date_trunc ( \"quarter\" , i )) + F . expr ( \"Interval 3 months\" ) + F . expr ( \"Interval -1 day\" ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_year ( idf, list_of_cols, output_mode='append') Extract the last day of the year of given timestamp/date columns. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearEnd\", e.g. column X is appended as X_yearEnd. (Default value = \"append\") Returns DataFrame Expand source code def end_of_year ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the year of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearEnd\", e.g. column X is appended as X_yearEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_year\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_yearEnd\" , F . concat_ws ( \"-\" , F . year ( i ), F . lit ( 12 ), F . lit ( 31 )) . cast ( \"date\" ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_leapYear ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are in a leap year. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isleapYear\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isleapYear\", e.g. column X is appended as X_isleapYear. (Default value = \"append\") Returns DataFrame Expand source code def is_leapYear ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are in a leap year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isleapYear\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isleapYear\", e.g. column X is appended as X_isleapYear. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_leapYear\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf def check ( year ): if year is None : return None if calendar . isleap ( year ): return 1 else : return 0 f_check = F . udf ( check , T . IntegerType ()) odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isleapYear\" , f_check ( F . year ( i ))) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_monthEnd ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are the last day of a month. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthEnd\", e.g. column X is appended as X_ismonthEnd. (Default value = \"append\") Returns DataFrame Expand source code def is_monthEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a month. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthEnd\", e.g. column X is appended as X_ismonthEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_monthEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_month ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_ismonthEnd\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_monthEnd\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_monthEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_monthStart ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are the first day of a month. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthStart\", e.g. column X is appended as X_ismonthStart. (Default value = \"append\") Returns DataFrame Expand source code def is_monthStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a month. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthStart\", e.g. column X is appended as X_ismonthStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_monthStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_month ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_ismonthStart\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_monthStart\" ), 1 ) . otherwise ( F . when ( F . col ( i ) . isNull (), None ) . otherwise ( 0 ) ) ), ) . drop ( i + \"_monthStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_quarterEnd ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are the last day of a quarter. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterEnd\", e.g. column X is appended as X_isquarterEnd. (Default value = \"append\") Returns DataFrame Expand source code def is_quarterEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a quarter. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterEnd\", e.g. column X is appended as X_isquarterEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_quarterEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_quarter ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isquarterEnd\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_quarterEnd\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_quarterEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_quarterStart ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are the first day of a quarter. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterStart\", e.g. column X is appended as X_isquarterStart. (Default value = \"append\") Returns DataFrame Expand source code def is_quarterStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a quarter. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterStart\", e.g. column X is appended as X_isquarterStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_quarterStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_quarter ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isquarterStart\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_quarterStart\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_quarterStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_selectedHour ( idf, list_of_cols, start_hour, end_hour, output_mode='append') Check if the hour component of given timestamp columns are between start hour (inclusive) and end hour ( inclusive). The derived values are 1 if True and 0 if False. Start hour can be larger than end hour, for example, start_hour=22 and end_hour=3 can be used to check whether the hour component is in [22, 23, 0, 1, 2, 3]. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". start_hour The starting hour of the hour range (inclusive) end_hour The ending hour of the hour range (inclusive) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isselectedHour\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isselectedHour\", e.g. column X is appended as X_isselectedHour. (Default value = \"append\") Returns DataFrame Expand source code def is_selectedHour ( idf , list_of_cols , start_hour , end_hour , output_mode = \"append\" ): \"\"\" Check if the hour component of given timestamp columns are between start hour (inclusive) and end hour ( inclusive). The derived values are 1 if True and 0 if False. Start hour can be larger than end hour, for example, start_hour=22 and end_hour=3 can be used to check whether the hour component is in [22, 23, 0, 1, 2, 3]. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". start_hour The starting hour of the hour range (inclusive) end_hour The ending hour of the hour range (inclusive) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isselectedHour\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isselectedHour\", e.g. column X is appended as X_isselectedHour. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_selectedHour\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"start_hour\" : start_hour , \"end_hour\" : end_hour , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf if start_hour < end_hour : list_of_hrs = range ( start_hour , end_hour + 1 ) elif start_hour > end_hour : list_of_hrs = list ( range ( start_hour , 24 )) + list ( range ( 0 , end_hour + 1 )) else : list_of_hrs = [ start_hour ] for i in list_of_cols : odf = odf . withColumn ( i + \"_isselectedHour\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . hour ( F . col ( i )) . isin ( * list_of_hrs ), 1 ) . otherwise ( 0 ) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_weekend ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are on weekends. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isweekend\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isweekend\", e.g. column X is appended as X_isweekend. (Default value = \"append\") Returns DataFrame Expand source code def is_weekend ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are on weekends. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isweekend\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isweekend\", e.g. column X is appended as X_isweekend. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_weekend\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isweekend\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . dayofweek ( F . col ( i )) . isin ([ 1 , 7 ]), 1 ) . otherwise ( 0 ) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearEnd ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are the last day of a year. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearEnd\", e.g. column X is appended as X_isyearEnd. (Default value = \"append\") Returns DataFrame Expand source code def is_yearEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearEnd\", e.g. column X is appended as X_isyearEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_year ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isyearEnd\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_yearEnd\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_yearEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearFirstHalf ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are in the first half of a year. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isFirstHalf\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isFirstHalf\", e.g. column X is appended as X_isFirstHalf. (Default value = \"append\") Returns DataFrame Expand source code def is_yearFirstHalf ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are in the first half of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isFirstHalf\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isFirstHalf\", e.g. column X is appended as X_isFirstHalf. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearFirstHalf\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isFirstHalf\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . month ( F . col ( i )) . isin ( * range ( 1 , 7 )), 1 ) . otherwise ( 0 ) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearStart ( idf, list_of_cols, output_mode='append') Check if values in given timestamp/date columns are the first day of a year. The derived values are 1 if True and 0 if False. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearStart\", e.g. column X is appended as X_isyearStart. (Default value = \"append\") Returns DataFrame Expand source code def is_yearStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearStart\", e.g. column X is appended as X_isyearStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_year ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isyearStart\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_yearStart\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_yearStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def lagged_ts ( idf, list_of_cols, lag, output_type='ts', tsdiff_unit='days', partition_col='', output_mode='append') lagged_ts returns the values that are lag rows before the current rows, and None if there is less than lag rows before the current rows. If output_type is \"ts_diff\", an additional column is generated with values being the time difference between the original timestamp and the lagged timestamp in given unit tsdiff_unit . Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". lag Number of row(s) to extend. output_type \"ts\", \"ts_diff\". \"ts\" option generats a lag column for each input column having the value that is rows before the current row, and None if there is less than rows before the current row. \"ts_diff\" option generates the lag column in the same way as the \"ts\" option. On top of that, it appends a column which represents the time_diff between the original and the lag column. (Default value = \"ts\") tsdiff_unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the time_diff if output_type=\"ts_diff\". (Default value = \"days\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column: lag for \"ts\" output_type, lag diff for \"ts_diff\" output_type. \u201cappend\u201d option appends derived column to the input dataset, e.g. given output_type=\"ts_diff\", lag=5, tsdiff_unit=\"days\", column X is appended as X_X_lag5_daydiff. (Default value = \"append\") Returns DataFrame Expand source code def lagged_ts ( idf , list_of_cols , lag , output_type = \"ts\" , tsdiff_unit = \"days\" , partition_col = \"\" , output_mode = \"append\" , ): \"\"\" lagged_ts returns the values that are *lag* rows before the current rows, and None if there is less than *lag* rows before the current rows. If output_type is \"ts_diff\", an additional column is generated with values being the time difference between the original timestamp and the lagged timestamp in given unit *tsdiff_unit*. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". lag Number of row(s) to extend. output_type \"ts\", \"ts_diff\". \"ts\" option generats a lag column for each input column having the value that is <lag> rows before the current row, and None if there is less than <lag> rows before the current row. \"ts_diff\" option generates the lag column in the same way as the \"ts\" option. On top of that, it appends a column which represents the time_diff between the original and the lag column. (Default value = \"ts\") tsdiff_unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the time_diff if output_type=\"ts_diff\". (Default value = \"days\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column: <col>_lag<lag> for \"ts\" output_type, <col>_<col>_lag<lag>_<tsdiff_unit>diff for \"ts_diff\" output_type. \u201cappend\u201d option appends derived column to the input dataset, e.g. given output_type=\"ts_diff\", lag=5, tsdiff_unit=\"days\", column X is appended as X_X_lag5_daydiff. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"lagged_ts\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"lag\" : lag , \"output_type\" : output_type , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : if partition_col : window = Window . partitionBy ( partition_col ) . orderBy ( i ) else : window = Window . partitionBy () . orderBy ( i ) lag = int ( lag ) odf = odf . withColumn ( i + \"_lag\" + str ( lag ), F . lag ( F . col ( i ), lag ) . over ( window )) if output_type == \"ts_diff\" : odf = time_diff ( odf , i , i + \"_lag\" + str ( lag ), unit = tsdiff_unit , output_mode = \"append\" ) . drop ( i + \"_lag\" + str ( lag )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_month ( idf, list_of_cols, output_mode='append') Extract the first day of the month of given timestamp/date columns. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthStart\", e.g. column X is appended as X_monthStart. (Default value = \"append\") Returns DataFrame Expand source code def start_of_month ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the month of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthStart\", e.g. column X is appended as X_monthStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_month\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_monthStart\" , F . trunc ( i , \"month\" )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_quarter ( idf, list_of_cols, output_mode='append') Extract the first day of the quarter of given timestamp/date columns. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterStart. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterStart\", e.g. column X is appended as X_quarterStart. (Default value = \"append\") Returns DataFrame Expand source code def start_of_quarter ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the quarter of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterStart. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterStart\", e.g. column X is appended as X_quarterStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_quarter\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_quarterStart\" , F . to_date ( F . date_trunc ( \"quarter\" , i ))) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_year ( idf, list_of_cols, output_mode='append') Extract the first day of the year of given timestamp/date columns. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearStart\", e.g. column X is appended as X_yearStart. (Default value = \"append\") Returns DataFrame Expand source code def start_of_year ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the year of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearStart\", e.g. column X is appended as X_yearStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_year\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_yearStart\" , F . trunc ( i , \"year\" )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def string_to_timestamp ( spark, idf, list_of_cols, input_format='%Y-%m-%d %H:%M:%S', output_type='ts', output_mode='replace') Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to TimestampType or DateType columns. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_type \"ts\", \"dt\" \"ts\" option returns result in T.TimestampType() \"dt\" option returns result in T.DateType() (Default value = \"ts\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns DataFrame Expand source code def string_to_timestamp ( spark , idf , list_of_cols , input_format = \"%Y-%m- %d %H:%M:%S\" , output_type = \"ts\" , output_mode = \"replace\" , ): \"\"\" Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to TimestampType or DateType columns. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_type \"ts\", \"dt\" \"ts\" option returns result in T.TimestampType() \"dt\" option returns result in T.DateType() (Default value = \"ts\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"string_to_timestamp\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"output_type\" : output_type , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) def conversion ( col , form ): if col is None : return None output = pytz . timezone ( localtz ) . localize ( dt . strptime ( str ( col ), form )) return output data_type = { \"ts\" : T . TimestampType (), \"dt\" : T . DateType ()} f_conversion = F . udf ( conversion , data_type [ output_type ]) odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_ts\" } odf = odf . withColumn ( modify_col [ output_mode ], f_conversion ( F . col ( i ), F . lit ( input_format )) ) return odf def timeUnits_extraction ( idf, list_of_cols, units, output_mode='append') Extract the unit(s) of given timestamp columns as integer. Currently the following units are supported: hour, minute, second, dayofmonth, dayofweek, dayofyear, weekofyear, month, quarter, year. Multiple units can be calculated at the same time by inputting a list of units or a string of units separated by pipe delimiter \u201c|\u201d. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". units List of unit(s) to extract. Alternatively, unit(s) can be specified in a string format, where different units are separated by pipe delimiter \u201c|\u201d e.g., \"hour|minute\". Supported units to extract: \"hour\", \"minute\", \"second\",\"dayofmonth\",\"dayofweek\", \"dayofyear\",\"weekofyear\",\"month\",\"quarter\",\"year\". \"all\" can be passed to compute all supported metrics. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \" \", e.g. column X is replaced with X_second for units=\"second\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \" \", e.g. column X is appended as X_second for units=\"second\". (Default value = \"append\") Returns DataFrame Expand source code def timeUnits_extraction ( idf , list_of_cols , units , output_mode = \"append\" ): \"\"\" Extract the unit(s) of given timestamp columns as integer. Currently the following units are supported: hour, minute, second, dayofmonth, dayofweek, dayofyear, weekofyear, month, quarter, year. Multiple units can be calculated at the same time by inputting a list of units or a string of units separated by pipe delimiter \u201c|\u201d. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". units List of unit(s) to extract. Alternatively, unit(s) can be specified in a string format, where different units are separated by pipe delimiter \u201c|\u201d e.g., \"hour|minute\". Supported units to extract: \"hour\", \"minute\", \"second\",\"dayofmonth\",\"dayofweek\", \"dayofyear\",\"weekofyear\",\"month\",\"quarter\",\"year\". \"all\" can be passed to compute all supported metrics. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_<unit>\", e.g. column X is replaced with X_second for units=\"second\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_<unit>\", e.g. column X is appended as X_second for units=\"second\". (Default value = \"append\") Returns ------- DataFrame \"\"\" all_units = [ \"hour\" , \"minute\" , \"second\" , \"dayofmonth\" , \"dayofweek\" , \"dayofyear\" , \"weekofyear\" , \"month\" , \"quarter\" , \"year\" , ] if units == \"all\" : units = all_units if isinstance ( units , str ): units = [ x . strip () for x in units . split ( \"|\" )] list_of_cols = argument_checker ( \"timeUnits_extraction\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"units\" : units , \"all_units\" : all_units , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : for e in units : func = getattr ( F , e ) odf = odf . withColumn ( i + \"_\" + e , func ( i )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def time_diff ( idf, ts1, ts2, unit, output_mode='append') Calculate the time difference between 2 timestamp columns (Timestamp 1 - Timestamp 2) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters idf Input Dataframe ts1 First column to calculate the difference ts2 Second column to calculate the difference. unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column diff, e.g. Given ts1=X, ts2=Y , X and Y are replaced with X_Y_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with name = diff, e.g. Given ts1=X, ts2=Y, X_Y_daydiff is appended for unit=\"day\". (Default value = \"append\") Returns DataFrame Expand source code def time_diff ( idf , ts1 , ts2 , unit , output_mode = \"append\" ): \"\"\" Calculate the time difference between 2 timestamp columns (Timestamp 1 - Timestamp 2) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe ts1 First column to calculate the difference ts2 Second column to calculate the difference. unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column <ts1>_<ts2>_<unit>diff, e.g. Given ts1=X, ts2=Y , X and Y are replaced with X_Y_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with name = <ts1>_<ts2>_<unit>diff, e.g. Given ts1=X, ts2=Y, X_Y_daydiff is appended for unit=\"day\". (Default value = \"append\") Returns ------- DataFrame \"\"\" argument_checker ( \"time_diff\" , { \"list_of_cols\" : [ ts1 , ts2 ], \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) factor_mapping = { \"second\" : 1 , \"minute\" : 60 , \"hour\" : 3600 , \"day\" : 86400 , \"week\" : 604800 , \"month\" : 2628000 , \"year\" : 31536000 , } if unit in factor_mapping . keys (): factor = factor_mapping [ unit ] elif unit in [( e + \"s\" ) for e in factor_mapping . keys ()]: unit = unit [: - 1 ] factor = factor_mapping [ unit ] else : raise TypeError ( \"Invalid input of unit\" ) odf = idf . withColumn ( ts1 + \"_\" + ts2 + \"_\" + unit + \"diff\" , F . abs ( ( F . col ( ts1 ) . cast ( T . TimestampType ()) . cast ( \"double\" ) - F . col ( ts2 ) . cast ( T . TimestampType ()) . cast ( \"double\" ) ) ) / factor , ) if output_mode == \"replace\" : odf = odf . drop ( ts1 , ts2 ) return odf def time_elapsed ( idf, list_of_cols, unit, output_mode='append') Calculate time difference between the current and the given timestamp (Current - Given Timestamp) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \" diff\", e.g. column X is replaced with X_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \" diff\", e.g. column X is appended as X_daydiff for unit=\"day\". (Default value = \"append\") Returns DataFrame Expand source code def time_elapsed ( idf , list_of_cols , unit , output_mode = \"append\" ): \"\"\" Calculate time difference between the current and the given timestamp (Current - Given Timestamp) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_<unit>diff\", e.g. column X is replaced with X_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_<unit>diff\", e.g. column X is appended as X_daydiff for unit=\"day\". (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"time_elapsed\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf factor_mapping = { \"second\" : 1 , \"minute\" : 60 , \"hour\" : 3600 , \"day\" : 86400 , \"week\" : 604800 , \"month\" : 2628000 , \"year\" : 31536000 , } if unit in factor_mapping . keys (): factor = factor_mapping [ unit ] elif unit in [( e + \"s\" ) for e in factor_mapping . keys ()]: unit = unit [: - 1 ] factor = factor_mapping [ unit ] else : raise TypeError ( \"Invalid input of unit\" ) odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_\" + unit + \"diff\" , F . abs ( ( F . lit ( F . current_timestamp ()) . cast ( \"double\" ) - F . col ( i ) . cast ( T . TimestampType ()) . cast ( \"double\" ) ) ) / factor , ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def timestamp_comparison ( spark, idf, list_of_cols, comparison_type, comparison_value, comparison_format='%Y-%m-%d %H:%M:%S', output_mode='append') Compare timestamp columns with a given timestamp/date value (comparison_value) of given format ( comparison_format). Supported comparison types include greater_than, less_than, greaterThan_equalTo and lessThan_equalTo. The derived values are 1 if True and 0 if False. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". comparison_type greater_than\", \"less_than\", \"greaterThan_equalTo\", \"lessThan_equalTo\" The comparison type of the transformation. comparison_value The timestamp / date value to compare with in string. comparison_format The format of comparison_value in string. (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_compared\", e.g. column X is replaced with X_compared. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_compared\", e.g. column X is appended as X_compared. (Default value = \"append\") Returns DataFrame Expand source code def timestamp_comparison ( spark , idf , list_of_cols , comparison_type , comparison_value , comparison_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"append\" , ): \"\"\" Compare timestamp columns with a given timestamp/date value (comparison_value) of given format ( comparison_format). Supported comparison types include greater_than, less_than, greaterThan_equalTo and lessThan_equalTo. The derived values are 1 if True and 0 if False. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". comparison_type greater_than\", \"less_than\", \"greaterThan_equalTo\", \"lessThan_equalTo\" The comparison type of the transformation. comparison_value The timestamp / date value to compare with in string. comparison_format The format of comparison_value in string. (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_compared\", e.g. column X is replaced with X_compared. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_compared\", e.g. column X is appended as X_compared. (Default value = \"append\") Returns ------- DataFrame \"\"\" all_types = [ \"greater_than\" , \"less_than\" , \"greaterThan_equalTo\" , \"lessThan_equalTo\" ] list_of_cols = argument_checker ( \"timestamp_comparison\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"comparison_type\" : comparison_type , \"all_types\" : all_types , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) base_ts = pytz . timezone ( localtz ) . localize ( dt . strptime ( comparison_value , comparison_format ) ) odf = idf for i in list_of_cols : if comparison_type == \"greater_than\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) > F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) elif comparison_type == \"less_than\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) < F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) elif comparison_type == \"greaterThan_equalTo\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) >= F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) else : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) <= F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def timestamp_to_string ( spark, idf, list_of_cols, output_format='%Y-%m-%d %H:%M:%S', output_mode='replace') Convert timestamp/date columns to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default) Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". Columns must be of Datetime type or String type in \"%Y-%m-%d %H:%M:%S\" format. output_format Format of the output column(s) (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_str\" e.g. column X is appended as X_str. (Default value = \"replace\") Returns DataFrame Expand source code def timestamp_to_string ( spark , idf , list_of_cols , output_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"replace\" ): \"\"\" Convert timestamp/date columns to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default) Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". Columns must be of Datetime type or String type in \"%Y-%m-%d %H:%M:%S\" format. output_format Format of the output column(s) (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_str\" e.g. column X is appended as X_str. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"timestamp_to_string\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) def conversion ( col , form ): if col is None : return None output = col . astimezone ( pytz . timezone ( localtz )) . strftime ( form ) return output f_conversion = F . udf ( conversion , T . StringType ()) odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_str\" } odf = odf . withColumn ( modify_col [ output_mode ], f_conversion ( F . col ( i ) . cast ( T . TimestampType ()), F . lit ( output_format )), ) return odf def timestamp_to_unix ( spark, idf, list_of_cols, precision='s', tz='local', output_mode='replace') Convert timestamp columns in a specified time zone to Unix time stamp in seconds or milliseconds. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" option returns the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" option returns the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". Timezone of the input column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_unix\" e.g. column X is appended as X_unix. (Default value = \"replace\") Returns DataFrame Expand source code def timestamp_to_unix ( spark , idf , list_of_cols , precision = \"s\" , tz = \"local\" , output_mode = \"replace\" ): \"\"\" Convert timestamp columns in a specified time zone to Unix time stamp in seconds or milliseconds. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" option returns the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" option returns the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". Timezone of the input column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_unix\" e.g. column X is appended as X_unix. (Default value = \"replace\") Returns ------- DataFrame \"\"\" tz = tz . lower () list_of_cols = argument_checker ( \"timestamp_to_unix\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"precision\" : precision , \"tz\" : tz , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) factor = { \"ms\" : 1000 , \"s\" : 1 } odf = idf for i in list_of_cols : if ( tz in ( \"gmt\" , \"utc\" )) & ( localtz . lower () not in ( \"gmt\" , \"utc\" )): odf = odf . withColumn ( i + \"_local\" , F . from_utc_timestamp ( i , localtz )) else : odf = odf . withColumn ( i + \"_local\" , F . col ( i )) modify_col = { \"replace\" : i , \"append\" : i + \"_unix\" } odf = odf . withColumn ( modify_col [ output_mode ], ( F . col ( i + \"_local\" ) . cast ( T . TimestampType ()) . cast ( \"double\" ) * factor [ precision ] ) . cast ( \"long\" ), ) . drop ( i + \"_local\" ) return odf def timezone_conversion ( spark, idf, list_of_cols, given_tz, output_tz, output_mode='replace') Convert timestamp columns from the given timezone (given_tz) to the output timezone (output_tz). Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". given_tz Timezone of the input column(s). If \"local\", the timezone of the spark session will be used. output_tz Timezone of the output column(s). If \"local\", the timezone of the spark session will be used. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_tzconverted\" e.g. column X is appended as X_tzconverted. (Default value = \"replace\") Returns DataFrame Expand source code def timezone_conversion ( spark , idf , list_of_cols , given_tz , output_tz , output_mode = \"replace\" ): \"\"\" Convert timestamp columns from the given timezone (given_tz) to the output timezone (output_tz). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". given_tz Timezone of the input column(s). If \"local\", the timezone of the spark session will be used. output_tz Timezone of the output column(s). If \"local\", the timezone of the spark session will be used. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_tzconverted\" e.g. column X is appended as X_tzconverted. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"timezone_conversion\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf given_tz = given_tz . upper () output_tz = output_tz . upper () localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) if given_tz == \"LOCAL\" : given_tz = localtz if output_tz == \"LOCAL\" : output_tz = localtz odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_tzconverted\" } odf = odf . withColumn ( modify_col [ output_mode ], F . from_utc_timestamp ( F . to_utc_timestamp ( i , given_tz ), output_tz ), ) return odf def unix_to_timestamp ( spark, idf, list_of_cols, precision='s', tz='local', output_mode='replace') Convert the number of seconds or milliseconds from unix epoch (1970-01-01 00:00:00 UTC) to a timestamp column in the specified time zone. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" treats the input columns as the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" treats the input columns as the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". timezone of the output column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns DataFrame Expand source code def unix_to_timestamp ( spark , idf , list_of_cols , precision = \"s\" , tz = \"local\" , output_mode = \"replace\" ): \"\"\" Convert the number of seconds or milliseconds from unix epoch (1970-01-01 00:00:00 UTC) to a timestamp column in the specified time zone. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" treats the input columns as the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" treats the input columns as the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". timezone of the output column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" tz = tz . lower () list_of_cols = argument_checker ( \"unix_to_timestamp\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"precision\" : precision , \"tz\" : tz , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) factor = { \"ms\" : 1000 , \"s\" : 1 } odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_ts\" } odf = odf . withColumn ( modify_col [ output_mode ], F . to_timestamp ( F . col ( i ) / factor [ precision ]) ) if ( tz in ( \"gmt\" , \"utc\" )) & ( localtz . lower () not in ( \"gmt\" , \"utc\" )): odf = odf . withColumn ( modify_col [ output_mode ], F . to_utc_timestamp ( modify_col [ output_mode ], localtz ), ) return odf def window_aggregator ( idf, list_of_cols, list_of_aggs, order_col, window_type='expanding', window_size='unbounded', partition_col='', output_mode='append') window_aggregator calcuates a list of aggregate metrics for all input columns over a window frame (expanding by default, or rolling type) ordered by the given timestamp column and partitioned by partition_col (\"\" by default, to indicate no partition). Window size needs to be provided as an integer for rolling window type. The following aggregate metrics are supported: count, min, max, sum, mean, median. Parameters idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\", \"mean\", \"median\" order_col Timestamp Column to order window window_type \"expanding\", \"rolling\" \"expanding\" option has a fixed lower bound (first row in the partition) \"rolling\" option has a fixed window size defined by window_size param (Default value = \"expanding\") window_size window size for rolling window type. Integer value with value >= 1. (Default value = \"unbounded\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column(s) with metric name as postfix. \u201cappend\u201d option appends derived column(s) to the input dataset with metric name as postfix, e.g. \"_count\", \"_mean\". (Default value = \"append\") Returns DataFrame Expand source code def window_aggregator ( idf , list_of_cols , list_of_aggs , order_col , window_type = \"expanding\" , window_size = \"unbounded\" , partition_col = \"\" , output_mode = \"append\" , ): \"\"\" window_aggregator calcuates a list of aggregate metrics for all input columns over a window frame (expanding by default, or rolling type) ordered by the given timestamp column and partitioned by partition_col (\"\" by default, to indicate no partition). Window size needs to be provided as an integer for rolling window type. The following aggregate metrics are supported: count, min, max, sum, mean, median. Parameters ---------- idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\", \"mean\", \"median\" order_col Timestamp Column to order window window_type \"expanding\", \"rolling\" \"expanding\" option has a fixed lower bound (first row in the partition) \"rolling\" option has a fixed window size defined by window_size param (Default value = \"expanding\") window_size window size for rolling window type. Integer value with value >= 1. (Default value = \"unbounded\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column(s) with metric name as postfix. \u201cappend\u201d option appends derived column(s) to the input dataset with metric name as postfix, e.g. \"_count\", \"_mean\". (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_aggs , str ): list_of_aggs = [ x . strip () for x in list_of_aggs . split ( \"|\" )] all_aggs = [ \"count\" , \"min\" , \"max\" , \"sum\" , \"mean\" , \"median\" ] list_of_cols = argument_checker ( \"window_aggregator\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"list_of_aggs\" : list_of_aggs , \"all_aggs\" : all_aggs , \"output_mode\" : output_mode , \"window_type\" : window_type , \"window_size\" : window_size , }, ) if not list_of_cols : return idf odf = idf window_upper = ( Window . unboundedPreceding if window_type == \"expanding\" else - int ( window_size ) ) if partition_col : window = ( Window . partitionBy ( partition_col ) . orderBy ( order_col ) . rowsBetween ( window_upper , 0 ) ) else : window = Window . partitionBy () . orderBy ( order_col ) . rowsBetween ( window_upper , 0 ) def agg_funcs ( col ): mapping = { \"count\" : F . count ( col ) . over ( window ) . alias ( col + \"_count\" ), \"min\" : F . min ( col ) . over ( window ) . alias ( col + \"_min\" ), \"max\" : F . max ( col ) . over ( window ) . alias ( col + \"_max\" ), \"sum\" : F . sum ( col ) . over ( window ) . alias ( col + \"_sum\" ), \"mean\" : F . mean ( col ) . over ( window ) . alias ( col + \"_mean\" ), \"median\" : F . expr ( \"percentile_approx(\" + col + \", 0.5)\" ) . over ( window ) . alias ( col + \"_median\" ), } derived_cols = [] for agg in list_of_aggs : derived_cols . append ( mapping [ agg ]) return derived_cols for i in list_of_cols : derived_cols = agg_funcs ( i ) odf = odf . select ( odf . columns + derived_cols ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf","title":"<code>datetime</code>"},{"location":"api/data_transformer/datetime.html#datetime","text":"Datetime module supports various transformations related to date and timestamp datatype columns. All available functions in this release can be classified into the following 4 categories: Conversion: - Between Timestamp and Epoch (timestamp_to_unix and unix_to_timestamp) - Between Timestamp and String (timestamp_to_string and string_to_timestamp) - Between Date Formats (dateformat_conversion) - Between Time Zones (timezone_conversion) Calculation: - Time difference - [Timestamp 1 - Timestamp 2] (time_diff) - Time elapsed - [Current - Given Timestamp] (time_elapsed) - Adding/subtracting time units (adding_timeUnits) - Aggregate features at X granularity level (aggregator) - Aggregate features with window frame (window_aggregator) - Lagged features - lagged date and time diff from the lagged date (lagged_ts) Extraction: - Time component extraction (timeUnits_extraction) - Start/end of month/year/quarter (start_of_month, end_of_month, start_of_year, end_of_year, start_of_quarter and end_of_quarter) Binary features: - Timestamp comparison (timestamp_comparison) - Is start/end of month/year/quarter nor not (is_monthStart, is_monthEnd, is_yearStart, is_yearEnd, is_quarterStart, is_quarterEnd) - Is first half of the year/selected hours/leap year/weekend or not (is_yearFirstHalf, is_selectedHour, is_leapYear and is_weekend) Expand source code \"\"\" Datetime module supports various transformations related to date and timestamp datatype columns. All available functions in this release can be classified into the following 4 categories: Conversion: - Between Timestamp and Epoch (timestamp_to_unix and unix_to_timestamp) - Between Timestamp and String (timestamp_to_string and string_to_timestamp) - Between Date Formats (dateformat_conversion) - Between Time Zones (timezone_conversion) Calculation: - Time difference - [Timestamp 1 - Timestamp 2] (time_diff) - Time elapsed - [Current - Given Timestamp] (time_elapsed) - Adding/subtracting time units (adding_timeUnits) - Aggregate features at X granularity level (aggregator) - Aggregate features with window frame (window_aggregator) - Lagged features - lagged date and time diff from the lagged date (lagged_ts) Extraction: - Time component extraction (timeUnits_extraction) - Start/end of month/year/quarter (start_of_month, end_of_month, start_of_year, end_of_year, start_of_quarter and end_of_quarter) Binary features: - Timestamp comparison (timestamp_comparison) - Is start/end of month/year/quarter nor not (is_monthStart, is_monthEnd, is_yearStart, is_yearEnd, is_quarterStart, is_quarterEnd) - Is first half of the year/selected hours/leap year/weekend or not (is_yearFirstHalf, is_selectedHour, is_leapYear and is_weekend) \"\"\" import calendar import warnings from datetime import datetime as dt import pytz from pyspark.sql import Window from pyspark.sql import functions as F from pyspark.sql import types as T def argument_checker ( func_name , args ): \"\"\" Parameters ---------- func_name function name for which argument needs to be check args arguments to check in dictionary format Returns ------- List list of columns to analyze \"\"\" list_of_cols = args [ \"list_of_cols\" ] all_columns = args [ \"all_columns\" ] if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if any ( x not in all_columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No timestamp conversion - No column(s) to convert\" ) return [] if func_name not in [ \"aggregator\" ]: if args [ \"output_mode\" ] not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if func_name in [ \"timestamp_to_unix\" , \"unix_to_timestamp\" ]: if args [ \"precision\" ] not in ( \"ms\" , \"s\" ): raise TypeError ( \"Invalid input for precision\" ) if args [ \"tz\" ] not in ( \"local\" , \"gmt\" , \"utc\" ): raise TypeError ( \"Invalid input for timezone\" ) if func_name in [ \"string_to_timestamp\" ]: if args [ \"output_type\" ] not in ( \"ts\" , \"dt\" ): raise TypeError ( \"Invalid input for output_type\" ) if func_name in [ \"timeUnits_extraction\" ]: if any ( x not in args [ \"all_units\" ] for x in args [ \"units\" ]): raise TypeError ( \"Invalid input for Unit(s)\" ) if func_name in [ \"adding_timeUnits\" ]: if args [ \"unit\" ] not in ( args [ \"all_units\" ] + [( e + \"s\" ) for e in args [ \"all_units\" ]] ): raise TypeError ( \"Invalid input for Unit\" ) if func_name in [ \"timestamp_comparison\" ]: if args [ \"comparison_type\" ] not in args [ \"all_types\" ]: raise TypeError ( \"Invalid input for comparison_type\" ) if func_name in [ \"is_selectedHour\" ]: hours = list ( range ( 0 , 24 )) if args [ \"start_hour\" ] not in hours : raise TypeError ( \"Invalid input for start_hour\" ) if args [ \"end_hour\" ] not in hours : raise TypeError ( \"Invalid input for end_hour\" ) if func_name in [ \"window_aggregator\" ]: if any ( x not in args [ \"all_aggs\" ] for x in args [ \"list_of_aggs\" ]): raise TypeError ( \"Invalid input for Aggregate Function(s)\" ) if args [ \"window_type\" ] not in ( \"expanding\" , \"rolling\" ): raise TypeError ( \"Invalid input for Window Type\" ) if ( args [ \"window_type\" ] == \"rolling\" ) & ( not str ( args [ \"window_size\" ]) . isnumeric () ): raise TypeError ( \"Invalid input for Window Size\" ) if func_name in [ \"aggregator\" ]: if any ( x not in args [ \"all_aggs\" ] for x in args [ \"list_of_aggs\" ]): raise TypeError ( \"Invalid input for Aggregate Function(s)\" ) if args [ \"time_col\" ] not in all_columns : raise TypeError ( \"Invalid input for time_col\" ) if func_name in [ \"lagged_ts\" ]: if not str ( args [ \"lag\" ]) . isnumeric (): raise TypeError ( \"Invalid input for Lag\" ) if args [ \"output_type\" ] not in ( \"ts\" , \"ts_diff\" ): raise TypeError ( \"Invalid input for output_type\" ) return list_of_cols def timestamp_to_unix ( spark , idf , list_of_cols , precision = \"s\" , tz = \"local\" , output_mode = \"replace\" ): \"\"\" Convert timestamp columns in a specified time zone to Unix time stamp in seconds or milliseconds. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" option returns the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" option returns the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". Timezone of the input column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_unix\" e.g. column X is appended as X_unix. (Default value = \"replace\") Returns ------- DataFrame \"\"\" tz = tz . lower () list_of_cols = argument_checker ( \"timestamp_to_unix\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"precision\" : precision , \"tz\" : tz , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) factor = { \"ms\" : 1000 , \"s\" : 1 } odf = idf for i in list_of_cols : if ( tz in ( \"gmt\" , \"utc\" )) & ( localtz . lower () not in ( \"gmt\" , \"utc\" )): odf = odf . withColumn ( i + \"_local\" , F . from_utc_timestamp ( i , localtz )) else : odf = odf . withColumn ( i + \"_local\" , F . col ( i )) modify_col = { \"replace\" : i , \"append\" : i + \"_unix\" } odf = odf . withColumn ( modify_col [ output_mode ], ( F . col ( i + \"_local\" ) . cast ( T . TimestampType ()) . cast ( \"double\" ) * factor [ precision ] ) . cast ( \"long\" ), ) . drop ( i + \"_local\" ) return odf def unix_to_timestamp ( spark , idf , list_of_cols , precision = \"s\" , tz = \"local\" , output_mode = \"replace\" ): \"\"\" Convert the number of seconds or milliseconds from unix epoch (1970-01-01 00:00:00 UTC) to a timestamp column in the specified time zone. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". precision \"ms\", \"s\". \"ms\" treats the input columns as the number of milliseconds from the unix epoch (1970-01-01 00:00:00 UTC) . \"s\" treats the input columns as the number of seconds from the unix epoch. (Default value = \"s\") tz \"local\", \"gmt\", \"utc\". timezone of the output column(s) (Default value = \"local\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" tz = tz . lower () list_of_cols = argument_checker ( \"unix_to_timestamp\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"precision\" : precision , \"tz\" : tz , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) factor = { \"ms\" : 1000 , \"s\" : 1 } odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_ts\" } odf = odf . withColumn ( modify_col [ output_mode ], F . to_timestamp ( F . col ( i ) / factor [ precision ]) ) if ( tz in ( \"gmt\" , \"utc\" )) & ( localtz . lower () not in ( \"gmt\" , \"utc\" )): odf = odf . withColumn ( modify_col [ output_mode ], F . to_utc_timestamp ( modify_col [ output_mode ], localtz ), ) return odf def timezone_conversion ( spark , idf , list_of_cols , given_tz , output_tz , output_mode = \"replace\" ): \"\"\" Convert timestamp columns from the given timezone (given_tz) to the output timezone (output_tz). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". given_tz Timezone of the input column(s). If \"local\", the timezone of the spark session will be used. output_tz Timezone of the output column(s). If \"local\", the timezone of the spark session will be used. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_tzconverted\" e.g. column X is appended as X_tzconverted. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"timezone_conversion\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf given_tz = given_tz . upper () output_tz = output_tz . upper () localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) if given_tz == \"LOCAL\" : given_tz = localtz if output_tz == \"LOCAL\" : output_tz = localtz odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_tzconverted\" } odf = odf . withColumn ( modify_col [ output_mode ], F . from_utc_timestamp ( F . to_utc_timestamp ( i , given_tz ), output_tz ), ) return odf def string_to_timestamp ( spark , idf , list_of_cols , input_format = \"%Y-%m- %d %H:%M:%S\" , output_type = \"ts\" , output_mode = \"replace\" , ): \"\"\" Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to TimestampType or DateType columns. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_type \"ts\", \"dt\" \"ts\" option returns result in T.TimestampType() \"dt\" option returns result in T.DateType() (Default value = \"ts\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"string_to_timestamp\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"output_type\" : output_type , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) def conversion ( col , form ): if col is None : return None output = pytz . timezone ( localtz ) . localize ( dt . strptime ( str ( col ), form )) return output data_type = { \"ts\" : T . TimestampType (), \"dt\" : T . DateType ()} f_conversion = F . udf ( conversion , data_type [ output_type ]) odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_ts\" } odf = odf . withColumn ( modify_col [ output_mode ], f_conversion ( F . col ( i ), F . lit ( input_format )) ) return odf def timestamp_to_string ( spark , idf , list_of_cols , output_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"replace\" ): \"\"\" Convert timestamp/date columns to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default) Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". Columns must be of Datetime type or String type in \"%Y-%m-%d %H:%M:%S\" format. output_format Format of the output column(s) (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_str\" e.g. column X is appended as X_str. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"timestamp_to_string\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) def conversion ( col , form ): if col is None : return None output = col . astimezone ( pytz . timezone ( localtz )) . strftime ( form ) return output f_conversion = F . udf ( conversion , T . StringType ()) odf = idf for i in list_of_cols : modify_col = { \"replace\" : i , \"append\" : i + \"_str\" } odf = odf . withColumn ( modify_col [ output_mode ], f_conversion ( F . col ( i ) . cast ( T . TimestampType ()), F . lit ( output_format )), ) return odf def dateformat_conversion ( spark , idf , list_of_cols , input_format = \"%Y-%m- %d %H:%M:%S\" , output_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"replace\" , ): \"\"\" Convert time string columns with given input format (\"%Y-%m-%d %H:%M:%S\", by default) to time string columns with given output format (\"%Y-%m-%d %H:%M:%S\", by default). Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". input_format Format of the input column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_format Format of the output column(s) in string (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ts\" e.g. column X is appended as X_ts. (Default value = \"replace\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"dateformat_conversion\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf_tmp = string_to_timestamp ( spark , idf , list_of_cols , input_format = input_format , output_type = \"ts\" , output_mode = output_mode , ) appended_cols = { \"append\" : [ col + \"_ts\" for col in list_of_cols ], \"replace\" : list_of_cols , } odf = timestamp_to_string ( spark , odf_tmp , appended_cols [ output_mode ], output_format = output_format , output_mode = \"replace\" , ) return odf def timeUnits_extraction ( idf , list_of_cols , units , output_mode = \"append\" ): \"\"\" Extract the unit(s) of given timestamp columns as integer. Currently the following units are supported: hour, minute, second, dayofmonth, dayofweek, dayofyear, weekofyear, month, quarter, year. Multiple units can be calculated at the same time by inputting a list of units or a string of units separated by pipe delimiter \u201c|\u201d. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". units List of unit(s) to extract. Alternatively, unit(s) can be specified in a string format, where different units are separated by pipe delimiter \u201c|\u201d e.g., \"hour|minute\". Supported units to extract: \"hour\", \"minute\", \"second\",\"dayofmonth\",\"dayofweek\", \"dayofyear\",\"weekofyear\",\"month\",\"quarter\",\"year\". \"all\" can be passed to compute all supported metrics. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_<unit>\", e.g. column X is replaced with X_second for units=\"second\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_<unit>\", e.g. column X is appended as X_second for units=\"second\". (Default value = \"append\") Returns ------- DataFrame \"\"\" all_units = [ \"hour\" , \"minute\" , \"second\" , \"dayofmonth\" , \"dayofweek\" , \"dayofyear\" , \"weekofyear\" , \"month\" , \"quarter\" , \"year\" , ] if units == \"all\" : units = all_units if isinstance ( units , str ): units = [ x . strip () for x in units . split ( \"|\" )] list_of_cols = argument_checker ( \"timeUnits_extraction\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"units\" : units , \"all_units\" : all_units , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : for e in units : func = getattr ( F , e ) odf = odf . withColumn ( i + \"_\" + e , func ( i )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def time_diff ( idf , ts1 , ts2 , unit , output_mode = \"append\" ): \"\"\" Calculate the time difference between 2 timestamp columns (Timestamp 1 - Timestamp 2) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe ts1 First column to calculate the difference ts2 Second column to calculate the difference. unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column <ts1>_<ts2>_<unit>diff, e.g. Given ts1=X, ts2=Y , X and Y are replaced with X_Y_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with name = <ts1>_<ts2>_<unit>diff, e.g. Given ts1=X, ts2=Y, X_Y_daydiff is appended for unit=\"day\". (Default value = \"append\") Returns ------- DataFrame \"\"\" argument_checker ( \"time_diff\" , { \"list_of_cols\" : [ ts1 , ts2 ], \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) factor_mapping = { \"second\" : 1 , \"minute\" : 60 , \"hour\" : 3600 , \"day\" : 86400 , \"week\" : 604800 , \"month\" : 2628000 , \"year\" : 31536000 , } if unit in factor_mapping . keys (): factor = factor_mapping [ unit ] elif unit in [( e + \"s\" ) for e in factor_mapping . keys ()]: unit = unit [: - 1 ] factor = factor_mapping [ unit ] else : raise TypeError ( \"Invalid input of unit\" ) odf = idf . withColumn ( ts1 + \"_\" + ts2 + \"_\" + unit + \"diff\" , F . abs ( ( F . col ( ts1 ) . cast ( T . TimestampType ()) . cast ( \"double\" ) - F . col ( ts2 ) . cast ( T . TimestampType ()) . cast ( \"double\" ) ) ) / factor , ) if output_mode == \"replace\" : odf = odf . drop ( ts1 , ts2 ) return odf def time_elapsed ( idf , list_of_cols , unit , output_mode = \"append\" ): \"\"\" Calculate time difference between the current and the given timestamp (Current - Given Timestamp) in a given unit. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the output values. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_<unit>diff\", e.g. column X is replaced with X_daydiff for unit=\"day\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_<unit>diff\", e.g. column X is appended as X_daydiff for unit=\"day\". (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"time_elapsed\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf factor_mapping = { \"second\" : 1 , \"minute\" : 60 , \"hour\" : 3600 , \"day\" : 86400 , \"week\" : 604800 , \"month\" : 2628000 , \"year\" : 31536000 , } if unit in factor_mapping . keys (): factor = factor_mapping [ unit ] elif unit in [( e + \"s\" ) for e in factor_mapping . keys ()]: unit = unit [: - 1 ] factor = factor_mapping [ unit ] else : raise TypeError ( \"Invalid input of unit\" ) odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_\" + unit + \"diff\" , F . abs ( ( F . lit ( F . current_timestamp ()) . cast ( \"double\" ) - F . col ( i ) . cast ( T . TimestampType ()) . cast ( \"double\" ) ) ) / factor , ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def adding_timeUnits ( idf , list_of_cols , unit , unit_value , output_mode = \"append\" ): \"\"\" Add or subtract given time units to/from timestamp columns. Currently the following units are supported: second, minute, hour, day, week, month, year. Subtraction can be performed by setting a negative unit_value. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the added value. unit_value The value to be added to input column(s). output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_adjusted\", e.g. column X is replaced with X_adjusted. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_adjusted\", e.g. column X is appended as X_adjusted. (Default value = \"append\") Returns ------- DataFrame \"\"\" all_units = [ \"hour\" , \"minute\" , \"second\" , \"day\" , \"week\" , \"month\" , \"year\" ] list_of_cols = argument_checker ( \"adding_timeUnits\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"unit\" : unit , \"all_units\" : all_units , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_adjusted\" , F . col ( i ) . cast ( T . TimestampType ()) + F . expr ( \"Interval \" + str ( unit_value ) + \" \" + unit ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def timestamp_comparison ( spark , idf , list_of_cols , comparison_type , comparison_value , comparison_format = \"%Y-%m- %d %H:%M:%S\" , output_mode = \"append\" , ): \"\"\" Compare timestamp columns with a given timestamp/date value (comparison_value) of given format ( comparison_format). Supported comparison types include greater_than, less_than, greaterThan_equalTo and lessThan_equalTo. The derived values are 1 if True and 0 if False. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". comparison_type greater_than\", \"less_than\", \"greaterThan_equalTo\", \"lessThan_equalTo\" The comparison type of the transformation. comparison_value The timestamp / date value to compare with in string. comparison_format The format of comparison_value in string. (Default value = \"%Y-%m-%d %H:%M:%S\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived columns with a postfix \"_compared\", e.g. column X is replaced with X_compared. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_compared\", e.g. column X is appended as X_compared. (Default value = \"append\") Returns ------- DataFrame \"\"\" all_types = [ \"greater_than\" , \"less_than\" , \"greaterThan_equalTo\" , \"lessThan_equalTo\" ] list_of_cols = argument_checker ( \"timestamp_comparison\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , \"comparison_type\" : comparison_type , \"all_types\" : all_types , }, ) if not list_of_cols : return idf localtz = ( spark . sql ( \"SET spark.sql.session.timeZone\" ) . select ( \"value\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) base_ts = pytz . timezone ( localtz ) . localize ( dt . strptime ( comparison_value , comparison_format ) ) odf = idf for i in list_of_cols : if comparison_type == \"greater_than\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) > F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) elif comparison_type == \"less_than\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) < F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) elif comparison_type == \"greaterThan_equalTo\" : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) >= F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) else : odf = odf . withColumn ( i + \"_compared\" , F . when ( F . col ( i ) <= F . lit ( base_ts ), 1 ) . otherwise ( 0 ) ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_month ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the month of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthStart\", e.g. column X is appended as X_monthStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_month\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_monthStart\" , F . trunc ( i , \"month\" )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_monthStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a month. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthStart\", e.g. column X is appended as X_ismonthStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_monthStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_month ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_ismonthStart\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_monthStart\" ), 1 ) . otherwise ( F . when ( F . col ( i ) . isNull (), None ) . otherwise ( 0 ) ) ), ) . drop ( i + \"_monthStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_month ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the month of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_monthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_monthEnd\", e.g. column X is appended as X_monthEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_month\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_monthEnd\" , F . last_day ( i )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_monthEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a month. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_ismonthEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_ismonthEnd\", e.g. column X is appended as X_ismonthEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_monthEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_month ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_ismonthEnd\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_monthEnd\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_monthEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_year ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the year of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearStart\", e.g. column X is appended as X_yearStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_year\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_yearStart\" , F . trunc ( i , \"year\" )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearStart\", e.g. column X is appended as X_isyearStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_year ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isyearStart\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_yearStart\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_yearStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_year ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the year of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_yearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_yearEnd\", e.g. column X is appended as X_yearEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_year\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_yearEnd\" , F . concat_ws ( \"-\" , F . year ( i ), F . lit ( 12 ), F . lit ( 31 )) . cast ( \"date\" ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isyearEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isyearEnd\", e.g. column X is appended as X_isyearEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_year ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isyearEnd\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_yearEnd\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_yearEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def start_of_quarter ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the first day of the quarter of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterStart. \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterStart\", e.g. column X is appended as X_quarterStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"start_of_quarter\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_quarterStart\" , F . to_date ( F . date_trunc ( \"quarter\" , i ))) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_quarterStart ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the first day of a quarter. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterStart\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterStart\", e.g. column X is appended as X_isquarterStart. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_quarterStart\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = start_of_quarter ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isquarterStart\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_quarterStart\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_quarterStart\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def end_of_quarter ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Extract the last day of the quarter of given timestamp/date columns. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_quarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_quarterEnd\", e.g. column X is appended as X_quarterEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"end_of_quarter\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_quarterEnd\" , F . to_date ( F . date_trunc ( \"quarter\" , i )) + F . expr ( \"Interval 3 months\" ) + F . expr ( \"Interval -1 day\" ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_quarterEnd ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are the last day of a quarter. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isquarterEnd\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isquarterEnd\", e.g. column X is appended as X_isquarterEnd. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_quarterEnd\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = end_of_quarter ( idf , list_of_cols , output_mode = \"append\" ) for i in list_of_cols : odf = odf . withColumn ( i + \"_isquarterEnd\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . to_date ( F . col ( i )) == F . col ( i + \"_quarterEnd\" ), 1 ) . otherwise ( 0 ) ), ) . drop ( i + \"_quarterEnd\" ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_yearFirstHalf ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are in the first half of a year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isFirstHalf\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isFirstHalf\", e.g. column X is appended as X_isFirstHalf. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_yearFirstHalf\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isFirstHalf\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . month ( F . col ( i )) . isin ( * range ( 1 , 7 )), 1 ) . otherwise ( 0 ) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_selectedHour ( idf , list_of_cols , start_hour , end_hour , output_mode = \"append\" ): \"\"\" Check if the hour component of given timestamp columns are between start hour (inclusive) and end hour ( inclusive). The derived values are 1 if True and 0 if False. Start hour can be larger than end hour, for example, start_hour=22 and end_hour=3 can be used to check whether the hour component is in [22, 23, 0, 1, 2, 3]. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". start_hour The starting hour of the hour range (inclusive) end_hour The ending hour of the hour range (inclusive) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isselectedHour\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isselectedHour\", e.g. column X is appended as X_isselectedHour. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_selectedHour\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"start_hour\" : start_hour , \"end_hour\" : end_hour , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf if start_hour < end_hour : list_of_hrs = range ( start_hour , end_hour + 1 ) elif start_hour > end_hour : list_of_hrs = list ( range ( start_hour , 24 )) + list ( range ( 0 , end_hour + 1 )) else : list_of_hrs = [ start_hour ] for i in list_of_cols : odf = odf . withColumn ( i + \"_isselectedHour\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . hour ( F . col ( i )) . isin ( * list_of_hrs ), 1 ) . otherwise ( 0 ) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_leapYear ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are in a leap year. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isleapYear\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isleapYear\", e.g. column X is appended as X_isleapYear. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_leapYear\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf def check ( year ): if year is None : return None if calendar . isleap ( year ): return 1 else : return 0 f_check = F . udf ( check , T . IntegerType ()) odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isleapYear\" , f_check ( F . year ( i ))) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def is_weekend ( idf , list_of_cols , output_mode = \"append\" ): \"\"\" Check if values in given timestamp/date columns are on weekends. The derived values are 1 if True and 0 if False. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column with a postfix \"_isweekend\". \u201cappend\u201d option appends derived column to the input dataset with a postfix \"_isweekend\", e.g. column X is appended as X_isweekend. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"is_weekend\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : odf = odf . withColumn ( i + \"_isweekend\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . when ( F . dayofweek ( F . col ( i )) . isin ([ 1 , 7 ]), 1 ) . otherwise ( 0 ) ), ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def aggregator ( spark , idf , list_of_cols , list_of_aggs , time_col , granularity_format = \"%Y-%m- %d \" ): \"\"\" aggregator performs groupBy over the timestamp/date column and calcuates a list of aggregate metrics over all input columns. The timestamp column is firstly converted to the given granularity format (\"%Y-%m-%d\", by default) before applying groupBy and the conversion step can be skipped by setting granularity format to be an empty string. The following aggregate metrics are supported: count, min, max, sum, mean, median, stddev, countDistinct, sumDistinct, collect_list, collect_set. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\",\"mean\",\"median\",\"stddev\", \"countDistinct\",\"sumDistinct\",\"collect_list\",\"collect_set\". time_col Timestamp) Column to group by. granularity_format Format to be applied to time_col before groupBy. The default value is '%Y-%m-%d', which means grouping by the date component of time_col. Alternatively, '' can be used if no formatting is necessary. Returns ------- DataFrame \"\"\" all_aggs = [ \"count\" , \"min\" , \"max\" , \"sum\" , \"mean\" , \"median\" , \"stddev\" , \"countDistinct\" , \"sumDistinct\" , \"collect_list\" , \"collect_set\" , ] if isinstance ( list_of_aggs , str ): list_of_aggs = [ x . strip () for x in list_of_aggs . split ( \"|\" )] list_of_cols = argument_checker ( \"aggregator\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"list_of_aggs\" : list_of_aggs , \"all_aggs\" : all_aggs , \"time_col\" : time_col , }, ) if not list_of_cols : return idf if granularity_format != \"\" : idf = timestamp_to_string ( spark , idf , time_col , output_format = granularity_format , output_mode = \"replace\" , ) def agg_funcs ( col , agg ): mapping = { \"count\" : F . count ( col ) . alias ( col + \"_count\" ), \"min\" : F . min ( col ) . alias ( col + \"_min\" ), \"max\" : F . max ( col ) . alias ( col + \"_max\" ), \"sum\" : F . sum ( col ) . alias ( col + \"_sum\" ), \"mean\" : F . mean ( col ) . alias ( col + \"_mean\" ), \"median\" : F . expr ( \"percentile_approx(\" + col + \", 0.5)\" ) . alias ( col + \"_median\" ), \"stddev\" : F . stddev ( col ) . alias ( col + \"_stddev\" ), \"countDistinct\" : F . countDistinct ( col ) . alias ( col + \"_countDistinct\" ), \"sumDistinct\" : F . sumDistinct ( col ) . alias ( col + \"_sumDistinct\" ), \"collect_list\" : F . collect_list ( col ) . alias ( col + \"_collect_list\" ), \"collect_set\" : F . collect_set ( col ) . alias ( col + \"_collect_set\" ), } return mapping [ agg ] derived_cols = [] for i in list_of_cols : for j in list_of_aggs : derived_cols . append ( agg_funcs ( i , j )) odf = idf . groupBy ( time_col ) . agg ( * derived_cols ) return odf def window_aggregator ( idf , list_of_cols , list_of_aggs , order_col , window_type = \"expanding\" , window_size = \"unbounded\" , partition_col = \"\" , output_mode = \"append\" , ): \"\"\" window_aggregator calcuates a list of aggregate metrics for all input columns over a window frame (expanding by default, or rolling type) ordered by the given timestamp column and partitioned by partition_col (\"\" by default, to indicate no partition). Window size needs to be provided as an integer for rolling window type. The following aggregate metrics are supported: count, min, max, sum, mean, median. Parameters ---------- idf Input Dataframe list_of_cols List of columns to aggregate e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". list_of_aggs List of aggregate metrics to compute e.g., [\"f1\",\"f2\"]. Alternatively, metrics can be specified in a string format, where different metrics are separated by pipe delimiter \u201c|\u201d e.g., \"f1|f2\". Supported metrics: \"count\", \"min\", \"max\", \"sum\", \"mean\", \"median\" order_col Timestamp Column to order window window_type \"expanding\", \"rolling\" \"expanding\" option has a fixed lower bound (first row in the partition) \"rolling\" option has a fixed window size defined by window_size param (Default value = \"expanding\") window_size window size for rolling window type. Integer value with value >= 1. (Default value = \"unbounded\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column(s) with metric name as postfix. \u201cappend\u201d option appends derived column(s) to the input dataset with metric name as postfix, e.g. \"_count\", \"_mean\". (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_aggs , str ): list_of_aggs = [ x . strip () for x in list_of_aggs . split ( \"|\" )] all_aggs = [ \"count\" , \"min\" , \"max\" , \"sum\" , \"mean\" , \"median\" ] list_of_cols = argument_checker ( \"window_aggregator\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"list_of_aggs\" : list_of_aggs , \"all_aggs\" : all_aggs , \"output_mode\" : output_mode , \"window_type\" : window_type , \"window_size\" : window_size , }, ) if not list_of_cols : return idf odf = idf window_upper = ( Window . unboundedPreceding if window_type == \"expanding\" else - int ( window_size ) ) if partition_col : window = ( Window . partitionBy ( partition_col ) . orderBy ( order_col ) . rowsBetween ( window_upper , 0 ) ) else : window = Window . partitionBy () . orderBy ( order_col ) . rowsBetween ( window_upper , 0 ) def agg_funcs ( col ): mapping = { \"count\" : F . count ( col ) . over ( window ) . alias ( col + \"_count\" ), \"min\" : F . min ( col ) . over ( window ) . alias ( col + \"_min\" ), \"max\" : F . max ( col ) . over ( window ) . alias ( col + \"_max\" ), \"sum\" : F . sum ( col ) . over ( window ) . alias ( col + \"_sum\" ), \"mean\" : F . mean ( col ) . over ( window ) . alias ( col + \"_mean\" ), \"median\" : F . expr ( \"percentile_approx(\" + col + \", 0.5)\" ) . over ( window ) . alias ( col + \"_median\" ), } derived_cols = [] for agg in list_of_aggs : derived_cols . append ( mapping [ agg ]) return derived_cols for i in list_of_cols : derived_cols = agg_funcs ( i ) odf = odf . select ( odf . columns + derived_cols ) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf def lagged_ts ( idf , list_of_cols , lag , output_type = \"ts\" , tsdiff_unit = \"days\" , partition_col = \"\" , output_mode = \"append\" , ): \"\"\" lagged_ts returns the values that are *lag* rows before the current rows, and None if there is less than *lag* rows before the current rows. If output_type is \"ts_diff\", an additional column is generated with values being the time difference between the original timestamp and the lagged timestamp in given unit *tsdiff_unit*. Currently the following units are supported: second, minute, hour, day, week, month, year. Parameters ---------- idf Input Dataframe list_of_cols List of columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". lag Number of row(s) to extend. output_type \"ts\", \"ts_diff\". \"ts\" option generats a lag column for each input column having the value that is <lag> rows before the current row, and None if there is less than <lag> rows before the current row. \"ts_diff\" option generates the lag column in the same way as the \"ts\" option. On top of that, it appends a column which represents the time_diff between the original and the lag column. (Default value = \"ts\") tsdiff_unit \"second\", \"minute\", \"hour\", \"day\", \"week\", \"month\", \"year\". Unit of the time_diff if output_type=\"ts_diff\". (Default value = \"days\") partition_col Rows partitioned by this column before creating window. (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with derived column: <col>_lag<lag> for \"ts\" output_type, <col>_<col>_lag<lag>_<tsdiff_unit>diff for \"ts_diff\" output_type. \u201cappend\u201d option appends derived column to the input dataset, e.g. given output_type=\"ts_diff\", lag=5, tsdiff_unit=\"days\", column X is appended as X_X_lag5_daydiff. (Default value = \"append\") Returns ------- DataFrame \"\"\" list_of_cols = argument_checker ( \"lagged_ts\" , { \"list_of_cols\" : list_of_cols , \"all_columns\" : idf . columns , \"lag\" : lag , \"output_type\" : output_type , \"output_mode\" : output_mode , }, ) if not list_of_cols : return idf odf = idf for i in list_of_cols : if partition_col : window = Window . partitionBy ( partition_col ) . orderBy ( i ) else : window = Window . partitionBy () . orderBy ( i ) lag = int ( lag ) odf = odf . withColumn ( i + \"_lag\" + str ( lag ), F . lag ( F . col ( i ), lag ) . over ( window )) if output_type == \"ts_diff\" : odf = time_diff ( odf , i , i + \"_lag\" + str ( lag ), unit = tsdiff_unit , output_mode = \"append\" ) . drop ( i + \"_lag\" + str ( lag )) if output_mode == \"replace\" : odf = odf . drop ( i ) return odf","title":"datetime"},{"location":"api/data_transformer/datetime.html#functions","text":"def adding_timeUnits ( idf, list_of_cols, unit, unit_value, output_mode='append') Add or subtract given time units to/from timestamp columns. Currently the following units are supported: second, minute, hour, day, week, month, year. Subtraction can be performed by setting a negative unit_value.","title":"Functions"},{"location":"api/data_transformer/geo_utils.html","text":"geo_utils Expand source code from math import sin , cos , atan2 , asin , radians , degrees , sqrt import pygeohash as pgh from geopy import distance from scipy import spatial import numbers import warnings from pyspark.sql import functions as F from pyspark.sql import types as T EARTH_RADIUS = 6371009 UNIT_FACTOR = { \"m\" : 1.0 , \"km\" : 1000.0 } def in_range ( loc , loc_format = \"dd\" ): \"\"\" This function helps to check if the input location is in range based on loc_format. Parameters ---------- loc Location to check if in range. If loc_format is \"dd\", [lat, lon] format is required. If loc_format is \"dms\", [[d1,m1,s1], [d2,m2,s2]] format is required. If loc_format is \"radian\", [lat_radian, lon_radian] format is required. If loc_format is \"cartesian\", [x, y, z] format is required. If loc_format is \"geohash\", string format is required. loc_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". (Default value = \"dd\") \"\"\" if loc_format == \"dd\" : try : lat , lon = [ int ( float ( i )) for i in loc ] except : lat , lon = None , None else : try : lat , lon = [ int ( float ( i )) for i in to_latlon_decimal_degrees ( loc , loc_format ) ] except : lat , lon = None , None if None not in [ lat , lon ]: if lat > 90 or lat < - 90 or lon > 180 or lon < - 180 : warnings . warn ( \"Rows may contain unintended values due to longitude and/or latitude values being out of the \" \"valid range\" ) def to_latlon_decimal_degrees ( loc , input_format , radius = EARTH_RADIUS ): \"\"\" This function helps to format input location into [lat,lon] format Parameters ---------- loc Location to format. If input_format is \"dd\", [lat, lon] format is required. If input_format is \"dms\", [[d1,m1,s1], [d2,m2,s2]] format is required. If input_format is \"radian\", [lat_radian, lon_radian] format is required. If input_format is \"cartesian\", [x, y, z] format is required. If input_format is \"geohash\", string format is required. input_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". radius Radius of Earth. (Default value = EARTH_RADIUS) Returns ------- List Formatted location: [lat, lon] \"\"\" if loc is None : return None if isinstance ( loc , ( list , tuple )): if any ( i is None for i in loc ): return None if isinstance ( loc [ 0 ], ( list , tuple )): if any ( i is None for i in loc [ 0 ] + loc [ 1 ]): return None if input_format == \"dd\" : # loc = [lat, lon] try : lat = float ( loc [ 0 ]) lon = float ( loc [ 1 ]) except : lat , lon = None , None warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) elif input_format == \"dms\" : # loc = [[d1,m1,s1], [d2,m2,s2]] try : d1 , m1 , s1 , d2 , m2 , s2 = [ float ( i ) for i in ( loc [ 0 ] + loc [ 1 ])] lat = d1 + m1 / 60 + s1 / 3600 lon = d2 + m2 / 60 + s2 / 3600 except : lat , lon = None , None warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) elif input_format == \"radian\" : # loc = [lat_radian, lon_radian] try : lat = degrees ( float ( loc [ 0 ])) lon = degrees ( float ( loc [ 1 ])) except : lat , lon = None , None warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) elif input_format == \"cartesian\" : # loc = [x, y, z] try : x , y , z = [ float ( i ) for i in loc ] lat = degrees ( float ( asin ( z / radius ))) lon = degrees ( float ( atan2 ( y , x ))) except : lat , lon = None , None warnings . warn ( \"Rows dropped due to invalid cartesian values\" ) elif input_format == \"geohash\" : # loc = geohash try : lat , lon = list ( pgh . decode ( loc )) except : lat , lon = None , None warnings . warn ( \"Rows dropped due to an invalid geohash entry\" ) in_range (( lat , lon )) return [ lat , lon ] def decimal_degrees_to_degrees_minutes_seconds ( dd ): \"\"\" This function helps to divide float value dd in decimal degree into [degreee, minute, second] Parameters ---------- dd Float value in decimal degree. Returns ------- List [degree, minute, second] \"\"\" if dd is None : return [ None , None , None ] else : minute , second = divmod ( dd * 3600 , 60 ) degree , minute = divmod ( minute , 60 ) return [ degree , minute , second ] def from_latlon_decimal_degrees ( loc , output_format , radius = EARTH_RADIUS , geohash_precision = 8 ): \"\"\" This function helps to transform [lat,lon] locations into desired output_format. Parameters ---------- loc Location to format with format [lat, lon]. output_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". radius Radius of Earth. (Default value = EARTH_RADIUS) geohash_precision Precision of the resultant geohash. This argument is only used when output_format is \"geohash\". (Default value = 8) Returns ------- String if output_format is \"geohash\", list otherwise. [lat, lon] if output_format is \"dd\". [[d1,m1,s1], [d2,m2,s2]] if output_format is \"dms\". [lat_radian, lon_radian] if output_format is \"radian\". [x, y, z] if output_format is \"cartesian\". string if output_format is \"geohash\". \"\"\" # loc = [lat, lon] if loc is None : lat , lon = None , None else : lat , lon = loc [ 0 ], loc [ 1 ] if output_format == \"dd\" : return [ lat , lon ] elif output_format == \"dms\" : return [ decimal_degrees_to_degrees_minutes_seconds ( lat ), decimal_degrees_to_degrees_minutes_seconds ( lon ), ] elif output_format == \"radian\" : if ( lat is None ) | ( lon is None ): return [ None , None ] else : lat_rad = radians ( float ( lat )) lon_rad = radians ( float ( lon )) return [ lat_rad , lon_rad ] elif output_format == \"cartesian\" : if ( lat is None ) | ( lon is None ): return [ None , None , None ] else : lat_rad = radians ( float ( lat )) lon_rad = radians ( float ( lon )) x = radius * cos ( lat_rad ) * cos ( lon_rad ) y = radius * cos ( lat_rad ) * sin ( lon_rad ) z = radius * sin ( lat_rad ) return [ x , y , z ] elif output_format == \"geohash\" : if ( lat is None ) | ( lon is None ): return None else : return pgh . encode ( lat , lon , geohash_precision ) def haversine_distance ( loc1 , loc2 , loc_format , unit = \"m\" , radius = EARTH_RADIUS ): \"\"\" This function helps to calculate the haversine distance between loc1 and loc2. Parameters ---------- loc1 The first location. If loc_format is \"dd\", [lat, lon] format is required. If loc_format is \"radian\", [lat_radian, lon_radian] format is required. loc2 The second location . If loc_format is \"dd\", [lat, lon] format is required. If loc_format is \"radian\", [lat_radian, lon_radian] format is required. loc_format \"dd\", \"radian\". unit \"m\", \"km\". Unit of the result. (Default value = \"m\") radius Radius of Earth. (Default value = EARTH_RADIUS) Returns ------- Float \"\"\" # loc1 = [lat1, lon1]; loc2 = [lat2, lon2] if None in [ loc1 , loc2 ]: return None if None in loc1 + loc2 : return None if loc_format not in [ \"dd\" , \"radian\" ]: raise TypeError ( \"Invalid input for loc_format\" ) try : lat1 , lon1 = float ( loc1 [ 0 ]), float ( loc1 [ 1 ]) lat2 , lon2 = float ( loc2 [ 0 ]), float ( loc2 [ 1 ]) except : return None in_range (( lat1 , lon1 ), loc_format ) in_range (( lat2 , lon2 ), loc_format ) if loc_format == \"dd\" : lat1 , lon1 = radians ( lat1 ), radians ( lon1 ) lat2 , lon2 = radians ( lat2 ), radians ( lon2 ) dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 c = 2 * atan2 ( sqrt ( a ), sqrt ( 1 - a )) distance = radius * c / UNIT_FACTOR [ unit ] return distance def vincenty_distance ( loc1 , loc2 , unit = \"m\" , ellipsoid = \"WGS-84\" ): \"\"\" Vincenty's formulae are two related iterative methods used in geodesy to calculate the distance between two points on the surface of a spheroid. Parameters ---------- loc1 The first location. [lat, lon] format is required. loc2 The second location. [lat, lon] format is required. unit \"m\", \"km\". Unit of the result. (Default value = \"m\") ellipsoid \"WGS-84\", \"GRS-80\", \"Airy (1830)\", \"Intl 1924\", \"Clarke (1880)\", \"GRS-67\". The ellipsoidal model to use. For more information, please refer to geopy.distance.ELLIPSOIDS. (Default value = \"WGS-84\") Returns ------- Float \"\"\" if None in [ loc1 , loc2 ]: return None if None in loc1 + loc2 : return None in_range ( loc1 ) in_range ( loc2 ) try : loc_distance = distance . distance ( loc1 , loc2 , ellipsoid = ellipsoid ) except : return None if unit == \"m\" : return loc_distance . m else : return loc_distance . km def euclidean_distance ( loc1 , loc2 , unit = \"m\" ): \"\"\" The Euclidean distance between 2 lists loc1 and loc2, is defined as .. math:: {\\\\|loc1-loc2\\\\|}_2 Parameters ---------- loc1 The first location. [x1, y1, z1] format is required. loc2 The second location. [x2, y2, z2] format is required. unit \"m\", \"km\". Unit of the result. (Default value = \"m\") Returns ------- Float \"\"\" if None in [ loc1 , loc2 ]: return None if None in loc1 + loc2 : return None try : loc1 = [ float ( i ) for i in loc1 ] loc2 = [ float ( i ) for i in loc2 ] except : return None in_range ( loc1 , \"cartesian\" ) in_range ( loc2 , \"cartesian\" ) # loc1 = [x1, y1, z1]; loc2 = [x2, y2, z2] euclidean_distance = spatial . distance . euclidean ( loc1 , loc2 ) if unit == \"km\" : euclidean_distance /= 1000 return euclidean_distance def point_in_polygon ( x , y , polygon ): \"\"\" This function helps to check whether (x,y) is inside a polygon Parameters ---------- x x coordinate/longitude y y coordinate/latitude polygon polygon consists of list of (x,y)s Returns ------- Integer 1 if (x, y) is in the polygon and 0 otherwise. \"\"\" if ( x is None ) | ( y is None ): return None try : x = float ( x ) y = float ( y ) except : return None in_range (( y , x )) counter = 0 for index , poly in enumerate ( polygon ): # Check whether x and y are numbers if not isinstance ( x , numbers . Number ) or not isinstance ( y , numbers . Number ): raise TypeError ( \"Input coordinate should be of type float\" ) # Check whether poly is list of (x,y)s if any ([ not isinstance ( i , numbers . Number ) for point in poly for i in point ]): # Polygon from multipolygon have extra bracket - that need to be removed poly = poly [ 0 ] if any ( [ not isinstance ( i , numbers . Number ) for point in poly for i in point ] ): raise TypeError ( \"The polygon is invalid\" ) # Check if point is a vertex test_vertex = ( x , y ) if isinstance ( poly [ 0 ], tuple ) else [ x , y ] if test_vertex in poly : return 1 # Check if point is on a boundary poly_length = len ( poly ) for i in range ( poly_length - 1 ): p1 = poly [ i ] p2 = poly [ i + 1 ] if ( p1 [ 1 ] == p2 [ 1 ] and p1 [ 1 ] == y and ( min ( p1 [ 0 ], p2 [ 0 ]) <= x <= max ( p1 [ 0 ], p2 [ 0 ])) ): return 1 if ( p1 [ 0 ] == p2 [ 0 ] and p1 [ 0 ] == x and ( min ( p1 [ 1 ], p2 [ 1 ]) <= y <= max ( p1 [ 1 ], p2 [ 1 ])) ): return 1 # Check if the point is inside for i in range ( poly_length ): p1x , p1y = poly [ i ] p2x , p2y = poly [( i + 1 ) % poly_length ] if y > min ( p1y , p2y ): if y <= max ( p1y , p2y ): if x <= max ( p1x , p2x ): if p1y != p2y : xints = ( y - p1y ) * ( p2x - p1x ) / ( p2y - p1y ) + p1x if p1x == p2x or x <= xints : counter += 1 if counter % 2 == 0 : return 0 else : return 1 def point_in_polygons ( x , y , polygon_list , south_west_loc = [], north_east_loc = []): \"\"\" This function helps to check whether (x,y) is inside any polygon in a list of polygon Parameters ---------- x x coordinate/longitude y y coordinate/latitude polygon_list A list of polygon(s) south_west_loc The south-west point (x_sw, y_sw) of the bounding box of the polygons, if available. 0 will be directly returned if x < x_sw or y < y_sw (Default value = []) north_east_loc The north-east point (x_ne, y_ne) of the bounding box of the polygons, if available. (Default value = []) 0 will be directly returned if x > x_ne or y > y_ne (Default value = []) Returns ------- Integer 1 if (x, y) is inside any polygon of polygon_list and 0 otherwise. \"\"\" if ( x is None ) | ( y is None ): return None try : x = float ( x ) y = float ( y ) except : warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) return None in_range (( y , x )) if south_west_loc : if ( x < south_west_loc [ 0 ]) or ( y < south_west_loc [ 1 ]): return 0 if north_east_loc : if ( x > north_east_loc [ 0 ]) or ( y > north_east_loc [ 1 ]): return 0 flag_list = [] for polygon in polygon_list : flag_list . append ( point_in_polygon ( x , y , polygon )) return int ( any ( flag_list )) def f_point_in_polygons ( polygon_list , south_west_loc = [], north_east_loc = []): return F . udf ( lambda x , y : point_in_polygons ( x , y , polygon_list , south_west_loc , north_east_loc ), T . IntegerType (), ) COUNTRY_BOUNDING_BOXES = { \"AW\" : ( \"Aruba\" , ( - 70.2809842 , 12.1702998 , - 69.6409842 , 12.8102998 )), \"AF\" : ( \"Afghanistan\" , ( 60.5176034 , 29.3772 , 74.889862 , 38.4910682 )), \"AO\" : ( \"Angola\" , ( 11.4609793 , - 18.038945 , 24.0878856 , - 4.3880634 )), \"AI\" : ( \"Anguilla\" , ( - 63.6391992 , 18.0615454 , - 62.7125449 , 18.7951194 )), \"AL\" : ( \"Albania\" , ( 19.1246095 , 39.6448625 , 21.0574335 , 42.6610848 )), \"AD\" : ( \"Andorra\" , ( 1.4135781 , 42.4288238 , 1.7863837 , 42.6559357 )), \"AE\" : ( \"United Arab Emirates\" , ( 51.498 , 22.6444 , 56.3834 , 26.2822 )), \"AR\" : ( \"Argentina\" , ( - 73.5600329 , - 55.1850761 , - 53.6374515 , - 21.781168 )), \"AM\" : ( \"Armenia\" , ( 43.4471395 , 38.8404775 , 46.6333087 , 41.300712 )), \"AS\" : ( \"American Samoa\" , ( - 171.2951296 , - 14.7608358 , - 167.9322899 , - 10.8449746 )), \"AQ\" : ( \"Antarctica\" , ( - 180.0 , - 85.0511287 , 180.0 , - 60.0 )), \"AG\" : ( \"Antigua and Barbuda\" , ( - 62.5536517 , 16.7573901 , - 61.447857 , 17.929 )), \"AU\" : ( \"Australia\" , ( 72.2460938 , - 55.3228175 , 168.2249543 , - 9.0882278 )), \"AT\" : ( \"Austria\" , ( 9.5307487 , 46.3722761 , 17.160776 , 49.0205305 )), \"AZ\" : ( \"Azerbaijan\" , ( 44.7633701 , 38.3929551 , 51.0090302 , 41.9502947 )), \"BI\" : ( \"Burundi\" , ( 29.0007401 , - 4.4693155 , 30.8498462 , - 2.3096796 )), \"BE\" : ( \"Belgium\" , ( 2.3889137 , 49.4969821 , 6.408097 , 51.5516667 )), \"BJ\" : ( \"Benin\" , ( 0.776667 , 6.0398696 , 3.843343 , 12.4092447 )), \"BF\" : ( \"Burkina Faso\" , ( - 5.5132416 , 9.4104718 , 2.4089717 , 15.084 )), \"BD\" : ( \"Bangladesh\" , ( 88.0075306 , 20.3756582 , 92.6804979 , 26.6382534 )), \"BG\" : ( \"Bulgaria\" , ( 22.3571459 , 41.2353929 , 28.8875409 , 44.2167064 )), \"BH\" : ( \"Bahrain\" , ( 50.2697989 , 25.535 , 50.9233693 , 26.6872444 )), \"BS\" : ( \"Bahamas\" , ( - 80.7001941 , 20.7059846 , - 72.4477521 , 27.4734551 )), \"BA\" : ( \"Bosnia and Herzegovina\" , ( 15.7287433 , 42.5553114 , 19.6237311 , 45.2764135 )), \"BL\" : ( \"Saint Barth\u00e9lemy\" , ( - 63.06639 , 17.670931 , - 62.5844019 , 18.1375569 )), \"BY\" : ( \"Belarus\" , ( 23.1783344 , 51.2575982 , 32.7627809 , 56.17218 )), \"BZ\" : ( \"Belize\" , ( - 89.2262083 , 15.8857286 , - 87.3098494 , 18.496001 )), \"BM\" : ( \"Bermuda\" , ( - 65.1232222 , 32.0469651 , - 64.4109842 , 32.5913693 )), \"BO\" : ( \"Bolivia (Plurinational State of)\" , ( - 69.6450073 , - 22.8982742 , - 57.453 , - 9.6689438 ), ), \"BR\" : ( \"Brazil\" , ( - 73.9830625 , - 33.8689056 , - 28.6341164 , 5.2842873 )), \"BB\" : ( \"Barbados\" , ( - 59.8562115 , 12.845 , - 59.2147175 , 13.535 )), \"BN\" : ( \"Brunei Darussalam\" , ( 114.0758734 , 4.002508 , 115.3635623 , 5.1011857 )), \"BT\" : ( \"Bhutan\" , ( 88.7464724 , 26.702016 , 92.1252321 , 28.246987 )), \"BW\" : ( \"Botswana\" , ( 19.9986474 , - 26.9059669 , 29.375304 , - 17.778137 )), \"CF\" : ( \"Central African Republic\" , ( 14.4155426 , 2.2156553 , 27.4540764 , 11.001389 )), \"CA\" : ( \"Canada\" , ( - 141.00275 , 41.6765556 , - 52.3231981 , 83.3362128 )), \"CC\" : ( \"Cocos (Keeling) Islands\" , ( 96.612524 , - 12.4055983 , 97.1357343 , - 11.6213132 ), ), \"CH\" : ( \"Switzerland\" , ( 5.9559113 , 45.817995 , 10.4922941 , 47.8084648 )), \"CL\" : ( \"Chile\" , ( - 109.6795789 , - 56.725 , - 66.0753474 , - 17.4983998 )), \"CN\" : ( \"China\" , ( 73.4997347 , 8.8383436 , 134.7754563 , 53.5608154 )), \"CI\" : ( \"C\u00f4te d'Ivoire\" , ( - 8.601725 , 4.1621205 , - 2.493031 , 10.740197 )), \"CM\" : ( \"Cameroon\" , ( 8.3822176 , 1.6546659 , 16.1921476 , 13.083333 )), \"CD\" : ( \"Congo, Democratic Republic of the\" , ( 12.039074 , - 13.459035 , 31.3056758 , 5.3920026 ), ), \"CG\" : ( \"Congo\" , ( 11.0048205 , - 5.149089 , 18.643611 , 3.713056 )), \"CK\" : ( \"Cook Islands\" , ( - 166.0856468 , - 22.15807 , - 157.1089329 , - 8.7168792 )), \"CO\" : ( \"Colombia\" , ( - 82.1243666 , - 4.2316872 , - 66.8511907 , 16.0571269 )), \"KM\" : ( \"Comoros\" , ( 43.025305 , - 12.621 , 44.7451922 , - 11.165 )), \"CV\" : ( \"Cabo Verde\" , ( - 25.3609478 , 14.8031546 , - 22.6673416 , 17.2053108 )), \"CR\" : ( \"Costa Rica\" , ( - 87.2722647 , 5.3329698 , - 82.5060208 , 11.2195684 )), \"CU\" : ( \"Cuba\" , ( - 85.1679702 , 19.6275294 , - 73.9190004 , 23.4816972 )), \"CX\" : ( \"Christmas Island\" , ( 105.5336422 , - 10.5698515 , 105.7130159 , - 10.4123553 )), \"KY\" : ( \"Cayman Islands\" , ( - 81.6313748 , 19.0620619 , - 79.5110954 , 19.9573759 )), \"CY\" : ( \"Cyprus\" , ( 32.0227581 , 34.4383706 , 34.8553182 , 35.913252 )), \"CZ\" : ( \"Czechia\" , ( 12.0905901 , 48.5518083 , 18.859216 , 51.0557036 )), \"DE\" : ( \"Germany\" , ( 5.8663153 , 47.2701114 , 15.0419319 , 55.099161 )), \"DJ\" : ( \"Djibouti\" , ( 41.7713139 , 10.9149547 , 43.6579046 , 12.7923081 )), \"DM\" : ( \"Dominica\" , ( - 61.6869184 , 15.0074207 , - 61.0329895 , 15.7872222 )), \"DK\" : ( \"Denmark\" , ( 7.7153255 , 54.4516667 , 15.5530641 , 57.9524297 )), \"DO\" : ( \"Dominican Republic\" , ( - 72.0574706 , 17.2701708 , - 68.1101463 , 21.303433 )), \"DZ\" : ( \"Algeria\" , ( - 8.668908 , 18.968147 , 11.997337 , 37.2962055 )), \"EC\" : ( \"Ecuador\" , ( - 92.2072392 , - 5.0159314 , - 75.192504 , 1.8835964 )), \"EG\" : ( \"Egypt\" , ( 24.6499112 , 22.0 , 37.1153517 , 31.8330854 )), \"ER\" : ( \"Eritrea\" , ( 36.4333653 , 12.3548219 , 43.3001714 , 18.0709917 )), \"EH\" : ( \"Western Sahara\" , ( - 17.3494721 , 20.556883 , - 8.666389 , 27.6666834 )), \"ES\" : ( \"Spain\" , ( - 18.3936845 , 27.4335426 , 4.5918885 , 43.9933088 )), \"EE\" : ( \"Estonia\" , ( 21.3826069 , 57.5092997 , 28.2100175 , 59.9383754 )), \"ET\" : ( \"Ethiopia\" , ( 32.9975838 , 3.397448 , 47.9823797 , 14.8940537 )), \"FI\" : ( \"Finland\" , ( 19.0832098 , 59.4541578 , 31.5867071 , 70.0922939 )), \"FJ\" : ( \"Fiji\" , ( 172.0 , - 21.9434274 , - 178.5 , - 12.2613866 )), \"FK\" : ( \"Falkland Islands (Malvinas)\" , ( - 61.7726772 , - 53.1186766 , - 57.3662367 , - 50.7973007 ), ), \"FR\" : ( \"France\" , ( - 5.4534286 , 41.2632185 , 9.8678344 , 51.268318 )), \"FO\" : ( \"Faroe Islands\" , ( - 7.6882939 , 61.3915553 , - 6.2565525 , 62.3942991 )), \"FM\" : ( \"Micronesia (Federated States of)\" , ( 137.2234512 , 0.827 , 163.2364054 , 10.291 ), ), \"GA\" : ( \"Gabon\" , ( 8.5002246 , - 4.1012261 , 14.539444 , 2.3182171 )), \"GB\" : ( \"United Kingdom of Great Britain and Northern Ireland\" , ( - 14.015517 , 49.674 , 2.0919117 , 61.061 ), ), \"GE\" : ( \"Georgia\" , ( 39.8844803 , 41.0552922 , 46.7365373 , 43.5864294 )), \"GG\" : ( \"Guernsey\" , ( - 2.6751703 , 49.4155331 , - 2.501814 , 49.5090776 )), \"GH\" : ( \"Ghana\" , ( - 3.260786 , 4.5392525 , 1.2732942 , 11.1748562 )), \"GI\" : ( \"Gibraltar\" , ( - 5.3941295 , 36.100807 , - 5.3141295 , 36.180807 )), \"GN\" : ( \"Guinea\" , ( - 15.5680508 , 7.1906045 , - 7.6381993 , 12.67563 )), \"GM\" : ( \"Gambia\" , ( - 17.0288254 , 13.061 , - 13.797778 , 13.8253137 )), \"GW\" : ( \"Guinea-Bissau\" , ( - 16.894523 , 10.6514215 , - 13.6348777 , 12.6862384 )), \"GQ\" : ( \"Equatorial Guinea\" , ( 5.4172943 , - 1.6732196 , 11.3598628 , 3.989 )), \"GR\" : ( \"Greece\" , ( 19.2477876 , 34.7006096 , 29.7296986 , 41.7488862 )), \"GD\" : ( \"Grenada\" , ( - 62.0065868 , 11.786 , - 61.1732143 , 12.5966532 )), \"GL\" : ( \"Greenland\" , ( - 74.1250416 , 59.515387 , - 10.0288759 , 83.875172 )), \"GT\" : ( \"Guatemala\" , ( - 92.3105242 , 13.6345804 , - 88.1755849 , 17.8165947 )), \"GU\" : ( \"Guam\" , ( 144.563426 , 13.182335 , 145.009167 , 13.706179 )), \"GY\" : ( \"Guyana\" , ( - 61.414905 , 1.1710017 , - 56.4689543 , 8.6038842 )), \"HK\" : ( \"Hong Kong\" , ( 114.0028131 , 22.1193278 , 114.3228131 , 22.4393278 )), \"HN\" : ( \"Honduras\" , ( - 89.3568207 , 12.9808485 , - 82.1729621 , 17.619526 )), \"HR\" : ( \"Croatia\" , ( 13.2104814 , 42.1765993 , 19.4470842 , 46.555029 )), \"HT\" : ( \"Haiti\" , ( - 75.2384618 , 17.9099291 , - 71.6217461 , 20.2181368 )), \"HU\" : ( \"Hungary\" , ( 16.1138867 , 45.737128 , 22.8977094 , 48.585257 )), \"ID\" : ( \"Indonesia\" , ( 94.7717124 , - 11.2085669 , 141.0194444 , 6.2744496 )), \"IM\" : ( \"Isle of Man\" , ( - 4.7946845 , 54.0539576 , - 4.3076853 , 54.4178705 )), \"IN\" : ( \"India\" , ( 68.1113787 , 6.5546079 , 97.395561 , 35.6745457 )), \"IO\" : ( \"British Indian Ocean Territory\" , ( 71.036504 , - 7.6454079 , 72.7020157 , - 5.037066 ), ), \"IE\" : ( \"Ireland\" , ( - 11.0133788 , 51.222 , - 5.6582363 , 55.636 )), \"IR\" : ( \"Iran (Islamic Republic of)\" , ( 44.0318908 , 24.8465103 , 63.3332704 , 39.7816502 ), ), \"IQ\" : ( \"Iraq\" , ( 38.7936719 , 29.0585661 , 48.8412702 , 37.380932 )), \"IS\" : ( \"Iceland\" , ( - 25.0135069 , 63.0859177 , - 12.8046162 , 67.353 )), \"IL\" : ( \"Israel\" , ( 34.2674994 , 29.4533796 , 35.8950234 , 33.3356317 )), \"IT\" : ( \"Italy\" , ( 6.6272658 , 35.2889616 , 18.7844746 , 47.0921462 )), \"JM\" : ( \"Jamaica\" , ( - 78.5782366 , 16.5899443 , - 75.7541143 , 18.7256394 )), \"JE\" : ( \"Jersey\" , ( - 2.254512 , 49.1625179 , - 2.0104193 , 49.2621288 )), \"JO\" : ( \"Jordan\" , ( 34.8844372 , 29.183401 , 39.3012981 , 33.3750617 )), \"JP\" : ( \"Japan\" , ( 122.7141754 , 20.2145811 , 154.205541 , 45.7112046 )), \"KZ\" : ( \"Kazakhstan\" , ( 46.4932179 , 40.5686476 , 87.3156316 , 55.4421701 )), \"KE\" : ( \"Kenya\" , ( 33.9098987 , - 4.8995204 , 41.899578 , 4.62 )), \"KG\" : ( \"Kyrgyzstan\" , ( 69.2649523 , 39.1728437 , 80.2295793 , 43.2667971 )), \"KH\" : ( \"Cambodia\" , ( 102.3338282 , 9.4752639 , 107.6276788 , 14.6904224 )), \"KI\" : ( \"Kiribati\" , ( - 179.1645388 , - 7.0516717 , - 164.1645388 , 7.9483283 )), \"KN\" : ( \"Saint Kitts and Nevis\" , ( - 63.051129 , 16.895 , - 62.3303519 , 17.6158146 )), \"KR\" : ( \"Korea, Republic of\" , ( 124.354847 , 32.9104556 , 132.1467806 , 38.623477 )), \"KW\" : ( \"Kuwait\" , ( 46.5526837 , 28.5243622 , 49.0046809 , 30.1038082 )), \"LA\" : ( \"Lao People's Democratic Republic\" , ( 100.0843247 , 13.9096752 , 107.6349989 , 22.5086717 ), ), \"LB\" : ( \"Lebanon\" , ( 34.8825667 , 33.0479858 , 36.625 , 34.6923543 )), \"LR\" : ( \"Liberia\" , ( - 11.6080764 , 4.1555907 , - 7.367323 , 8.5519861 )), \"LY\" : ( \"Libya\" , ( 9.391081 , 19.5008138 , 25.3770629 , 33.3545898 )), \"LC\" : ( \"Saint Lucia\" , ( - 61.2853867 , 13.508 , - 60.6669363 , 14.2725 )), \"LI\" : ( \"Liechtenstein\" , ( 9.4716736 , 47.0484291 , 9.6357143 , 47.270581 )), \"LK\" : ( \"Sri Lanka\" , ( 79.3959205 , 5.719 , 82.0810141 , 10.035 )), \"LS\" : ( \"Lesotho\" , ( 27.0114632 , - 30.6772773 , 29.4557099 , - 28.570615 )), \"LT\" : ( \"Lithuania\" , ( 20.653783 , 53.8967893 , 26.8355198 , 56.4504213 )), \"LU\" : ( \"Luxembourg\" , ( 4.9684415 , 49.4969821 , 6.0344254 , 50.430377 )), \"LV\" : ( \"Latvia\" , ( 20.6715407 , 55.6746505 , 28.2414904 , 58.0855688 )), \"MO\" : ( \"Macao\" , ( 113.5281666 , 22.0766667 , 113.6301389 , 22.2170361 )), \"MF\" : ( \"Saint Martin (French part)\" , ( - 63.3605643 , 17.8963535 , - 62.7644063 , 18.1902778 ), ), \"MA\" : ( \"Morocco\" , ( - 17.2551456 , 21.3365321 , - 0.998429 , 36.0505269 )), \"MC\" : ( \"Monaco\" , ( 7.4090279 , 43.7247599 , 7.4398704 , 43.7519311 )), \"MD\" : ( \"Moldova, Republic of\" , ( 26.6162189 , 45.4674139 , 30.1636756 , 48.4918695 )), \"MG\" : ( \"Madagascar\" , ( 43.2202072 , - 25.6071002 , 50.4862553 , - 11.9519693 )), \"MV\" : ( \"Maldives\" , ( 72.3554187 , - 0.9074935 , 73.9700962 , 7.3106246 )), \"MX\" : ( \"Mexico\" , ( - 118.59919 , 14.3886243 , - 86.493266 , 32.7186553 )), \"MH\" : ( \"Marshall Islands\" , ( 163.4985095 , - 0.5481258 , 178.4985095 , 14.4518742 )), \"MK\" : ( \"North Macedonia\" , ( 20.4529023 , 40.8536596 , 23.034051 , 42.3735359 )), \"ML\" : ( \"Mali\" , ( - 12.2402835 , 10.147811 , 4.2673828 , 25.001084 )), \"MT\" : ( \"Malta\" , ( 13.9324226 , 35.6029696 , 14.8267966 , 36.2852706 )), \"MM\" : ( \"Myanmar\" , ( 92.1719423 , 9.4399432 , 101.1700796 , 28.547835 )), \"ME\" : ( \"Montenegro\" , ( 18.4195781 , 41.7495999 , 20.3561641 , 43.5585061 )), \"MN\" : ( \"Mongolia\" , ( 87.73762 , 41.5800276 , 119.931949 , 52.1496 )), \"MP\" : ( \"Northern Mariana Islands\" , ( 144.813338 , 14.036565 , 146.154418 , 20.616556 )), \"MZ\" : ( \"Mozambique\" , ( 30.2138197 , - 26.9209427 , 41.0545908 , - 10.3252149 )), \"MR\" : ( \"Mauritania\" , ( - 17.068081 , 14.7209909 , - 4.8333344 , 27.314942 )), \"MS\" : ( \"Montserrat\" , ( - 62.450667 , 16.475 , - 61.9353818 , 17.0152978 )), \"MU\" : ( \"Mauritius\" , ( 56.3825151 , - 20.725 , 63.7151319 , - 10.138 )), \"MW\" : ( \"Malawi\" , ( 32.6703616 , - 17.1296031 , 35.9185731 , - 9.3683261 )), \"MY\" : ( \"Malaysia\" , ( 105.3471939 , - 5.1076241 , 120.3471939 , 9.8923759 )), \"YT\" : ( \"Mayotte\" , ( 45.0183298 , - 13.0210119 , 45.2999917 , - 12.6365902 )), \"NA\" : ( \"Namibia\" , ( 11.5280384 , - 28.96945 , 25.2617671 , - 16.9634855 )), \"NC\" : ( \"New Caledonia\" , ( 162.6034343 , - 23.2217509 , 167.8109827 , - 17.6868616 )), \"NE\" : ( \"Niger\" , ( 0.1689653 , 11.693756 , 15.996667 , 23.517178 )), \"NG\" : ( \"Nigeria\" , ( 2.676932 , 4.0690959 , 14.678014 , 13.885645 )), \"NI\" : ( \"Nicaragua\" , ( - 87.901532 , 10.7076565 , - 82.6227023 , 15.0331183 )), \"NU\" : ( \"Niue\" , ( - 170.1595029 , - 19.3548665 , - 169.5647229 , - 18.7534559 )), \"NL\" : ( \"Netherlands\" , ( 1.9193492 , 50.7295671 , 7.2274985 , 53.7253321 )), \"NO\" : ( \"Norway\" , ( 4.0875274 , 57.7590052 , 31.7614911 , 71.3848787 )), \"NP\" : ( \"Nepal\" , ( 80.0586226 , 26.3477581 , 88.2015257 , 30.446945 )), \"NR\" : ( \"Nauru\" , ( 166.9091794 , - 0.5541334 , 166.9589235 , - 0.5025906 )), \"NZ\" : ( \"New Zealand\" , ( - 179.059153 , - 52.8213687 , 179.3643594 , - 29.0303303 )), \"OM\" : ( \"Oman\" , ( 52 , 16.4649608 , 60.054577 , 26.7026737 )), \"PK\" : ( \"Pakistan\" , ( 60.872855 , 23.5393916 , 77.1203914 , 37.084107 )), \"PA\" : ( \"Panama\" , ( - 83.0517245 , 7.0338679 , - 77.1393779 , 9.8701757 )), \"PN\" : ( \"Pitcairn\" , ( - 130.8049862 , - 25.1306736 , - 124.717534 , - 23.8655769 )), \"PE\" : ( \"Peru\" , ( - 84.6356535 , - 20.1984472 , - 68.6519906 , - 0.0392818 )), \"PH\" : ( \"Philippines\" , ( 114.0952145 , 4.2158064 , 126.8072562 , 21.3217806 )), \"PW\" : ( \"Palau\" , ( 131.0685462 , 2.748 , 134.7714735 , 8.222 )), \"PG\" : ( \"Papua New Guinea\" , ( 136.7489081 , - 13.1816069 , 151.7489081 , 1.8183931 )), \"PL\" : ( \"Poland\" , ( 14.1229707 , 49.0020468 , 24.145783 , 55.0336963 )), \"PR\" : ( \"Puerto Rico\" , ( - 67.271492 , 17.9268695 , - 65.5897525 , 18.5159789 )), \"KP\" : ( \"Korea (Democratic People's Republic of)\" , ( 124.0913902 , 37.5867855 , 130.924647 , 43.0089642 ), ), \"PT\" : ( \"Portugal\" , ( - 31.5575303 , 29.8288021 , - 6.1891593 , 42.1543112 )), \"PY\" : ( \"Paraguay\" , ( - 62.6442036 , - 27.6063935 , - 54.258 , - 19.2876472 )), \"PS\" : ( \"Palestine, State of\" , ( 34.0689732 , 31.2201289 , 35.5739235 , 32.5521479 )), \"PF\" : ( \"French Polynesia\" , ( - 154.9360599 , - 28.0990232 , - 134.244799 , - 7.6592173 )), \"QA\" : ( \"Qatar\" , ( 50.5675 , 24.4707534 , 52.638011 , 26.3830212 )), \"RE\" : ( \"R\u00e9union\" , ( 55.2164268 , - 21.3897308 , 55.8366924 , - 20.8717136 )), \"RO\" : ( \"Romania\" , ( 20.2619773 , 43.618682 , 30.0454257 , 48.2653964 )), \"RU\" : ( \"Russian Federation\" , ( 19.6389 , 41.1850968 , 180 , 82.0586232 )), \"RW\" : ( \"Rwanda\" , ( 28.8617546 , - 2.8389804 , 30.8990738 , - 1.0474083 )), \"SA\" : ( \"Saudi Arabia\" , ( 34.4571718 , 16.29 , 55.6666851 , 32.1543377 )), \"SD\" : ( \"Sudan\" , ( 21.8145046 , 8.685278 , 39.0576252 , 22.224918 )), \"SN\" : ( \"Senegal\" , ( - 17.7862419 , 12.2372838 , - 11.3458996 , 16.6919712 )), \"SG\" : ( \"Singapore\" , ( 103.6920359 , 1.1304753 , 104.0120359 , 1.4504753 )), \"SH\" : ( \"Saint Helena, Ascension and Tristan da Cunha\" , ( - 5.9973424 , - 16.23 , - 5.4234153 , - 15.704 ), ), \"SJ\" : ( \"Svalbard and Jan Mayen\" , ( - 9.6848146 , 70.6260825 , 34.6891253 , 81.028076 )), \"SB\" : ( \"Solomon Islands\" , ( 155.3190556 , - 13.2424298 , 170.3964667 , - 4.81085 )), \"SL\" : ( \"Sierra Leone\" , ( - 13.5003389 , 6.755 , - 10.271683 , 9.999973 )), \"SV\" : ( \"El Salvador\" , ( - 90.1790975 , 12.976046 , - 87.6351394 , 14.4510488 )), \"SM\" : ( \"San Marino\" , ( 12.4033246 , 43.8937002 , 12.5160665 , 43.992093 )), \"SO\" : ( \"Somalia\" , ( 40.98918 , - 1.8031969 , 51.6177696 , 12.1889121 )), \"PM\" : ( \"Saint Pierre and Miquelon\" , ( - 56.6972961 , 46.5507173 , - 55.9033333 , 47.365 )), \"RS\" : ( \"Serbia\" , ( 18.8142875 , 42.2322435 , 23.006309 , 46.1900524 )), \"ST\" : ( \"Sao Tome and Principe\" , ( 6.260642 , - 0.2135137 , 7.6704783 , 1.9257601 )), \"SR\" : ( \"Suriname\" , ( - 58.070833 , 1.8312802 , - 53.8433358 , 6.225 )), \"SK\" : ( \"Slovakia\" , ( 16.8331891 , 47.7314286 , 22.56571 , 49.6138162 )), \"SI\" : ( \"Slovenia\" , ( 13.3754696 , 45.4214242 , 16.5967702 , 46.8766816 )), \"SE\" : ( \"Sweden\" , ( 10.5930952 , 55.1331192 , 24.1776819 , 69.0599699 )), \"SZ\" : ( \"Eswatini\" , ( 30.7908 , - 27.3175201 , 32.1349923 , - 25.71876 )), \"SC\" : ( \"Seychelles\" , ( 45.9988759 , - 10.4649258 , 56.4979396 , - 3.512 )), \"SY\" : ( \"Syrian Arab Republic\" , ( 35.4714427 , 32.311354 , 42.3745687 , 37.3184589 )), \"TC\" : ( \"Turks and Caicos Islands\" , ( - 72.6799046 , 20.9553418 , - 70.8643591 , 22.1630989 ), ), \"TD\" : ( \"Chad\" , ( 13.47348 , 7.44107 , 24.0 , 23.4975 )), \"TG\" : ( \"Togo\" , ( - 0.1439746 , 5.926547 , 1.8087605 , 11.1395102 )), \"TH\" : ( \"Thailand\" , ( 97.3438072 , 5.612851 , 105.636812 , 20.4648337 )), \"TJ\" : ( \"Tajikistan\" , ( 67.3332775 , 36.6711153 , 75.1539563 , 41.0450935 )), \"TK\" : ( \"Tokelau\" , ( - 172.7213673 , - 9.6442499 , - 170.9797586 , - 8.3328631 )), \"TM\" : ( \"Turkmenistan\" , ( 52.335076 , 35.129093 , 66.6895177 , 42.7975571 )), \"TL\" : ( \"Timor-Leste\" , ( 124.0415703 , - 9.5642775 , 127.5335392 , - 8.0895459 )), \"TO\" : ( \"Tonga\" , ( - 179.3866055 , - 24.1034499 , - 173.5295458 , - 15.3655722 )), \"TT\" : ( \"Trinidad and Tobago\" , ( - 62.083056 , 9.8732106 , - 60.2895848 , 11.5628372 )), \"TN\" : ( \"Tunisia\" , ( 7.5219807 , 30.230236 , 11.8801133 , 37.7612052 )), \"TR\" : ( \"Turkey\" , ( 25.6212891 , 35.8076804 , 44.8176638 , 42.297 )), \"TV\" : ( \"Tuvalu\" , ( 175.1590468 , - 9.9939389 , 178.7344938 , - 5.4369611 )), \"TW\" : ( \"Taiwan, Province of China\" , ( 114.3599058 , 10.374269 , 122.297 , 26.4372222 )), \"TZ\" : ( \"Tanzania, United Republic of\" , ( 29.3269773 , - 11.761254 , 40.6584071 , - 0.9854812 ), ), \"UG\" : ( \"Uganda\" , ( 29.573433 , - 1.4823179 , 35.000308 , 4.2340766 )), \"UA\" : ( \"Ukraine\" , ( 22.137059 , 44.184598 , 40.2275801 , 52.3791473 )), \"UY\" : ( \"Uruguay\" , ( - 58.4948438 , - 35.7824481 , - 53.0755833 , - 30.0853962 )), \"US\" : ( \"United States of America\" , ( - 125.0011 , 24.9493 , - 66.9326 , 49.5904 )), \"UZ\" : ( \"Uzbekistan\" , ( 55.9977865 , 37.1821164 , 73.1397362 , 45.590118 )), \"VA\" : ( \"Holy See\" , ( 12.4457442 , 41.9002044 , 12.4583653 , 41.9073912 )), \"VC\" : ( \"Saint Vincent and the Grenadines\" , ( - 61.6657471 , 12.5166548 , - 60.9094146 , 13.583 ), ), \"VE\" : ( \"Venezuela (Bolivarian Republic of)\" , ( - 73.3529632 , 0.647529 , - 59.5427079 , 15.9158431 ), ), \"VG\" : ( \"Virgin Islands (British)\" , ( - 65.159094 , 17.623468 , - 64.512674 , 18.464984 )), \"VI\" : ( \"Virgin Islands (U.S.)\" , ( - 65.159094 , 17.623468 , - 64.512674 , 18.464984 )), \"VN\" : ( \"Viet Nam\" , ( 102.14441 , 8.1790665 , 114.3337595 , 23.393395 )), \"VU\" : ( \"Vanuatu\" , ( 166.3355255 , - 20.4627425 , 170.449982 , - 12.8713777 )), \"WF\" : ( \"Wallis and Futuna\" , ( - 178.3873749 , - 14.5630748 , - 175.9190391 , - 12.9827961 )), \"WS\" : ( \"Samoa\" , ( - 173.0091864 , - 14.2770916 , - 171.1929229 , - 13.2381892 )), \"YE\" : ( \"Yemen\" , ( 41.60825 , 11.9084802 , 54.7389375 , 19.0 )), \"ZA\" : ( \"South Africa\" , ( 16.3335213 , - 47.1788335 , 38.2898954 , - 22.1250301 )), \"ZM\" : ( \"Zambia\" , ( 21.9993509 , - 18.0765945 , 33.701111 , - 8.2712822 )), \"ZW\" : ( \"Zimbabwe\" , ( 25.2373 , - 22.4241096 , 33.0683413 , - 15.6097033 )), } def point_in_country_approx ( lat , lon , country ): c = COUNTRY_BOUNDING_BOXES [ country ] if ( lat is None ) | ( lon is None ): return None try : lat = float ( lat ) lon = float ( lon ) except : warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) return None in_range (( lat , lon )) if ( c [ 1 ][ 1 ] <= lat <= c [ 1 ][ 3 ]) and ( c [ 1 ][ 0 ] <= lon <= c [ 1 ][ 2 ]): return 1 else : return 0 Functions def decimal_degrees_to_degrees_minutes_seconds ( dd) This function helps to divide float value dd in decimal degree into [degreee, minute, second] Parameters dd Float value in decimal degree. Returns List [degree, minute, second] Expand source code def decimal_degrees_to_degrees_minutes_seconds ( dd ): \"\"\" This function helps to divide float value dd in decimal degree into [degreee, minute, second] Parameters ---------- dd Float value in decimal degree. Returns ------- List [degree, minute, second] \"\"\" if dd is None : return [ None , None , None ] else : minute , second = divmod ( dd * 3600 , 60 ) degree , minute = divmod ( minute , 60 ) return [ degree , minute , second ] def euclidean_distance ( loc1, loc2, unit='m') The Euclidean distance between 2 lists loc1 and loc2, is defined as [ {|loc1-loc2|}_2 ] Parameters loc1 The first location. [x1, y1, z1] format is required. loc2 The second location. [x2, y2, z2] format is required. unit \"m\", \"km\". Unit of the result. (Default value = \"m\") Returns Float Expand source code def euclidean_distance ( loc1 , loc2 , unit = \"m\" ): \"\"\" The Euclidean distance between 2 lists loc1 and loc2, is defined as .. math:: {\\\\|loc1-loc2\\\\|}_2 Parameters ---------- loc1 The first location. [x1, y1, z1] format is required. loc2 The second location. [x2, y2, z2] format is required. unit \"m\", \"km\". Unit of the result. (Default value = \"m\") Returns ------- Float \"\"\" if None in [ loc1 , loc2 ]: return None if None in loc1 + loc2 : return None try : loc1 = [ float ( i ) for i in loc1 ] loc2 = [ float ( i ) for i in loc2 ] except : return None in_range ( loc1 , \"cartesian\" ) in_range ( loc2 , \"cartesian\" ) # loc1 = [x1, y1, z1]; loc2 = [x2, y2, z2] euclidean_distance = spatial . distance . euclidean ( loc1 , loc2 ) if unit == \"km\" : euclidean_distance /= 1000 return euclidean_distance def f_point_in_polygons ( polygon_list, south_west_loc=[], north_east_loc=[]) Expand source code def f_point_in_polygons ( polygon_list , south_west_loc = [], north_east_loc = []): return F . udf ( lambda x , y : point_in_polygons ( x , y , polygon_list , south_west_loc , north_east_loc ), T . IntegerType (), ) def from_latlon_decimal_degrees ( loc, output_format, radius=6371009, geohash_precision=8) This function helps to transform [lat,lon] locations into desired output_format. Parameters loc Location to format with format [lat, lon]. output_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". radius Radius of Earth. (Default value = EARTH_RADIUS) geohash_precision Precision of the resultant geohash. This argument is only used when output_format is \"geohash\". (Default value = 8) Returns String if output_format is \"geohash\", list otherwise. [lat, lon] if output_format is \"dd\". [[d1,m1,s1], [d2,m2,s2]] if output_format is \"dms\". [lat_radian, lon_radian] if output_format is \"radian\". [x, y, z] if output_format is \"cartesian\". string if output_format is \"geohash\". Expand source code def from_latlon_decimal_degrees ( loc , output_format , radius = EARTH_RADIUS , geohash_precision = 8 ): \"\"\" This function helps to transform [lat,lon] locations into desired output_format. Parameters ---------- loc Location to format with format [lat, lon]. output_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". radius Radius of Earth. (Default value = EARTH_RADIUS) geohash_precision Precision of the resultant geohash. This argument is only used when output_format is \"geohash\". (Default value = 8) Returns ------- String if output_format is \"geohash\", list otherwise. [lat, lon] if output_format is \"dd\". [[d1,m1,s1], [d2,m2,s2]] if output_format is \"dms\". [lat_radian, lon_radian] if output_format is \"radian\". [x, y, z] if output_format is \"cartesian\". string if output_format is \"geohash\". \"\"\" # loc = [lat, lon] if loc is None : lat , lon = None , None else : lat , lon = loc [ 0 ], loc [ 1 ] if output_format == \"dd\" : return [ lat , lon ] elif output_format == \"dms\" : return [ decimal_degrees_to_degrees_minutes_seconds ( lat ), decimal_degrees_to_degrees_minutes_seconds ( lon ), ] elif output_format == \"radian\" : if ( lat is None ) | ( lon is None ): return [ None , None ] else : lat_rad = radians ( float ( lat )) lon_rad = radians ( float ( lon )) return [ lat_rad , lon_rad ] elif output_format == \"cartesian\" : if ( lat is None ) | ( lon is None ): return [ None , None , None ] else : lat_rad = radians ( float ( lat )) lon_rad = radians ( float ( lon )) x = radius * cos ( lat_rad ) * cos ( lon_rad ) y = radius * cos ( lat_rad ) * sin ( lon_rad ) z = radius * sin ( lat_rad ) return [ x , y , z ] elif output_format == \"geohash\" : if ( lat is None ) | ( lon is None ): return None else : return pgh . encode ( lat , lon , geohash_precision ) def haversine_distance ( loc1, loc2, loc_format, unit='m', radius=6371009) This function helps to calculate the haversine distance between loc1 and loc2. Parameters loc1 The first location. If loc_format is \"dd\", [lat, lon] format is required. If loc_format is \"radian\", [lat_radian, lon_radian] format is required. loc2 The second location . If loc_format is \"dd\", [lat, lon] format is required. If loc_format is \"radian\", [lat_radian, lon_radian] format is required. loc_format \"dd\", \"radian\". unit \"m\", \"km\". Unit of the result. (Default value = \"m\") radius Radius of Earth. (Default value = EARTH_RADIUS) Returns Float Expand source code def haversine_distance ( loc1 , loc2 , loc_format , unit = \"m\" , radius = EARTH_RADIUS ): \"\"\" This function helps to calculate the haversine distance between loc1 and loc2. Parameters ---------- loc1 The first location. If loc_format is \"dd\", [lat, lon] format is required. If loc_format is \"radian\", [lat_radian, lon_radian] format is required. loc2 The second location . If loc_format is \"dd\", [lat, lon] format is required. If loc_format is \"radian\", [lat_radian, lon_radian] format is required. loc_format \"dd\", \"radian\". unit \"m\", \"km\". Unit of the result. (Default value = \"m\") radius Radius of Earth. (Default value = EARTH_RADIUS) Returns ------- Float \"\"\" # loc1 = [lat1, lon1]; loc2 = [lat2, lon2] if None in [ loc1 , loc2 ]: return None if None in loc1 + loc2 : return None if loc_format not in [ \"dd\" , \"radian\" ]: raise TypeError ( \"Invalid input for loc_format\" ) try : lat1 , lon1 = float ( loc1 [ 0 ]), float ( loc1 [ 1 ]) lat2 , lon2 = float ( loc2 [ 0 ]), float ( loc2 [ 1 ]) except : return None in_range (( lat1 , lon1 ), loc_format ) in_range (( lat2 , lon2 ), loc_format ) if loc_format == \"dd\" : lat1 , lon1 = radians ( lat1 ), radians ( lon1 ) lat2 , lon2 = radians ( lat2 ), radians ( lon2 ) dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 c = 2 * atan2 ( sqrt ( a ), sqrt ( 1 - a )) distance = radius * c / UNIT_FACTOR [ unit ] return distance def in_range ( loc, loc_format='dd') This function helps to check if the input location is in range based on loc_format. Parameters loc Location to check if in range. If loc_format is \"dd\", [lat, lon] format is required. If loc_format is \"dms\", [[d1,m1,s1], [d2,m2,s2]] format is required. If loc_format is \"radian\", [lat_radian, lon_radian] format is required. If loc_format is \"cartesian\", [x, y, z] format is required. If loc_format is \"geohash\", string format is required. loc_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". (Default value = \"dd\") Expand source code def in_range ( loc , loc_format = \"dd\" ): \"\"\" This function helps to check if the input location is in range based on loc_format. Parameters ---------- loc Location to check if in range. If loc_format is \"dd\", [lat, lon] format is required. If loc_format is \"dms\", [[d1,m1,s1], [d2,m2,s2]] format is required. If loc_format is \"radian\", [lat_radian, lon_radian] format is required. If loc_format is \"cartesian\", [x, y, z] format is required. If loc_format is \"geohash\", string format is required. loc_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". (Default value = \"dd\") \"\"\" if loc_format == \"dd\" : try : lat , lon = [ int ( float ( i )) for i in loc ] except : lat , lon = None , None else : try : lat , lon = [ int ( float ( i )) for i in to_latlon_decimal_degrees ( loc , loc_format ) ] except : lat , lon = None , None if None not in [ lat , lon ]: if lat > 90 or lat < - 90 or lon > 180 or lon < - 180 : warnings . warn ( \"Rows may contain unintended values due to longitude and/or latitude values being out of the \" \"valid range\" ) def point_in_country_approx ( lat, lon, country) Expand source code def point_in_country_approx ( lat , lon , country ): c = COUNTRY_BOUNDING_BOXES [ country ] if ( lat is None ) | ( lon is None ): return None try : lat = float ( lat ) lon = float ( lon ) except : warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) return None in_range (( lat , lon )) if ( c [ 1 ][ 1 ] <= lat <= c [ 1 ][ 3 ]) and ( c [ 1 ][ 0 ] <= lon <= c [ 1 ][ 2 ]): return 1 else : return 0 def point_in_polygon ( x, y, polygon) This function helps to check whether (x,y) is inside a polygon Parameters x x coordinate/longitude y y coordinate/latitude polygon polygon consists of list of (x,y)s Returns Integer 1 if (x, y) is in the polygon and 0 otherwise. Expand source code def point_in_polygon ( x , y , polygon ): \"\"\" This function helps to check whether (x,y) is inside a polygon Parameters ---------- x x coordinate/longitude y y coordinate/latitude polygon polygon consists of list of (x,y)s Returns ------- Integer 1 if (x, y) is in the polygon and 0 otherwise. \"\"\" if ( x is None ) | ( y is None ): return None try : x = float ( x ) y = float ( y ) except : return None in_range (( y , x )) counter = 0 for index , poly in enumerate ( polygon ): # Check whether x and y are numbers if not isinstance ( x , numbers . Number ) or not isinstance ( y , numbers . Number ): raise TypeError ( \"Input coordinate should be of type float\" ) # Check whether poly is list of (x,y)s if any ([ not isinstance ( i , numbers . Number ) for point in poly for i in point ]): # Polygon from multipolygon have extra bracket - that need to be removed poly = poly [ 0 ] if any ( [ not isinstance ( i , numbers . Number ) for point in poly for i in point ] ): raise TypeError ( \"The polygon is invalid\" ) # Check if point is a vertex test_vertex = ( x , y ) if isinstance ( poly [ 0 ], tuple ) else [ x , y ] if test_vertex in poly : return 1 # Check if point is on a boundary poly_length = len ( poly ) for i in range ( poly_length - 1 ): p1 = poly [ i ] p2 = poly [ i + 1 ] if ( p1 [ 1 ] == p2 [ 1 ] and p1 [ 1 ] == y and ( min ( p1 [ 0 ], p2 [ 0 ]) <= x <= max ( p1 [ 0 ], p2 [ 0 ])) ): return 1 if ( p1 [ 0 ] == p2 [ 0 ] and p1 [ 0 ] == x and ( min ( p1 [ 1 ], p2 [ 1 ]) <= y <= max ( p1 [ 1 ], p2 [ 1 ])) ): return 1 # Check if the point is inside for i in range ( poly_length ): p1x , p1y = poly [ i ] p2x , p2y = poly [( i + 1 ) % poly_length ] if y > min ( p1y , p2y ): if y <= max ( p1y , p2y ): if x <= max ( p1x , p2x ): if p1y != p2y : xints = ( y - p1y ) * ( p2x - p1x ) / ( p2y - p1y ) + p1x if p1x == p2x or x <= xints : counter += 1 if counter % 2 == 0 : return 0 else : return 1 def point_in_polygons ( x, y, polygon_list, south_west_loc=[], north_east_loc=[]) This function helps to check whether (x,y) is inside any polygon in a list of polygon Parameters x x coordinate/longitude y y coordinate/latitude polygon_list A list of polygon(s) south_west_loc The south-west point (x_sw, y_sw) of the bounding box of the polygons, if available. 0 will be directly returned if x < x_sw or y < y_sw (Default value = []) north_east_loc The north-east point (x_ne, y_ne) of the bounding box of the polygons, if available. (Default value = []) 0 will be directly returned if x > x_ne or y > y_ne (Default value = []) Returns Integer 1 if (x, y) is inside any polygon of polygon_list and 0 otherwise. Expand source code def point_in_polygons ( x , y , polygon_list , south_west_loc = [], north_east_loc = []): \"\"\" This function helps to check whether (x,y) is inside any polygon in a list of polygon Parameters ---------- x x coordinate/longitude y y coordinate/latitude polygon_list A list of polygon(s) south_west_loc The south-west point (x_sw, y_sw) of the bounding box of the polygons, if available. 0 will be directly returned if x < x_sw or y < y_sw (Default value = []) north_east_loc The north-east point (x_ne, y_ne) of the bounding box of the polygons, if available. (Default value = []) 0 will be directly returned if x > x_ne or y > y_ne (Default value = []) Returns ------- Integer 1 if (x, y) is inside any polygon of polygon_list and 0 otherwise. \"\"\" if ( x is None ) | ( y is None ): return None try : x = float ( x ) y = float ( y ) except : warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) return None in_range (( y , x )) if south_west_loc : if ( x < south_west_loc [ 0 ]) or ( y < south_west_loc [ 1 ]): return 0 if north_east_loc : if ( x > north_east_loc [ 0 ]) or ( y > north_east_loc [ 1 ]): return 0 flag_list = [] for polygon in polygon_list : flag_list . append ( point_in_polygon ( x , y , polygon )) return int ( any ( flag_list )) def to_latlon_decimal_degrees ( loc, input_format, radius=6371009) This function helps to format input location into [lat,lon] format Parameters loc Location to format. If input_format is \"dd\", [lat, lon] format is required. If input_format is \"dms\", [[d1,m1,s1], [d2,m2,s2]] format is required. If input_format is \"radian\", [lat_radian, lon_radian] format is required. If input_format is \"cartesian\", [x, y, z] format is required. If input_format is \"geohash\", string format is required. input_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". radius Radius of Earth. (Default value = EARTH_RADIUS) Returns List Formatted location: [lat, lon] Expand source code def to_latlon_decimal_degrees ( loc , input_format , radius = EARTH_RADIUS ): \"\"\" This function helps to format input location into [lat,lon] format Parameters ---------- loc Location to format. If input_format is \"dd\", [lat, lon] format is required. If input_format is \"dms\", [[d1,m1,s1], [d2,m2,s2]] format is required. If input_format is \"radian\", [lat_radian, lon_radian] format is required. If input_format is \"cartesian\", [x, y, z] format is required. If input_format is \"geohash\", string format is required. input_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". radius Radius of Earth. (Default value = EARTH_RADIUS) Returns ------- List Formatted location: [lat, lon] \"\"\" if loc is None : return None if isinstance ( loc , ( list , tuple )): if any ( i is None for i in loc ): return None if isinstance ( loc [ 0 ], ( list , tuple )): if any ( i is None for i in loc [ 0 ] + loc [ 1 ]): return None if input_format == \"dd\" : # loc = [lat, lon] try : lat = float ( loc [ 0 ]) lon = float ( loc [ 1 ]) except : lat , lon = None , None warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) elif input_format == \"dms\" : # loc = [[d1,m1,s1], [d2,m2,s2]] try : d1 , m1 , s1 , d2 , m2 , s2 = [ float ( i ) for i in ( loc [ 0 ] + loc [ 1 ])] lat = d1 + m1 / 60 + s1 / 3600 lon = d2 + m2 / 60 + s2 / 3600 except : lat , lon = None , None warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) elif input_format == \"radian\" : # loc = [lat_radian, lon_radian] try : lat = degrees ( float ( loc [ 0 ])) lon = degrees ( float ( loc [ 1 ])) except : lat , lon = None , None warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) elif input_format == \"cartesian\" : # loc = [x, y, z] try : x , y , z = [ float ( i ) for i in loc ] lat = degrees ( float ( asin ( z / radius ))) lon = degrees ( float ( atan2 ( y , x ))) except : lat , lon = None , None warnings . warn ( \"Rows dropped due to invalid cartesian values\" ) elif input_format == \"geohash\" : # loc = geohash try : lat , lon = list ( pgh . decode ( loc )) except : lat , lon = None , None warnings . warn ( \"Rows dropped due to an invalid geohash entry\" ) in_range (( lat , lon )) return [ lat , lon ] def vincenty_distance ( loc1, loc2, unit='m', ellipsoid='WGS-84') Vincenty's formulae are two related iterative methods used in geodesy to calculate the distance between two points on the surface of a spheroid. Parameters loc1 The first location. [lat, lon] format is required. loc2 The second location. [lat, lon] format is required. unit \"m\", \"km\". Unit of the result. (Default value = \"m\") ellipsoid \"WGS-84\", \"GRS-80\", \"Airy (1830)\", \"Intl 1924\", \"Clarke (1880)\", \"GRS-67\". The ellipsoidal model to use. For more information, please refer to geopy.distance.ELLIPSOIDS. (Default value = \"WGS-84\") Returns Float Expand source code def vincenty_distance ( loc1 , loc2 , unit = \"m\" , ellipsoid = \"WGS-84\" ): \"\"\" Vincenty's formulae are two related iterative methods used in geodesy to calculate the distance between two points on the surface of a spheroid. Parameters ---------- loc1 The first location. [lat, lon] format is required. loc2 The second location. [lat, lon] format is required. unit \"m\", \"km\". Unit of the result. (Default value = \"m\") ellipsoid \"WGS-84\", \"GRS-80\", \"Airy (1830)\", \"Intl 1924\", \"Clarke (1880)\", \"GRS-67\". The ellipsoidal model to use. For more information, please refer to geopy.distance.ELLIPSOIDS. (Default value = \"WGS-84\") Returns ------- Float \"\"\" if None in [ loc1 , loc2 ]: return None if None in loc1 + loc2 : return None in_range ( loc1 ) in_range ( loc2 ) try : loc_distance = distance . distance ( loc1 , loc2 , ellipsoid = ellipsoid ) except : return None if unit == \"m\" : return loc_distance . m else : return loc_distance . km","title":"<code>geo_utils</code>"},{"location":"api/data_transformer/geo_utils.html#geo_utils","text":"Expand source code from math import sin , cos , atan2 , asin , radians , degrees , sqrt import pygeohash as pgh from geopy import distance from scipy import spatial import numbers import warnings from pyspark.sql import functions as F from pyspark.sql import types as T EARTH_RADIUS = 6371009 UNIT_FACTOR = { \"m\" : 1.0 , \"km\" : 1000.0 } def in_range ( loc , loc_format = \"dd\" ): \"\"\" This function helps to check if the input location is in range based on loc_format. Parameters ---------- loc Location to check if in range. If loc_format is \"dd\", [lat, lon] format is required. If loc_format is \"dms\", [[d1,m1,s1], [d2,m2,s2]] format is required. If loc_format is \"radian\", [lat_radian, lon_radian] format is required. If loc_format is \"cartesian\", [x, y, z] format is required. If loc_format is \"geohash\", string format is required. loc_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". (Default value = \"dd\") \"\"\" if loc_format == \"dd\" : try : lat , lon = [ int ( float ( i )) for i in loc ] except : lat , lon = None , None else : try : lat , lon = [ int ( float ( i )) for i in to_latlon_decimal_degrees ( loc , loc_format ) ] except : lat , lon = None , None if None not in [ lat , lon ]: if lat > 90 or lat < - 90 or lon > 180 or lon < - 180 : warnings . warn ( \"Rows may contain unintended values due to longitude and/or latitude values being out of the \" \"valid range\" ) def to_latlon_decimal_degrees ( loc , input_format , radius = EARTH_RADIUS ): \"\"\" This function helps to format input location into [lat,lon] format Parameters ---------- loc Location to format. If input_format is \"dd\", [lat, lon] format is required. If input_format is \"dms\", [[d1,m1,s1], [d2,m2,s2]] format is required. If input_format is \"radian\", [lat_radian, lon_radian] format is required. If input_format is \"cartesian\", [x, y, z] format is required. If input_format is \"geohash\", string format is required. input_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". radius Radius of Earth. (Default value = EARTH_RADIUS) Returns ------- List Formatted location: [lat, lon] \"\"\" if loc is None : return None if isinstance ( loc , ( list , tuple )): if any ( i is None for i in loc ): return None if isinstance ( loc [ 0 ], ( list , tuple )): if any ( i is None for i in loc [ 0 ] + loc [ 1 ]): return None if input_format == \"dd\" : # loc = [lat, lon] try : lat = float ( loc [ 0 ]) lon = float ( loc [ 1 ]) except : lat , lon = None , None warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) elif input_format == \"dms\" : # loc = [[d1,m1,s1], [d2,m2,s2]] try : d1 , m1 , s1 , d2 , m2 , s2 = [ float ( i ) for i in ( loc [ 0 ] + loc [ 1 ])] lat = d1 + m1 / 60 + s1 / 3600 lon = d2 + m2 / 60 + s2 / 3600 except : lat , lon = None , None warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) elif input_format == \"radian\" : # loc = [lat_radian, lon_radian] try : lat = degrees ( float ( loc [ 0 ])) lon = degrees ( float ( loc [ 1 ])) except : lat , lon = None , None warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) elif input_format == \"cartesian\" : # loc = [x, y, z] try : x , y , z = [ float ( i ) for i in loc ] lat = degrees ( float ( asin ( z / radius ))) lon = degrees ( float ( atan2 ( y , x ))) except : lat , lon = None , None warnings . warn ( \"Rows dropped due to invalid cartesian values\" ) elif input_format == \"geohash\" : # loc = geohash try : lat , lon = list ( pgh . decode ( loc )) except : lat , lon = None , None warnings . warn ( \"Rows dropped due to an invalid geohash entry\" ) in_range (( lat , lon )) return [ lat , lon ] def decimal_degrees_to_degrees_minutes_seconds ( dd ): \"\"\" This function helps to divide float value dd in decimal degree into [degreee, minute, second] Parameters ---------- dd Float value in decimal degree. Returns ------- List [degree, minute, second] \"\"\" if dd is None : return [ None , None , None ] else : minute , second = divmod ( dd * 3600 , 60 ) degree , minute = divmod ( minute , 60 ) return [ degree , minute , second ] def from_latlon_decimal_degrees ( loc , output_format , radius = EARTH_RADIUS , geohash_precision = 8 ): \"\"\" This function helps to transform [lat,lon] locations into desired output_format. Parameters ---------- loc Location to format with format [lat, lon]. output_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". radius Radius of Earth. (Default value = EARTH_RADIUS) geohash_precision Precision of the resultant geohash. This argument is only used when output_format is \"geohash\". (Default value = 8) Returns ------- String if output_format is \"geohash\", list otherwise. [lat, lon] if output_format is \"dd\". [[d1,m1,s1], [d2,m2,s2]] if output_format is \"dms\". [lat_radian, lon_radian] if output_format is \"radian\". [x, y, z] if output_format is \"cartesian\". string if output_format is \"geohash\". \"\"\" # loc = [lat, lon] if loc is None : lat , lon = None , None else : lat , lon = loc [ 0 ], loc [ 1 ] if output_format == \"dd\" : return [ lat , lon ] elif output_format == \"dms\" : return [ decimal_degrees_to_degrees_minutes_seconds ( lat ), decimal_degrees_to_degrees_minutes_seconds ( lon ), ] elif output_format == \"radian\" : if ( lat is None ) | ( lon is None ): return [ None , None ] else : lat_rad = radians ( float ( lat )) lon_rad = radians ( float ( lon )) return [ lat_rad , lon_rad ] elif output_format == \"cartesian\" : if ( lat is None ) | ( lon is None ): return [ None , None , None ] else : lat_rad = radians ( float ( lat )) lon_rad = radians ( float ( lon )) x = radius * cos ( lat_rad ) * cos ( lon_rad ) y = radius * cos ( lat_rad ) * sin ( lon_rad ) z = radius * sin ( lat_rad ) return [ x , y , z ] elif output_format == \"geohash\" : if ( lat is None ) | ( lon is None ): return None else : return pgh . encode ( lat , lon , geohash_precision ) def haversine_distance ( loc1 , loc2 , loc_format , unit = \"m\" , radius = EARTH_RADIUS ): \"\"\" This function helps to calculate the haversine distance between loc1 and loc2. Parameters ---------- loc1 The first location. If loc_format is \"dd\", [lat, lon] format is required. If loc_format is \"radian\", [lat_radian, lon_radian] format is required. loc2 The second location . If loc_format is \"dd\", [lat, lon] format is required. If loc_format is \"radian\", [lat_radian, lon_radian] format is required. loc_format \"dd\", \"radian\". unit \"m\", \"km\". Unit of the result. (Default value = \"m\") radius Radius of Earth. (Default value = EARTH_RADIUS) Returns ------- Float \"\"\" # loc1 = [lat1, lon1]; loc2 = [lat2, lon2] if None in [ loc1 , loc2 ]: return None if None in loc1 + loc2 : return None if loc_format not in [ \"dd\" , \"radian\" ]: raise TypeError ( \"Invalid input for loc_format\" ) try : lat1 , lon1 = float ( loc1 [ 0 ]), float ( loc1 [ 1 ]) lat2 , lon2 = float ( loc2 [ 0 ]), float ( loc2 [ 1 ]) except : return None in_range (( lat1 , lon1 ), loc_format ) in_range (( lat2 , lon2 ), loc_format ) if loc_format == \"dd\" : lat1 , lon1 = radians ( lat1 ), radians ( lon1 ) lat2 , lon2 = radians ( lat2 ), radians ( lon2 ) dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 c = 2 * atan2 ( sqrt ( a ), sqrt ( 1 - a )) distance = radius * c / UNIT_FACTOR [ unit ] return distance def vincenty_distance ( loc1 , loc2 , unit = \"m\" , ellipsoid = \"WGS-84\" ): \"\"\" Vincenty's formulae are two related iterative methods used in geodesy to calculate the distance between two points on the surface of a spheroid. Parameters ---------- loc1 The first location. [lat, lon] format is required. loc2 The second location. [lat, lon] format is required. unit \"m\", \"km\". Unit of the result. (Default value = \"m\") ellipsoid \"WGS-84\", \"GRS-80\", \"Airy (1830)\", \"Intl 1924\", \"Clarke (1880)\", \"GRS-67\". The ellipsoidal model to use. For more information, please refer to geopy.distance.ELLIPSOIDS. (Default value = \"WGS-84\") Returns ------- Float \"\"\" if None in [ loc1 , loc2 ]: return None if None in loc1 + loc2 : return None in_range ( loc1 ) in_range ( loc2 ) try : loc_distance = distance . distance ( loc1 , loc2 , ellipsoid = ellipsoid ) except : return None if unit == \"m\" : return loc_distance . m else : return loc_distance . km def euclidean_distance ( loc1 , loc2 , unit = \"m\" ): \"\"\" The Euclidean distance between 2 lists loc1 and loc2, is defined as .. math:: {\\\\|loc1-loc2\\\\|}_2 Parameters ---------- loc1 The first location. [x1, y1, z1] format is required. loc2 The second location. [x2, y2, z2] format is required. unit \"m\", \"km\". Unit of the result. (Default value = \"m\") Returns ------- Float \"\"\" if None in [ loc1 , loc2 ]: return None if None in loc1 + loc2 : return None try : loc1 = [ float ( i ) for i in loc1 ] loc2 = [ float ( i ) for i in loc2 ] except : return None in_range ( loc1 , \"cartesian\" ) in_range ( loc2 , \"cartesian\" ) # loc1 = [x1, y1, z1]; loc2 = [x2, y2, z2] euclidean_distance = spatial . distance . euclidean ( loc1 , loc2 ) if unit == \"km\" : euclidean_distance /= 1000 return euclidean_distance def point_in_polygon ( x , y , polygon ): \"\"\" This function helps to check whether (x,y) is inside a polygon Parameters ---------- x x coordinate/longitude y y coordinate/latitude polygon polygon consists of list of (x,y)s Returns ------- Integer 1 if (x, y) is in the polygon and 0 otherwise. \"\"\" if ( x is None ) | ( y is None ): return None try : x = float ( x ) y = float ( y ) except : return None in_range (( y , x )) counter = 0 for index , poly in enumerate ( polygon ): # Check whether x and y are numbers if not isinstance ( x , numbers . Number ) or not isinstance ( y , numbers . Number ): raise TypeError ( \"Input coordinate should be of type float\" ) # Check whether poly is list of (x,y)s if any ([ not isinstance ( i , numbers . Number ) for point in poly for i in point ]): # Polygon from multipolygon have extra bracket - that need to be removed poly = poly [ 0 ] if any ( [ not isinstance ( i , numbers . Number ) for point in poly for i in point ] ): raise TypeError ( \"The polygon is invalid\" ) # Check if point is a vertex test_vertex = ( x , y ) if isinstance ( poly [ 0 ], tuple ) else [ x , y ] if test_vertex in poly : return 1 # Check if point is on a boundary poly_length = len ( poly ) for i in range ( poly_length - 1 ): p1 = poly [ i ] p2 = poly [ i + 1 ] if ( p1 [ 1 ] == p2 [ 1 ] and p1 [ 1 ] == y and ( min ( p1 [ 0 ], p2 [ 0 ]) <= x <= max ( p1 [ 0 ], p2 [ 0 ])) ): return 1 if ( p1 [ 0 ] == p2 [ 0 ] and p1 [ 0 ] == x and ( min ( p1 [ 1 ], p2 [ 1 ]) <= y <= max ( p1 [ 1 ], p2 [ 1 ])) ): return 1 # Check if the point is inside for i in range ( poly_length ): p1x , p1y = poly [ i ] p2x , p2y = poly [( i + 1 ) % poly_length ] if y > min ( p1y , p2y ): if y <= max ( p1y , p2y ): if x <= max ( p1x , p2x ): if p1y != p2y : xints = ( y - p1y ) * ( p2x - p1x ) / ( p2y - p1y ) + p1x if p1x == p2x or x <= xints : counter += 1 if counter % 2 == 0 : return 0 else : return 1 def point_in_polygons ( x , y , polygon_list , south_west_loc = [], north_east_loc = []): \"\"\" This function helps to check whether (x,y) is inside any polygon in a list of polygon Parameters ---------- x x coordinate/longitude y y coordinate/latitude polygon_list A list of polygon(s) south_west_loc The south-west point (x_sw, y_sw) of the bounding box of the polygons, if available. 0 will be directly returned if x < x_sw or y < y_sw (Default value = []) north_east_loc The north-east point (x_ne, y_ne) of the bounding box of the polygons, if available. (Default value = []) 0 will be directly returned if x > x_ne or y > y_ne (Default value = []) Returns ------- Integer 1 if (x, y) is inside any polygon of polygon_list and 0 otherwise. \"\"\" if ( x is None ) | ( y is None ): return None try : x = float ( x ) y = float ( y ) except : warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) return None in_range (( y , x )) if south_west_loc : if ( x < south_west_loc [ 0 ]) or ( y < south_west_loc [ 1 ]): return 0 if north_east_loc : if ( x > north_east_loc [ 0 ]) or ( y > north_east_loc [ 1 ]): return 0 flag_list = [] for polygon in polygon_list : flag_list . append ( point_in_polygon ( x , y , polygon )) return int ( any ( flag_list )) def f_point_in_polygons ( polygon_list , south_west_loc = [], north_east_loc = []): return F . udf ( lambda x , y : point_in_polygons ( x , y , polygon_list , south_west_loc , north_east_loc ), T . IntegerType (), ) COUNTRY_BOUNDING_BOXES = { \"AW\" : ( \"Aruba\" , ( - 70.2809842 , 12.1702998 , - 69.6409842 , 12.8102998 )), \"AF\" : ( \"Afghanistan\" , ( 60.5176034 , 29.3772 , 74.889862 , 38.4910682 )), \"AO\" : ( \"Angola\" , ( 11.4609793 , - 18.038945 , 24.0878856 , - 4.3880634 )), \"AI\" : ( \"Anguilla\" , ( - 63.6391992 , 18.0615454 , - 62.7125449 , 18.7951194 )), \"AL\" : ( \"Albania\" , ( 19.1246095 , 39.6448625 , 21.0574335 , 42.6610848 )), \"AD\" : ( \"Andorra\" , ( 1.4135781 , 42.4288238 , 1.7863837 , 42.6559357 )), \"AE\" : ( \"United Arab Emirates\" , ( 51.498 , 22.6444 , 56.3834 , 26.2822 )), \"AR\" : ( \"Argentina\" , ( - 73.5600329 , - 55.1850761 , - 53.6374515 , - 21.781168 )), \"AM\" : ( \"Armenia\" , ( 43.4471395 , 38.8404775 , 46.6333087 , 41.300712 )), \"AS\" : ( \"American Samoa\" , ( - 171.2951296 , - 14.7608358 , - 167.9322899 , - 10.8449746 )), \"AQ\" : ( \"Antarctica\" , ( - 180.0 , - 85.0511287 , 180.0 , - 60.0 )), \"AG\" : ( \"Antigua and Barbuda\" , ( - 62.5536517 , 16.7573901 , - 61.447857 , 17.929 )), \"AU\" : ( \"Australia\" , ( 72.2460938 , - 55.3228175 , 168.2249543 , - 9.0882278 )), \"AT\" : ( \"Austria\" , ( 9.5307487 , 46.3722761 , 17.160776 , 49.0205305 )), \"AZ\" : ( \"Azerbaijan\" , ( 44.7633701 , 38.3929551 , 51.0090302 , 41.9502947 )), \"BI\" : ( \"Burundi\" , ( 29.0007401 , - 4.4693155 , 30.8498462 , - 2.3096796 )), \"BE\" : ( \"Belgium\" , ( 2.3889137 , 49.4969821 , 6.408097 , 51.5516667 )), \"BJ\" : ( \"Benin\" , ( 0.776667 , 6.0398696 , 3.843343 , 12.4092447 )), \"BF\" : ( \"Burkina Faso\" , ( - 5.5132416 , 9.4104718 , 2.4089717 , 15.084 )), \"BD\" : ( \"Bangladesh\" , ( 88.0075306 , 20.3756582 , 92.6804979 , 26.6382534 )), \"BG\" : ( \"Bulgaria\" , ( 22.3571459 , 41.2353929 , 28.8875409 , 44.2167064 )), \"BH\" : ( \"Bahrain\" , ( 50.2697989 , 25.535 , 50.9233693 , 26.6872444 )), \"BS\" : ( \"Bahamas\" , ( - 80.7001941 , 20.7059846 , - 72.4477521 , 27.4734551 )), \"BA\" : ( \"Bosnia and Herzegovina\" , ( 15.7287433 , 42.5553114 , 19.6237311 , 45.2764135 )), \"BL\" : ( \"Saint Barth\u00e9lemy\" , ( - 63.06639 , 17.670931 , - 62.5844019 , 18.1375569 )), \"BY\" : ( \"Belarus\" , ( 23.1783344 , 51.2575982 , 32.7627809 , 56.17218 )), \"BZ\" : ( \"Belize\" , ( - 89.2262083 , 15.8857286 , - 87.3098494 , 18.496001 )), \"BM\" : ( \"Bermuda\" , ( - 65.1232222 , 32.0469651 , - 64.4109842 , 32.5913693 )), \"BO\" : ( \"Bolivia (Plurinational State of)\" , ( - 69.6450073 , - 22.8982742 , - 57.453 , - 9.6689438 ), ), \"BR\" : ( \"Brazil\" , ( - 73.9830625 , - 33.8689056 , - 28.6341164 , 5.2842873 )), \"BB\" : ( \"Barbados\" , ( - 59.8562115 , 12.845 , - 59.2147175 , 13.535 )), \"BN\" : ( \"Brunei Darussalam\" , ( 114.0758734 , 4.002508 , 115.3635623 , 5.1011857 )), \"BT\" : ( \"Bhutan\" , ( 88.7464724 , 26.702016 , 92.1252321 , 28.246987 )), \"BW\" : ( \"Botswana\" , ( 19.9986474 , - 26.9059669 , 29.375304 , - 17.778137 )), \"CF\" : ( \"Central African Republic\" , ( 14.4155426 , 2.2156553 , 27.4540764 , 11.001389 )), \"CA\" : ( \"Canada\" , ( - 141.00275 , 41.6765556 , - 52.3231981 , 83.3362128 )), \"CC\" : ( \"Cocos (Keeling) Islands\" , ( 96.612524 , - 12.4055983 , 97.1357343 , - 11.6213132 ), ), \"CH\" : ( \"Switzerland\" , ( 5.9559113 , 45.817995 , 10.4922941 , 47.8084648 )), \"CL\" : ( \"Chile\" , ( - 109.6795789 , - 56.725 , - 66.0753474 , - 17.4983998 )), \"CN\" : ( \"China\" , ( 73.4997347 , 8.8383436 , 134.7754563 , 53.5608154 )), \"CI\" : ( \"C\u00f4te d'Ivoire\" , ( - 8.601725 , 4.1621205 , - 2.493031 , 10.740197 )), \"CM\" : ( \"Cameroon\" , ( 8.3822176 , 1.6546659 , 16.1921476 , 13.083333 )), \"CD\" : ( \"Congo, Democratic Republic of the\" , ( 12.039074 , - 13.459035 , 31.3056758 , 5.3920026 ), ), \"CG\" : ( \"Congo\" , ( 11.0048205 , - 5.149089 , 18.643611 , 3.713056 )), \"CK\" : ( \"Cook Islands\" , ( - 166.0856468 , - 22.15807 , - 157.1089329 , - 8.7168792 )), \"CO\" : ( \"Colombia\" , ( - 82.1243666 , - 4.2316872 , - 66.8511907 , 16.0571269 )), \"KM\" : ( \"Comoros\" , ( 43.025305 , - 12.621 , 44.7451922 , - 11.165 )), \"CV\" : ( \"Cabo Verde\" , ( - 25.3609478 , 14.8031546 , - 22.6673416 , 17.2053108 )), \"CR\" : ( \"Costa Rica\" , ( - 87.2722647 , 5.3329698 , - 82.5060208 , 11.2195684 )), \"CU\" : ( \"Cuba\" , ( - 85.1679702 , 19.6275294 , - 73.9190004 , 23.4816972 )), \"CX\" : ( \"Christmas Island\" , ( 105.5336422 , - 10.5698515 , 105.7130159 , - 10.4123553 )), \"KY\" : ( \"Cayman Islands\" , ( - 81.6313748 , 19.0620619 , - 79.5110954 , 19.9573759 )), \"CY\" : ( \"Cyprus\" , ( 32.0227581 , 34.4383706 , 34.8553182 , 35.913252 )), \"CZ\" : ( \"Czechia\" , ( 12.0905901 , 48.5518083 , 18.859216 , 51.0557036 )), \"DE\" : ( \"Germany\" , ( 5.8663153 , 47.2701114 , 15.0419319 , 55.099161 )), \"DJ\" : ( \"Djibouti\" , ( 41.7713139 , 10.9149547 , 43.6579046 , 12.7923081 )), \"DM\" : ( \"Dominica\" , ( - 61.6869184 , 15.0074207 , - 61.0329895 , 15.7872222 )), \"DK\" : ( \"Denmark\" , ( 7.7153255 , 54.4516667 , 15.5530641 , 57.9524297 )), \"DO\" : ( \"Dominican Republic\" , ( - 72.0574706 , 17.2701708 , - 68.1101463 , 21.303433 )), \"DZ\" : ( \"Algeria\" , ( - 8.668908 , 18.968147 , 11.997337 , 37.2962055 )), \"EC\" : ( \"Ecuador\" , ( - 92.2072392 , - 5.0159314 , - 75.192504 , 1.8835964 )), \"EG\" : ( \"Egypt\" , ( 24.6499112 , 22.0 , 37.1153517 , 31.8330854 )), \"ER\" : ( \"Eritrea\" , ( 36.4333653 , 12.3548219 , 43.3001714 , 18.0709917 )), \"EH\" : ( \"Western Sahara\" , ( - 17.3494721 , 20.556883 , - 8.666389 , 27.6666834 )), \"ES\" : ( \"Spain\" , ( - 18.3936845 , 27.4335426 , 4.5918885 , 43.9933088 )), \"EE\" : ( \"Estonia\" , ( 21.3826069 , 57.5092997 , 28.2100175 , 59.9383754 )), \"ET\" : ( \"Ethiopia\" , ( 32.9975838 , 3.397448 , 47.9823797 , 14.8940537 )), \"FI\" : ( \"Finland\" , ( 19.0832098 , 59.4541578 , 31.5867071 , 70.0922939 )), \"FJ\" : ( \"Fiji\" , ( 172.0 , - 21.9434274 , - 178.5 , - 12.2613866 )), \"FK\" : ( \"Falkland Islands (Malvinas)\" , ( - 61.7726772 , - 53.1186766 , - 57.3662367 , - 50.7973007 ), ), \"FR\" : ( \"France\" , ( - 5.4534286 , 41.2632185 , 9.8678344 , 51.268318 )), \"FO\" : ( \"Faroe Islands\" , ( - 7.6882939 , 61.3915553 , - 6.2565525 , 62.3942991 )), \"FM\" : ( \"Micronesia (Federated States of)\" , ( 137.2234512 , 0.827 , 163.2364054 , 10.291 ), ), \"GA\" : ( \"Gabon\" , ( 8.5002246 , - 4.1012261 , 14.539444 , 2.3182171 )), \"GB\" : ( \"United Kingdom of Great Britain and Northern Ireland\" , ( - 14.015517 , 49.674 , 2.0919117 , 61.061 ), ), \"GE\" : ( \"Georgia\" , ( 39.8844803 , 41.0552922 , 46.7365373 , 43.5864294 )), \"GG\" : ( \"Guernsey\" , ( - 2.6751703 , 49.4155331 , - 2.501814 , 49.5090776 )), \"GH\" : ( \"Ghana\" , ( - 3.260786 , 4.5392525 , 1.2732942 , 11.1748562 )), \"GI\" : ( \"Gibraltar\" , ( - 5.3941295 , 36.100807 , - 5.3141295 , 36.180807 )), \"GN\" : ( \"Guinea\" , ( - 15.5680508 , 7.1906045 , - 7.6381993 , 12.67563 )), \"GM\" : ( \"Gambia\" , ( - 17.0288254 , 13.061 , - 13.797778 , 13.8253137 )), \"GW\" : ( \"Guinea-Bissau\" , ( - 16.894523 , 10.6514215 , - 13.6348777 , 12.6862384 )), \"GQ\" : ( \"Equatorial Guinea\" , ( 5.4172943 , - 1.6732196 , 11.3598628 , 3.989 )), \"GR\" : ( \"Greece\" , ( 19.2477876 , 34.7006096 , 29.7296986 , 41.7488862 )), \"GD\" : ( \"Grenada\" , ( - 62.0065868 , 11.786 , - 61.1732143 , 12.5966532 )), \"GL\" : ( \"Greenland\" , ( - 74.1250416 , 59.515387 , - 10.0288759 , 83.875172 )), \"GT\" : ( \"Guatemala\" , ( - 92.3105242 , 13.6345804 , - 88.1755849 , 17.8165947 )), \"GU\" : ( \"Guam\" , ( 144.563426 , 13.182335 , 145.009167 , 13.706179 )), \"GY\" : ( \"Guyana\" , ( - 61.414905 , 1.1710017 , - 56.4689543 , 8.6038842 )), \"HK\" : ( \"Hong Kong\" , ( 114.0028131 , 22.1193278 , 114.3228131 , 22.4393278 )), \"HN\" : ( \"Honduras\" , ( - 89.3568207 , 12.9808485 , - 82.1729621 , 17.619526 )), \"HR\" : ( \"Croatia\" , ( 13.2104814 , 42.1765993 , 19.4470842 , 46.555029 )), \"HT\" : ( \"Haiti\" , ( - 75.2384618 , 17.9099291 , - 71.6217461 , 20.2181368 )), \"HU\" : ( \"Hungary\" , ( 16.1138867 , 45.737128 , 22.8977094 , 48.585257 )), \"ID\" : ( \"Indonesia\" , ( 94.7717124 , - 11.2085669 , 141.0194444 , 6.2744496 )), \"IM\" : ( \"Isle of Man\" , ( - 4.7946845 , 54.0539576 , - 4.3076853 , 54.4178705 )), \"IN\" : ( \"India\" , ( 68.1113787 , 6.5546079 , 97.395561 , 35.6745457 )), \"IO\" : ( \"British Indian Ocean Territory\" , ( 71.036504 , - 7.6454079 , 72.7020157 , - 5.037066 ), ), \"IE\" : ( \"Ireland\" , ( - 11.0133788 , 51.222 , - 5.6582363 , 55.636 )), \"IR\" : ( \"Iran (Islamic Republic of)\" , ( 44.0318908 , 24.8465103 , 63.3332704 , 39.7816502 ), ), \"IQ\" : ( \"Iraq\" , ( 38.7936719 , 29.0585661 , 48.8412702 , 37.380932 )), \"IS\" : ( \"Iceland\" , ( - 25.0135069 , 63.0859177 , - 12.8046162 , 67.353 )), \"IL\" : ( \"Israel\" , ( 34.2674994 , 29.4533796 , 35.8950234 , 33.3356317 )), \"IT\" : ( \"Italy\" , ( 6.6272658 , 35.2889616 , 18.7844746 , 47.0921462 )), \"JM\" : ( \"Jamaica\" , ( - 78.5782366 , 16.5899443 , - 75.7541143 , 18.7256394 )), \"JE\" : ( \"Jersey\" , ( - 2.254512 , 49.1625179 , - 2.0104193 , 49.2621288 )), \"JO\" : ( \"Jordan\" , ( 34.8844372 , 29.183401 , 39.3012981 , 33.3750617 )), \"JP\" : ( \"Japan\" , ( 122.7141754 , 20.2145811 , 154.205541 , 45.7112046 )), \"KZ\" : ( \"Kazakhstan\" , ( 46.4932179 , 40.5686476 , 87.3156316 , 55.4421701 )), \"KE\" : ( \"Kenya\" , ( 33.9098987 , - 4.8995204 , 41.899578 , 4.62 )), \"KG\" : ( \"Kyrgyzstan\" , ( 69.2649523 , 39.1728437 , 80.2295793 , 43.2667971 )), \"KH\" : ( \"Cambodia\" , ( 102.3338282 , 9.4752639 , 107.6276788 , 14.6904224 )), \"KI\" : ( \"Kiribati\" , ( - 179.1645388 , - 7.0516717 , - 164.1645388 , 7.9483283 )), \"KN\" : ( \"Saint Kitts and Nevis\" , ( - 63.051129 , 16.895 , - 62.3303519 , 17.6158146 )), \"KR\" : ( \"Korea, Republic of\" , ( 124.354847 , 32.9104556 , 132.1467806 , 38.623477 )), \"KW\" : ( \"Kuwait\" , ( 46.5526837 , 28.5243622 , 49.0046809 , 30.1038082 )), \"LA\" : ( \"Lao People's Democratic Republic\" , ( 100.0843247 , 13.9096752 , 107.6349989 , 22.5086717 ), ), \"LB\" : ( \"Lebanon\" , ( 34.8825667 , 33.0479858 , 36.625 , 34.6923543 )), \"LR\" : ( \"Liberia\" , ( - 11.6080764 , 4.1555907 , - 7.367323 , 8.5519861 )), \"LY\" : ( \"Libya\" , ( 9.391081 , 19.5008138 , 25.3770629 , 33.3545898 )), \"LC\" : ( \"Saint Lucia\" , ( - 61.2853867 , 13.508 , - 60.6669363 , 14.2725 )), \"LI\" : ( \"Liechtenstein\" , ( 9.4716736 , 47.0484291 , 9.6357143 , 47.270581 )), \"LK\" : ( \"Sri Lanka\" , ( 79.3959205 , 5.719 , 82.0810141 , 10.035 )), \"LS\" : ( \"Lesotho\" , ( 27.0114632 , - 30.6772773 , 29.4557099 , - 28.570615 )), \"LT\" : ( \"Lithuania\" , ( 20.653783 , 53.8967893 , 26.8355198 , 56.4504213 )), \"LU\" : ( \"Luxembourg\" , ( 4.9684415 , 49.4969821 , 6.0344254 , 50.430377 )), \"LV\" : ( \"Latvia\" , ( 20.6715407 , 55.6746505 , 28.2414904 , 58.0855688 )), \"MO\" : ( \"Macao\" , ( 113.5281666 , 22.0766667 , 113.6301389 , 22.2170361 )), \"MF\" : ( \"Saint Martin (French part)\" , ( - 63.3605643 , 17.8963535 , - 62.7644063 , 18.1902778 ), ), \"MA\" : ( \"Morocco\" , ( - 17.2551456 , 21.3365321 , - 0.998429 , 36.0505269 )), \"MC\" : ( \"Monaco\" , ( 7.4090279 , 43.7247599 , 7.4398704 , 43.7519311 )), \"MD\" : ( \"Moldova, Republic of\" , ( 26.6162189 , 45.4674139 , 30.1636756 , 48.4918695 )), \"MG\" : ( \"Madagascar\" , ( 43.2202072 , - 25.6071002 , 50.4862553 , - 11.9519693 )), \"MV\" : ( \"Maldives\" , ( 72.3554187 , - 0.9074935 , 73.9700962 , 7.3106246 )), \"MX\" : ( \"Mexico\" , ( - 118.59919 , 14.3886243 , - 86.493266 , 32.7186553 )), \"MH\" : ( \"Marshall Islands\" , ( 163.4985095 , - 0.5481258 , 178.4985095 , 14.4518742 )), \"MK\" : ( \"North Macedonia\" , ( 20.4529023 , 40.8536596 , 23.034051 , 42.3735359 )), \"ML\" : ( \"Mali\" , ( - 12.2402835 , 10.147811 , 4.2673828 , 25.001084 )), \"MT\" : ( \"Malta\" , ( 13.9324226 , 35.6029696 , 14.8267966 , 36.2852706 )), \"MM\" : ( \"Myanmar\" , ( 92.1719423 , 9.4399432 , 101.1700796 , 28.547835 )), \"ME\" : ( \"Montenegro\" , ( 18.4195781 , 41.7495999 , 20.3561641 , 43.5585061 )), \"MN\" : ( \"Mongolia\" , ( 87.73762 , 41.5800276 , 119.931949 , 52.1496 )), \"MP\" : ( \"Northern Mariana Islands\" , ( 144.813338 , 14.036565 , 146.154418 , 20.616556 )), \"MZ\" : ( \"Mozambique\" , ( 30.2138197 , - 26.9209427 , 41.0545908 , - 10.3252149 )), \"MR\" : ( \"Mauritania\" , ( - 17.068081 , 14.7209909 , - 4.8333344 , 27.314942 )), \"MS\" : ( \"Montserrat\" , ( - 62.450667 , 16.475 , - 61.9353818 , 17.0152978 )), \"MU\" : ( \"Mauritius\" , ( 56.3825151 , - 20.725 , 63.7151319 , - 10.138 )), \"MW\" : ( \"Malawi\" , ( 32.6703616 , - 17.1296031 , 35.9185731 , - 9.3683261 )), \"MY\" : ( \"Malaysia\" , ( 105.3471939 , - 5.1076241 , 120.3471939 , 9.8923759 )), \"YT\" : ( \"Mayotte\" , ( 45.0183298 , - 13.0210119 , 45.2999917 , - 12.6365902 )), \"NA\" : ( \"Namibia\" , ( 11.5280384 , - 28.96945 , 25.2617671 , - 16.9634855 )), \"NC\" : ( \"New Caledonia\" , ( 162.6034343 , - 23.2217509 , 167.8109827 , - 17.6868616 )), \"NE\" : ( \"Niger\" , ( 0.1689653 , 11.693756 , 15.996667 , 23.517178 )), \"NG\" : ( \"Nigeria\" , ( 2.676932 , 4.0690959 , 14.678014 , 13.885645 )), \"NI\" : ( \"Nicaragua\" , ( - 87.901532 , 10.7076565 , - 82.6227023 , 15.0331183 )), \"NU\" : ( \"Niue\" , ( - 170.1595029 , - 19.3548665 , - 169.5647229 , - 18.7534559 )), \"NL\" : ( \"Netherlands\" , ( 1.9193492 , 50.7295671 , 7.2274985 , 53.7253321 )), \"NO\" : ( \"Norway\" , ( 4.0875274 , 57.7590052 , 31.7614911 , 71.3848787 )), \"NP\" : ( \"Nepal\" , ( 80.0586226 , 26.3477581 , 88.2015257 , 30.446945 )), \"NR\" : ( \"Nauru\" , ( 166.9091794 , - 0.5541334 , 166.9589235 , - 0.5025906 )), \"NZ\" : ( \"New Zealand\" , ( - 179.059153 , - 52.8213687 , 179.3643594 , - 29.0303303 )), \"OM\" : ( \"Oman\" , ( 52 , 16.4649608 , 60.054577 , 26.7026737 )), \"PK\" : ( \"Pakistan\" , ( 60.872855 , 23.5393916 , 77.1203914 , 37.084107 )), \"PA\" : ( \"Panama\" , ( - 83.0517245 , 7.0338679 , - 77.1393779 , 9.8701757 )), \"PN\" : ( \"Pitcairn\" , ( - 130.8049862 , - 25.1306736 , - 124.717534 , - 23.8655769 )), \"PE\" : ( \"Peru\" , ( - 84.6356535 , - 20.1984472 , - 68.6519906 , - 0.0392818 )), \"PH\" : ( \"Philippines\" , ( 114.0952145 , 4.2158064 , 126.8072562 , 21.3217806 )), \"PW\" : ( \"Palau\" , ( 131.0685462 , 2.748 , 134.7714735 , 8.222 )), \"PG\" : ( \"Papua New Guinea\" , ( 136.7489081 , - 13.1816069 , 151.7489081 , 1.8183931 )), \"PL\" : ( \"Poland\" , ( 14.1229707 , 49.0020468 , 24.145783 , 55.0336963 )), \"PR\" : ( \"Puerto Rico\" , ( - 67.271492 , 17.9268695 , - 65.5897525 , 18.5159789 )), \"KP\" : ( \"Korea (Democratic People's Republic of)\" , ( 124.0913902 , 37.5867855 , 130.924647 , 43.0089642 ), ), \"PT\" : ( \"Portugal\" , ( - 31.5575303 , 29.8288021 , - 6.1891593 , 42.1543112 )), \"PY\" : ( \"Paraguay\" , ( - 62.6442036 , - 27.6063935 , - 54.258 , - 19.2876472 )), \"PS\" : ( \"Palestine, State of\" , ( 34.0689732 , 31.2201289 , 35.5739235 , 32.5521479 )), \"PF\" : ( \"French Polynesia\" , ( - 154.9360599 , - 28.0990232 , - 134.244799 , - 7.6592173 )), \"QA\" : ( \"Qatar\" , ( 50.5675 , 24.4707534 , 52.638011 , 26.3830212 )), \"RE\" : ( \"R\u00e9union\" , ( 55.2164268 , - 21.3897308 , 55.8366924 , - 20.8717136 )), \"RO\" : ( \"Romania\" , ( 20.2619773 , 43.618682 , 30.0454257 , 48.2653964 )), \"RU\" : ( \"Russian Federation\" , ( 19.6389 , 41.1850968 , 180 , 82.0586232 )), \"RW\" : ( \"Rwanda\" , ( 28.8617546 , - 2.8389804 , 30.8990738 , - 1.0474083 )), \"SA\" : ( \"Saudi Arabia\" , ( 34.4571718 , 16.29 , 55.6666851 , 32.1543377 )), \"SD\" : ( \"Sudan\" , ( 21.8145046 , 8.685278 , 39.0576252 , 22.224918 )), \"SN\" : ( \"Senegal\" , ( - 17.7862419 , 12.2372838 , - 11.3458996 , 16.6919712 )), \"SG\" : ( \"Singapore\" , ( 103.6920359 , 1.1304753 , 104.0120359 , 1.4504753 )), \"SH\" : ( \"Saint Helena, Ascension and Tristan da Cunha\" , ( - 5.9973424 , - 16.23 , - 5.4234153 , - 15.704 ), ), \"SJ\" : ( \"Svalbard and Jan Mayen\" , ( - 9.6848146 , 70.6260825 , 34.6891253 , 81.028076 )), \"SB\" : ( \"Solomon Islands\" , ( 155.3190556 , - 13.2424298 , 170.3964667 , - 4.81085 )), \"SL\" : ( \"Sierra Leone\" , ( - 13.5003389 , 6.755 , - 10.271683 , 9.999973 )), \"SV\" : ( \"El Salvador\" , ( - 90.1790975 , 12.976046 , - 87.6351394 , 14.4510488 )), \"SM\" : ( \"San Marino\" , ( 12.4033246 , 43.8937002 , 12.5160665 , 43.992093 )), \"SO\" : ( \"Somalia\" , ( 40.98918 , - 1.8031969 , 51.6177696 , 12.1889121 )), \"PM\" : ( \"Saint Pierre and Miquelon\" , ( - 56.6972961 , 46.5507173 , - 55.9033333 , 47.365 )), \"RS\" : ( \"Serbia\" , ( 18.8142875 , 42.2322435 , 23.006309 , 46.1900524 )), \"ST\" : ( \"Sao Tome and Principe\" , ( 6.260642 , - 0.2135137 , 7.6704783 , 1.9257601 )), \"SR\" : ( \"Suriname\" , ( - 58.070833 , 1.8312802 , - 53.8433358 , 6.225 )), \"SK\" : ( \"Slovakia\" , ( 16.8331891 , 47.7314286 , 22.56571 , 49.6138162 )), \"SI\" : ( \"Slovenia\" , ( 13.3754696 , 45.4214242 , 16.5967702 , 46.8766816 )), \"SE\" : ( \"Sweden\" , ( 10.5930952 , 55.1331192 , 24.1776819 , 69.0599699 )), \"SZ\" : ( \"Eswatini\" , ( 30.7908 , - 27.3175201 , 32.1349923 , - 25.71876 )), \"SC\" : ( \"Seychelles\" , ( 45.9988759 , - 10.4649258 , 56.4979396 , - 3.512 )), \"SY\" : ( \"Syrian Arab Republic\" , ( 35.4714427 , 32.311354 , 42.3745687 , 37.3184589 )), \"TC\" : ( \"Turks and Caicos Islands\" , ( - 72.6799046 , 20.9553418 , - 70.8643591 , 22.1630989 ), ), \"TD\" : ( \"Chad\" , ( 13.47348 , 7.44107 , 24.0 , 23.4975 )), \"TG\" : ( \"Togo\" , ( - 0.1439746 , 5.926547 , 1.8087605 , 11.1395102 )), \"TH\" : ( \"Thailand\" , ( 97.3438072 , 5.612851 , 105.636812 , 20.4648337 )), \"TJ\" : ( \"Tajikistan\" , ( 67.3332775 , 36.6711153 , 75.1539563 , 41.0450935 )), \"TK\" : ( \"Tokelau\" , ( - 172.7213673 , - 9.6442499 , - 170.9797586 , - 8.3328631 )), \"TM\" : ( \"Turkmenistan\" , ( 52.335076 , 35.129093 , 66.6895177 , 42.7975571 )), \"TL\" : ( \"Timor-Leste\" , ( 124.0415703 , - 9.5642775 , 127.5335392 , - 8.0895459 )), \"TO\" : ( \"Tonga\" , ( - 179.3866055 , - 24.1034499 , - 173.5295458 , - 15.3655722 )), \"TT\" : ( \"Trinidad and Tobago\" , ( - 62.083056 , 9.8732106 , - 60.2895848 , 11.5628372 )), \"TN\" : ( \"Tunisia\" , ( 7.5219807 , 30.230236 , 11.8801133 , 37.7612052 )), \"TR\" : ( \"Turkey\" , ( 25.6212891 , 35.8076804 , 44.8176638 , 42.297 )), \"TV\" : ( \"Tuvalu\" , ( 175.1590468 , - 9.9939389 , 178.7344938 , - 5.4369611 )), \"TW\" : ( \"Taiwan, Province of China\" , ( 114.3599058 , 10.374269 , 122.297 , 26.4372222 )), \"TZ\" : ( \"Tanzania, United Republic of\" , ( 29.3269773 , - 11.761254 , 40.6584071 , - 0.9854812 ), ), \"UG\" : ( \"Uganda\" , ( 29.573433 , - 1.4823179 , 35.000308 , 4.2340766 )), \"UA\" : ( \"Ukraine\" , ( 22.137059 , 44.184598 , 40.2275801 , 52.3791473 )), \"UY\" : ( \"Uruguay\" , ( - 58.4948438 , - 35.7824481 , - 53.0755833 , - 30.0853962 )), \"US\" : ( \"United States of America\" , ( - 125.0011 , 24.9493 , - 66.9326 , 49.5904 )), \"UZ\" : ( \"Uzbekistan\" , ( 55.9977865 , 37.1821164 , 73.1397362 , 45.590118 )), \"VA\" : ( \"Holy See\" , ( 12.4457442 , 41.9002044 , 12.4583653 , 41.9073912 )), \"VC\" : ( \"Saint Vincent and the Grenadines\" , ( - 61.6657471 , 12.5166548 , - 60.9094146 , 13.583 ), ), \"VE\" : ( \"Venezuela (Bolivarian Republic of)\" , ( - 73.3529632 , 0.647529 , - 59.5427079 , 15.9158431 ), ), \"VG\" : ( \"Virgin Islands (British)\" , ( - 65.159094 , 17.623468 , - 64.512674 , 18.464984 )), \"VI\" : ( \"Virgin Islands (U.S.)\" , ( - 65.159094 , 17.623468 , - 64.512674 , 18.464984 )), \"VN\" : ( \"Viet Nam\" , ( 102.14441 , 8.1790665 , 114.3337595 , 23.393395 )), \"VU\" : ( \"Vanuatu\" , ( 166.3355255 , - 20.4627425 , 170.449982 , - 12.8713777 )), \"WF\" : ( \"Wallis and Futuna\" , ( - 178.3873749 , - 14.5630748 , - 175.9190391 , - 12.9827961 )), \"WS\" : ( \"Samoa\" , ( - 173.0091864 , - 14.2770916 , - 171.1929229 , - 13.2381892 )), \"YE\" : ( \"Yemen\" , ( 41.60825 , 11.9084802 , 54.7389375 , 19.0 )), \"ZA\" : ( \"South Africa\" , ( 16.3335213 , - 47.1788335 , 38.2898954 , - 22.1250301 )), \"ZM\" : ( \"Zambia\" , ( 21.9993509 , - 18.0765945 , 33.701111 , - 8.2712822 )), \"ZW\" : ( \"Zimbabwe\" , ( 25.2373 , - 22.4241096 , 33.0683413 , - 15.6097033 )), } def point_in_country_approx ( lat , lon , country ): c = COUNTRY_BOUNDING_BOXES [ country ] if ( lat is None ) | ( lon is None ): return None try : lat = float ( lat ) lon = float ( lon ) except : warnings . warn ( \"Rows dropped due to invalid longitude and/or latitude values\" ) return None in_range (( lat , lon )) if ( c [ 1 ][ 1 ] <= lat <= c [ 1 ][ 3 ]) and ( c [ 1 ][ 0 ] <= lon <= c [ 1 ][ 2 ]): return 1 else : return 0","title":"geo_utils"},{"location":"api/data_transformer/geo_utils.html#functions","text":"def decimal_degrees_to_degrees_minutes_seconds ( dd) This function helps to divide float value dd in decimal degree into [degreee, minute, second] Parameters dd Float value in decimal degree.","title":"Functions"},{"location":"api/data_transformer/geospatial.html","text":"geospatial The geospatial module supports transformation & calculation functions for geospatial fields, such as transforming between different formats, controlling geohash values precisions, checking if a location is inside a country, calculating centroid and Radius of Gyration, and reverse latitude-longitude pairs into address. Functions supported through this module are listed below: - geo_format_latlon - geo_format_cartesian - geo_format_geohash - location_distance - geohash_precision_control - location_in_polygon - location_in_country - centroid - weighted_centroid - rog_calculation - reverse_geocoding Expand source code \"\"\" The geospatial module supports transformation & calculation functions for geospatial fields, such as transforming between different formats, controlling geohash values precisions, checking if a location is inside a country, calculating centroid and Radius of Gyration, and reverse latitude-longitude pairs into address. Functions supported through this module are listed below: - geo_format_latlon - geo_format_cartesian - geo_format_geohash - location_distance - geohash_precision_control - location_in_polygon - location_in_country - centroid - weighted_centroid - rog_calculation - reverse_geocoding \"\"\" from math import sin , cos , sqrt , atan2 , pi , radians from loguru import logger from pyspark.sql import functions as F from pyspark.sql import types as T import reverse_geocoder as rg import warnings from anovos.data_ingest.data_ingest import recast_column from anovos.data_transformer.geo_utils import ( EARTH_RADIUS , from_latlon_decimal_degrees , to_latlon_decimal_degrees , haversine_distance , vincenty_distance , euclidean_distance , f_point_in_polygons , point_in_country_approx , ) def geo_format_latlon ( idf , list_of_lat , list_of_lon , input_format , output_format , result_prefix = [], optional_configs = { \"geohash_precision\" : 8 , \"radius\" : EARTH_RADIUS }, output_mode = \"append\" , ): \"\"\" This function is the main function to convert the input data's location columns from lat,lon format to desired format based on output_format. If output_mode is set to True, the original location columns will be dropped. For each location column, \"to_latlon_decimal_degrees\" will be called to convert them to decimal degrees, and then \"from_latlon_decimal_degrees\" will be called to transform decimal degrees to output_format If output_format is \"dd\" or \"dms\" or \"radian\", 2 new columns containing \"_lat_\", \"_long_\" in the column names will be created. If output_format is \"cartesian\", 3 new columns containing \"_x_\", \"_y_\", \"_z_\" in the column names will be created. If output_format is \"geohash\", 1 new column containing \"_geohash\" in the column name will be created. Parameters ---------- idf Input Dataframe. list_of_lat List of columns representing latitude e.g., [\"lat1\",\"lat2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lat2\". list_of_lon List of columns representing longitude e.g., [\"lon1\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lon1|lon2\". list_of_lon must have the same length as list_of_lat such that i-th element of list_of_lat and i-th element of list_of_lon form a lat-lon pair to format. input_format \"dd\", \"dms\", \"radian\". \"dd\" represents latitude and longitude in decimal degrees. \"dms\" represents latitude and longitude in degrees minutes second. \"radian\" represents latitude and longitude in radians. output_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". \"cartesian\" represents the Cartesian coordinates of the point in three-dimensional space. \"geohash\" represents geocoded locations. result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_lat and list_of_lon. If it is empty, <lat>_<lon> will be used for each lat-lon pair. For example, list_of_lat is \"lat1|lat2\", list_of_lon is \"L1|L2\". Case 1: result_prefix = \"L1|L2\". If output_format is \"dd\", \"dms\" or \"radian\", new columns will be named as L1_lat_<output_format>, L1_lon_<output_format>, L2_lat_<output_format>, L2_lon_<output_format>. If output_format is \"cartesian\", new columns will be named as L1_x, L1_y, L1_z, L2_x, L2_y, L2_z. If output_format is \"geohash\", new columns will be named as L1_geohash and L2_geohash. Case 2: result_prefix = []. Prefixes \"L1\" and \"L2\" in above column names will be replaced by \"lat1_lon1\" and \"lat2_lon2\". (Default value = []) optional_configs The following keys can be used: - geohash_precision: precision of the resultant geohash. This key is only used when output_format is \"geohash\". (Default value = 8) - radius: radius of Earth. Necessary only when output_format is \"cartesian\". (Default value = EARTH_RADIUS) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" geohash_precision = int ( optional_configs . get ( \"geohash_precision\" , 8 )) radius = optional_configs . get ( \"radius\" , EARTH_RADIUS ) if isinstance ( list_of_lat , str ): list_of_lat = [ x . strip () for x in list_of_lat . split ( \"|\" )] if isinstance ( list_of_lon , str ): list_of_lon = [ x . strip () for x in list_of_lon . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_lat + list_of_lon ): raise TypeError ( \"Invalid input for list_of_lat or list_of_lon\" ) format_list = [ \"dd\" , \"dms\" , \"radian\" , \"cartesian\" , \"geohash\" ] if ( input_format not in format_list [: 3 ]) or ( output_format not in format_list ): raise TypeError ( \"Invalid input for input_format or output_format\" ) if len ( list_of_lat ) != len ( list_of_lon ): raise TypeError ( \"list_of_lat and list_of_lon must have the same length\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_lat )): raise TypeError ( \"result_prefix must have the same length as list_of_lat and list_of_lon if it is not empty\" ) f_to_latlon_dd = F . udf ( lambda loc : to_latlon_decimal_degrees ( loc , input_format , radius ), T . ArrayType ( T . FloatType ()), ) from_latlon_dd_ = lambda loc : from_latlon_decimal_degrees ( loc , output_format , radius , geohash_precision ) if output_format in [ \"dd\" , \"radian\" , \"cartesian\" ]: f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . FloatType ())) elif output_format == \"dms\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . ArrayType ( T . FloatType ())) ) elif output_format == \"geohash\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . StringType ()) odf = idf for i , ( lat , lon ) in enumerate ( zip ( list_of_lat , list_of_lon )): col = result_prefix [ i ] if result_prefix else ( lat + \"_\" + lon ) odf = odf . withColumn ( col + \"_temp\" , F . array ( lat , lon )) . withColumn ( col + \"_\" + output_format , f_from_latlon_dd ( f_to_latlon_dd ( col + \"_temp\" )) ) if output_format in [ \"dd\" , \"dms\" , \"radian\" ]: odf = ( odf . withColumn ( col + \"_lat_\" + output_format , F . col ( col + \"_\" + output_format )[ 0 ] ) . withColumn ( col + \"_lon_\" + output_format , F . col ( col + \"_\" + output_format )[ 1 ] ) . drop ( col + \"_\" + output_format ) ) if output_format == \"cartesian\" : odf = ( odf . withColumn ( col + \"_x\" , F . col ( col + \"_\" + output_format )[ 0 ]) . withColumn ( col + \"_y\" , F . col ( col + \"_\" + output_format )[ 1 ]) . withColumn ( col + \"_z\" , F . col ( col + \"_\" + output_format )[ 2 ]) . drop ( col + \"_\" + output_format ) ) odf = odf . drop ( col + \"_temp\" ) if output_mode == \"replace\" : odf = odf . drop ( lat , lon ) return odf def geo_format_cartesian ( idf , list_of_x , list_of_y , list_of_z , output_format , result_prefix = [], optional_configs = { \"geohash_precision\" : 8 , \"radius\" : EARTH_RADIUS }, output_mode = \"append\" , ): \"\"\" This function helps to convert the input data's location columns from cartesian format to desired format based on output_format. If output_mode is set to True, the original location columns will be dropped. For each location column, \"to_latlon_decimal_degrees\" will be called to convert them to decimal degrees, and then \"from_latlon_decimal_degrees\" will be called to transform decimal degrees to output_format If output_format is \"dd\" or \"dms\" or \"radian\", 2 new columns containing \"_lat_\", \"_long_\" in the column names will be created. If output_format is \"geohash\", 1 new column containing \"_geohash\" in the column name will be created. Parameters ---------- idf Input Dataframe. list_of_x List of columns representing x axis values e.g., [\"x1\",\"x2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"x1|x2\". list_of_y List of columns representing y axis values e.g., [\"y1\",\"y2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"y1|y2\". list_of_z List of columns representing z axis values e.g., [\"z1\",\"z2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"z1|z2\". list_of_x, list_of_y and list_of_z must have the same length such that the i-th element of 3 lists form an x-y-z pair to format. output_format \"dd\", \"dms\", \"radian\", \"geohash\" \"dd\" represents latitude and longitude in decimal degrees. \"dms\" represents latitude and longitude in degrees minutes second. \"radian\" represents latitude and longitude in radians. \"geohash\" represents geocoded locations. result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_x, list_of_y and list_of_z. If it is empty, <x>_<y>_<z> will be used for each x-y-z pair. For example, list_of_x is \"x1|x2\", list_of_y is \"y1|y2\" and list_of_z is \"z1|z2\" Case 1: result_prefix = \"L1|L2\". If output_format is \"dd\", \"dms\" or \"radian\", new columns will be named as L1_lat_<output_format>, L1_lon_<output_format>, L2_lat_<output_format>, L2_lon_<output_format>. If output_format is \"geohash\", new columns will be named as L1_geohash and L2_geohash. Case 2: result_prefix = []. Prefixes \"L1\" and \"L2\" in above column names will be replaced by \"x1_y1_z1\" and \"x2_y2_z2\". (Default value = []) optional_configs The following keys can be used: - geohash_precision: precision of the resultant geohash. This key is only used when output_format is \"geohash\". (Default value = 8) - radius: radius of Earth. (Default value = EARTH_RADIUS) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" geohash_precision = int ( optional_configs . get ( \"geohash_precision\" , 8 )) radius = optional_configs . get ( \"radius\" , EARTH_RADIUS ) if isinstance ( list_of_x , str ): list_of_x = [ x . strip () for x in list_of_x . split ( \"|\" )] if isinstance ( list_of_y , str ): list_of_y = [ x . strip () for x in list_of_y . split ( \"|\" )] if isinstance ( list_of_z , str ): list_of_z = [ x . strip () for x in list_of_z . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_x + list_of_y + list_of_z ): raise TypeError ( \"Invalid input for list_of_x or list_of_y or list_of_z\" ) format_list = [ \"dd\" , \"dms\" , \"radian\" , \"geohash\" ] if output_format not in format_list : raise TypeError ( \"Invalid input for output_format\" ) if len ({ len ( list_of_x ), len ( list_of_y ), len ( list_of_z )}) != 1 : raise TypeError ( \"list_of_x, list_of_y and list_of_z must have the same length\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_x )): raise TypeError ( \"result_prefix must have the same length as list_of_x, list_of_y and list_of_y if it is not empty\" ) f_to_latlon_dd = F . udf ( lambda loc : to_latlon_decimal_degrees ( loc , \"cartesian\" , radius ), T . ArrayType ( T . FloatType ()), ) from_latlon_dd_ = lambda loc : from_latlon_decimal_degrees ( loc , output_format , radius , geohash_precision ) if output_format in [ \"dd\" , \"radian\" ]: f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . FloatType ())) elif output_format == \"dms\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . ArrayType ( T . FloatType ())) ) elif output_format == \"geohash\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . StringType ()) odf = idf for i , ( x , y , z ) in enumerate ( zip ( list_of_x , list_of_y , list_of_z )): col = result_prefix [ i ] if result_prefix else ( x + \"_\" + y + \"_\" + z ) odf = odf . withColumn ( col + \"_temp\" , F . array ( x , y , z )) . withColumn ( col + \"_\" + output_format , f_from_latlon_dd ( f_to_latlon_dd ( col + \"_temp\" )) ) if output_format in [ \"dd\" , \"dms\" , \"radian\" ]: odf = ( odf . withColumn ( col + \"_lat_\" + output_format , F . col ( col + \"_\" + output_format )[ 0 ] ) . withColumn ( col + \"_lon_\" + output_format , F . col ( col + \"_\" + output_format )[ 1 ] ) . drop ( col + \"_\" + output_format ) ) odf = odf . drop ( col + \"_temp\" ) if output_mode == \"replace\" : odf = odf . drop ( x , y , z ) return odf def geo_format_geohash ( idf , list_of_geohash , output_format , result_prefix = [], optional_configs = { \"radius\" : EARTH_RADIUS }, output_mode = \"append\" , ): \"\"\" This function is the main function to convert the input data's location columns from geohash format to desired format based on output_format. If output_mode is set to True, the original location columns will be dropped. For each location column, \"to_latlon_decimal_degrees\" will be called to convert them to decimal degrees, and then \"from_latlon_decimal_degrees\" will be called to transform decimal degrees to output_format If output_format is \"dd\" or \"dms\" or \"radian\", 2 new columns containing \"_lat_\", \"_long_\" in the column names will be created. If output_format is \"cartesian\", 3 new columns containing \"_x_\", \"_y_\", \"_z_\" in the column names will be created. Parameters ---------- idf Input Dataframe. list_of_geohash List of columns representing geohash e.g., [\"gh1\",\"gh2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"gh1|gh2\". output_format \"dd\", \"dms\", \"radian\", \"cartesian\" \"dd\" represents latitude and longitude in decimal degrees. \"dms\" represents latitude and longitude in degrees minutes second. \"radian\" represents latitude and longitude in radians. \"cartesian\" represents the Cartesian coordinates of the point in three-dimensional space. result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_geohash. If it is empty, <geohash column name> will be used as the prefix for transformed columns. For example, list_of_geohash is \"gh1|gh2\". Case 1: result_prefix = \"L1|L2\". If output_format is \"dd\", \"dms\" or \"radian\", new columns will be named as L1_lat_<output_format>, L1_lon_<output_format>, L2_lat_<output_format>, L2_lon_<output_format>. If output_format is \"cartesian\", new columns will be named as L1_x, L1_y, L1_z, L2_x, L2_y, L2_z. Case 2: result_prefix = []. Prefixes \"L1\" and \"L2\" in above column names will be replaced by \"gh1\" and \"gh2\". (Default value = []) optional_configs The following key can be used: - radius: radius of Earth. Necessary only when output_format is \"cartesian\". (Default value = EARTH_RADIUS) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" radius = optional_configs . get ( \"radius\" , EARTH_RADIUS ) if isinstance ( list_of_geohash , str ): list_of_geohash = [ x . strip () for x in list_of_geohash . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_geohash ): raise TypeError ( \"Invalid input for list_of_geohash\" ) format_list = [ \"dd\" , \"dms\" , \"radian\" , \"cartesian\" ] if output_format not in format_list : raise TypeError ( \"Invalid input for output_format\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_geohash )): raise TypeError ( \"result_prefix must have the same length as list_of_geohash if it is not empty\" ) f_to_latlon_dd = F . udf ( lambda loc : to_latlon_decimal_degrees ( loc , \"geohash\" , radius ), T . ArrayType ( T . FloatType ()), ) from_latlon_dd_ = lambda loc : from_latlon_decimal_degrees ( loc , output_format , radius ) if output_format in [ \"dd\" , \"radian\" , \"cartesian\" ]: f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . FloatType ())) elif output_format == \"dms\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . ArrayType ( T . FloatType ())) ) odf = idf for i , geohash in enumerate ( list_of_geohash ): col = result_prefix [ i ] if result_prefix else geohash odf = odf . withColumn ( col + \"_\" + output_format , f_from_latlon_dd ( f_to_latlon_dd ( geohash )) ) if output_format in [ \"dd\" , \"dms\" , \"radian\" ]: odf = ( odf . withColumn ( col + \"_lat_\" + output_format , F . col ( col + \"_\" + output_format )[ 0 ] ) . withColumn ( col + \"_lon_\" + output_format , F . col ( col + \"_\" + output_format )[ 1 ] ) . drop ( col + \"_\" + output_format ) ) if output_format == \"cartesian\" : odf = ( odf . withColumn ( col + \"_x\" , F . col ( col + \"_\" + output_format )[ 0 ]) . withColumn ( col + \"_y\" , F . col ( col + \"_\" + output_format )[ 1 ]) . withColumn ( col + \"_z\" , F . col ( col + \"_\" + output_format )[ 2 ]) . drop ( col + \"_\" + output_format ) ) if output_mode == \"replace\" : odf = odf . drop ( geohash ) return odf def location_distance ( idf , list_of_cols_loc1 , list_of_cols_loc2 , loc_format = \"dd\" , result_prefix = \"\" , distance_type = \"haversine\" , unit = \"m\" , optional_configs = { \"radius\" : EARTH_RADIUS , \"vincenty_model\" : \"WGS-84\" }, output_mode = \"append\" , ): \"\"\" This function calculates the distance between 2 locations, and the distance formula is determined by distance_type. If distance_type = \"vincenty\", thed loc_format should be \"dd\", and list_of_cols_loc1, list_of_cols_loc2 should be in [lat1, lon1] and [lat2, lon2] format respectively. \"vincenty_distance\" function will be called to calculate the distance. If distance_type = \"haversine\", then loc_format should be \"radian\", and list_of_cols_loc1, list_of_cols_loc2 should be in [lat1, lon1] and [lat2, lon2] format respectively. \"haversine_distance\" function will be called to calculate the distance. If distance_type = \"euclidean\", then loc_format should be \"cartesian\", and list_of_cols_loc1, list_of_cols_loc2 should be in [x1, y1, z1] and [x2, y2, z2] format respectively. \"euclidean_distance\" function will be called to calculate the distance. If loc_format does not match with distance_type's desired format, necessary conversion of location columns will be performed with the help of \"geo_format_latlon\", \"geo_format_cartesian\" and \"geo_format_geohash\" functions. Parameters ---------- idf Input Dataframe. list_of_cols_loc1 List of columns to express the first location e.g., [\"lat1\",\"lon1\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lon1\". list_of_cols_loc2 List of columns to express the second location e.g., [\"lat2\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat2|lon2\". loc_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". (Default value = \"dd\") result_prefix Prefix for the newly generated column. It must be a string or a list with one element. If it is empty, <list_of_cols_loc1 joined by '_'>_<list_of_cols_loc2 joined by '_'> will be used as the prefix. For example, list_of_cols_loc1 is \"lat1|lon1\", list_of_cols_loc2 is \"lat2|lon2\". Case 1: result_prefix = \"L1_L2\": the new column will be named as L1_L2_distance. Case 2: result_prefix = []: the new column will be named as lat1_lon1_lat2_lon2_distance. (Default value = '') distance_type \"vincenty\", \"haversine\", \"euclidean\". (Default value = \"haversine\") \"vincenty\" option calculates the distance between two points on the surface of a spheroid. \"haversine\" option calculates the great-circle distance between two points on a sphere. \"euclidean\" option calculates the length of the line segment between two points. unit \"m\", \"km\". Unit of the result. (Default value = \"m\") optional_configs The following keys can be used: - radius: radius of Earth. Necessary only when output_format is \"cartesian\". (Default value = EARTH_RADIUS) - vincenty_model: The ellipsoidal model to use. Supported values: \"WGS-84\", \"GRS-80\", \"Airy (1830)\", \"Intl 1924\", \"Clarke (1880)\", \"GRS-67\". For more information, please refer to geopy.distance.ELLIPSOIDS. (Default value = \"WGS-84\") output_mode \"replace\", \"append\". \"replace\" option replaces original columns with transformed column. \"append\" option appends the transformed column to the input dataset with name \"<loc1>_<loc2>_distance\". (Default value = \"append\") Returns ------- DataFrame \"\"\" radius = optional_configs . get ( \"radius\" , EARTH_RADIUS ) vincenty_model = optional_configs . get ( \"vincenty_model\" , \"WGS-84\" ) if isinstance ( list_of_cols_loc1 , str ): list_of_cols_loc1 = [ x . strip () for x in list_of_cols_loc1 . split ( \"|\" )] if isinstance ( list_of_cols_loc2 , str ): list_of_cols_loc2 = [ x . strip () for x in list_of_cols_loc2 . split ( \"|\" )] if isinstance ( result_prefix , list ): if len ( result_prefix ) > 1 : raise TypeError ( \"If result_prefix is a list, it can contain maximally 1 element\" ) elif len ( result_prefix ) == 1 : result_prefix = result_prefix [ 0 ] if any ( i not in idf . columns for i in list_of_cols_loc1 + list_of_cols_loc2 ): raise TypeError ( \"Invalid input for list_of_cols_loc1 or list_of_cols_loc2\" ) if distance_type not in [ \"vincenty\" , \"haversine\" , \"euclidean\" ]: raise TypeError ( \"Invalid input for distance_type\" ) if loc_format not in [ \"dd\" , \"dms\" , \"radian\" , \"cartesian\" , \"geohash\" ]: raise TypeError ( \"Invalid input for loc_format\" ) format_mapping = { \"vincenty\" : \"dd\" , \"haversine\" : \"radian\" , \"euclidean\" : \"cartesian\" } format_required = format_mapping [ distance_type ] if loc_format != format_required : if loc_format in [ \"dd\" , \"dms\" , \"radian\" ]: idf = geo_format_latlon ( idf , list_of_lat = [ list_of_cols_loc1 [ 0 ], list_of_cols_loc2 [ 0 ]], list_of_lon = [ list_of_cols_loc1 [ 1 ], list_of_cols_loc2 [ 1 ]], input_format = loc_format , output_format = format_required , result_prefix = [ \"temp_loc1\" , \"temp_loc2\" ], optional_configs = { \"radius\" : radius }, output_mode = \"append\" , ) elif loc_format == \"cartesian\" : idf = geo_format_cartesian ( idf , list_of_x = [ list_of_cols_loc1 [ 0 ], list_of_cols_loc2 [ 0 ]], list_of_y = [ list_of_cols_loc1 [ 1 ], list_of_cols_loc2 [ 1 ]], list_of_z = [ list_of_cols_loc1 [ 2 ], list_of_cols_loc2 [ 2 ]], output_format = format_required , result_prefix = [ \"temp_loc1\" , \"temp_loc2\" ], optional_configs = { \"radius\" : radius }, output_mode = \"append\" , ) elif loc_format == \"geohash\" : idf = geo_format_geohash ( idf , list_of_geohash = [ list_of_cols_loc1 [ 0 ], list_of_cols_loc2 [ 0 ]], output_format = format_required , result_prefix = [ \"temp_loc1\" , \"temp_loc2\" ], optional_configs = { \"radius\" : radius }, output_mode = \"append\" , ) if format_required == \"dd\" : loc1 , loc2 = [ \"temp_loc1_lat_dd\" , \"temp_loc1_lon_dd\" ], [ \"temp_loc2_lat_dd\" , \"temp_loc2_lon_dd\" , ] elif format_required == \"radian\" : loc1 , loc2 = [ \"temp_loc1_lat_radian\" , \"temp_loc1_lon_radian\" ], [ \"temp_loc2_lat_radian\" , \"temp_loc2_lon_radian\" , ] elif format_required == \"cartesian\" : loc1 , loc2 = [ \"temp_loc1_x\" , \"temp_loc1_y\" , \"temp_loc1_z\" ], [ \"temp_loc2_x\" , \"temp_loc2_y\" , \"temp_loc2_z\" , ] idf = ( idf . withColumn ( \"temp_loc1\" , F . array ( * loc1 )) . withColumn ( \"temp_loc2\" , F . array ( * loc2 )) . drop ( * ( loc1 + loc2 )) ) else : idf = idf . withColumn ( \"temp_loc1\" , F . array ( * list_of_cols_loc1 )) . withColumn ( \"temp_loc2\" , F . array ( * list_of_cols_loc2 ) ) if distance_type == \"vincenty\" : compute_distance = lambda x1 , x2 : vincenty_distance ( x1 , x2 , unit , vincenty_model ) elif distance_type == \"haversine\" : compute_distance = lambda x1 , x2 : haversine_distance ( x1 , x2 , \"radian\" , unit , radius ) else : compute_distance = lambda x1 , x2 : euclidean_distance ( x1 , x2 , unit ) f_compute_distance = F . udf ( compute_distance , T . FloatType ()) col_prefix = ( result_prefix if result_prefix else \"_\" . join ( list_of_cols_loc1 ) + \"_\" + \"_\" . join ( list_of_cols_loc2 ) ) odf = idf . withColumn ( col_prefix + \"_distance\" , f_compute_distance ( \"temp_loc1\" , \"temp_loc2\" ) ) . drop ( \"temp_loc1\" , \"temp_loc2\" ) if output_mode == \"replace\" : odf = odf . drop ( * ( list_of_cols_loc1 + list_of_cols_loc2 )) return odf def geohash_precision_control ( idf , list_of_geohash , output_precision = 8 , km_max_error = None , output_mode = \"append\" ): \"\"\" This function controls the precision of input data's geohash columns. Parameters ---------- idf Input Dataframe. list_of_geohash List of columns in geohash format e.g., [\"gh1\",\"gh2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"gh1|gh2\". output_precision Precision of the transformed geohash in the output dataframe. (Default value = 8) km_max_error Maximum permissible error in kilometers. If km_max_error is specified, output_precision will be ignored and km_max_error will be mapped to an output_precision according to the following dictionary: {2500: 1, 630: 2, 78: 3, 20: 4, 2.4: 5, 0.61: 6, 0.076: 7, 0.019: 8, 0.0024: 9, 0.00060: 10, 0.000074: 11}. (Default value = None) output_mode \"replace\", \"append\". \"replace\" option replaces original columns with transformed column. \"append\" option appends the transformed column to the input dataset with postfix \"_precision_<output_precision>\". (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_geohash , str ): list_of_geohash = [ x . strip () for x in list_of_geohash . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_geohash ): raise TypeError ( \"Invalid input for list_of_geohash\" ) error_precision_mapping = { 2500 : 1 , 630 : 2 , 78 : 3 , 20 : 4 , 2.4 : 5 , 0.61 : 6 , 0.076 : 7 , 0.019 : 8 , 0.0024 : 9 , 0.00060 : 10 , 0.000074 : 11 , } if km_max_error is not None : output_precision = 12 for key , val in error_precision_mapping . items (): if km_max_error >= key : output_precision = val break output_precision = int ( output_precision ) logger . info ( \"Precision of the output geohashes will be capped at \" + str ( output_precision ) + \".\" ) odf = idf for i , geohash in enumerate ( list_of_geohash ): if output_mode == \"replace\" : col_name = geohash else : col_name = geohash + \"_precision_\" + str ( output_precision ) odf = odf . withColumn ( col_name , F . substring ( geohash , 1 , output_precision )) return odf def location_in_polygon ( idf , list_of_lat , list_of_lon , polygon , result_prefix = [], output_mode = \"append\" ): \"\"\" This function checks whether each lat-lon pair is insided a GeoJSON object. The following types of GeoJSON objects are supported by this function: Polygon, MultiPolygon, Feature or FeatureCollection Parameters ---------- idf Input Dataframe. list_of_lat List of columns representing latitude e.g., [\"lat1\",\"lat2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lat2\". list_of_lon List of columns representing longitude e.g., [\"lon1\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lon1|lon2\". list_of_lon must have the same length as list_of_lat such that i-th element of list_of_lat and i-th element of list_of_lon form a lat-lon pair to format. polygon The following types of GeoJSON objects are supported: Polygon, MultiPolygon, Feature or FeatureCollection result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_lat and list_of_lon. If it is empty, <lat>_<lon> will be used for each lat-lon pair. For example, list_of_lat is \"lat1|lat2\", list_of_lon is \"lon1|lon2\". Case 1: result_prefix = \"L1|L2\". New columns will be named as L1_in_poly and L2_in_poly. Calse 2: result_prefix = []. New columns will be named as lat1_lon1_in_poly and lat2_lon2_in_poly. (Default value = []) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_lat , str ): list_of_lat = [ x . strip () for x in list_of_lat . split ( \"|\" )] if isinstance ( list_of_lon , str ): list_of_lon = [ x . strip () for x in list_of_lon . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_lat + list_of_lon ): raise TypeError ( \"Invalid input for list_of_lat or list_of_lon\" ) if len ( list_of_lat ) != len ( list_of_lon ): raise TypeError ( \"list_of_lat and list_of_lon must have the same length\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_lat )): raise TypeError ( \"result_prefix must have the same length as list_of_lat and list_of_lon if it is not empty\" ) if \"coordinates\" in polygon . keys (): polygon_list = polygon [ \"coordinates\" ] if polygon [ \"type\" ] == \"Polygon\" : polygon_list = [ polygon_list ] elif \"geometry\" in polygon . keys (): polygon_list = [ polygon [ \"geometry\" ][ \"coordinates\" ]] elif \"features\" in polygon . keys (): polygon_list = [] for poly in polygon [ \"features\" ]: polygon_list . append ( poly [ \"geometry\" ][ \"coordinates\" ]) odf = idf for i , ( lat , lon ) in enumerate ( zip ( list_of_lat , list_of_lon )): col = result_prefix [ i ] if result_prefix else ( lat + \"_\" + lon ) odf = odf . withColumn ( col + \"_in_poly\" , f_point_in_polygons ( polygon_list )( F . col ( lon ), F . col ( lat )) ) if output_mode == \"replace\" : odf = odf . drop ( lat , lon ) return odf def location_in_country ( spark , idf , list_of_lat , list_of_lon , country , country_shapefile_path = \"\" , method_type = \"approx\" , result_prefix = [], output_mode = \"append\" , ): \"\"\" This function checks whether each lat-lon pair is insided a country. Two ways of checking are supported: \"approx\" (using the bounding box of a country) and \"exact\" (using the shapefile of a country). Parameters ---------- spark Spark Session idf Input Dataframe. list_of_lat List of columns representing latitude e.g., [\"lat1\",\"lat2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lat2\". list_of_lon List of columns representing longitude e.g., [\"lon1\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lon1|lon2\". list_of_lon must have the same length as list_of_lat such that i-th element of list_of_lat and i-th element of list_of_lon form a lat-lon pair to format. country The Alpha-2 country code. country_shapefile_path The geojson file with a FeatureCollection object containing polygons for each country. One example file country_polygons.geojson can be downloaded from Anovos GitHub repository: https://github.com/anovos/anovos/tree/main/data/ method_type \"approx\", \"exact\". \"approx\" uses the bounding box of a country to estimate whether a location is inside the country \"exact\" uses the shapefile of a country to calculate whether a location is inside the country result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_lat and list_of_lon. If it is empty, <lat>_<lon> will be used for each lat-lon pair. For example, list_of_lat is \"lat1|lat2\", list_of_lon is \"lon1|lon2\". Case 1: result_prefix = \"L1|L2\", country=\"US\" New columns will be named as L1_in_US and L2_in_US. Calse 2: result_prefix = [], country=\"US\" New columns will be named as lat1_lon1_in_US and lat2_lon2_in_US. (Default value = []) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_lat , str ): list_of_lat = [ x . strip () for x in list_of_lat . split ( \"|\" )] if isinstance ( list_of_lon , str ): list_of_lon = [ x . strip () for x in list_of_lon . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_lat + list_of_lon ): raise TypeError ( \"Invalid input for list_of_lat or list_of_lon\" ) if len ( list_of_lat ) != len ( list_of_lon ): raise TypeError ( \"list_of_lat and list_of_lon must have the same length\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_lat )): raise TypeError ( \"result_prefix must have the same length as list_of_lat and list_of_lon if it is not empty\" ) if method_type not in ( \"approx\" , \"exact\" ): raise TypeError ( \"Invalid input for method_type.\" ) f_point_in_country_approx = F . udf ( point_in_country_approx , T . IntegerType ()) if method_type == \"exact\" : def zip_feats ( x , y ): \"\"\"zipping two features (in list form) elementwise\"\"\" return zip ( x , y ) f_zip_feats = F . udf ( zip_feats , T . ArrayType ( T . StructType ( [ T . StructField ( \"first\" , T . StringType ()), T . StructField ( \"second\" , T . ArrayType ( T . ArrayType ( T . ArrayType ( T . ArrayType ( T . DoubleType ()))) ), ), ] ) ), ) geo_data = spark . read . json ( country_shapefile_path , multiLine = True ) . withColumn ( \"tmp\" , f_zip_feats ( \"features.properties.ISO_A2\" , \"features.geometry.coordinates\" ), ) polygon_list = ( geo_data . select ( F . explode ( F . col ( \"tmp\" )) . alias ( \"country_coord\" )) . withColumn ( \"country_code\" , F . col ( \"country_coord\" ) . getItem ( \"first\" )) . withColumn ( \"coordinates\" , F . col ( \"country_coord\" ) . getItem ( \"second\" )) . where ( F . col ( \"country_code\" ) == country ) . select ( \"coordinates\" ) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) print ( \"No. of polygon: \" + str ( len ( polygon_list ))) min_lon , min_lat = polygon_list [ 0 ][ 0 ][ 0 ] max_lon , max_lat = polygon_list [ 0 ][ 0 ][ 0 ] for polygon in polygon_list : exterior = polygon [ 0 ] for loc in exterior : if loc [ 0 ] < min_lon : min_lon = loc [ 0 ] elif loc [ 0 ] > max_lon : max_lon = loc [ 0 ] if loc [ 1 ] < min_lat : min_lat = loc [ 1 ] elif loc [ 1 ] > max_lat : max_lat = loc [ 1 ] odf = idf for i , ( lat , lon ) in enumerate ( zip ( list_of_lat , list_of_lon )): col = result_prefix [ i ] if result_prefix else ( lat + \"_\" + lon ) if method_type == \"exact\" : odf = odf . withColumn ( col + \"_in_\" + country + \"_exact\" , f_point_in_polygons ( polygon_list , [ min_lon , min_lat ], [ max_lon , max_lat ] )( F . col ( lon ), F . col ( lat )), ) else : odf = odf . withColumn ( col + \"_in_\" + country + \"_approx\" , f_point_in_country_approx ( F . col ( lat ), F . col ( lon ), F . lit ( country )), ) if output_mode == \"replace\" : odf = odf . drop ( lat , lon ) return odf def centroid ( idf , lat_col , long_col , id_col = None ): \"\"\" This function calculates the centroid of a given DataFrame using lat-long pairs based on its identifier column (if applicable) Parameters ---------- idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data id_col Column in the input DataFrame that contains identifier for the DataFrame (Default is None) If id_col=None, the function will calculate centriod of latitude and longitude for the whole dataset. Returns ------- odf : DataFrame Output DataFrame, which contains lat_centroid and long_centroid and identifier (if applicable) \"\"\" if id_col not in idf . columns and id_col : raise TypeError ( \"Invalid input for id_col\" ) if lat_col not in idf . columns : raise TypeError ( \"Invalid input for lat_col\" ) if long_col not in idf . columns : raise TypeError ( \"Invalid input for long_col\" ) idf = recast_column ( idf , list_of_cols = [ lat_col , long_col ], list_of_dtypes = [ \"double\" , \"double\" ] ) if idf != idf . dropna ( subset = ( lat_col , long_col )): warnings . warn ( \"Rows dropped due to null value in longitude and/or latitude values\" ) idf = idf . dropna ( subset = ( lat_col , long_col )) if not idf . where ( ( F . col ( lat_col ) > 90 ) | ( F . col ( lat_col ) < - 90 ) | ( F . col ( long_col ) > 180 ) | ( F . col ( long_col ) < - 180 ) ) . rdd . isEmpty (): warnings . warn ( \"Rows dropped due to longitude and/or latitude values being out of the valid range\" ) idf = idf . where ( ( F . col ( lat_col ) <= 90 ) & ( F . col ( lat_col ) >= - 90 ) & ( F . col ( long_col ) <= 180 ) & ( F . col ( long_col ) >= - 180 ) ) if idf . rdd . isEmpty (): warnings . warn ( \"No reverse_geocoding Computation - No valid latitude/longitude row(s) to compute\" ) return idf def degree_to_radian ( deg ): return deg * pi / 180 f_degree_to_radian = F . udf ( degree_to_radian , T . FloatType ()) idf_rad = ( idf . withColumn ( \"lat_rad\" , f_degree_to_radian ( lat_col )) . withColumn ( \"long_rad\" , f_degree_to_radian ( long_col )) . withColumn ( \"x\" , F . cos ( \"lat_rad\" ) * F . cos ( \"long_rad\" )) . withColumn ( \"y\" , F . cos ( \"lat_rad\" ) * F . sin ( \"long_rad\" )) . withColumn ( \"z\" , F . sin ( \"lat_rad\" )) ) if id_col : idf_groupby = idf_rad . groupby ( id_col ) . agg ( F . sum ( \"x\" ) . alias ( \"x_group\" ), F . sum ( \"y\" ) . alias ( \"y_group\" ), F . sum ( \"z\" ) . alias ( \"z_group\" ), ) odf = ( idf_groupby . withColumn ( \"hyp\" , F . sqrt ( F . col ( \"x_group\" ) * F . col ( \"x_group\" ) + F . col ( \"y_group\" ) * F . col ( \"y_group\" ) ), ) . withColumn ( lat_col + \"_centroid\" , F . atan2 ( F . col ( \"z_group\" ), F . col ( \"hyp\" )) * 180 / pi , ) . withColumn ( long_col + \"_centroid\" , F . atan2 ( F . col ( \"y_group\" ), F . col ( \"x_group\" )) * 180 / pi , ) . select ( id_col , lat_col + \"_centroid\" , long_col + \"_centroid\" ) ) else : idf_groupby = idf_rad . groupby () . agg ( F . sum ( \"x\" ) . alias ( \"x_group\" ), F . sum ( \"y\" ) . alias ( \"y_group\" ), F . sum ( \"z\" ) . alias ( \"z_group\" ), ) odf = ( idf_groupby . withColumn ( \"hyp\" , F . sqrt ( F . col ( \"x_group\" ) * F . col ( \"x_group\" ) + F . col ( \"y_group\" ) * F . col ( \"y_group\" ) ), ) . withColumn ( lat_col + \"_centroid\" , F . atan2 ( F . col ( \"z_group\" ), F . col ( \"hyp\" )) * 180 / pi , ) . withColumn ( long_col + \"_centroid\" , F . atan2 ( F . col ( \"y_group\" ), F . col ( \"x_group\" )) * 180 / pi , ) . select ( lat_col + \"_centroid\" , long_col + \"_centroid\" ) ) return odf def weighted_centroid ( idf , id_col , lat_col , long_col ): \"\"\" This function calculates the weighted centroid of a given DataFrame using lat-long pairs based on its identifier column Parameters ---------- idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data id_col Column in the input DataFrame that contains identifier for the DataFrame Returns ------- odf : DataFrame Output DataFrame, which contains weighted lat_centroid and long_centroid and identifier \"\"\" if id_col not in idf . columns : raise TypeError ( \"Invalid input for id_col\" ) if lat_col not in idf . columns : raise TypeError ( \"Invalid input for lat_col\" ) if long_col not in idf . columns : raise TypeError ( \"Invalid input for long_col\" ) idf = recast_column ( idf , list_of_cols = [ lat_col , long_col ], list_of_dtypes = [ \"double\" , \"double\" ] ) if idf != idf . dropna ( subset = ( lat_col , long_col )): warnings . warn ( \"Rows dropped due to null value in longitude and/or latitude values\" ) idf = idf . dropna ( subset = ( lat_col , long_col )) if not idf . where ( ( F . col ( lat_col ) > 90 ) | ( F . col ( lat_col ) < - 90 ) | ( F . col ( long_col ) > 180 ) | ( F . col ( long_col ) < - 180 ) ) . rdd . isEmpty (): warnings . warn ( \"Rows dropped due to longitude and/or latitude values being out of the valid range\" ) idf = idf . where ( ( F . col ( lat_col ) <= 90 ) & ( F . col ( lat_col ) >= - 90 ) & ( F . col ( long_col ) <= 180 ) & ( F . col ( long_col ) >= - 180 ) ) if idf . rdd . isEmpty (): warnings . warn ( \"No reverse_geocoding Computation - No valid latitude/longitude row(s) to compute\" ) return idf def degree_to_radian ( deg ): return deg * pi / 180 f_degree_to_radian = F . udf ( degree_to_radian , T . FloatType ()) idf_rad = ( idf . withColumn ( \"lat_rad\" , f_degree_to_radian ( lat_col )) . withColumn ( \"long_rad\" , f_degree_to_radian ( long_col )) . withColumn ( \"x\" , F . cos ( \"lat_rad\" ) * F . cos ( \"long_rad\" )) . withColumn ( \"y\" , F . cos ( \"lat_rad\" ) * F . sin ( \"long_rad\" )) . withColumn ( \"z\" , F . sin ( \"lat_rad\" )) ) idf_groupby = ( idf_rad . groupby ( id_col ) . agg ( F . sum ( \"x\" ) . alias ( \"x_group\" ), F . sum ( \"y\" ) . alias ( \"y_group\" ), F . sum ( \"z\" ) . alias ( \"z_group\" ), F . count ( id_col ) . alias ( \"weight_group\" ), ) . withColumn ( \"weighted_x\" , F . col ( \"x_group\" ) * F . col ( \"weight_group\" )) . withColumn ( \"weighted_y\" , F . col ( \"y_group\" ) * F . col ( \"weight_group\" )) . withColumn ( \"weighted_z\" , F . col ( \"z_group\" ) * F . col ( \"weight_group\" )) ) total_weight = ( idf_groupby . groupBy () . agg ( F . sum ( \"weight_group\" )) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) total_x = ( idf_groupby . groupBy () . agg ( F . sum ( \"weighted_x\" )) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) total_y = ( idf_groupby . groupBy () . agg ( F . sum ( \"weighted_y\" )) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) total_z = ( idf_groupby . groupBy () . agg ( F . sum ( \"weighted_z\" )) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) x = total_x / total_weight y = total_y / total_weight z = total_z / total_weight hyp = sqrt ( x * x + y * y ) lat_centroid , long_centroid = atan2 ( z , hyp ) * 180 / pi , atan2 ( y , x ) * 180 / pi odf = ( idf_groupby . select ( id_col ) . withColumn ( lat_col + \"_centroid\" , F . lit ( lat_centroid )) . withColumn ( long_col + \"_centroid\" , F . lit ( long_centroid )) ) return odf def rog_calculation ( idf , lat_col , long_col , id_col = None ): \"\"\" This function calculates the Radius of Gyration (in meter) of a given DataFrame, based on its identifier column (if applicable) Parameters ---------- idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data id_col Column in the input DataFrame that contains identifier for the DataFrame (Default is None) Returns ------- odf : DataFrame Output DataFrame, which contains Radius of Gyration (in meter) and identifier (if applicable) \"\"\" if id_col not in idf . columns and id_col : raise TypeError ( \"Invalid input for id_col\" ) if lat_col not in idf . columns : raise TypeError ( \"Invalid input for lat_col\" ) if long_col not in idf . columns : raise TypeError ( \"Invalid input for long_col\" ) idf = recast_column ( idf , list_of_cols = [ lat_col , long_col ], list_of_dtypes = [ \"double\" , \"double\" ] ) if idf != idf . dropna ( subset = ( lat_col , long_col )): warnings . warn ( \"Rows dropped due to null value in longitude and/or latitude values\" ) idf = idf . dropna ( subset = ( lat_col , long_col )) if not idf . where ( ( F . col ( lat_col ) > 90 ) | ( F . col ( lat_col ) < - 90 ) | ( F . col ( long_col ) > 180 ) | ( F . col ( long_col ) < - 180 ) ) . rdd . isEmpty (): warnings . warn ( \"Rows dropped due to longitude and/or latitude values being out of the valid range\" ) idf = idf . where ( ( F . col ( lat_col ) <= 90 ) & ( F . col ( lat_col ) >= - 90 ) & ( F . col ( long_col ) <= 180 ) & ( F . col ( long_col ) >= - 180 ) ) if idf . rdd . isEmpty (): warnings . warn ( \"No reverse_geocoding Computation - No valid latitude/longitude row(s) to compute\" ) return idf def getHaversineDist ( lat1 , lon1 , lat2 , lon2 ): R = 6378126 # approximate radius of earth in m lat1 = radians ( float ( lat1 )) lon1 = radians ( float ( lon1 )) lat2 = radians ( float ( lat2 )) lon2 = radians ( float ( lon2 )) dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 c = 2 * atan2 ( sqrt ( a ), sqrt ( 1 - a )) distance = R * c return distance udf_harver_dist = F . udf ( getHaversineDist , T . FloatType ()) if id_col : idf_centroid = centroid ( idf , lat_col , long_col , id_col ) idf_join = idf_centroid . join ( idf , id_col , \"inner\" ) idf_calc = idf_join . withColumn ( \"distance\" , udf_harver_dist ( F . col ( lat_col ), F . col ( long_col ), F . col ( lat_col + \"_centroid\" ), F . col ( long_col + \"_centroid\" ), ), ) odf = idf_calc . groupby ( id_col ) . agg ( F . mean ( \"distance\" ) . alias ( \"radius_of_gyration\" ) ) else : centroid_info = centroid ( idf , lat_col , long_col , id_col ) . rdd . collect () lat_centroid = centroid_info [ 0 ][ 0 ] long_centroid = centroid_info [ 0 ][ 1 ] idf_join = idf . withColumn ( lat_col + \"_centroid\" , F . lit ( lat_centroid ) ) . withColumn ( long_col + \"_centroid\" , F . lit ( long_centroid )) idf_calc = idf_join . withColumn ( \"distance\" , udf_harver_dist ( F . col ( lat_col ), F . col ( long_col ), F . col ( lat_col + \"_centroid\" ), F . col ( long_col + \"_centroid\" ), ), ) odf = idf_calc . groupby () . agg ( F . mean ( \"distance\" ) . alias ( \"radius_of_gyration\" )) return odf def reverse_geocoding ( idf , lat_col , long_col ): \"\"\" This function reverses the input latitude and longitude of a given DataFrame into address Parameters ---------- idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data Returns ------- odf : DataFrame Output DataFrame, which contains latitude, longitude and address appropriately \"\"\" if lat_col not in idf . columns : raise TypeError ( \"Invalid input for lat_col\" ) if long_col not in idf . columns : raise TypeError ( \"Invalid input for long_col\" ) idf = recast_column ( idf , list_of_cols = [ lat_col , long_col ], list_of_dtypes = [ \"double\" , \"double\" ] ) if idf != idf . dropna ( subset = ( lat_col , long_col )): warnings . warn ( \"Rows dropped due to null value in longitude and/or latitude values\" ) idf = idf . dropna ( subset = ( lat_col , long_col )) if not idf . where ( ( F . col ( lat_col ) > 90 ) | ( F . col ( lat_col ) < - 90 ) | ( F . col ( long_col ) > 180 ) | ( F . col ( long_col ) < - 180 ) ) . rdd . isEmpty (): warnings . warn ( \"Rows dropped due to longitude and/or latitude values being out of the valid range\" ) idf = idf . where ( ( F . col ( lat_col ) <= 90 ) & ( F . col ( lat_col ) >= - 90 ) & ( F . col ( long_col ) <= 180 ) & ( F . col ( long_col ) >= - 180 ) ) if idf . rdd . isEmpty (): warnings . warn ( \"No reverse_geocoding Computation - No valid latitude/longitude row(s) to compute\" ) return idf def reverse_geocode ( lat , long ): coordinates = ( float ( lat ), float ( long )) location = rg . search ( coordinates , mode = 1 ) if location : return ( str ( location [ 0 ][ \"name\" ]) + \",\" + str ( location [ 0 ][ \"admin1\" ]) + \",\" + str ( location [ 0 ][ \"cc\" ]) ) else : return \"N/A\" udf_reverse_geocode = F . udf ( reverse_geocode ) odf = ( idf . withColumn ( \"info\" , udf_reverse_geocode ( F . col ( lat_col ), F . col ( long_col ))) . select ( lat_col , long_col , \"info\" ) . withColumn ( \"name_of_place\" , F . split ( F . col ( \"info\" ), \",\" ) . getItem ( 0 )) . withColumn ( \"region\" , F . split ( F . col ( \"info\" ), \",\" ) . getItem ( 1 )) . withColumn ( \"country_code\" , F . split ( F . col ( \"info\" ), \",\" ) . getItem ( 2 )) . drop ( \"info\" ) ) return odf Functions def centroid ( idf, lat_col, long_col, id_col=None) This function calculates the centroid of a given DataFrame using lat-long pairs based on its identifier column (if applicable) Parameters idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data id_col Column in the input DataFrame that contains identifier for the DataFrame (Default is None) If id_col=None, the function will calculate centriod of latitude and longitude for the whole dataset. Returns odf :\u2002 DataFrame Output DataFrame, which contains lat_centroid and long_centroid and identifier (if applicable) Expand source code def centroid ( idf , lat_col , long_col , id_col = None ): \"\"\" This function calculates the centroid of a given DataFrame using lat-long pairs based on its identifier column (if applicable) Parameters ---------- idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data id_col Column in the input DataFrame that contains identifier for the DataFrame (Default is None) If id_col=None, the function will calculate centriod of latitude and longitude for the whole dataset. Returns ------- odf : DataFrame Output DataFrame, which contains lat_centroid and long_centroid and identifier (if applicable) \"\"\" if id_col not in idf . columns and id_col : raise TypeError ( \"Invalid input for id_col\" ) if lat_col not in idf . columns : raise TypeError ( \"Invalid input for lat_col\" ) if long_col not in idf . columns : raise TypeError ( \"Invalid input for long_col\" ) idf = recast_column ( idf , list_of_cols = [ lat_col , long_col ], list_of_dtypes = [ \"double\" , \"double\" ] ) if idf != idf . dropna ( subset = ( lat_col , long_col )): warnings . warn ( \"Rows dropped due to null value in longitude and/or latitude values\" ) idf = idf . dropna ( subset = ( lat_col , long_col )) if not idf . where ( ( F . col ( lat_col ) > 90 ) | ( F . col ( lat_col ) < - 90 ) | ( F . col ( long_col ) > 180 ) | ( F . col ( long_col ) < - 180 ) ) . rdd . isEmpty (): warnings . warn ( \"Rows dropped due to longitude and/or latitude values being out of the valid range\" ) idf = idf . where ( ( F . col ( lat_col ) <= 90 ) & ( F . col ( lat_col ) >= - 90 ) & ( F . col ( long_col ) <= 180 ) & ( F . col ( long_col ) >= - 180 ) ) if idf . rdd . isEmpty (): warnings . warn ( \"No reverse_geocoding Computation - No valid latitude/longitude row(s) to compute\" ) return idf def degree_to_radian ( deg ): return deg * pi / 180 f_degree_to_radian = F . udf ( degree_to_radian , T . FloatType ()) idf_rad = ( idf . withColumn ( \"lat_rad\" , f_degree_to_radian ( lat_col )) . withColumn ( \"long_rad\" , f_degree_to_radian ( long_col )) . withColumn ( \"x\" , F . cos ( \"lat_rad\" ) * F . cos ( \"long_rad\" )) . withColumn ( \"y\" , F . cos ( \"lat_rad\" ) * F . sin ( \"long_rad\" )) . withColumn ( \"z\" , F . sin ( \"lat_rad\" )) ) if id_col : idf_groupby = idf_rad . groupby ( id_col ) . agg ( F . sum ( \"x\" ) . alias ( \"x_group\" ), F . sum ( \"y\" ) . alias ( \"y_group\" ), F . sum ( \"z\" ) . alias ( \"z_group\" ), ) odf = ( idf_groupby . withColumn ( \"hyp\" , F . sqrt ( F . col ( \"x_group\" ) * F . col ( \"x_group\" ) + F . col ( \"y_group\" ) * F . col ( \"y_group\" ) ), ) . withColumn ( lat_col + \"_centroid\" , F . atan2 ( F . col ( \"z_group\" ), F . col ( \"hyp\" )) * 180 / pi , ) . withColumn ( long_col + \"_centroid\" , F . atan2 ( F . col ( \"y_group\" ), F . col ( \"x_group\" )) * 180 / pi , ) . select ( id_col , lat_col + \"_centroid\" , long_col + \"_centroid\" ) ) else : idf_groupby = idf_rad . groupby () . agg ( F . sum ( \"x\" ) . alias ( \"x_group\" ), F . sum ( \"y\" ) . alias ( \"y_group\" ), F . sum ( \"z\" ) . alias ( \"z_group\" ), ) odf = ( idf_groupby . withColumn ( \"hyp\" , F . sqrt ( F . col ( \"x_group\" ) * F . col ( \"x_group\" ) + F . col ( \"y_group\" ) * F . col ( \"y_group\" ) ), ) . withColumn ( lat_col + \"_centroid\" , F . atan2 ( F . col ( \"z_group\" ), F . col ( \"hyp\" )) * 180 / pi , ) . withColumn ( long_col + \"_centroid\" , F . atan2 ( F . col ( \"y_group\" ), F . col ( \"x_group\" )) * 180 / pi , ) . select ( lat_col + \"_centroid\" , long_col + \"_centroid\" ) ) return odf def geo_format_cartesian ( idf, list_of_x, list_of_y, list_of_z, output_format, result_prefix=[], optional_configs={'geohash_precision': 8, 'radius': 6371009}, output_mode='append') This function helps to convert the input data's location columns from cartesian format to desired format based on output_format. If output_mode is set to True, the original location columns will be dropped. For each location column, \"to_latlon_decimal_degrees\" will be called to convert them to decimal degrees, and then \"from_latlon_decimal_degrees\" will be called to transform decimal degrees to output_format If output_format is \"dd\" or \"dms\" or \"radian\", 2 new columns containing \" lat \", \" long \" in the column names will be created. If output_format is \"geohash\", 1 new column containing \"_geohash\" in the column name will be created. Parameters idf Input Dataframe. list_of_x List of columns representing x axis values e.g., [\"x1\",\"x2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"x1|x2\". list_of_y List of columns representing y axis values e.g., [\"y1\",\"y2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"y1|y2\". list_of_z List of columns representing z axis values e.g., [\"z1\",\"z2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"z1|z2\". list_of_x, list_of_y and list_of_z must have the same length such that the i-th element of 3 lists form an x-y-z pair to format. output_format \"dd\", \"dms\", \"radian\", \"geohash\" \"dd\" represents latitude and longitude in decimal degrees. \"dms\" represents latitude and longitude in degrees minutes second. \"radian\" represents latitude and longitude in radians. \"geohash\" represents geocoded locations. result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_x, list_of_y and list_of_z. If it is empty, will be used for each x-y-z pair. For example, list_of_x is \"x1|x2\", list_of_y is \"y1|y2\" and list_of_z is \"z1|z2\" Case 1: result_prefix = \"L1|L2\". If output_format is \"dd\", \"dms\" or \"radian\", new columns will be named as L1_lat_ , L1_lon_ , L2_lat_ , L2_lon_ . If output_format is \"geohash\", new columns will be named as L1_geohash and L2_geohash. Case 2: result_prefix = []. Prefixes \"L1\" and \"L2\" in above column names will be replaced by \"x1_y1_z1\" and \"x2_y2_z2\". (Default value = []) optional_configs The following keys can be used: - geohash_precision: precision of the resultant geohash. This key is only used when output_format is \"geohash\". (Default value = 8) - radius: radius of Earth. (Default value = EARTH_RADIUS) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns DataFrame Expand source code def geo_format_cartesian ( idf , list_of_x , list_of_y , list_of_z , output_format , result_prefix = [], optional_configs = { \"geohash_precision\" : 8 , \"radius\" : EARTH_RADIUS }, output_mode = \"append\" , ): \"\"\" This function helps to convert the input data's location columns from cartesian format to desired format based on output_format. If output_mode is set to True, the original location columns will be dropped. For each location column, \"to_latlon_decimal_degrees\" will be called to convert them to decimal degrees, and then \"from_latlon_decimal_degrees\" will be called to transform decimal degrees to output_format If output_format is \"dd\" or \"dms\" or \"radian\", 2 new columns containing \"_lat_\", \"_long_\" in the column names will be created. If output_format is \"geohash\", 1 new column containing \"_geohash\" in the column name will be created. Parameters ---------- idf Input Dataframe. list_of_x List of columns representing x axis values e.g., [\"x1\",\"x2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"x1|x2\". list_of_y List of columns representing y axis values e.g., [\"y1\",\"y2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"y1|y2\". list_of_z List of columns representing z axis values e.g., [\"z1\",\"z2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"z1|z2\". list_of_x, list_of_y and list_of_z must have the same length such that the i-th element of 3 lists form an x-y-z pair to format. output_format \"dd\", \"dms\", \"radian\", \"geohash\" \"dd\" represents latitude and longitude in decimal degrees. \"dms\" represents latitude and longitude in degrees minutes second. \"radian\" represents latitude and longitude in radians. \"geohash\" represents geocoded locations. result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_x, list_of_y and list_of_z. If it is empty, <x>_<y>_<z> will be used for each x-y-z pair. For example, list_of_x is \"x1|x2\", list_of_y is \"y1|y2\" and list_of_z is \"z1|z2\" Case 1: result_prefix = \"L1|L2\". If output_format is \"dd\", \"dms\" or \"radian\", new columns will be named as L1_lat_<output_format>, L1_lon_<output_format>, L2_lat_<output_format>, L2_lon_<output_format>. If output_format is \"geohash\", new columns will be named as L1_geohash and L2_geohash. Case 2: result_prefix = []. Prefixes \"L1\" and \"L2\" in above column names will be replaced by \"x1_y1_z1\" and \"x2_y2_z2\". (Default value = []) optional_configs The following keys can be used: - geohash_precision: precision of the resultant geohash. This key is only used when output_format is \"geohash\". (Default value = 8) - radius: radius of Earth. (Default value = EARTH_RADIUS) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" geohash_precision = int ( optional_configs . get ( \"geohash_precision\" , 8 )) radius = optional_configs . get ( \"radius\" , EARTH_RADIUS ) if isinstance ( list_of_x , str ): list_of_x = [ x . strip () for x in list_of_x . split ( \"|\" )] if isinstance ( list_of_y , str ): list_of_y = [ x . strip () for x in list_of_y . split ( \"|\" )] if isinstance ( list_of_z , str ): list_of_z = [ x . strip () for x in list_of_z . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_x + list_of_y + list_of_z ): raise TypeError ( \"Invalid input for list_of_x or list_of_y or list_of_z\" ) format_list = [ \"dd\" , \"dms\" , \"radian\" , \"geohash\" ] if output_format not in format_list : raise TypeError ( \"Invalid input for output_format\" ) if len ({ len ( list_of_x ), len ( list_of_y ), len ( list_of_z )}) != 1 : raise TypeError ( \"list_of_x, list_of_y and list_of_z must have the same length\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_x )): raise TypeError ( \"result_prefix must have the same length as list_of_x, list_of_y and list_of_y if it is not empty\" ) f_to_latlon_dd = F . udf ( lambda loc : to_latlon_decimal_degrees ( loc , \"cartesian\" , radius ), T . ArrayType ( T . FloatType ()), ) from_latlon_dd_ = lambda loc : from_latlon_decimal_degrees ( loc , output_format , radius , geohash_precision ) if output_format in [ \"dd\" , \"radian\" ]: f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . FloatType ())) elif output_format == \"dms\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . ArrayType ( T . FloatType ())) ) elif output_format == \"geohash\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . StringType ()) odf = idf for i , ( x , y , z ) in enumerate ( zip ( list_of_x , list_of_y , list_of_z )): col = result_prefix [ i ] if result_prefix else ( x + \"_\" + y + \"_\" + z ) odf = odf . withColumn ( col + \"_temp\" , F . array ( x , y , z )) . withColumn ( col + \"_\" + output_format , f_from_latlon_dd ( f_to_latlon_dd ( col + \"_temp\" )) ) if output_format in [ \"dd\" , \"dms\" , \"radian\" ]: odf = ( odf . withColumn ( col + \"_lat_\" + output_format , F . col ( col + \"_\" + output_format )[ 0 ] ) . withColumn ( col + \"_lon_\" + output_format , F . col ( col + \"_\" + output_format )[ 1 ] ) . drop ( col + \"_\" + output_format ) ) odf = odf . drop ( col + \"_temp\" ) if output_mode == \"replace\" : odf = odf . drop ( x , y , z ) return odf def geo_format_geohash ( idf, list_of_geohash, output_format, result_prefix=[], optional_configs={'radius': 6371009}, output_mode='append') This function is the main function to convert the input data's location columns from geohash format to desired format based on output_format. If output_mode is set to True, the original location columns will be dropped. For each location column, \"to_latlon_decimal_degrees\" will be called to convert them to decimal degrees, and then \"from_latlon_decimal_degrees\" will be called to transform decimal degrees to output_format If output_format is \"dd\" or \"dms\" or \"radian\", 2 new columns containing \" lat \", \" long \" in the column names will be created. If output_format is \"cartesian\", 3 new columns containing \" x \", \" y \", \" z \" in the column names will be created. Parameters idf Input Dataframe. list_of_geohash List of columns representing geohash e.g., [\"gh1\",\"gh2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"gh1|gh2\". output_format \"dd\", \"dms\", \"radian\", \"cartesian\" \"dd\" represents latitude and longitude in decimal degrees. \"dms\" represents latitude and longitude in degrees minutes second. \"radian\" represents latitude and longitude in radians. \"cartesian\" represents the Cartesian coordinates of the point in three-dimensional space. result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_geohash. If it is empty, will be used as the prefix for transformed columns. For example, list_of_geohash is \"gh1|gh2\". Case 1: result_prefix = \"L1|L2\". If output_format is \"dd\", \"dms\" or \"radian\", new columns will be named as L1_lat_ , L1_lon_ , L2_lat_ , L2_lon_ . If output_format is \"cartesian\", new columns will be named as L1_x, L1_y, L1_z, L2_x, L2_y, L2_z. Case 2: result_prefix = []. Prefixes \"L1\" and \"L2\" in above column names will be replaced by \"gh1\" and \"gh2\". (Default value = []) optional_configs The following key can be used: - radius: radius of Earth. Necessary only when output_format is \"cartesian\". (Default value = EARTH_RADIUS) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns DataFrame Expand source code def geo_format_geohash ( idf , list_of_geohash , output_format , result_prefix = [], optional_configs = { \"radius\" : EARTH_RADIUS }, output_mode = \"append\" , ): \"\"\" This function is the main function to convert the input data's location columns from geohash format to desired format based on output_format. If output_mode is set to True, the original location columns will be dropped. For each location column, \"to_latlon_decimal_degrees\" will be called to convert them to decimal degrees, and then \"from_latlon_decimal_degrees\" will be called to transform decimal degrees to output_format If output_format is \"dd\" or \"dms\" or \"radian\", 2 new columns containing \"_lat_\", \"_long_\" in the column names will be created. If output_format is \"cartesian\", 3 new columns containing \"_x_\", \"_y_\", \"_z_\" in the column names will be created. Parameters ---------- idf Input Dataframe. list_of_geohash List of columns representing geohash e.g., [\"gh1\",\"gh2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"gh1|gh2\". output_format \"dd\", \"dms\", \"radian\", \"cartesian\" \"dd\" represents latitude and longitude in decimal degrees. \"dms\" represents latitude and longitude in degrees minutes second. \"radian\" represents latitude and longitude in radians. \"cartesian\" represents the Cartesian coordinates of the point in three-dimensional space. result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_geohash. If it is empty, <geohash column name> will be used as the prefix for transformed columns. For example, list_of_geohash is \"gh1|gh2\". Case 1: result_prefix = \"L1|L2\". If output_format is \"dd\", \"dms\" or \"radian\", new columns will be named as L1_lat_<output_format>, L1_lon_<output_format>, L2_lat_<output_format>, L2_lon_<output_format>. If output_format is \"cartesian\", new columns will be named as L1_x, L1_y, L1_z, L2_x, L2_y, L2_z. Case 2: result_prefix = []. Prefixes \"L1\" and \"L2\" in above column names will be replaced by \"gh1\" and \"gh2\". (Default value = []) optional_configs The following key can be used: - radius: radius of Earth. Necessary only when output_format is \"cartesian\". (Default value = EARTH_RADIUS) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" radius = optional_configs . get ( \"radius\" , EARTH_RADIUS ) if isinstance ( list_of_geohash , str ): list_of_geohash = [ x . strip () for x in list_of_geohash . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_geohash ): raise TypeError ( \"Invalid input for list_of_geohash\" ) format_list = [ \"dd\" , \"dms\" , \"radian\" , \"cartesian\" ] if output_format not in format_list : raise TypeError ( \"Invalid input for output_format\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_geohash )): raise TypeError ( \"result_prefix must have the same length as list_of_geohash if it is not empty\" ) f_to_latlon_dd = F . udf ( lambda loc : to_latlon_decimal_degrees ( loc , \"geohash\" , radius ), T . ArrayType ( T . FloatType ()), ) from_latlon_dd_ = lambda loc : from_latlon_decimal_degrees ( loc , output_format , radius ) if output_format in [ \"dd\" , \"radian\" , \"cartesian\" ]: f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . FloatType ())) elif output_format == \"dms\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . ArrayType ( T . FloatType ())) ) odf = idf for i , geohash in enumerate ( list_of_geohash ): col = result_prefix [ i ] if result_prefix else geohash odf = odf . withColumn ( col + \"_\" + output_format , f_from_latlon_dd ( f_to_latlon_dd ( geohash )) ) if output_format in [ \"dd\" , \"dms\" , \"radian\" ]: odf = ( odf . withColumn ( col + \"_lat_\" + output_format , F . col ( col + \"_\" + output_format )[ 0 ] ) . withColumn ( col + \"_lon_\" + output_format , F . col ( col + \"_\" + output_format )[ 1 ] ) . drop ( col + \"_\" + output_format ) ) if output_format == \"cartesian\" : odf = ( odf . withColumn ( col + \"_x\" , F . col ( col + \"_\" + output_format )[ 0 ]) . withColumn ( col + \"_y\" , F . col ( col + \"_\" + output_format )[ 1 ]) . withColumn ( col + \"_z\" , F . col ( col + \"_\" + output_format )[ 2 ]) . drop ( col + \"_\" + output_format ) ) if output_mode == \"replace\" : odf = odf . drop ( geohash ) return odf def geo_format_latlon ( idf, list_of_lat, list_of_lon, input_format, output_format, result_prefix=[], optional_configs={'geohash_precision': 8, 'radius': 6371009}, output_mode='append') This function is the main function to convert the input data's location columns from lat,lon format to desired format based on output_format. If output_mode is set to True, the original location columns will be dropped. For each location column, \"to_latlon_decimal_degrees\" will be called to convert them to decimal degrees, and then \"from_latlon_decimal_degrees\" will be called to transform decimal degrees to output_format If output_format is \"dd\" or \"dms\" or \"radian\", 2 new columns containing \" lat \", \" long \" in the column names will be created. If output_format is \"cartesian\", 3 new columns containing \" x \", \" y \", \" z \" in the column names will be created. If output_format is \"geohash\", 1 new column containing \"_geohash\" in the column name will be created. Parameters idf Input Dataframe. list_of_lat List of columns representing latitude e.g., [\"lat1\",\"lat2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lat2\". list_of_lon List of columns representing longitude e.g., [\"lon1\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lon1|lon2\". list_of_lon must have the same length as list_of_lat such that i-th element of list_of_lat and i-th element of list_of_lon form a lat-lon pair to format. input_format \"dd\", \"dms\", \"radian\". \"dd\" represents latitude and longitude in decimal degrees. \"dms\" represents latitude and longitude in degrees minutes second. \"radian\" represents latitude and longitude in radians. output_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". \"cartesian\" represents the Cartesian coordinates of the point in three-dimensional space. \"geohash\" represents geocoded locations. result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_lat and list_of_lon. If it is empty, will be used for each lat-lon pair. For example, list_of_lat is \"lat1|lat2\", list_of_lon is \"L1|L2\". Case 1: result_prefix = \"L1|L2\". If output_format is \"dd\", \"dms\" or \"radian\", new columns will be named as L1_lat , L1_lon_ , L2_lat_ , L2_lon_ . If output_format is \"cartesian\", new columns will be named as L1_x, L1_y, L1_z, L2_x, L2_y, L2_z. If output_format is \"geohash\", new columns will be named as L1_geohash and L2_geohash. Case 2: result_prefix = []. Prefixes \"L1\" and \"L2\" in above column names will be replaced by \"lat1_lon1\" and \"lat2_lon2\". (Default value = []) optional_configs The following keys can be used: - geohash_precision: precision of the resultant geohash. This key is only used when output_format is \"geohash\". (Default value = 8) - radius: radius of Earth. Necessary only when output_format is \"cartesian\". (Default value = EARTH_RADIUS) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns DataFrame Expand source code def geo_format_latlon ( idf , list_of_lat , list_of_lon , input_format , output_format , result_prefix = [], optional_configs = { \"geohash_precision\" : 8 , \"radius\" : EARTH_RADIUS }, output_mode = \"append\" , ): \"\"\" This function is the main function to convert the input data's location columns from lat,lon format to desired format based on output_format. If output_mode is set to True, the original location columns will be dropped. For each location column, \"to_latlon_decimal_degrees\" will be called to convert them to decimal degrees, and then \"from_latlon_decimal_degrees\" will be called to transform decimal degrees to output_format If output_format is \"dd\" or \"dms\" or \"radian\", 2 new columns containing \"_lat_\", \"_long_\" in the column names will be created. If output_format is \"cartesian\", 3 new columns containing \"_x_\", \"_y_\", \"_z_\" in the column names will be created. If output_format is \"geohash\", 1 new column containing \"_geohash\" in the column name will be created. Parameters ---------- idf Input Dataframe. list_of_lat List of columns representing latitude e.g., [\"lat1\",\"lat2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lat2\". list_of_lon List of columns representing longitude e.g., [\"lon1\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lon1|lon2\". list_of_lon must have the same length as list_of_lat such that i-th element of list_of_lat and i-th element of list_of_lon form a lat-lon pair to format. input_format \"dd\", \"dms\", \"radian\". \"dd\" represents latitude and longitude in decimal degrees. \"dms\" represents latitude and longitude in degrees minutes second. \"radian\" represents latitude and longitude in radians. output_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". \"cartesian\" represents the Cartesian coordinates of the point in three-dimensional space. \"geohash\" represents geocoded locations. result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_lat and list_of_lon. If it is empty, <lat>_<lon> will be used for each lat-lon pair. For example, list_of_lat is \"lat1|lat2\", list_of_lon is \"L1|L2\". Case 1: result_prefix = \"L1|L2\". If output_format is \"dd\", \"dms\" or \"radian\", new columns will be named as L1_lat_<output_format>, L1_lon_<output_format>, L2_lat_<output_format>, L2_lon_<output_format>. If output_format is \"cartesian\", new columns will be named as L1_x, L1_y, L1_z, L2_x, L2_y, L2_z. If output_format is \"geohash\", new columns will be named as L1_geohash and L2_geohash. Case 2: result_prefix = []. Prefixes \"L1\" and \"L2\" in above column names will be replaced by \"lat1_lon1\" and \"lat2_lon2\". (Default value = []) optional_configs The following keys can be used: - geohash_precision: precision of the resultant geohash. This key is only used when output_format is \"geohash\". (Default value = 8) - radius: radius of Earth. Necessary only when output_format is \"cartesian\". (Default value = EARTH_RADIUS) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" geohash_precision = int ( optional_configs . get ( \"geohash_precision\" , 8 )) radius = optional_configs . get ( \"radius\" , EARTH_RADIUS ) if isinstance ( list_of_lat , str ): list_of_lat = [ x . strip () for x in list_of_lat . split ( \"|\" )] if isinstance ( list_of_lon , str ): list_of_lon = [ x . strip () for x in list_of_lon . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_lat + list_of_lon ): raise TypeError ( \"Invalid input for list_of_lat or list_of_lon\" ) format_list = [ \"dd\" , \"dms\" , \"radian\" , \"cartesian\" , \"geohash\" ] if ( input_format not in format_list [: 3 ]) or ( output_format not in format_list ): raise TypeError ( \"Invalid input for input_format or output_format\" ) if len ( list_of_lat ) != len ( list_of_lon ): raise TypeError ( \"list_of_lat and list_of_lon must have the same length\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_lat )): raise TypeError ( \"result_prefix must have the same length as list_of_lat and list_of_lon if it is not empty\" ) f_to_latlon_dd = F . udf ( lambda loc : to_latlon_decimal_degrees ( loc , input_format , radius ), T . ArrayType ( T . FloatType ()), ) from_latlon_dd_ = lambda loc : from_latlon_decimal_degrees ( loc , output_format , radius , geohash_precision ) if output_format in [ \"dd\" , \"radian\" , \"cartesian\" ]: f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . FloatType ())) elif output_format == \"dms\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . ArrayType ( T . FloatType ())) ) elif output_format == \"geohash\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . StringType ()) odf = idf for i , ( lat , lon ) in enumerate ( zip ( list_of_lat , list_of_lon )): col = result_prefix [ i ] if result_prefix else ( lat + \"_\" + lon ) odf = odf . withColumn ( col + \"_temp\" , F . array ( lat , lon )) . withColumn ( col + \"_\" + output_format , f_from_latlon_dd ( f_to_latlon_dd ( col + \"_temp\" )) ) if output_format in [ \"dd\" , \"dms\" , \"radian\" ]: odf = ( odf . withColumn ( col + \"_lat_\" + output_format , F . col ( col + \"_\" + output_format )[ 0 ] ) . withColumn ( col + \"_lon_\" + output_format , F . col ( col + \"_\" + output_format )[ 1 ] ) . drop ( col + \"_\" + output_format ) ) if output_format == \"cartesian\" : odf = ( odf . withColumn ( col + \"_x\" , F . col ( col + \"_\" + output_format )[ 0 ]) . withColumn ( col + \"_y\" , F . col ( col + \"_\" + output_format )[ 1 ]) . withColumn ( col + \"_z\" , F . col ( col + \"_\" + output_format )[ 2 ]) . drop ( col + \"_\" + output_format ) ) odf = odf . drop ( col + \"_temp\" ) if output_mode == \"replace\" : odf = odf . drop ( lat , lon ) return odf def geohash_precision_control ( idf, list_of_geohash, output_precision=8, km_max_error=None, output_mode='append') This function controls the precision of input data's geohash columns. Parameters idf Input Dataframe. list_of_geohash List of columns in geohash format e.g., [\"gh1\",\"gh2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"gh1|gh2\". output_precision Precision of the transformed geohash in the output dataframe. (Default value = 8) km_max_error Maximum permissible error in kilometers. If km_max_error is specified, output_precision will be ignored and km_max_error will be mapped to an output_precision according to the following dictionary: {2500: 1, 630: 2, 78: 3, 20: 4, 2.4: 5, 0.61: 6, 0.076: 7, 0.019: 8, 0.0024: 9, 0.00060: 10, 0.000074: 11}. (Default value = None) output_mode \"replace\", \"append\". \"replace\" option replaces original columns with transformed column. \"append\" option appends the transformed column to the input dataset with postfix \" precision \". (Default value = \"append\") Returns DataFrame Expand source code def geohash_precision_control ( idf , list_of_geohash , output_precision = 8 , km_max_error = None , output_mode = \"append\" ): \"\"\" This function controls the precision of input data's geohash columns. Parameters ---------- idf Input Dataframe. list_of_geohash List of columns in geohash format e.g., [\"gh1\",\"gh2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"gh1|gh2\". output_precision Precision of the transformed geohash in the output dataframe. (Default value = 8) km_max_error Maximum permissible error in kilometers. If km_max_error is specified, output_precision will be ignored and km_max_error will be mapped to an output_precision according to the following dictionary: {2500: 1, 630: 2, 78: 3, 20: 4, 2.4: 5, 0.61: 6, 0.076: 7, 0.019: 8, 0.0024: 9, 0.00060: 10, 0.000074: 11}. (Default value = None) output_mode \"replace\", \"append\". \"replace\" option replaces original columns with transformed column. \"append\" option appends the transformed column to the input dataset with postfix \"_precision_<output_precision>\". (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_geohash , str ): list_of_geohash = [ x . strip () for x in list_of_geohash . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_geohash ): raise TypeError ( \"Invalid input for list_of_geohash\" ) error_precision_mapping = { 2500 : 1 , 630 : 2 , 78 : 3 , 20 : 4 , 2.4 : 5 , 0.61 : 6 , 0.076 : 7 , 0.019 : 8 , 0.0024 : 9 , 0.00060 : 10 , 0.000074 : 11 , } if km_max_error is not None : output_precision = 12 for key , val in error_precision_mapping . items (): if km_max_error >= key : output_precision = val break output_precision = int ( output_precision ) logger . info ( \"Precision of the output geohashes will be capped at \" + str ( output_precision ) + \".\" ) odf = idf for i , geohash in enumerate ( list_of_geohash ): if output_mode == \"replace\" : col_name = geohash else : col_name = geohash + \"_precision_\" + str ( output_precision ) odf = odf . withColumn ( col_name , F . substring ( geohash , 1 , output_precision )) return odf def location_distance ( idf, list_of_cols_loc1, list_of_cols_loc2, loc_format='dd', result_prefix='', distance_type='haversine', unit='m', optional_configs={'radius': 6371009, 'vincenty_model': 'WGS-84'}, output_mode='append') This function calculates the distance between 2 locations, and the distance formula is determined by distance_type. If distance_type = \"vincenty\", thed loc_format should be \"dd\", and list_of_cols_loc1, list_of_cols_loc2 should be in [lat1, lon1] and [lat2, lon2] format respectively. \"vincenty_distance\" function will be called to calculate the distance. If distance_type = \"haversine\", then loc_format should be \"radian\", and list_of_cols_loc1, list_of_cols_loc2 should be in [lat1, lon1] and [lat2, lon2] format respectively. \"haversine_distance\" function will be called to calculate the distance. If distance_type = \"euclidean\", then loc_format should be \"cartesian\", and list_of_cols_loc1, list_of_cols_loc2 should be in [x1, y1, z1] and [x2, y2, z2] format respectively. \"euclidean_distance\" function will be called to calculate the distance. If loc_format does not match with distance_type's desired format, necessary conversion of location columns will be performed with the help of \"geo_format_latlon\", \"geo_format_cartesian\" and \"geo_format_geohash\" functions. Parameters idf Input Dataframe. list_of_cols_loc1 List of columns to express the first location e.g., [\"lat1\",\"lon1\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lon1\". list_of_cols_loc2 List of columns to express the second location e.g., [\"lat2\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat2|lon2\". loc_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". (Default value = \"dd\") result_prefix Prefix for the newly generated column. It must be a string or a list with one element. If it is empty, _ will be used as the prefix. For example, list_of_cols_loc1 is \"lat1|lon1\", list_of_cols_loc2 is \"lat2|lon2\". Case 1: result_prefix = \"L1_L2\": the new column will be named as L1_L2_distance. Case 2: result_prefix = []: the new column will be named as lat1_lon1_lat2_lon2_distance. (Default value = '') distance_type \"vincenty\", \"haversine\", \"euclidean\". (Default value = \"haversine\") \"vincenty\" option calculates the distance between two points on the surface of a spheroid. \"haversine\" option calculates the great-circle distance between two points on a sphere. \"euclidean\" option calculates the length of the line segment between two points. unit \"m\", \"km\". Unit of the result. (Default value = \"m\") optional_configs The following keys can be used: - radius: radius of Earth. Necessary only when output_format is \"cartesian\". (Default value = EARTH_RADIUS) - vincenty_model: The ellipsoidal model to use. Supported values: \"WGS-84\", \"GRS-80\", \"Airy (1830)\", \"Intl 1924\", \"Clarke (1880)\", \"GRS-67\". For more information, please refer to geopy.distance.ELLIPSOIDS. (Default value = \"WGS-84\") output_mode \"replace\", \"append\". \"replace\" option replaces original columns with transformed column. \"append\" option appends the transformed column to the input dataset with name \" _ _distance\". (Default value = \"append\") Returns DataFrame Expand source code def location_distance ( idf , list_of_cols_loc1 , list_of_cols_loc2 , loc_format = \"dd\" , result_prefix = \"\" , distance_type = \"haversine\" , unit = \"m\" , optional_configs = { \"radius\" : EARTH_RADIUS , \"vincenty_model\" : \"WGS-84\" }, output_mode = \"append\" , ): \"\"\" This function calculates the distance between 2 locations, and the distance formula is determined by distance_type. If distance_type = \"vincenty\", thed loc_format should be \"dd\", and list_of_cols_loc1, list_of_cols_loc2 should be in [lat1, lon1] and [lat2, lon2] format respectively. \"vincenty_distance\" function will be called to calculate the distance. If distance_type = \"haversine\", then loc_format should be \"radian\", and list_of_cols_loc1, list_of_cols_loc2 should be in [lat1, lon1] and [lat2, lon2] format respectively. \"haversine_distance\" function will be called to calculate the distance. If distance_type = \"euclidean\", then loc_format should be \"cartesian\", and list_of_cols_loc1, list_of_cols_loc2 should be in [x1, y1, z1] and [x2, y2, z2] format respectively. \"euclidean_distance\" function will be called to calculate the distance. If loc_format does not match with distance_type's desired format, necessary conversion of location columns will be performed with the help of \"geo_format_latlon\", \"geo_format_cartesian\" and \"geo_format_geohash\" functions. Parameters ---------- idf Input Dataframe. list_of_cols_loc1 List of columns to express the first location e.g., [\"lat1\",\"lon1\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lon1\". list_of_cols_loc2 List of columns to express the second location e.g., [\"lat2\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat2|lon2\". loc_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". (Default value = \"dd\") result_prefix Prefix for the newly generated column. It must be a string or a list with one element. If it is empty, <list_of_cols_loc1 joined by '_'>_<list_of_cols_loc2 joined by '_'> will be used as the prefix. For example, list_of_cols_loc1 is \"lat1|lon1\", list_of_cols_loc2 is \"lat2|lon2\". Case 1: result_prefix = \"L1_L2\": the new column will be named as L1_L2_distance. Case 2: result_prefix = []: the new column will be named as lat1_lon1_lat2_lon2_distance. (Default value = '') distance_type \"vincenty\", \"haversine\", \"euclidean\". (Default value = \"haversine\") \"vincenty\" option calculates the distance between two points on the surface of a spheroid. \"haversine\" option calculates the great-circle distance between two points on a sphere. \"euclidean\" option calculates the length of the line segment between two points. unit \"m\", \"km\". Unit of the result. (Default value = \"m\") optional_configs The following keys can be used: - radius: radius of Earth. Necessary only when output_format is \"cartesian\". (Default value = EARTH_RADIUS) - vincenty_model: The ellipsoidal model to use. Supported values: \"WGS-84\", \"GRS-80\", \"Airy (1830)\", \"Intl 1924\", \"Clarke (1880)\", \"GRS-67\". For more information, please refer to geopy.distance.ELLIPSOIDS. (Default value = \"WGS-84\") output_mode \"replace\", \"append\". \"replace\" option replaces original columns with transformed column. \"append\" option appends the transformed column to the input dataset with name \"<loc1>_<loc2>_distance\". (Default value = \"append\") Returns ------- DataFrame \"\"\" radius = optional_configs . get ( \"radius\" , EARTH_RADIUS ) vincenty_model = optional_configs . get ( \"vincenty_model\" , \"WGS-84\" ) if isinstance ( list_of_cols_loc1 , str ): list_of_cols_loc1 = [ x . strip () for x in list_of_cols_loc1 . split ( \"|\" )] if isinstance ( list_of_cols_loc2 , str ): list_of_cols_loc2 = [ x . strip () for x in list_of_cols_loc2 . split ( \"|\" )] if isinstance ( result_prefix , list ): if len ( result_prefix ) > 1 : raise TypeError ( \"If result_prefix is a list, it can contain maximally 1 element\" ) elif len ( result_prefix ) == 1 : result_prefix = result_prefix [ 0 ] if any ( i not in idf . columns for i in list_of_cols_loc1 + list_of_cols_loc2 ): raise TypeError ( \"Invalid input for list_of_cols_loc1 or list_of_cols_loc2\" ) if distance_type not in [ \"vincenty\" , \"haversine\" , \"euclidean\" ]: raise TypeError ( \"Invalid input for distance_type\" ) if loc_format not in [ \"dd\" , \"dms\" , \"radian\" , \"cartesian\" , \"geohash\" ]: raise TypeError ( \"Invalid input for loc_format\" ) format_mapping = { \"vincenty\" : \"dd\" , \"haversine\" : \"radian\" , \"euclidean\" : \"cartesian\" } format_required = format_mapping [ distance_type ] if loc_format != format_required : if loc_format in [ \"dd\" , \"dms\" , \"radian\" ]: idf = geo_format_latlon ( idf , list_of_lat = [ list_of_cols_loc1 [ 0 ], list_of_cols_loc2 [ 0 ]], list_of_lon = [ list_of_cols_loc1 [ 1 ], list_of_cols_loc2 [ 1 ]], input_format = loc_format , output_format = format_required , result_prefix = [ \"temp_loc1\" , \"temp_loc2\" ], optional_configs = { \"radius\" : radius }, output_mode = \"append\" , ) elif loc_format == \"cartesian\" : idf = geo_format_cartesian ( idf , list_of_x = [ list_of_cols_loc1 [ 0 ], list_of_cols_loc2 [ 0 ]], list_of_y = [ list_of_cols_loc1 [ 1 ], list_of_cols_loc2 [ 1 ]], list_of_z = [ list_of_cols_loc1 [ 2 ], list_of_cols_loc2 [ 2 ]], output_format = format_required , result_prefix = [ \"temp_loc1\" , \"temp_loc2\" ], optional_configs = { \"radius\" : radius }, output_mode = \"append\" , ) elif loc_format == \"geohash\" : idf = geo_format_geohash ( idf , list_of_geohash = [ list_of_cols_loc1 [ 0 ], list_of_cols_loc2 [ 0 ]], output_format = format_required , result_prefix = [ \"temp_loc1\" , \"temp_loc2\" ], optional_configs = { \"radius\" : radius }, output_mode = \"append\" , ) if format_required == \"dd\" : loc1 , loc2 = [ \"temp_loc1_lat_dd\" , \"temp_loc1_lon_dd\" ], [ \"temp_loc2_lat_dd\" , \"temp_loc2_lon_dd\" , ] elif format_required == \"radian\" : loc1 , loc2 = [ \"temp_loc1_lat_radian\" , \"temp_loc1_lon_radian\" ], [ \"temp_loc2_lat_radian\" , \"temp_loc2_lon_radian\" , ] elif format_required == \"cartesian\" : loc1 , loc2 = [ \"temp_loc1_x\" , \"temp_loc1_y\" , \"temp_loc1_z\" ], [ \"temp_loc2_x\" , \"temp_loc2_y\" , \"temp_loc2_z\" , ] idf = ( idf . withColumn ( \"temp_loc1\" , F . array ( * loc1 )) . withColumn ( \"temp_loc2\" , F . array ( * loc2 )) . drop ( * ( loc1 + loc2 )) ) else : idf = idf . withColumn ( \"temp_loc1\" , F . array ( * list_of_cols_loc1 )) . withColumn ( \"temp_loc2\" , F . array ( * list_of_cols_loc2 ) ) if distance_type == \"vincenty\" : compute_distance = lambda x1 , x2 : vincenty_distance ( x1 , x2 , unit , vincenty_model ) elif distance_type == \"haversine\" : compute_distance = lambda x1 , x2 : haversine_distance ( x1 , x2 , \"radian\" , unit , radius ) else : compute_distance = lambda x1 , x2 : euclidean_distance ( x1 , x2 , unit ) f_compute_distance = F . udf ( compute_distance , T . FloatType ()) col_prefix = ( result_prefix if result_prefix else \"_\" . join ( list_of_cols_loc1 ) + \"_\" + \"_\" . join ( list_of_cols_loc2 ) ) odf = idf . withColumn ( col_prefix + \"_distance\" , f_compute_distance ( \"temp_loc1\" , \"temp_loc2\" ) ) . drop ( \"temp_loc1\" , \"temp_loc2\" ) if output_mode == \"replace\" : odf = odf . drop ( * ( list_of_cols_loc1 + list_of_cols_loc2 )) return odf def location_in_country ( spark, idf, list_of_lat, list_of_lon, country, country_shapefile_path='', method_type='approx', result_prefix=[], output_mode='append') This function checks whether each lat-lon pair is insided a country. Two ways of checking are supported: \"approx\" (using the bounding box of a country) and \"exact\" (using the shapefile of a country). Parameters spark Spark Session idf Input Dataframe. list_of_lat List of columns representing latitude e.g., [\"lat1\",\"lat2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lat2\". list_of_lon List of columns representing longitude e.g., [\"lon1\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lon1|lon2\". list_of_lon must have the same length as list_of_lat such that i-th element of list_of_lat and i-th element of list_of_lon form a lat-lon pair to format. country The Alpha-2 country code. country_shapefile_path The geojson file with a FeatureCollection object containing polygons for each country. One example file country_polygons.geojson can be downloaded from Anovos GitHub repository: https://github.com/anovos/anovos/tree/main/data/ method_type \"approx\", \"exact\". \"approx\" uses the bounding box of a country to estimate whether a location is inside the country \"exact\" uses the shapefile of a country to calculate whether a location is inside the country result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_lat and list_of_lon. If it is empty, _ will be used for each lat-lon pair. For example, list_of_lat is \"lat1|lat2\", list_of_lon is \"lon1|lon2\". Case 1: result_prefix = \"L1|L2\", country=\"US\" New columns will be named as L1_in_US and L2_in_US. Calse 2: result_prefix = [], country=\"US\" New columns will be named as lat1_lon1_in_US and lat2_lon2_in_US. (Default value = []) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns DataFrame Expand source code def location_in_country ( spark , idf , list_of_lat , list_of_lon , country , country_shapefile_path = \"\" , method_type = \"approx\" , result_prefix = [], output_mode = \"append\" , ): \"\"\" This function checks whether each lat-lon pair is insided a country. Two ways of checking are supported: \"approx\" (using the bounding box of a country) and \"exact\" (using the shapefile of a country). Parameters ---------- spark Spark Session idf Input Dataframe. list_of_lat List of columns representing latitude e.g., [\"lat1\",\"lat2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lat2\". list_of_lon List of columns representing longitude e.g., [\"lon1\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lon1|lon2\". list_of_lon must have the same length as list_of_lat such that i-th element of list_of_lat and i-th element of list_of_lon form a lat-lon pair to format. country The Alpha-2 country code. country_shapefile_path The geojson file with a FeatureCollection object containing polygons for each country. One example file country_polygons.geojson can be downloaded from Anovos GitHub repository: https://github.com/anovos/anovos/tree/main/data/ method_type \"approx\", \"exact\". \"approx\" uses the bounding box of a country to estimate whether a location is inside the country \"exact\" uses the shapefile of a country to calculate whether a location is inside the country result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_lat and list_of_lon. If it is empty, <lat>_<lon> will be used for each lat-lon pair. For example, list_of_lat is \"lat1|lat2\", list_of_lon is \"lon1|lon2\". Case 1: result_prefix = \"L1|L2\", country=\"US\" New columns will be named as L1_in_US and L2_in_US. Calse 2: result_prefix = [], country=\"US\" New columns will be named as lat1_lon1_in_US and lat2_lon2_in_US. (Default value = []) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_lat , str ): list_of_lat = [ x . strip () for x in list_of_lat . split ( \"|\" )] if isinstance ( list_of_lon , str ): list_of_lon = [ x . strip () for x in list_of_lon . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_lat + list_of_lon ): raise TypeError ( \"Invalid input for list_of_lat or list_of_lon\" ) if len ( list_of_lat ) != len ( list_of_lon ): raise TypeError ( \"list_of_lat and list_of_lon must have the same length\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_lat )): raise TypeError ( \"result_prefix must have the same length as list_of_lat and list_of_lon if it is not empty\" ) if method_type not in ( \"approx\" , \"exact\" ): raise TypeError ( \"Invalid input for method_type.\" ) f_point_in_country_approx = F . udf ( point_in_country_approx , T . IntegerType ()) if method_type == \"exact\" : def zip_feats ( x , y ): \"\"\"zipping two features (in list form) elementwise\"\"\" return zip ( x , y ) f_zip_feats = F . udf ( zip_feats , T . ArrayType ( T . StructType ( [ T . StructField ( \"first\" , T . StringType ()), T . StructField ( \"second\" , T . ArrayType ( T . ArrayType ( T . ArrayType ( T . ArrayType ( T . DoubleType ()))) ), ), ] ) ), ) geo_data = spark . read . json ( country_shapefile_path , multiLine = True ) . withColumn ( \"tmp\" , f_zip_feats ( \"features.properties.ISO_A2\" , \"features.geometry.coordinates\" ), ) polygon_list = ( geo_data . select ( F . explode ( F . col ( \"tmp\" )) . alias ( \"country_coord\" )) . withColumn ( \"country_code\" , F . col ( \"country_coord\" ) . getItem ( \"first\" )) . withColumn ( \"coordinates\" , F . col ( \"country_coord\" ) . getItem ( \"second\" )) . where ( F . col ( \"country_code\" ) == country ) . select ( \"coordinates\" ) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) print ( \"No. of polygon: \" + str ( len ( polygon_list ))) min_lon , min_lat = polygon_list [ 0 ][ 0 ][ 0 ] max_lon , max_lat = polygon_list [ 0 ][ 0 ][ 0 ] for polygon in polygon_list : exterior = polygon [ 0 ] for loc in exterior : if loc [ 0 ] < min_lon : min_lon = loc [ 0 ] elif loc [ 0 ] > max_lon : max_lon = loc [ 0 ] if loc [ 1 ] < min_lat : min_lat = loc [ 1 ] elif loc [ 1 ] > max_lat : max_lat = loc [ 1 ] odf = idf for i , ( lat , lon ) in enumerate ( zip ( list_of_lat , list_of_lon )): col = result_prefix [ i ] if result_prefix else ( lat + \"_\" + lon ) if method_type == \"exact\" : odf = odf . withColumn ( col + \"_in_\" + country + \"_exact\" , f_point_in_polygons ( polygon_list , [ min_lon , min_lat ], [ max_lon , max_lat ] )( F . col ( lon ), F . col ( lat )), ) else : odf = odf . withColumn ( col + \"_in_\" + country + \"_approx\" , f_point_in_country_approx ( F . col ( lat ), F . col ( lon ), F . lit ( country )), ) if output_mode == \"replace\" : odf = odf . drop ( lat , lon ) return odf def location_in_polygon ( idf, list_of_lat, list_of_lon, polygon, result_prefix=[], output_mode='append') This function checks whether each lat-lon pair is insided a GeoJSON object. The following types of GeoJSON objects are supported by this function: Polygon, MultiPolygon, Feature or FeatureCollection Parameters idf Input Dataframe. list_of_lat List of columns representing latitude e.g., [\"lat1\",\"lat2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lat2\". list_of_lon List of columns representing longitude e.g., [\"lon1\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lon1|lon2\". list_of_lon must have the same length as list_of_lat such that i-th element of list_of_lat and i-th element of list_of_lon form a lat-lon pair to format. polygon The following types of GeoJSON objects are supported: Polygon, MultiPolygon, Feature or FeatureCollection result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_lat and list_of_lon. If it is empty, _ will be used for each lat-lon pair. For example, list_of_lat is \"lat1|lat2\", list_of_lon is \"lon1|lon2\". Case 1: result_prefix = \"L1|L2\". New columns will be named as L1_in_poly and L2_in_poly. Calse 2: result_prefix = []. New columns will be named as lat1_lon1_in_poly and lat2_lon2_in_poly. (Default value = []) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns DataFrame Expand source code def location_in_polygon ( idf , list_of_lat , list_of_lon , polygon , result_prefix = [], output_mode = \"append\" ): \"\"\" This function checks whether each lat-lon pair is insided a GeoJSON object. The following types of GeoJSON objects are supported by this function: Polygon, MultiPolygon, Feature or FeatureCollection Parameters ---------- idf Input Dataframe. list_of_lat List of columns representing latitude e.g., [\"lat1\",\"lat2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lat2\". list_of_lon List of columns representing longitude e.g., [\"lon1\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lon1|lon2\". list_of_lon must have the same length as list_of_lat such that i-th element of list_of_lat and i-th element of list_of_lon form a lat-lon pair to format. polygon The following types of GeoJSON objects are supported: Polygon, MultiPolygon, Feature or FeatureCollection result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_lat and list_of_lon. If it is empty, <lat>_<lon> will be used for each lat-lon pair. For example, list_of_lat is \"lat1|lat2\", list_of_lon is \"lon1|lon2\". Case 1: result_prefix = \"L1|L2\". New columns will be named as L1_in_poly and L2_in_poly. Calse 2: result_prefix = []. New columns will be named as lat1_lon1_in_poly and lat2_lon2_in_poly. (Default value = []) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_lat , str ): list_of_lat = [ x . strip () for x in list_of_lat . split ( \"|\" )] if isinstance ( list_of_lon , str ): list_of_lon = [ x . strip () for x in list_of_lon . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_lat + list_of_lon ): raise TypeError ( \"Invalid input for list_of_lat or list_of_lon\" ) if len ( list_of_lat ) != len ( list_of_lon ): raise TypeError ( \"list_of_lat and list_of_lon must have the same length\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_lat )): raise TypeError ( \"result_prefix must have the same length as list_of_lat and list_of_lon if it is not empty\" ) if \"coordinates\" in polygon . keys (): polygon_list = polygon [ \"coordinates\" ] if polygon [ \"type\" ] == \"Polygon\" : polygon_list = [ polygon_list ] elif \"geometry\" in polygon . keys (): polygon_list = [ polygon [ \"geometry\" ][ \"coordinates\" ]] elif \"features\" in polygon . keys (): polygon_list = [] for poly in polygon [ \"features\" ]: polygon_list . append ( poly [ \"geometry\" ][ \"coordinates\" ]) odf = idf for i , ( lat , lon ) in enumerate ( zip ( list_of_lat , list_of_lon )): col = result_prefix [ i ] if result_prefix else ( lat + \"_\" + lon ) odf = odf . withColumn ( col + \"_in_poly\" , f_point_in_polygons ( polygon_list )( F . col ( lon ), F . col ( lat )) ) if output_mode == \"replace\" : odf = odf . drop ( lat , lon ) return odf def reverse_geocoding ( idf, lat_col, long_col) This function reverses the input latitude and longitude of a given DataFrame into address Parameters idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data Returns odf :\u2002 DataFrame Output DataFrame, which contains latitude, longitude and address appropriately Expand source code def reverse_geocoding ( idf , lat_col , long_col ): \"\"\" This function reverses the input latitude and longitude of a given DataFrame into address Parameters ---------- idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data Returns ------- odf : DataFrame Output DataFrame, which contains latitude, longitude and address appropriately \"\"\" if lat_col not in idf . columns : raise TypeError ( \"Invalid input for lat_col\" ) if long_col not in idf . columns : raise TypeError ( \"Invalid input for long_col\" ) idf = recast_column ( idf , list_of_cols = [ lat_col , long_col ], list_of_dtypes = [ \"double\" , \"double\" ] ) if idf != idf . dropna ( subset = ( lat_col , long_col )): warnings . warn ( \"Rows dropped due to null value in longitude and/or latitude values\" ) idf = idf . dropna ( subset = ( lat_col , long_col )) if not idf . where ( ( F . col ( lat_col ) > 90 ) | ( F . col ( lat_col ) < - 90 ) | ( F . col ( long_col ) > 180 ) | ( F . col ( long_col ) < - 180 ) ) . rdd . isEmpty (): warnings . warn ( \"Rows dropped due to longitude and/or latitude values being out of the valid range\" ) idf = idf . where ( ( F . col ( lat_col ) <= 90 ) & ( F . col ( lat_col ) >= - 90 ) & ( F . col ( long_col ) <= 180 ) & ( F . col ( long_col ) >= - 180 ) ) if idf . rdd . isEmpty (): warnings . warn ( \"No reverse_geocoding Computation - No valid latitude/longitude row(s) to compute\" ) return idf def reverse_geocode ( lat , long ): coordinates = ( float ( lat ), float ( long )) location = rg . search ( coordinates , mode = 1 ) if location : return ( str ( location [ 0 ][ \"name\" ]) + \",\" + str ( location [ 0 ][ \"admin1\" ]) + \",\" + str ( location [ 0 ][ \"cc\" ]) ) else : return \"N/A\" udf_reverse_geocode = F . udf ( reverse_geocode ) odf = ( idf . withColumn ( \"info\" , udf_reverse_geocode ( F . col ( lat_col ), F . col ( long_col ))) . select ( lat_col , long_col , \"info\" ) . withColumn ( \"name_of_place\" , F . split ( F . col ( \"info\" ), \",\" ) . getItem ( 0 )) . withColumn ( \"region\" , F . split ( F . col ( \"info\" ), \",\" ) . getItem ( 1 )) . withColumn ( \"country_code\" , F . split ( F . col ( \"info\" ), \",\" ) . getItem ( 2 )) . drop ( \"info\" ) ) return odf def rog_calculation ( idf, lat_col, long_col, id_col=None) This function calculates the Radius of Gyration (in meter) of a given DataFrame, based on its identifier column (if applicable) Parameters idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data id_col Column in the input DataFrame that contains identifier for the DataFrame (Default is None) Returns odf :\u2002 DataFrame Output DataFrame, which contains Radius of Gyration (in meter) and identifier (if applicable) Expand source code def rog_calculation ( idf , lat_col , long_col , id_col = None ): \"\"\" This function calculates the Radius of Gyration (in meter) of a given DataFrame, based on its identifier column (if applicable) Parameters ---------- idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data id_col Column in the input DataFrame that contains identifier for the DataFrame (Default is None) Returns ------- odf : DataFrame Output DataFrame, which contains Radius of Gyration (in meter) and identifier (if applicable) \"\"\" if id_col not in idf . columns and id_col : raise TypeError ( \"Invalid input for id_col\" ) if lat_col not in idf . columns : raise TypeError ( \"Invalid input for lat_col\" ) if long_col not in idf . columns : raise TypeError ( \"Invalid input for long_col\" ) idf = recast_column ( idf , list_of_cols = [ lat_col , long_col ], list_of_dtypes = [ \"double\" , \"double\" ] ) if idf != idf . dropna ( subset = ( lat_col , long_col )): warnings . warn ( \"Rows dropped due to null value in longitude and/or latitude values\" ) idf = idf . dropna ( subset = ( lat_col , long_col )) if not idf . where ( ( F . col ( lat_col ) > 90 ) | ( F . col ( lat_col ) < - 90 ) | ( F . col ( long_col ) > 180 ) | ( F . col ( long_col ) < - 180 ) ) . rdd . isEmpty (): warnings . warn ( \"Rows dropped due to longitude and/or latitude values being out of the valid range\" ) idf = idf . where ( ( F . col ( lat_col ) <= 90 ) & ( F . col ( lat_col ) >= - 90 ) & ( F . col ( long_col ) <= 180 ) & ( F . col ( long_col ) >= - 180 ) ) if idf . rdd . isEmpty (): warnings . warn ( \"No reverse_geocoding Computation - No valid latitude/longitude row(s) to compute\" ) return idf def getHaversineDist ( lat1 , lon1 , lat2 , lon2 ): R = 6378126 # approximate radius of earth in m lat1 = radians ( float ( lat1 )) lon1 = radians ( float ( lon1 )) lat2 = radians ( float ( lat2 )) lon2 = radians ( float ( lon2 )) dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 c = 2 * atan2 ( sqrt ( a ), sqrt ( 1 - a )) distance = R * c return distance udf_harver_dist = F . udf ( getHaversineDist , T . FloatType ()) if id_col : idf_centroid = centroid ( idf , lat_col , long_col , id_col ) idf_join = idf_centroid . join ( idf , id_col , \"inner\" ) idf_calc = idf_join . withColumn ( \"distance\" , udf_harver_dist ( F . col ( lat_col ), F . col ( long_col ), F . col ( lat_col + \"_centroid\" ), F . col ( long_col + \"_centroid\" ), ), ) odf = idf_calc . groupby ( id_col ) . agg ( F . mean ( \"distance\" ) . alias ( \"radius_of_gyration\" ) ) else : centroid_info = centroid ( idf , lat_col , long_col , id_col ) . rdd . collect () lat_centroid = centroid_info [ 0 ][ 0 ] long_centroid = centroid_info [ 0 ][ 1 ] idf_join = idf . withColumn ( lat_col + \"_centroid\" , F . lit ( lat_centroid ) ) . withColumn ( long_col + \"_centroid\" , F . lit ( long_centroid )) idf_calc = idf_join . withColumn ( \"distance\" , udf_harver_dist ( F . col ( lat_col ), F . col ( long_col ), F . col ( lat_col + \"_centroid\" ), F . col ( long_col + \"_centroid\" ), ), ) odf = idf_calc . groupby () . agg ( F . mean ( \"distance\" ) . alias ( \"radius_of_gyration\" )) return odf def weighted_centroid ( idf, id_col, lat_col, long_col) This function calculates the weighted centroid of a given DataFrame using lat-long pairs based on its identifier column Parameters idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data id_col Column in the input DataFrame that contains identifier for the DataFrame Returns odf :\u2002 DataFrame Output DataFrame, which contains weighted lat_centroid and long_centroid and identifier Expand source code def weighted_centroid ( idf , id_col , lat_col , long_col ): \"\"\" This function calculates the weighted centroid of a given DataFrame using lat-long pairs based on its identifier column Parameters ---------- idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data id_col Column in the input DataFrame that contains identifier for the DataFrame Returns ------- odf : DataFrame Output DataFrame, which contains weighted lat_centroid and long_centroid and identifier \"\"\" if id_col not in idf . columns : raise TypeError ( \"Invalid input for id_col\" ) if lat_col not in idf . columns : raise TypeError ( \"Invalid input for lat_col\" ) if long_col not in idf . columns : raise TypeError ( \"Invalid input for long_col\" ) idf = recast_column ( idf , list_of_cols = [ lat_col , long_col ], list_of_dtypes = [ \"double\" , \"double\" ] ) if idf != idf . dropna ( subset = ( lat_col , long_col )): warnings . warn ( \"Rows dropped due to null value in longitude and/or latitude values\" ) idf = idf . dropna ( subset = ( lat_col , long_col )) if not idf . where ( ( F . col ( lat_col ) > 90 ) | ( F . col ( lat_col ) < - 90 ) | ( F . col ( long_col ) > 180 ) | ( F . col ( long_col ) < - 180 ) ) . rdd . isEmpty (): warnings . warn ( \"Rows dropped due to longitude and/or latitude values being out of the valid range\" ) idf = idf . where ( ( F . col ( lat_col ) <= 90 ) & ( F . col ( lat_col ) >= - 90 ) & ( F . col ( long_col ) <= 180 ) & ( F . col ( long_col ) >= - 180 ) ) if idf . rdd . isEmpty (): warnings . warn ( \"No reverse_geocoding Computation - No valid latitude/longitude row(s) to compute\" ) return idf def degree_to_radian ( deg ): return deg * pi / 180 f_degree_to_radian = F . udf ( degree_to_radian , T . FloatType ()) idf_rad = ( idf . withColumn ( \"lat_rad\" , f_degree_to_radian ( lat_col )) . withColumn ( \"long_rad\" , f_degree_to_radian ( long_col )) . withColumn ( \"x\" , F . cos ( \"lat_rad\" ) * F . cos ( \"long_rad\" )) . withColumn ( \"y\" , F . cos ( \"lat_rad\" ) * F . sin ( \"long_rad\" )) . withColumn ( \"z\" , F . sin ( \"lat_rad\" )) ) idf_groupby = ( idf_rad . groupby ( id_col ) . agg ( F . sum ( \"x\" ) . alias ( \"x_group\" ), F . sum ( \"y\" ) . alias ( \"y_group\" ), F . sum ( \"z\" ) . alias ( \"z_group\" ), F . count ( id_col ) . alias ( \"weight_group\" ), ) . withColumn ( \"weighted_x\" , F . col ( \"x_group\" ) * F . col ( \"weight_group\" )) . withColumn ( \"weighted_y\" , F . col ( \"y_group\" ) * F . col ( \"weight_group\" )) . withColumn ( \"weighted_z\" , F . col ( \"z_group\" ) * F . col ( \"weight_group\" )) ) total_weight = ( idf_groupby . groupBy () . agg ( F . sum ( \"weight_group\" )) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) total_x = ( idf_groupby . groupBy () . agg ( F . sum ( \"weighted_x\" )) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) total_y = ( idf_groupby . groupBy () . agg ( F . sum ( \"weighted_y\" )) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) total_z = ( idf_groupby . groupBy () . agg ( F . sum ( \"weighted_z\" )) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) x = total_x / total_weight y = total_y / total_weight z = total_z / total_weight hyp = sqrt ( x * x + y * y ) lat_centroid , long_centroid = atan2 ( z , hyp ) * 180 / pi , atan2 ( y , x ) * 180 / pi odf = ( idf_groupby . select ( id_col ) . withColumn ( lat_col + \"_centroid\" , F . lit ( lat_centroid )) . withColumn ( long_col + \"_centroid\" , F . lit ( long_centroid )) ) return odf","title":"<code>geospatial</code>"},{"location":"api/data_transformer/geospatial.html#geospatial","text":"The geospatial module supports transformation & calculation functions for geospatial fields, such as transforming between different formats, controlling geohash values precisions, checking if a location is inside a country, calculating centroid and Radius of Gyration, and reverse latitude-longitude pairs into address. Functions supported through this module are listed below: - geo_format_latlon - geo_format_cartesian - geo_format_geohash - location_distance - geohash_precision_control - location_in_polygon - location_in_country - centroid - weighted_centroid - rog_calculation - reverse_geocoding Expand source code \"\"\" The geospatial module supports transformation & calculation functions for geospatial fields, such as transforming between different formats, controlling geohash values precisions, checking if a location is inside a country, calculating centroid and Radius of Gyration, and reverse latitude-longitude pairs into address. Functions supported through this module are listed below: - geo_format_latlon - geo_format_cartesian - geo_format_geohash - location_distance - geohash_precision_control - location_in_polygon - location_in_country - centroid - weighted_centroid - rog_calculation - reverse_geocoding \"\"\" from math import sin , cos , sqrt , atan2 , pi , radians from loguru import logger from pyspark.sql import functions as F from pyspark.sql import types as T import reverse_geocoder as rg import warnings from anovos.data_ingest.data_ingest import recast_column from anovos.data_transformer.geo_utils import ( EARTH_RADIUS , from_latlon_decimal_degrees , to_latlon_decimal_degrees , haversine_distance , vincenty_distance , euclidean_distance , f_point_in_polygons , point_in_country_approx , ) def geo_format_latlon ( idf , list_of_lat , list_of_lon , input_format , output_format , result_prefix = [], optional_configs = { \"geohash_precision\" : 8 , \"radius\" : EARTH_RADIUS }, output_mode = \"append\" , ): \"\"\" This function is the main function to convert the input data's location columns from lat,lon format to desired format based on output_format. If output_mode is set to True, the original location columns will be dropped. For each location column, \"to_latlon_decimal_degrees\" will be called to convert them to decimal degrees, and then \"from_latlon_decimal_degrees\" will be called to transform decimal degrees to output_format If output_format is \"dd\" or \"dms\" or \"radian\", 2 new columns containing \"_lat_\", \"_long_\" in the column names will be created. If output_format is \"cartesian\", 3 new columns containing \"_x_\", \"_y_\", \"_z_\" in the column names will be created. If output_format is \"geohash\", 1 new column containing \"_geohash\" in the column name will be created. Parameters ---------- idf Input Dataframe. list_of_lat List of columns representing latitude e.g., [\"lat1\",\"lat2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lat2\". list_of_lon List of columns representing longitude e.g., [\"lon1\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lon1|lon2\". list_of_lon must have the same length as list_of_lat such that i-th element of list_of_lat and i-th element of list_of_lon form a lat-lon pair to format. input_format \"dd\", \"dms\", \"radian\". \"dd\" represents latitude and longitude in decimal degrees. \"dms\" represents latitude and longitude in degrees minutes second. \"radian\" represents latitude and longitude in radians. output_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". \"cartesian\" represents the Cartesian coordinates of the point in three-dimensional space. \"geohash\" represents geocoded locations. result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_lat and list_of_lon. If it is empty, <lat>_<lon> will be used for each lat-lon pair. For example, list_of_lat is \"lat1|lat2\", list_of_lon is \"L1|L2\". Case 1: result_prefix = \"L1|L2\". If output_format is \"dd\", \"dms\" or \"radian\", new columns will be named as L1_lat_<output_format>, L1_lon_<output_format>, L2_lat_<output_format>, L2_lon_<output_format>. If output_format is \"cartesian\", new columns will be named as L1_x, L1_y, L1_z, L2_x, L2_y, L2_z. If output_format is \"geohash\", new columns will be named as L1_geohash and L2_geohash. Case 2: result_prefix = []. Prefixes \"L1\" and \"L2\" in above column names will be replaced by \"lat1_lon1\" and \"lat2_lon2\". (Default value = []) optional_configs The following keys can be used: - geohash_precision: precision of the resultant geohash. This key is only used when output_format is \"geohash\". (Default value = 8) - radius: radius of Earth. Necessary only when output_format is \"cartesian\". (Default value = EARTH_RADIUS) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" geohash_precision = int ( optional_configs . get ( \"geohash_precision\" , 8 )) radius = optional_configs . get ( \"radius\" , EARTH_RADIUS ) if isinstance ( list_of_lat , str ): list_of_lat = [ x . strip () for x in list_of_lat . split ( \"|\" )] if isinstance ( list_of_lon , str ): list_of_lon = [ x . strip () for x in list_of_lon . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_lat + list_of_lon ): raise TypeError ( \"Invalid input for list_of_lat or list_of_lon\" ) format_list = [ \"dd\" , \"dms\" , \"radian\" , \"cartesian\" , \"geohash\" ] if ( input_format not in format_list [: 3 ]) or ( output_format not in format_list ): raise TypeError ( \"Invalid input for input_format or output_format\" ) if len ( list_of_lat ) != len ( list_of_lon ): raise TypeError ( \"list_of_lat and list_of_lon must have the same length\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_lat )): raise TypeError ( \"result_prefix must have the same length as list_of_lat and list_of_lon if it is not empty\" ) f_to_latlon_dd = F . udf ( lambda loc : to_latlon_decimal_degrees ( loc , input_format , radius ), T . ArrayType ( T . FloatType ()), ) from_latlon_dd_ = lambda loc : from_latlon_decimal_degrees ( loc , output_format , radius , geohash_precision ) if output_format in [ \"dd\" , \"radian\" , \"cartesian\" ]: f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . FloatType ())) elif output_format == \"dms\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . ArrayType ( T . FloatType ())) ) elif output_format == \"geohash\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . StringType ()) odf = idf for i , ( lat , lon ) in enumerate ( zip ( list_of_lat , list_of_lon )): col = result_prefix [ i ] if result_prefix else ( lat + \"_\" + lon ) odf = odf . withColumn ( col + \"_temp\" , F . array ( lat , lon )) . withColumn ( col + \"_\" + output_format , f_from_latlon_dd ( f_to_latlon_dd ( col + \"_temp\" )) ) if output_format in [ \"dd\" , \"dms\" , \"radian\" ]: odf = ( odf . withColumn ( col + \"_lat_\" + output_format , F . col ( col + \"_\" + output_format )[ 0 ] ) . withColumn ( col + \"_lon_\" + output_format , F . col ( col + \"_\" + output_format )[ 1 ] ) . drop ( col + \"_\" + output_format ) ) if output_format == \"cartesian\" : odf = ( odf . withColumn ( col + \"_x\" , F . col ( col + \"_\" + output_format )[ 0 ]) . withColumn ( col + \"_y\" , F . col ( col + \"_\" + output_format )[ 1 ]) . withColumn ( col + \"_z\" , F . col ( col + \"_\" + output_format )[ 2 ]) . drop ( col + \"_\" + output_format ) ) odf = odf . drop ( col + \"_temp\" ) if output_mode == \"replace\" : odf = odf . drop ( lat , lon ) return odf def geo_format_cartesian ( idf , list_of_x , list_of_y , list_of_z , output_format , result_prefix = [], optional_configs = { \"geohash_precision\" : 8 , \"radius\" : EARTH_RADIUS }, output_mode = \"append\" , ): \"\"\" This function helps to convert the input data's location columns from cartesian format to desired format based on output_format. If output_mode is set to True, the original location columns will be dropped. For each location column, \"to_latlon_decimal_degrees\" will be called to convert them to decimal degrees, and then \"from_latlon_decimal_degrees\" will be called to transform decimal degrees to output_format If output_format is \"dd\" or \"dms\" or \"radian\", 2 new columns containing \"_lat_\", \"_long_\" in the column names will be created. If output_format is \"geohash\", 1 new column containing \"_geohash\" in the column name will be created. Parameters ---------- idf Input Dataframe. list_of_x List of columns representing x axis values e.g., [\"x1\",\"x2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"x1|x2\". list_of_y List of columns representing y axis values e.g., [\"y1\",\"y2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"y1|y2\". list_of_z List of columns representing z axis values e.g., [\"z1\",\"z2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"z1|z2\". list_of_x, list_of_y and list_of_z must have the same length such that the i-th element of 3 lists form an x-y-z pair to format. output_format \"dd\", \"dms\", \"radian\", \"geohash\" \"dd\" represents latitude and longitude in decimal degrees. \"dms\" represents latitude and longitude in degrees minutes second. \"radian\" represents latitude and longitude in radians. \"geohash\" represents geocoded locations. result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_x, list_of_y and list_of_z. If it is empty, <x>_<y>_<z> will be used for each x-y-z pair. For example, list_of_x is \"x1|x2\", list_of_y is \"y1|y2\" and list_of_z is \"z1|z2\" Case 1: result_prefix = \"L1|L2\". If output_format is \"dd\", \"dms\" or \"radian\", new columns will be named as L1_lat_<output_format>, L1_lon_<output_format>, L2_lat_<output_format>, L2_lon_<output_format>. If output_format is \"geohash\", new columns will be named as L1_geohash and L2_geohash. Case 2: result_prefix = []. Prefixes \"L1\" and \"L2\" in above column names will be replaced by \"x1_y1_z1\" and \"x2_y2_z2\". (Default value = []) optional_configs The following keys can be used: - geohash_precision: precision of the resultant geohash. This key is only used when output_format is \"geohash\". (Default value = 8) - radius: radius of Earth. (Default value = EARTH_RADIUS) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" geohash_precision = int ( optional_configs . get ( \"geohash_precision\" , 8 )) radius = optional_configs . get ( \"radius\" , EARTH_RADIUS ) if isinstance ( list_of_x , str ): list_of_x = [ x . strip () for x in list_of_x . split ( \"|\" )] if isinstance ( list_of_y , str ): list_of_y = [ x . strip () for x in list_of_y . split ( \"|\" )] if isinstance ( list_of_z , str ): list_of_z = [ x . strip () for x in list_of_z . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_x + list_of_y + list_of_z ): raise TypeError ( \"Invalid input for list_of_x or list_of_y or list_of_z\" ) format_list = [ \"dd\" , \"dms\" , \"radian\" , \"geohash\" ] if output_format not in format_list : raise TypeError ( \"Invalid input for output_format\" ) if len ({ len ( list_of_x ), len ( list_of_y ), len ( list_of_z )}) != 1 : raise TypeError ( \"list_of_x, list_of_y and list_of_z must have the same length\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_x )): raise TypeError ( \"result_prefix must have the same length as list_of_x, list_of_y and list_of_y if it is not empty\" ) f_to_latlon_dd = F . udf ( lambda loc : to_latlon_decimal_degrees ( loc , \"cartesian\" , radius ), T . ArrayType ( T . FloatType ()), ) from_latlon_dd_ = lambda loc : from_latlon_decimal_degrees ( loc , output_format , radius , geohash_precision ) if output_format in [ \"dd\" , \"radian\" ]: f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . FloatType ())) elif output_format == \"dms\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . ArrayType ( T . FloatType ())) ) elif output_format == \"geohash\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . StringType ()) odf = idf for i , ( x , y , z ) in enumerate ( zip ( list_of_x , list_of_y , list_of_z )): col = result_prefix [ i ] if result_prefix else ( x + \"_\" + y + \"_\" + z ) odf = odf . withColumn ( col + \"_temp\" , F . array ( x , y , z )) . withColumn ( col + \"_\" + output_format , f_from_latlon_dd ( f_to_latlon_dd ( col + \"_temp\" )) ) if output_format in [ \"dd\" , \"dms\" , \"radian\" ]: odf = ( odf . withColumn ( col + \"_lat_\" + output_format , F . col ( col + \"_\" + output_format )[ 0 ] ) . withColumn ( col + \"_lon_\" + output_format , F . col ( col + \"_\" + output_format )[ 1 ] ) . drop ( col + \"_\" + output_format ) ) odf = odf . drop ( col + \"_temp\" ) if output_mode == \"replace\" : odf = odf . drop ( x , y , z ) return odf def geo_format_geohash ( idf , list_of_geohash , output_format , result_prefix = [], optional_configs = { \"radius\" : EARTH_RADIUS }, output_mode = \"append\" , ): \"\"\" This function is the main function to convert the input data's location columns from geohash format to desired format based on output_format. If output_mode is set to True, the original location columns will be dropped. For each location column, \"to_latlon_decimal_degrees\" will be called to convert them to decimal degrees, and then \"from_latlon_decimal_degrees\" will be called to transform decimal degrees to output_format If output_format is \"dd\" or \"dms\" or \"radian\", 2 new columns containing \"_lat_\", \"_long_\" in the column names will be created. If output_format is \"cartesian\", 3 new columns containing \"_x_\", \"_y_\", \"_z_\" in the column names will be created. Parameters ---------- idf Input Dataframe. list_of_geohash List of columns representing geohash e.g., [\"gh1\",\"gh2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"gh1|gh2\". output_format \"dd\", \"dms\", \"radian\", \"cartesian\" \"dd\" represents latitude and longitude in decimal degrees. \"dms\" represents latitude and longitude in degrees minutes second. \"radian\" represents latitude and longitude in radians. \"cartesian\" represents the Cartesian coordinates of the point in three-dimensional space. result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_geohash. If it is empty, <geohash column name> will be used as the prefix for transformed columns. For example, list_of_geohash is \"gh1|gh2\". Case 1: result_prefix = \"L1|L2\". If output_format is \"dd\", \"dms\" or \"radian\", new columns will be named as L1_lat_<output_format>, L1_lon_<output_format>, L2_lat_<output_format>, L2_lon_<output_format>. If output_format is \"cartesian\", new columns will be named as L1_x, L1_y, L1_z, L2_x, L2_y, L2_z. Case 2: result_prefix = []. Prefixes \"L1\" and \"L2\" in above column names will be replaced by \"gh1\" and \"gh2\". (Default value = []) optional_configs The following key can be used: - radius: radius of Earth. Necessary only when output_format is \"cartesian\". (Default value = EARTH_RADIUS) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" radius = optional_configs . get ( \"radius\" , EARTH_RADIUS ) if isinstance ( list_of_geohash , str ): list_of_geohash = [ x . strip () for x in list_of_geohash . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_geohash ): raise TypeError ( \"Invalid input for list_of_geohash\" ) format_list = [ \"dd\" , \"dms\" , \"radian\" , \"cartesian\" ] if output_format not in format_list : raise TypeError ( \"Invalid input for output_format\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_geohash )): raise TypeError ( \"result_prefix must have the same length as list_of_geohash if it is not empty\" ) f_to_latlon_dd = F . udf ( lambda loc : to_latlon_decimal_degrees ( loc , \"geohash\" , radius ), T . ArrayType ( T . FloatType ()), ) from_latlon_dd_ = lambda loc : from_latlon_decimal_degrees ( loc , output_format , radius ) if output_format in [ \"dd\" , \"radian\" , \"cartesian\" ]: f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . FloatType ())) elif output_format == \"dms\" : f_from_latlon_dd = F . udf ( from_latlon_dd_ , T . ArrayType ( T . ArrayType ( T . FloatType ())) ) odf = idf for i , geohash in enumerate ( list_of_geohash ): col = result_prefix [ i ] if result_prefix else geohash odf = odf . withColumn ( col + \"_\" + output_format , f_from_latlon_dd ( f_to_latlon_dd ( geohash )) ) if output_format in [ \"dd\" , \"dms\" , \"radian\" ]: odf = ( odf . withColumn ( col + \"_lat_\" + output_format , F . col ( col + \"_\" + output_format )[ 0 ] ) . withColumn ( col + \"_lon_\" + output_format , F . col ( col + \"_\" + output_format )[ 1 ] ) . drop ( col + \"_\" + output_format ) ) if output_format == \"cartesian\" : odf = ( odf . withColumn ( col + \"_x\" , F . col ( col + \"_\" + output_format )[ 0 ]) . withColumn ( col + \"_y\" , F . col ( col + \"_\" + output_format )[ 1 ]) . withColumn ( col + \"_z\" , F . col ( col + \"_\" + output_format )[ 2 ]) . drop ( col + \"_\" + output_format ) ) if output_mode == \"replace\" : odf = odf . drop ( geohash ) return odf def location_distance ( idf , list_of_cols_loc1 , list_of_cols_loc2 , loc_format = \"dd\" , result_prefix = \"\" , distance_type = \"haversine\" , unit = \"m\" , optional_configs = { \"radius\" : EARTH_RADIUS , \"vincenty_model\" : \"WGS-84\" }, output_mode = \"append\" , ): \"\"\" This function calculates the distance between 2 locations, and the distance formula is determined by distance_type. If distance_type = \"vincenty\", thed loc_format should be \"dd\", and list_of_cols_loc1, list_of_cols_loc2 should be in [lat1, lon1] and [lat2, lon2] format respectively. \"vincenty_distance\" function will be called to calculate the distance. If distance_type = \"haversine\", then loc_format should be \"radian\", and list_of_cols_loc1, list_of_cols_loc2 should be in [lat1, lon1] and [lat2, lon2] format respectively. \"haversine_distance\" function will be called to calculate the distance. If distance_type = \"euclidean\", then loc_format should be \"cartesian\", and list_of_cols_loc1, list_of_cols_loc2 should be in [x1, y1, z1] and [x2, y2, z2] format respectively. \"euclidean_distance\" function will be called to calculate the distance. If loc_format does not match with distance_type's desired format, necessary conversion of location columns will be performed with the help of \"geo_format_latlon\", \"geo_format_cartesian\" and \"geo_format_geohash\" functions. Parameters ---------- idf Input Dataframe. list_of_cols_loc1 List of columns to express the first location e.g., [\"lat1\",\"lon1\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lon1\". list_of_cols_loc2 List of columns to express the second location e.g., [\"lat2\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat2|lon2\". loc_format \"dd\", \"dms\", \"radian\", \"cartesian\", \"geohash\". (Default value = \"dd\") result_prefix Prefix for the newly generated column. It must be a string or a list with one element. If it is empty, <list_of_cols_loc1 joined by '_'>_<list_of_cols_loc2 joined by '_'> will be used as the prefix. For example, list_of_cols_loc1 is \"lat1|lon1\", list_of_cols_loc2 is \"lat2|lon2\". Case 1: result_prefix = \"L1_L2\": the new column will be named as L1_L2_distance. Case 2: result_prefix = []: the new column will be named as lat1_lon1_lat2_lon2_distance. (Default value = '') distance_type \"vincenty\", \"haversine\", \"euclidean\". (Default value = \"haversine\") \"vincenty\" option calculates the distance between two points on the surface of a spheroid. \"haversine\" option calculates the great-circle distance between two points on a sphere. \"euclidean\" option calculates the length of the line segment between two points. unit \"m\", \"km\". Unit of the result. (Default value = \"m\") optional_configs The following keys can be used: - radius: radius of Earth. Necessary only when output_format is \"cartesian\". (Default value = EARTH_RADIUS) - vincenty_model: The ellipsoidal model to use. Supported values: \"WGS-84\", \"GRS-80\", \"Airy (1830)\", \"Intl 1924\", \"Clarke (1880)\", \"GRS-67\". For more information, please refer to geopy.distance.ELLIPSOIDS. (Default value = \"WGS-84\") output_mode \"replace\", \"append\". \"replace\" option replaces original columns with transformed column. \"append\" option appends the transformed column to the input dataset with name \"<loc1>_<loc2>_distance\". (Default value = \"append\") Returns ------- DataFrame \"\"\" radius = optional_configs . get ( \"radius\" , EARTH_RADIUS ) vincenty_model = optional_configs . get ( \"vincenty_model\" , \"WGS-84\" ) if isinstance ( list_of_cols_loc1 , str ): list_of_cols_loc1 = [ x . strip () for x in list_of_cols_loc1 . split ( \"|\" )] if isinstance ( list_of_cols_loc2 , str ): list_of_cols_loc2 = [ x . strip () for x in list_of_cols_loc2 . split ( \"|\" )] if isinstance ( result_prefix , list ): if len ( result_prefix ) > 1 : raise TypeError ( \"If result_prefix is a list, it can contain maximally 1 element\" ) elif len ( result_prefix ) == 1 : result_prefix = result_prefix [ 0 ] if any ( i not in idf . columns for i in list_of_cols_loc1 + list_of_cols_loc2 ): raise TypeError ( \"Invalid input for list_of_cols_loc1 or list_of_cols_loc2\" ) if distance_type not in [ \"vincenty\" , \"haversine\" , \"euclidean\" ]: raise TypeError ( \"Invalid input for distance_type\" ) if loc_format not in [ \"dd\" , \"dms\" , \"radian\" , \"cartesian\" , \"geohash\" ]: raise TypeError ( \"Invalid input for loc_format\" ) format_mapping = { \"vincenty\" : \"dd\" , \"haversine\" : \"radian\" , \"euclidean\" : \"cartesian\" } format_required = format_mapping [ distance_type ] if loc_format != format_required : if loc_format in [ \"dd\" , \"dms\" , \"radian\" ]: idf = geo_format_latlon ( idf , list_of_lat = [ list_of_cols_loc1 [ 0 ], list_of_cols_loc2 [ 0 ]], list_of_lon = [ list_of_cols_loc1 [ 1 ], list_of_cols_loc2 [ 1 ]], input_format = loc_format , output_format = format_required , result_prefix = [ \"temp_loc1\" , \"temp_loc2\" ], optional_configs = { \"radius\" : radius }, output_mode = \"append\" , ) elif loc_format == \"cartesian\" : idf = geo_format_cartesian ( idf , list_of_x = [ list_of_cols_loc1 [ 0 ], list_of_cols_loc2 [ 0 ]], list_of_y = [ list_of_cols_loc1 [ 1 ], list_of_cols_loc2 [ 1 ]], list_of_z = [ list_of_cols_loc1 [ 2 ], list_of_cols_loc2 [ 2 ]], output_format = format_required , result_prefix = [ \"temp_loc1\" , \"temp_loc2\" ], optional_configs = { \"radius\" : radius }, output_mode = \"append\" , ) elif loc_format == \"geohash\" : idf = geo_format_geohash ( idf , list_of_geohash = [ list_of_cols_loc1 [ 0 ], list_of_cols_loc2 [ 0 ]], output_format = format_required , result_prefix = [ \"temp_loc1\" , \"temp_loc2\" ], optional_configs = { \"radius\" : radius }, output_mode = \"append\" , ) if format_required == \"dd\" : loc1 , loc2 = [ \"temp_loc1_lat_dd\" , \"temp_loc1_lon_dd\" ], [ \"temp_loc2_lat_dd\" , \"temp_loc2_lon_dd\" , ] elif format_required == \"radian\" : loc1 , loc2 = [ \"temp_loc1_lat_radian\" , \"temp_loc1_lon_radian\" ], [ \"temp_loc2_lat_radian\" , \"temp_loc2_lon_radian\" , ] elif format_required == \"cartesian\" : loc1 , loc2 = [ \"temp_loc1_x\" , \"temp_loc1_y\" , \"temp_loc1_z\" ], [ \"temp_loc2_x\" , \"temp_loc2_y\" , \"temp_loc2_z\" , ] idf = ( idf . withColumn ( \"temp_loc1\" , F . array ( * loc1 )) . withColumn ( \"temp_loc2\" , F . array ( * loc2 )) . drop ( * ( loc1 + loc2 )) ) else : idf = idf . withColumn ( \"temp_loc1\" , F . array ( * list_of_cols_loc1 )) . withColumn ( \"temp_loc2\" , F . array ( * list_of_cols_loc2 ) ) if distance_type == \"vincenty\" : compute_distance = lambda x1 , x2 : vincenty_distance ( x1 , x2 , unit , vincenty_model ) elif distance_type == \"haversine\" : compute_distance = lambda x1 , x2 : haversine_distance ( x1 , x2 , \"radian\" , unit , radius ) else : compute_distance = lambda x1 , x2 : euclidean_distance ( x1 , x2 , unit ) f_compute_distance = F . udf ( compute_distance , T . FloatType ()) col_prefix = ( result_prefix if result_prefix else \"_\" . join ( list_of_cols_loc1 ) + \"_\" + \"_\" . join ( list_of_cols_loc2 ) ) odf = idf . withColumn ( col_prefix + \"_distance\" , f_compute_distance ( \"temp_loc1\" , \"temp_loc2\" ) ) . drop ( \"temp_loc1\" , \"temp_loc2\" ) if output_mode == \"replace\" : odf = odf . drop ( * ( list_of_cols_loc1 + list_of_cols_loc2 )) return odf def geohash_precision_control ( idf , list_of_geohash , output_precision = 8 , km_max_error = None , output_mode = \"append\" ): \"\"\" This function controls the precision of input data's geohash columns. Parameters ---------- idf Input Dataframe. list_of_geohash List of columns in geohash format e.g., [\"gh1\",\"gh2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"gh1|gh2\". output_precision Precision of the transformed geohash in the output dataframe. (Default value = 8) km_max_error Maximum permissible error in kilometers. If km_max_error is specified, output_precision will be ignored and km_max_error will be mapped to an output_precision according to the following dictionary: {2500: 1, 630: 2, 78: 3, 20: 4, 2.4: 5, 0.61: 6, 0.076: 7, 0.019: 8, 0.0024: 9, 0.00060: 10, 0.000074: 11}. (Default value = None) output_mode \"replace\", \"append\". \"replace\" option replaces original columns with transformed column. \"append\" option appends the transformed column to the input dataset with postfix \"_precision_<output_precision>\". (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_geohash , str ): list_of_geohash = [ x . strip () for x in list_of_geohash . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_geohash ): raise TypeError ( \"Invalid input for list_of_geohash\" ) error_precision_mapping = { 2500 : 1 , 630 : 2 , 78 : 3 , 20 : 4 , 2.4 : 5 , 0.61 : 6 , 0.076 : 7 , 0.019 : 8 , 0.0024 : 9 , 0.00060 : 10 , 0.000074 : 11 , } if km_max_error is not None : output_precision = 12 for key , val in error_precision_mapping . items (): if km_max_error >= key : output_precision = val break output_precision = int ( output_precision ) logger . info ( \"Precision of the output geohashes will be capped at \" + str ( output_precision ) + \".\" ) odf = idf for i , geohash in enumerate ( list_of_geohash ): if output_mode == \"replace\" : col_name = geohash else : col_name = geohash + \"_precision_\" + str ( output_precision ) odf = odf . withColumn ( col_name , F . substring ( geohash , 1 , output_precision )) return odf def location_in_polygon ( idf , list_of_lat , list_of_lon , polygon , result_prefix = [], output_mode = \"append\" ): \"\"\" This function checks whether each lat-lon pair is insided a GeoJSON object. The following types of GeoJSON objects are supported by this function: Polygon, MultiPolygon, Feature or FeatureCollection Parameters ---------- idf Input Dataframe. list_of_lat List of columns representing latitude e.g., [\"lat1\",\"lat2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lat2\". list_of_lon List of columns representing longitude e.g., [\"lon1\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lon1|lon2\". list_of_lon must have the same length as list_of_lat such that i-th element of list_of_lat and i-th element of list_of_lon form a lat-lon pair to format. polygon The following types of GeoJSON objects are supported: Polygon, MultiPolygon, Feature or FeatureCollection result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_lat and list_of_lon. If it is empty, <lat>_<lon> will be used for each lat-lon pair. For example, list_of_lat is \"lat1|lat2\", list_of_lon is \"lon1|lon2\". Case 1: result_prefix = \"L1|L2\". New columns will be named as L1_in_poly and L2_in_poly. Calse 2: result_prefix = []. New columns will be named as lat1_lon1_in_poly and lat2_lon2_in_poly. (Default value = []) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_lat , str ): list_of_lat = [ x . strip () for x in list_of_lat . split ( \"|\" )] if isinstance ( list_of_lon , str ): list_of_lon = [ x . strip () for x in list_of_lon . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_lat + list_of_lon ): raise TypeError ( \"Invalid input for list_of_lat or list_of_lon\" ) if len ( list_of_lat ) != len ( list_of_lon ): raise TypeError ( \"list_of_lat and list_of_lon must have the same length\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_lat )): raise TypeError ( \"result_prefix must have the same length as list_of_lat and list_of_lon if it is not empty\" ) if \"coordinates\" in polygon . keys (): polygon_list = polygon [ \"coordinates\" ] if polygon [ \"type\" ] == \"Polygon\" : polygon_list = [ polygon_list ] elif \"geometry\" in polygon . keys (): polygon_list = [ polygon [ \"geometry\" ][ \"coordinates\" ]] elif \"features\" in polygon . keys (): polygon_list = [] for poly in polygon [ \"features\" ]: polygon_list . append ( poly [ \"geometry\" ][ \"coordinates\" ]) odf = idf for i , ( lat , lon ) in enumerate ( zip ( list_of_lat , list_of_lon )): col = result_prefix [ i ] if result_prefix else ( lat + \"_\" + lon ) odf = odf . withColumn ( col + \"_in_poly\" , f_point_in_polygons ( polygon_list )( F . col ( lon ), F . col ( lat )) ) if output_mode == \"replace\" : odf = odf . drop ( lat , lon ) return odf def location_in_country ( spark , idf , list_of_lat , list_of_lon , country , country_shapefile_path = \"\" , method_type = \"approx\" , result_prefix = [], output_mode = \"append\" , ): \"\"\" This function checks whether each lat-lon pair is insided a country. Two ways of checking are supported: \"approx\" (using the bounding box of a country) and \"exact\" (using the shapefile of a country). Parameters ---------- spark Spark Session idf Input Dataframe. list_of_lat List of columns representing latitude e.g., [\"lat1\",\"lat2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lat1|lat2\". list_of_lon List of columns representing longitude e.g., [\"lon1\",\"lon2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \"|\" e.g., \"lon1|lon2\". list_of_lon must have the same length as list_of_lat such that i-th element of list_of_lat and i-th element of list_of_lon form a lat-lon pair to format. country The Alpha-2 country code. country_shapefile_path The geojson file with a FeatureCollection object containing polygons for each country. One example file country_polygons.geojson can be downloaded from Anovos GitHub repository: https://github.com/anovos/anovos/tree/main/data/ method_type \"approx\", \"exact\". \"approx\" uses the bounding box of a country to estimate whether a location is inside the country \"exact\" uses the shapefile of a country to calculate whether a location is inside the country result_prefix List of prefixes for the newly generated column names. Alternatively, prefixes can be specified in a string format, where different prefixes are separated by pipe delimiter \"|\" e.g., \"pf1|pf2\". result_prefix must have the same length as list_of_lat and list_of_lon. If it is empty, <lat>_<lon> will be used for each lat-lon pair. For example, list_of_lat is \"lat1|lat2\", list_of_lon is \"lon1|lon2\". Case 1: result_prefix = \"L1|L2\", country=\"US\" New columns will be named as L1_in_US and L2_in_US. Calse 2: result_prefix = [], country=\"US\" New columns will be named as lat1_lon1_in_US and lat2_lon2_in_US. (Default value = []) output_mode \"replace\", \"append\". \"replace\" option appends transformed column to the input dataset and removes the original ones. \"append\" option appends transformed column to the input dataset. (Default value = \"append\") Returns ------- DataFrame \"\"\" if isinstance ( list_of_lat , str ): list_of_lat = [ x . strip () for x in list_of_lat . split ( \"|\" )] if isinstance ( list_of_lon , str ): list_of_lon = [ x . strip () for x in list_of_lon . split ( \"|\" )] if isinstance ( result_prefix , str ): result_prefix = [ x . strip () for x in result_prefix . split ( \"|\" )] if any ( x not in idf . columns for x in list_of_lat + list_of_lon ): raise TypeError ( \"Invalid input for list_of_lat or list_of_lon\" ) if len ( list_of_lat ) != len ( list_of_lon ): raise TypeError ( \"list_of_lat and list_of_lon must have the same length\" ) if result_prefix and ( len ( result_prefix ) != len ( list_of_lat )): raise TypeError ( \"result_prefix must have the same length as list_of_lat and list_of_lon if it is not empty\" ) if method_type not in ( \"approx\" , \"exact\" ): raise TypeError ( \"Invalid input for method_type.\" ) f_point_in_country_approx = F . udf ( point_in_country_approx , T . IntegerType ()) if method_type == \"exact\" : def zip_feats ( x , y ): \"\"\"zipping two features (in list form) elementwise\"\"\" return zip ( x , y ) f_zip_feats = F . udf ( zip_feats , T . ArrayType ( T . StructType ( [ T . StructField ( \"first\" , T . StringType ()), T . StructField ( \"second\" , T . ArrayType ( T . ArrayType ( T . ArrayType ( T . ArrayType ( T . DoubleType ()))) ), ), ] ) ), ) geo_data = spark . read . json ( country_shapefile_path , multiLine = True ) . withColumn ( \"tmp\" , f_zip_feats ( \"features.properties.ISO_A2\" , \"features.geometry.coordinates\" ), ) polygon_list = ( geo_data . select ( F . explode ( F . col ( \"tmp\" )) . alias ( \"country_coord\" )) . withColumn ( \"country_code\" , F . col ( \"country_coord\" ) . getItem ( \"first\" )) . withColumn ( \"coordinates\" , F . col ( \"country_coord\" ) . getItem ( \"second\" )) . where ( F . col ( \"country_code\" ) == country ) . select ( \"coordinates\" ) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) print ( \"No. of polygon: \" + str ( len ( polygon_list ))) min_lon , min_lat = polygon_list [ 0 ][ 0 ][ 0 ] max_lon , max_lat = polygon_list [ 0 ][ 0 ][ 0 ] for polygon in polygon_list : exterior = polygon [ 0 ] for loc in exterior : if loc [ 0 ] < min_lon : min_lon = loc [ 0 ] elif loc [ 0 ] > max_lon : max_lon = loc [ 0 ] if loc [ 1 ] < min_lat : min_lat = loc [ 1 ] elif loc [ 1 ] > max_lat : max_lat = loc [ 1 ] odf = idf for i , ( lat , lon ) in enumerate ( zip ( list_of_lat , list_of_lon )): col = result_prefix [ i ] if result_prefix else ( lat + \"_\" + lon ) if method_type == \"exact\" : odf = odf . withColumn ( col + \"_in_\" + country + \"_exact\" , f_point_in_polygons ( polygon_list , [ min_lon , min_lat ], [ max_lon , max_lat ] )( F . col ( lon ), F . col ( lat )), ) else : odf = odf . withColumn ( col + \"_in_\" + country + \"_approx\" , f_point_in_country_approx ( F . col ( lat ), F . col ( lon ), F . lit ( country )), ) if output_mode == \"replace\" : odf = odf . drop ( lat , lon ) return odf def centroid ( idf , lat_col , long_col , id_col = None ): \"\"\" This function calculates the centroid of a given DataFrame using lat-long pairs based on its identifier column (if applicable) Parameters ---------- idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data id_col Column in the input DataFrame that contains identifier for the DataFrame (Default is None) If id_col=None, the function will calculate centriod of latitude and longitude for the whole dataset. Returns ------- odf : DataFrame Output DataFrame, which contains lat_centroid and long_centroid and identifier (if applicable) \"\"\" if id_col not in idf . columns and id_col : raise TypeError ( \"Invalid input for id_col\" ) if lat_col not in idf . columns : raise TypeError ( \"Invalid input for lat_col\" ) if long_col not in idf . columns : raise TypeError ( \"Invalid input for long_col\" ) idf = recast_column ( idf , list_of_cols = [ lat_col , long_col ], list_of_dtypes = [ \"double\" , \"double\" ] ) if idf != idf . dropna ( subset = ( lat_col , long_col )): warnings . warn ( \"Rows dropped due to null value in longitude and/or latitude values\" ) idf = idf . dropna ( subset = ( lat_col , long_col )) if not idf . where ( ( F . col ( lat_col ) > 90 ) | ( F . col ( lat_col ) < - 90 ) | ( F . col ( long_col ) > 180 ) | ( F . col ( long_col ) < - 180 ) ) . rdd . isEmpty (): warnings . warn ( \"Rows dropped due to longitude and/or latitude values being out of the valid range\" ) idf = idf . where ( ( F . col ( lat_col ) <= 90 ) & ( F . col ( lat_col ) >= - 90 ) & ( F . col ( long_col ) <= 180 ) & ( F . col ( long_col ) >= - 180 ) ) if idf . rdd . isEmpty (): warnings . warn ( \"No reverse_geocoding Computation - No valid latitude/longitude row(s) to compute\" ) return idf def degree_to_radian ( deg ): return deg * pi / 180 f_degree_to_radian = F . udf ( degree_to_radian , T . FloatType ()) idf_rad = ( idf . withColumn ( \"lat_rad\" , f_degree_to_radian ( lat_col )) . withColumn ( \"long_rad\" , f_degree_to_radian ( long_col )) . withColumn ( \"x\" , F . cos ( \"lat_rad\" ) * F . cos ( \"long_rad\" )) . withColumn ( \"y\" , F . cos ( \"lat_rad\" ) * F . sin ( \"long_rad\" )) . withColumn ( \"z\" , F . sin ( \"lat_rad\" )) ) if id_col : idf_groupby = idf_rad . groupby ( id_col ) . agg ( F . sum ( \"x\" ) . alias ( \"x_group\" ), F . sum ( \"y\" ) . alias ( \"y_group\" ), F . sum ( \"z\" ) . alias ( \"z_group\" ), ) odf = ( idf_groupby . withColumn ( \"hyp\" , F . sqrt ( F . col ( \"x_group\" ) * F . col ( \"x_group\" ) + F . col ( \"y_group\" ) * F . col ( \"y_group\" ) ), ) . withColumn ( lat_col + \"_centroid\" , F . atan2 ( F . col ( \"z_group\" ), F . col ( \"hyp\" )) * 180 / pi , ) . withColumn ( long_col + \"_centroid\" , F . atan2 ( F . col ( \"y_group\" ), F . col ( \"x_group\" )) * 180 / pi , ) . select ( id_col , lat_col + \"_centroid\" , long_col + \"_centroid\" ) ) else : idf_groupby = idf_rad . groupby () . agg ( F . sum ( \"x\" ) . alias ( \"x_group\" ), F . sum ( \"y\" ) . alias ( \"y_group\" ), F . sum ( \"z\" ) . alias ( \"z_group\" ), ) odf = ( idf_groupby . withColumn ( \"hyp\" , F . sqrt ( F . col ( \"x_group\" ) * F . col ( \"x_group\" ) + F . col ( \"y_group\" ) * F . col ( \"y_group\" ) ), ) . withColumn ( lat_col + \"_centroid\" , F . atan2 ( F . col ( \"z_group\" ), F . col ( \"hyp\" )) * 180 / pi , ) . withColumn ( long_col + \"_centroid\" , F . atan2 ( F . col ( \"y_group\" ), F . col ( \"x_group\" )) * 180 / pi , ) . select ( lat_col + \"_centroid\" , long_col + \"_centroid\" ) ) return odf def weighted_centroid ( idf , id_col , lat_col , long_col ): \"\"\" This function calculates the weighted centroid of a given DataFrame using lat-long pairs based on its identifier column Parameters ---------- idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data id_col Column in the input DataFrame that contains identifier for the DataFrame Returns ------- odf : DataFrame Output DataFrame, which contains weighted lat_centroid and long_centroid and identifier \"\"\" if id_col not in idf . columns : raise TypeError ( \"Invalid input for id_col\" ) if lat_col not in idf . columns : raise TypeError ( \"Invalid input for lat_col\" ) if long_col not in idf . columns : raise TypeError ( \"Invalid input for long_col\" ) idf = recast_column ( idf , list_of_cols = [ lat_col , long_col ], list_of_dtypes = [ \"double\" , \"double\" ] ) if idf != idf . dropna ( subset = ( lat_col , long_col )): warnings . warn ( \"Rows dropped due to null value in longitude and/or latitude values\" ) idf = idf . dropna ( subset = ( lat_col , long_col )) if not idf . where ( ( F . col ( lat_col ) > 90 ) | ( F . col ( lat_col ) < - 90 ) | ( F . col ( long_col ) > 180 ) | ( F . col ( long_col ) < - 180 ) ) . rdd . isEmpty (): warnings . warn ( \"Rows dropped due to longitude and/or latitude values being out of the valid range\" ) idf = idf . where ( ( F . col ( lat_col ) <= 90 ) & ( F . col ( lat_col ) >= - 90 ) & ( F . col ( long_col ) <= 180 ) & ( F . col ( long_col ) >= - 180 ) ) if idf . rdd . isEmpty (): warnings . warn ( \"No reverse_geocoding Computation - No valid latitude/longitude row(s) to compute\" ) return idf def degree_to_radian ( deg ): return deg * pi / 180 f_degree_to_radian = F . udf ( degree_to_radian , T . FloatType ()) idf_rad = ( idf . withColumn ( \"lat_rad\" , f_degree_to_radian ( lat_col )) . withColumn ( \"long_rad\" , f_degree_to_radian ( long_col )) . withColumn ( \"x\" , F . cos ( \"lat_rad\" ) * F . cos ( \"long_rad\" )) . withColumn ( \"y\" , F . cos ( \"lat_rad\" ) * F . sin ( \"long_rad\" )) . withColumn ( \"z\" , F . sin ( \"lat_rad\" )) ) idf_groupby = ( idf_rad . groupby ( id_col ) . agg ( F . sum ( \"x\" ) . alias ( \"x_group\" ), F . sum ( \"y\" ) . alias ( \"y_group\" ), F . sum ( \"z\" ) . alias ( \"z_group\" ), F . count ( id_col ) . alias ( \"weight_group\" ), ) . withColumn ( \"weighted_x\" , F . col ( \"x_group\" ) * F . col ( \"weight_group\" )) . withColumn ( \"weighted_y\" , F . col ( \"y_group\" ) * F . col ( \"weight_group\" )) . withColumn ( \"weighted_z\" , F . col ( \"z_group\" ) * F . col ( \"weight_group\" )) ) total_weight = ( idf_groupby . groupBy () . agg ( F . sum ( \"weight_group\" )) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) total_x = ( idf_groupby . groupBy () . agg ( F . sum ( \"weighted_x\" )) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) total_y = ( idf_groupby . groupBy () . agg ( F . sum ( \"weighted_y\" )) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) total_z = ( idf_groupby . groupBy () . agg ( F . sum ( \"weighted_z\" )) . rdd . map ( lambda x : x [ 0 ]) . collect ()[ 0 ] ) x = total_x / total_weight y = total_y / total_weight z = total_z / total_weight hyp = sqrt ( x * x + y * y ) lat_centroid , long_centroid = atan2 ( z , hyp ) * 180 / pi , atan2 ( y , x ) * 180 / pi odf = ( idf_groupby . select ( id_col ) . withColumn ( lat_col + \"_centroid\" , F . lit ( lat_centroid )) . withColumn ( long_col + \"_centroid\" , F . lit ( long_centroid )) ) return odf def rog_calculation ( idf , lat_col , long_col , id_col = None ): \"\"\" This function calculates the Radius of Gyration (in meter) of a given DataFrame, based on its identifier column (if applicable) Parameters ---------- idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data id_col Column in the input DataFrame that contains identifier for the DataFrame (Default is None) Returns ------- odf : DataFrame Output DataFrame, which contains Radius of Gyration (in meter) and identifier (if applicable) \"\"\" if id_col not in idf . columns and id_col : raise TypeError ( \"Invalid input for id_col\" ) if lat_col not in idf . columns : raise TypeError ( \"Invalid input for lat_col\" ) if long_col not in idf . columns : raise TypeError ( \"Invalid input for long_col\" ) idf = recast_column ( idf , list_of_cols = [ lat_col , long_col ], list_of_dtypes = [ \"double\" , \"double\" ] ) if idf != idf . dropna ( subset = ( lat_col , long_col )): warnings . warn ( \"Rows dropped due to null value in longitude and/or latitude values\" ) idf = idf . dropna ( subset = ( lat_col , long_col )) if not idf . where ( ( F . col ( lat_col ) > 90 ) | ( F . col ( lat_col ) < - 90 ) | ( F . col ( long_col ) > 180 ) | ( F . col ( long_col ) < - 180 ) ) . rdd . isEmpty (): warnings . warn ( \"Rows dropped due to longitude and/or latitude values being out of the valid range\" ) idf = idf . where ( ( F . col ( lat_col ) <= 90 ) & ( F . col ( lat_col ) >= - 90 ) & ( F . col ( long_col ) <= 180 ) & ( F . col ( long_col ) >= - 180 ) ) if idf . rdd . isEmpty (): warnings . warn ( \"No reverse_geocoding Computation - No valid latitude/longitude row(s) to compute\" ) return idf def getHaversineDist ( lat1 , lon1 , lat2 , lon2 ): R = 6378126 # approximate radius of earth in m lat1 = radians ( float ( lat1 )) lon1 = radians ( float ( lon1 )) lat2 = radians ( float ( lat2 )) lon2 = radians ( float ( lon2 )) dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 c = 2 * atan2 ( sqrt ( a ), sqrt ( 1 - a )) distance = R * c return distance udf_harver_dist = F . udf ( getHaversineDist , T . FloatType ()) if id_col : idf_centroid = centroid ( idf , lat_col , long_col , id_col ) idf_join = idf_centroid . join ( idf , id_col , \"inner\" ) idf_calc = idf_join . withColumn ( \"distance\" , udf_harver_dist ( F . col ( lat_col ), F . col ( long_col ), F . col ( lat_col + \"_centroid\" ), F . col ( long_col + \"_centroid\" ), ), ) odf = idf_calc . groupby ( id_col ) . agg ( F . mean ( \"distance\" ) . alias ( \"radius_of_gyration\" ) ) else : centroid_info = centroid ( idf , lat_col , long_col , id_col ) . rdd . collect () lat_centroid = centroid_info [ 0 ][ 0 ] long_centroid = centroid_info [ 0 ][ 1 ] idf_join = idf . withColumn ( lat_col + \"_centroid\" , F . lit ( lat_centroid ) ) . withColumn ( long_col + \"_centroid\" , F . lit ( long_centroid )) idf_calc = idf_join . withColumn ( \"distance\" , udf_harver_dist ( F . col ( lat_col ), F . col ( long_col ), F . col ( lat_col + \"_centroid\" ), F . col ( long_col + \"_centroid\" ), ), ) odf = idf_calc . groupby () . agg ( F . mean ( \"distance\" ) . alias ( \"radius_of_gyration\" )) return odf def reverse_geocoding ( idf , lat_col , long_col ): \"\"\" This function reverses the input latitude and longitude of a given DataFrame into address Parameters ---------- idf Input Dataframe lat_col Column in the input DataFrame that contains latitude data long_col Column in the input DataFrame that contains longitude data Returns ------- odf : DataFrame Output DataFrame, which contains latitude, longitude and address appropriately \"\"\" if lat_col not in idf . columns : raise TypeError ( \"Invalid input for lat_col\" ) if long_col not in idf . columns : raise TypeError ( \"Invalid input for long_col\" ) idf = recast_column ( idf , list_of_cols = [ lat_col , long_col ], list_of_dtypes = [ \"double\" , \"double\" ] ) if idf != idf . dropna ( subset = ( lat_col , long_col )): warnings . warn ( \"Rows dropped due to null value in longitude and/or latitude values\" ) idf = idf . dropna ( subset = ( lat_col , long_col )) if not idf . where ( ( F . col ( lat_col ) > 90 ) | ( F . col ( lat_col ) < - 90 ) | ( F . col ( long_col ) > 180 ) | ( F . col ( long_col ) < - 180 ) ) . rdd . isEmpty (): warnings . warn ( \"Rows dropped due to longitude and/or latitude values being out of the valid range\" ) idf = idf . where ( ( F . col ( lat_col ) <= 90 ) & ( F . col ( lat_col ) >= - 90 ) & ( F . col ( long_col ) <= 180 ) & ( F . col ( long_col ) >= - 180 ) ) if idf . rdd . isEmpty (): warnings . warn ( \"No reverse_geocoding Computation - No valid latitude/longitude row(s) to compute\" ) return idf def reverse_geocode ( lat , long ): coordinates = ( float ( lat ), float ( long )) location = rg . search ( coordinates , mode = 1 ) if location : return ( str ( location [ 0 ][ \"name\" ]) + \",\" + str ( location [ 0 ][ \"admin1\" ]) + \",\" + str ( location [ 0 ][ \"cc\" ]) ) else : return \"N/A\" udf_reverse_geocode = F . udf ( reverse_geocode ) odf = ( idf . withColumn ( \"info\" , udf_reverse_geocode ( F . col ( lat_col ), F . col ( long_col ))) . select ( lat_col , long_col , \"info\" ) . withColumn ( \"name_of_place\" , F . split ( F . col ( \"info\" ), \",\" ) . getItem ( 0 )) . withColumn ( \"region\" , F . split ( F . col ( \"info\" ), \",\" ) . getItem ( 1 )) . withColumn ( \"country_code\" , F . split ( F . col ( \"info\" ), \",\" ) . getItem ( 2 )) . drop ( \"info\" ) ) return odf","title":"geospatial"},{"location":"api/data_transformer/geospatial.html#functions","text":"def centroid ( idf, lat_col, long_col, id_col=None) This function calculates the centroid of a given DataFrame using lat-long pairs based on its identifier column (if applicable)","title":"Functions"},{"location":"api/data_transformer/transformers.html","text":"transformers The data transformer module supports selected pre-processing & transformation functions, such as binning, encoding, scaling, imputation, to name a few, which are required for statistics generation and quality checks. Functions supported through this module are listed below: attribute_binning monotonic_binning cat_to_num_transformer cat_to_num_unsupervised cat_to_num_supervised z_standardization IQR_standardization normalization imputation_MMM imputation_sklearn imputation_matrixFactorization auto_imputation autoencoder_latentFeatures PCA_latentFeatures feature_transformation boxcox_transformation outlier_categories expression_parser Expand source code # coding=utf-8 \"\"\" The data transformer module supports selected pre-processing & transformation functions, such as binning, encoding, scaling, imputation, to name a few, which are required for statistics generation and quality checks. Functions supported through this module are listed below: - attribute_binning - monotonic_binning - cat_to_num_transformer - cat_to_num_unsupervised - cat_to_num_supervised - z_standardization - IQR_standardization - normalization - imputation_MMM - imputation_sklearn - imputation_matrixFactorization - auto_imputation - autoencoder_latentFeatures - PCA_latentFeatures - feature_transformation - boxcox_transformation - outlier_categories - expression_parser \"\"\" import copy import os import pickle import random import platform import subprocess import tempfile import warnings from itertools import chain import numpy as np import pandas as pd import pyspark from packaging import version from scipy import stats if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): from pyspark.ml.feature import OneHotEncoderEstimator as OneHotEncoder else : from pyspark.ml.feature import OneHotEncoder from pyspark.ml.evaluation import RegressionEvaluator from pyspark.ml.feature import ( PCA , Imputer , ImputerModel , IndexToString , MinMaxScaler , MinMaxScalerModel , PCAModel , StringIndexer , StringIndexerModel , VectorAssembler , ) from pyspark.ml.linalg import DenseVector from pyspark.ml.recommendation import ALS from pyspark.mllib.stat import Statistics from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql.window import Window from anovos.data_analyzer.stats_generator import ( missingCount_computation , uniqueCount_computation , ) from anovos.data_ingest.data_ingest import read_dataset , recast_column from anovos.data_ingest.data_sampling import data_sample from anovos.shared.utils import ends_with , attributeType_segregation , get_dtype # enable_iterative_imputer is prequisite for importing IterativeImputer # check the following issue for more details https://github.com/scikit-learn/scikit-learn/issues/16833 from sklearn.experimental import enable_iterative_imputer # noqa from sklearn.impute import KNNImputer , IterativeImputer if \"arm64\" not in platform . version () . lower (): import tensorflow from tensorflow.keras.models import load_model , Model from tensorflow.keras.layers import Dense , Input , BatchNormalization , LeakyReLU def attribute_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"equal_range\" , bin_size = 10 , bin_dtype = \"numerical\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Attribute binning (or discretization) is a method of numerical attribute into discrete (integer or categorical values) using pre-defined number of bins. This data pre-processing technique is used to reduce the effects of minor observation errors. Also, Binning introduces non-linearity and tends to improve the performance of the model. In this function, we are focussing on unsupervised way of binning i.e. without taking the target variable into account - Equal Range Binning, Equal Frequency Binning. In Equal Range method, each bin is of equal size/width and computed as: w = max- min / no. of bins *bins cutoff=[min, min+w,min+2w\u2026..,max-w,max]* whereas in Equal Frequency binning method, bins are created in such a way that each bin has equal no. of rows, though the width of bins may vary from each other. w = 1 / no. of bins *bins cutoff=[min, wthpctile, 2wthpctile\u2026.,max ]* Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Number of bins. (Default value = 10) bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") pre_existing_model Boolean argument \u2013 True or False. True if binning model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print the number of categories generated for each attribute (may or may be not same as bin_size) Returns ------- DataFrame Binned Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Binning Performed - No numerical column(s) to transform\" ) return idf if method_type not in ( \"equal_frequency\" , \"equal_range\" ): raise TypeError ( \"Invalid input for method_type\" ) if bin_size < 2 : raise TypeError ( \"Invalid input for bin_size\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/attribute_binning\" ) bin_cutoffs = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) bin_cutoffs . append ( mapped_value ) else : if method_type == \"equal_frequency\" : pctile_width = 1 / bin_size pctile_cutoff = [] for j in range ( 1 , bin_size ): pctile_cutoff . append ( j * pctile_width ) bin_cutoffs = idf . approxQuantile ( list_of_cols , pctile_cutoff , 0.01 ) else : funs = [ F . max , F . min ] exprs = [ f ( F . col ( c )) for f in funs for c in list_of_cols ] list_result = idf . groupby () . agg ( * exprs ) . rdd . flatMap ( lambda x : x ) . collect () bin_cutoffs = [] drop_col_process = [] for i in range ( int ( len ( list_result ) / 2 )): bin_cutoff = [] max_val = list_result [ i ] min_val = list_result [ i + int ( len ( list_result ) / 2 )] if not max_val and max_val != 0 : drop_col_process . append ( list_of_cols [ i ]) continue bin_width = ( max_val - min_val ) / bin_size for j in range ( 1 , bin_size ): bin_cutoff . append ( min_val + j * bin_width ) bin_cutoffs . append ( bin_cutoff ) if drop_col_process : warnings . warn ( \"Columns contains too much null values. Dropping \" + \", \" . join ( drop_col_process ) ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_col_process ]) ) if model_path != \"NA\" : df_model = spark . createDataFrame ( zip ( list_of_cols , bin_cutoffs ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . write . parquet ( model_path + \"/attribute_binning\" , mode = \"overwrite\" ) def bucket_label ( value , index ): if value is None : return None for i in range ( 0 , len ( bin_cutoffs [ index ])): if value <= bin_cutoffs [ index ][ i ]: if bin_dtype == \"numerical\" : return i + 1 else : if i == 0 : return \"<= \" + str ( round ( bin_cutoffs [ index ][ i ], 4 )) else : return ( str ( round ( bin_cutoffs [ index ][ i - 1 ], 4 )) + \"-\" + str ( round ( bin_cutoffs [ index ][ i ], 4 )) ) else : next if bin_dtype == \"numerical\" : return len ( bin_cutoffs [ 0 ]) + 1 else : return \"> \" + str ( round ( bin_cutoffs [ index ][ len ( bin_cutoffs [ 0 ]) - 1 ], 4 )) if bin_dtype == \"numerical\" : f_bucket_label = F . udf ( bucket_label , T . IntegerType ()) else : f_bucket_label = F . udf ( bucket_label , T . StringType ()) odf = idf for idx , i in enumerate ( list_of_cols ): odf = odf . withColumn ( i + \"_binned\" , f_bucket_label ( F . col ( i ), F . lit ( idx ))) if output_mode == \"replace\" : for col in list_of_cols : odf = odf . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_binned\" ) for i in list_of_cols ] uniqueCount_computation ( spark , odf , output_cols ) . show ( len ( output_cols ), False ) return odf def monotonic_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , bin_method = \"equal_range\" , bin_size = 10 , bin_dtype = \"numerical\" , output_mode = \"replace\" , ): \"\"\" This function constitutes supervised way of binning the numerical attribute into discrete (integer or categorical values) attribute. Instead of pre-defined fixed number of bins, number of bins are dynamically computed to ensure the monotonic nature of bins i.e. % event should increase or decrease with the bin. Monotonic nature of bins is evaluated by looking at spearman rank correlation, which should be either +1 or -1, between the bin index and % event. In case, the monotonic nature is not attained, user defined fixed number of bins are used for the binning. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) bin_method \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Default number of bins in case monotonicity is not achieved. bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") Returns ------- DataFrame Binned Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) odf = idf for col in list_of_cols : n = 20 r = 0 while n > 2 : tmp = ( attribute_binning ( spark , idf , [ col ], drop_cols = [], method_type = bin_method , bin_size = n , output_mode = \"append\" , ) . select ( label_col , col , col + \"_binned\" ) . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col + \"_binned\" ) . agg ( F . avg ( col ) . alias ( \"mean_val\" ), F . avg ( label_col ) . alias ( \"mean_label\" )) . dropna () ) r , p = stats . spearmanr ( tmp . toPandas ()[[ \"mean_val\" ]], tmp . toPandas ()[[ \"mean_label\" ]] ) if r == 1.0 : odf = attribute_binning ( spark , odf , [ col ], drop_cols = [], method_type = bin_method , bin_size = n , bin_dtype = bin_dtype , output_mode = output_mode , ) break n = n - 1 r = 0 if r < 1.0 : odf = attribute_binning ( spark , odf , [ col ], drop_cols = [], method_type = bin_method , bin_size = bin_size , bin_dtype = bin_dtype , output_mode = output_mode , ) return odf def cat_to_num_transformer ( spark , idf , list_of_cols , drop_cols , method_type , encoding , label_col , event_label ): \"\"\" This is method which helps converting a categorical attribute into numerical attribute(s) based on the analysis dataset. If there's a presence of label column then the relevant processing would happen through cat_to_num_supervised. However, for unsupervised scenario, the processing would happen through cat_to_num_unsupervised. Computation details can be referred from the respective functions. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. method_type Depending upon the use case the method type can be either Supervised or Unsupervised. For Supervised use case, label_col is mandatory. encoding \"label_encoding\" or \"onehot_encoding\" In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available that can be selected by index_order argument). In one-hot encoding, every unique value in the column will be added in a form of dummy/binary column. label_col Label/Target column event_label Value of (positive) event \"\"\" num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if len ( cat_cols ) > 0 : if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if ( method_type == \"supervised\" ) & ( label_col is not None ): odf = cat_to_num_supervised ( spark , idf , label_col = label_col , event_label = event_label ) odf = odf . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , F . lit ( 1 )) . otherwise ( F . lit ( 0 )), ) return odf elif ( method_type == \"unsupervised\" ) & ( label_col is None ): odf = cat_to_num_unsupervised ( spark , idf , method_type = encoding , index_order = \"frequencyDesc\" , ) return odf else : return idf def cat_to_num_unsupervised ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"label_encoding\" , index_order = \"frequencyDesc\" , cardinality_threshold = 50 , pre_existing_model = False , model_path = \"NA\" , stats_unique = {}, output_mode = \"replace\" , print_impact = False , ): \"\"\" This is unsupervised method of converting a categorical attribute into numerical attribute(s). This is among the most important transformations required for any modelling exercise, as most of the machine learning algorithms cannot process categorical values. It covers two popular encoding techniques \u2013 label encoding & one-hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available \u2013 can be selected by index_order argument). One of the pitfalls of using this technique is that the model may learn some spurious relationship, which doesn't exist or might not make any logical sense in the real world settings. In one-hot encoding, every unique value in the attribute will be added as a feature in a form of dummy/binary attribute. However, using this method on high cardinality attributes can further aggravate the dimensionality issue. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"label_encoding\" or \"onehot_encoding\" In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available that can be selected by index_order argument). In one-hot encoding, every unique value in the column will be added in a form of dummy/binary column. (Default value = 1) index_order \"frequencyDesc\", \"frequencyAsc\", \"alphabetDesc\", \"alphabetAsc\". Valid only for Label Encoding method_type. (Default value = \"frequencyDesc\") cardinality_threshold Defines threshold to skip columns with higher cardinality values from encoding - a warning is issued. (Default value = 50) pre_existing_model Boolean argument - True or False. True if encoding model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre existing model nor there is a need to save one. stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_index\" for label encoding e.g. column X is appended as X_index, or a postfix \"_{n}\" for one hot encoding, n varies from 0 to unique value count e.g. column X is appended as X_0, X_1, X_2 (n = 3 i.e. no. of unique values for X). (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the change in schema (one hot encoding) or descriptive statistics (label encoding) Returns ------- DataFrame Encoded Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"onehot_encoding\" , \"label_encoding\" ): raise TypeError ( \"Invalid input for method_type\" ) if index_order not in ( \"frequencyDesc\" , \"frequencyAsc\" , \"alphabetDesc\" , \"alphabetAsc\" , ): raise TypeError ( \"Invalid input for Encoding Index Order\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if stats_unique == {}: skip_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) > cardinality_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : skip_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) > cardinality_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) skip_cols = list ( set ([ e for e in skip_cols if e in list_of_cols and e not in drop_cols ]) ) if skip_cols : warnings . warn ( \"Columns dropped from encoding due to high cardinality: \" + \",\" . join ( skip_cols ) ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols + skip_cols ]) ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Encoding Computation - No categorical column(s) to transform\" ) return idf list_of_cols_vec = [] list_of_cols_idx = [] for i in list_of_cols : list_of_cols_vec . append ( i + \"_vec\" ) list_of_cols_idx . append ( i + \"_index\" ) odf_indexed = idf if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): for idx , i in enumerate ( list_of_cols ): if pre_existing_model : indexerModel = StringIndexerModel . load ( model_path + \"/cat_to_num_unsupervised/indexer-model/\" + i ) else : stringIndexer = StringIndexer ( inputCol = i , outputCol = i + \"_index\" , stringOrderType = index_order , handleInvalid = \"keep\" , ) indexerModel = stringIndexer . fit ( idf . select ( i )) if model_path != \"NA\" : indexerModel . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/indexer-model/\" + i ) odf_indexed = indexerModel . transform ( odf_indexed ) else : if pre_existing_model : indexerModel = StringIndexerModel . load ( model_path + \"/cat_to_num_unsupervised/indexer\" ) else : stringIndexer = StringIndexer ( inputCols = list_of_cols , outputCols = list_of_cols_idx , stringOrderType = index_order , handleInvalid = \"keep\" , ) indexerModel = stringIndexer . fit ( odf_indexed ) if model_path != \"NA\" : indexerModel . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/indexer\" ) odf_indexed = indexerModel . transform ( odf_indexed ) if method_type == \"onehot_encoding\" : if pre_existing_model : encoder = OneHotEncoder . load ( model_path + \"/cat_to_num_unsupervised/encoder\" ) else : encoder = OneHotEncoder ( inputCols = list_of_cols_idx , outputCols = list_of_cols_vec , handleInvalid = \"keep\" , ) if model_path != \"NA\" : encoder . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/encoder\" ) odf = encoder . fit ( odf_indexed ) . transform ( odf_indexed ) new_cols = [] odf_sample = odf . take ( 1 ) for i in list_of_cols : odf_schema = odf . schema uniq_cats = odf_sample [ 0 ] . asDict ()[ i + \"_vec\" ] . size for j in range ( 0 , uniq_cats ): odf_schema = odf_schema . add ( T . StructField ( i + \"_\" + str ( j ), T . IntegerType ()) ) new_cols . append ( i + \"_\" + str ( j )) odf = odf . rdd . map ( lambda x : ( * x , * ( DenseVector ( x [ i + \"_vec\" ]) . toArray () . astype ( int ) . tolist ()), ) ) . toDF ( schema = odf_schema ) if output_mode == \"replace\" : odf = odf . drop ( i , i + \"_vec\" , i + \"_index\" ) else : odf = odf . drop ( i + \"_vec\" , i + \"_index\" ) else : odf = odf_indexed for i in list_of_cols : odf = odf . withColumn ( i + \"_index\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . col ( i + \"_index\" ) . cast ( T . IntegerType ()) ), ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_index\" , i ) odf = odf . select ( idf . columns ) if print_impact : if method_type == \"label_encoding\" : if output_mode == \"append\" : new_cols = [ i + \"_index\" for i in list_of_cols ] else : new_cols = list_of_cols print ( \"Before\" ) idf . select ( list_of_cols ) . summary ( \"count\" , \"min\" , \"max\" ) . show ( 3 , False ) print ( \"After\" ) odf . select ( new_cols ) . summary ( \"count\" , \"min\" , \"max\" ) . show ( 3 , False ) if method_type == \"onehot_encoding\" : print ( \"Before\" ) idf . select ( list_of_cols ) . printSchema () print ( \"After\" ) if output_mode == \"append\" : odf . select ( list_of_cols + new_cols ) . printSchema () else : odf . select ( new_cols ) . printSchema () if skip_cols : print ( \"Columns dropped from encoding due to high cardinality: \" + \",\" . join ( skip_cols ) ) return odf def cat_to_num_supervised ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , persist = False , persist_option = pyspark . StorageLevel . MEMORY_AND_DISK , print_impact = False , ): \"\"\" This is a supervised method to convert a categorical attribute into a numerical attribute. It takes a label/target column to indicate whether the event is positive or negative. For each column, the positive event rate for each categorical value is used as the encoded numerical value. For example, there are 3 distinct values in a categorical attribute X: X1, X2 and X3. Within the input dataframe, there are - 15 positive events and 5 negative events with X==X1; - 10 positive events and 40 negative events with X==X2; - 20 positive events and 20 negative events with X==X3. Thus, value X1 is mapped to 15/(15+5) = 0.75, value X2 is mapped to 10/(10+40) = 0.2 and value X3 is mapped to 20/(20+20) = 0.5. This mapping will be applied to all values in attribute X. This encoding method can be helpful in avoiding creating too many dummy variables which may cause dimensionality issue and it also works with categorical attributes without an order or rank. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) pre_existing_model Boolean argument \u2013 True or False. True if model (original and mapped numerical value for each column) exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" is used to save the model in \"intermediate_data/\" folder for optimization purpose. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_encoded\" e.g. column X is appended as X_encoded. (Default value = \"replace\") persist Boolean argument - True or False. This parameter is for optimization purpose. If True, repeatedly used dataframe will be persisted (StorageLevel can be specified in persist_option). We recommend setting this parameter as True if at least one of the following criteria is True: (1) The underlying data source is in csv format (2) The transformation will be applicable to most columns. (Default value = False) persist_option A pyspark.StorageLevel instance. This parameter is useful only when persist is True. (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) print_impact True, False (Default value = False) This argument is to print out the descriptive statistics of encoded columns. Returns ------- DataFrame Encoded Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols elif isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e != label_col )]) ) if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Categorical Encoding - No categorical column(s) to transform\" ) return idf if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) odf = idf label_col_bool = label_col + \"_cat_to_num_sup_temp\" idf = idf . withColumn ( label_col_bool , F . when ( F . col ( label_col ) == event_label , \"1\" ) . otherwise ( \"0\" ), ) if model_path == \"NA\" : skip_if_error = True model_path = \"intermediate_data\" else : skip_if_error = False save_model = True if persist : idf = idf . persist ( persist_option ) for index , i in enumerate ( list_of_cols ): if pre_existing_model : df_tmp = spark . read . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , inferSchema = True , ) else : df_tmp = ( idf . select ( i , label_col_bool ) . groupBy ( i ) . pivot ( label_col_bool ) . count () . fillna ( 0 ) . withColumn ( i + \"_encoded\" , F . round ( F . col ( \"1\" ) / ( F . col ( \"1\" ) + F . col ( \"0\" )), 4 ) ) . drop ( * [ \"1\" , \"0\" ]) ) if save_model : try : df_tmp . coalesce ( 1 ) . write . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , mode = \"overwrite\" , ignoreLeadingWhiteSpace = False , ignoreTrailingWhiteSpace = False , ) df_tmp = spark . read . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , inferSchema = True , ) except Exception as error : if skip_if_error : warnings . warn ( \"For optimization purpose, we recommend specifying a valid model_path value to save the intermediate data. Saving to the default path - '\" + model_path + \"/cat_to_num_supervised/\" + i + \"' faced an error.\" ) save_model = False else : raise error if df_tmp . count () > 1 : odf = odf . join ( df_tmp , i , \"left_outer\" ) else : odf = odf . crossJoin ( df_tmp ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_encoded\" , i ) odf = odf . select ([ i for i in idf . columns if i != label_col_bool ]) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_encoded\" ) for i in list_of_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . summary ( \"count\" , \"min\" , \"max\" ) . show ( 3 , False ) print ( \"After: \" ) odf . select ( output_cols ) . summary ( \"count\" , \"min\" , \"max\" ) . show ( 3 , False ) if persist : idf . unpersist () return odf def z_standardization ( spark , idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Standardization is commonly used in data pre-processing process. z_standardization standardizes the selected attributes of an input dataframe by normalizing each attribute to have standard deviation of 1 and mean of 0. For each attribute, the standard deviation (s) and mean (u) are calculated and a sample x will be standardized into ( x-u)/s. If the standard deviation of an attribute is 0, it will be excluded in standardization and a warning will be shown. None values will be kept as None in the output dataframe. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (Mean/stddev for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Standardization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) parameters = [] excluded_cols = [] if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/z_standardization\" ) for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"feature\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : for i in list_of_cols : mean , stddev = idf . select ( F . mean ( i ), F . stddev ( i )) . first () parameters . append ( [ float ( mean ) if mean else None , float ( stddev ) if stddev else None ] ) if stddev : if round ( stddev , 5 ) == 0.0 : excluded_cols . append ( i ) else : excluded_cols . append ( i ) if len ( excluded_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from standardization because the standard deviation is zero:\" + str ( excluded_cols ) ) odf = idf for index , i in enumerate ( list_of_cols ): if i not in excluded_cols : modify_col = ( i + \"_scaled\" ) if ( output_mode == \"append\" ) else i odf = odf . withColumn ( modify_col , ( F . col ( i ) - parameters [ index ][ 0 ]) / parameters [ index ][ 1 ] ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( list_of_cols , parameters ), schema = [ \"feature\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/z_standardization\" , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [ ( i + \"_scaled\" ) for i in list_of_cols if i not in excluded_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def IQR_standardization ( spark , idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (25/50/75 percentile for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Standardization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/IQR_standardization\" ) parameters = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"feature\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : parameters = idf . approxQuantile ( list_of_cols , [ 0.25 , 0.5 , 0.75 ], 0.01 ) excluded_cols = [] for i , param in zip ( list_of_cols , parameters ): if len ( param ) > 0 : if round ( param [ 0 ], 5 ) == round ( param [ 2 ], 5 ): excluded_cols . append ( i ) else : excluded_cols . append ( i ) if len ( excluded_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from standardization because the 75th and 25th percentiles are the same:\" + str ( excluded_cols ) ) odf = idf for index , i in enumerate ( list_of_cols ): if i not in excluded_cols : modify_col = ( i + \"_scaled\" ) if ( output_mode == \"append\" ) else i odf = odf . withColumn ( modify_col , ( F . col ( i ) - parameters [ index ][ 1 ]) / ( parameters [ index ][ 2 ] - parameters [ index ][ 0 ]), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( list_of_cols , parameters ), schema = [ \"feature\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/IQR_standardization\" , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [ ( i + \"_scaled\" ) for i in list_of_cols if i not in excluded_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def normalization ( idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if normalization/scalar model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Normalization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) idf_id = idf . withColumn ( \"tempID\" , F . monotonically_increasing_id ()) idf_partial = idf_id . select ([ \"tempID\" ] + list_of_cols ) assembler = VectorAssembler ( inputCols = list_of_cols , outputCol = \"list_of_cols_vector\" , handleInvalid = \"keep\" ) assembled_data = assembler . transform ( idf_partial ) if pre_existing_model : scalerModel = MinMaxScalerModel . load ( model_path + \"/normalization\" ) else : scaler = MinMaxScaler ( inputCol = \"list_of_cols_vector\" , outputCol = \"list_of_cols_scaled\" ) scalerModel = scaler . fit ( assembled_data ) if model_path != \"NA\" : scalerModel . write () . overwrite () . save ( model_path + \"/normalization\" ) scaledData = scalerModel . transform ( assembled_data ) def vector_to_array ( v ): return v . toArray () . tolist () f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . FloatType ())) odf_partial = scaledData . withColumn ( \"list_of_cols_array\" , f_vector_to_array ( \"list_of_cols_scaled\" ) ) . drop ( * [ \"list_of_cols_scaled\" , \"list_of_cols_vector\" ]) odf_schema = odf_partial . schema for i in list_of_cols : odf_schema = odf_schema . add ( T . StructField ( i + \"_scaled\" , T . FloatType ())) odf_partial = ( odf_partial . rdd . map ( lambda x : ( * x , * x [ \"list_of_cols_array\" ])) . toDF ( schema = odf_schema ) . drop ( \"list_of_cols_array\" ) ) odf = idf_id . join ( odf_partial . drop ( * list_of_cols ), \"tempID\" , \"left_outer\" ) . select ( idf . columns + [ ( F . when ( F . isnan ( F . col ( i + \"_scaled\" )), None ) . otherwise ( F . col ( i + \"_scaled\" ) ) ) . alias ( i + \"_scaled\" ) for i in list_of_cols ] ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_scaled\" , i ) odf = odf . select ( idf . columns ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_scaled\" ) for i in list_of_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def imputation_MMM ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], method_type = \"median\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_missing = {}, stats_mode = {}, print_impact = False , ): \"\"\" This function handles missing value related issues by substituting null values by the measure of central tendency (mode for categorical features and mean/median for numerical features). For numerical attributes, it leverages [Imputer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer .html) functionality of Spark MLlib. Though, Imputer can be used for categorical attributes but this feature is available only in Spark3.x, therefore for categorical features, we compute mode or leverage mode computation from Measures of Central Tendency. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"median\", \"mean\" (valid only for for numerical columns attributes). Mode is only option for categorical columns. (Default value = \"median\") pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf ) else : missing_df = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) if ( len ( missing_cols ) == 0 ) & ( not pre_existing_model ) & ( model_path == \"NA\" ): return idf num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols + cat_cols if list_of_cols == \"missing\" : list_of_cols = [ x for x in missing_cols if x in num_cols + cat_cols ] if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Imputation performed- No column(s) to impute\" ) return idf if any ( x not in num_cols + cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"mode\" , \"mean\" , \"median\" ): raise TypeError ( \"Invalid input for method_type\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) odf = idf if len ( num_cols ) > 0 : recast_cols = [] recast_type = [] for i in num_cols : if get_dtype ( idf , i ) not in ( \"float\" , \"double\" ): odf = odf . withColumn ( i , F . col ( i ) . cast ( T . DoubleType ())) recast_cols . append ( i + \"_imputed\" ) recast_type . append ( get_dtype ( idf , i )) # For mode imputation if method_type == \"mode\" : if stats_mode == {}: parameters = [ str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) for i in num_cols ] else : mode_df = read_dataset ( spark , ** stats_mode ) . replace ( \"None\" , None ) mode_df_cols = list ( mode_df . select ( \"attribute\" ) . toPandas ()[ \"attribute\" ]) parameters = [] for i in num_cols : if i not in mode_df_cols : parameters . append ( str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) ) else : parameters . append ( mode_df . where ( F . col ( \"attribute\" ) == i ) . select ( \"mode\" ) . rdd . flatMap ( list ) . collect ()[ 0 ] ) for index , i in enumerate ( num_cols ): odf = odf . withColumn ( i + \"_imputed\" , F . when ( F . col ( i ) . isNull (), parameters [ index ]) . otherwise ( F . col ( i )), ) else : # For mean, median imputation # Building new imputer model or uploading the existing model if pre_existing_model : imputerModel = ImputerModel . load ( model_path + \"/imputation_MMM/num_imputer-model\" ) else : imputer = Imputer ( strategy = method_type , inputCols = num_cols , outputCols = [( e + \"_imputed\" ) for e in num_cols ], ) imputerModel = imputer . fit ( odf ) # Applying model # odf = recast_column(imputerModel.transform(odf), recast_cols, recast_type) odf = imputerModel . transform ( odf ) for i , j in zip ( recast_cols , recast_type ): odf = odf . withColumn ( i , F . col ( i ) . cast ( j )) # Saving model if required if ( not pre_existing_model ) & ( model_path != \"NA\" ): imputerModel . write () . overwrite () . save ( model_path + \"/imputation_MMM/num_imputer-model\" ) if len ( cat_cols ) > 0 : if pre_existing_model : df_model = spark . read . csv ( model_path + \"/imputation_MMM/cat_imputer\" , header = True , inferSchema = True , ) parameters = [] for i in cat_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : if stats_mode == {}: parameters = [ str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) for i in cat_cols ] else : mode_df = read_dataset ( spark , ** stats_mode ) . replace ( \"None\" , None ) parameters = [ mode_df . where ( F . col ( \"attribute\" ) == i ) . select ( \"mode\" ) . rdd . flatMap ( list ) . collect ()[ 0 ] for i in cat_cols ] for index , i in enumerate ( cat_cols ): odf = odf . withColumn ( i + \"_imputed\" , F . when ( F . col ( i ) . isNull (), parameters [ index ]) . otherwise ( F . col ( i )), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( cat_cols , parameters ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . repartition ( 1 ) . write . csv ( model_path + \"/imputation_MMM/cat_imputer\" , header = True , mode = \"overwrite\" , ) for i in num_cols + cat_cols : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) elif output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : output_cols = [ ( i + \"_imputed\" ) for i in [ e for e in ( num_cols + cat_cols ) if e in missing_cols ] ] odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def imputation_sklearn ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], missing_threshold = 1.0 , method_type = \"regression\" , use_sampling = True , sample_method = \"random\" , strata_cols = \"all\" , stratified_type = \"population\" , sample_size = 10000 , sample_seed = 42 , persist = True , persist_option = pyspark . StorageLevel . MEMORY_AND_DISK , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_missing = {}, run_type = \"local\" , auth_key = \"NA\" , print_impact = False , ): \"\"\" The function \"imputation_sklearn\" leverages sklearn imputer algorithms. Two methods are supported via this function: \u201cKNN\u201d and \u201cregression\u201d. \u201cKNN\u201d option trains a sklearn.impute.KNNImputer which is based on k-Nearest Neighbors algorithm. The missing values of a sample are imputed using the mean of its 5 nearest neighbors in the training set. \u201cregression\u201d option trains a sklearn.impute.IterativeImputer which models attribute to impute as a function of rest of the attributes and imputes using the estimation. Imputation is performed in an iterative way from attributes with fewest number of missing values to most. All the hyperparameters used in the above mentioned imputers are their default values. However, sklearn imputers are not scalable, which might be slow if the size of the input dataframe is large. In fact, if the input dataframe size exceeds 10 GigaBytes, the model fitting step powered by sklearn might fail. Thus, an input sample_size (the default value is 10,000) can be set to control the number of samples to be used to train the imputer. If the total number of input dataset exceeds sample_size, the rest of the samples will be imputed using the trained imputer in a scalable manner. This is one of the way to demonstrate how Anovos has been designed as a scalable feature engineering library. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) missing_threshold Float argument - If list_of_cols is \"missing\", this argument is used to determined the missing threshold for every column. The column that has more (count of missing value/ count of total value) >= missing_threshold will be excluded from the list of columns to be imputed. (Default value = 1.0) method_type \"KNN\", \"regression\". \"KNN\" option trains a sklearn.impute.KNNImputer. \"regression\" option trains a sklearn.impute.IterativeImputer (Default value = \"regression\") use_sampling Boolean argument - True or False. This argument is used to determine whether to use sampling on source and target dataset, True will enable the use of sample method, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) sample_method If use_sampling is True, this argument is used to determine the sampling method. \"stratified\" for Stratified sampling, \"random\" for Random Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"random\") strata_cols If use_sampling is True and sample_method is \"stratified\", this argument is used to determine the list of columns used to be treated as strata. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"all\") stratified_type If use_sampling is True and sample_method is \"stratified\", this argument is used to determine the stratified sampling method. \"population\" stands for Proportionate Stratified Sampling, \"balanced\" stands for Optimum Stratified Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"population\") sample_size If use_sampling is True, this argument is used to determine maximum rows for training the sklearn imputer (Default value = 10000) sample_seed If use_sampling is True, this argument is used to determine the seed of sampling method. (Default value = 42) persist Boolean argument - True or False. This argument is used to determine whether to persist on binning result of source and target dataset, True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) persist_option If persist is True, this argument is used to determine the type of persist. For all the pyspark.StorageLevel option available in persist, please refer to https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" if persist : idf = idf . persist ( persist_option ) num_cols = attributeType_segregation ( idf )[ 0 ] if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , num_cols ) else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( num_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns dropped from the imputation as all values are null: \" + \",\" . join ( empty_cols ) ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . where ( F . col ( \"missing_pct\" ) < missing_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = num_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e not in empty_cols )]) ) if len ( list_of_cols ) <= 1 : warnings . warn ( \"No Imputation Performed - No Column(s) or Insufficient Column(s) to Impute\" ) return idf if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) if ( ( len ([ e for e in list_of_cols if e in missing_cols ]) == 0 ) & ( not pre_existing_model ) & ( model_path == \"NA\" ) ): warnings . warn ( \"No Imputation Performed - No Column(s) to Impute and No Imputation Model to be saved\" ) return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"KNN\" , \"regression\" ): raise TypeError ( \"Invalid input for method_type\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : if run_type == \"emr\" : bash_cmd = \"aws s3 cp \" + model_path + \"/imputation_sklearn.sav .\" output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) elif run_type == \"ak8s\" : bash_cmd = ( 'azcopy cp \"' + model_path + \"/imputation_sklearn.sav\" + str ( auth_key ) + '\" .' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) else : imputer = pickle . load ( open ( model_path + \"/imputation_sklearn.sav\" , \"rb\" )) else : if use_sampling : count_idf = idf . count () if count_idf > sample_size : idf_model = data_sample ( idf , strata_cols = strata_cols , fraction = sample_size / count_idf , method_type = sample_method , stratified_type = stratified_type , seed_value = sample_seed , ) else : idf_model = idf else : idf_model = idf if persist : idf_model = idf_model . persist ( persist_option ) idf_pd = idf_model . select ( list_of_cols ) . toPandas () if method_type == \"KNN\" : imputer = KNNImputer ( n_neighbors = 5 , weights = \"uniform\" , metric = \"nan_euclidean\" ) if method_type == \"regression\" : imputer = IterativeImputer () imputer . fit ( idf_pd ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): if run_type == \"emr\" : pickle . dump ( imputer , open ( \"imputation_sklearn.sav\" , \"wb\" )) bash_cmd = ( \"aws s3 cp imputation_sklearn.sav \" + model_path + \"/imputation_sklearn.sav\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) elif run_type == \"ak8s\" : pickle . dump ( imputer , open ( \"imputation_sklearn.sav\" , \"wb\" )) bash_cmd = ( 'azcopy cp \"imputation_sklearn.sav\" \"' + ends_with ( model_path ) + \"imputation_sklearn.sav\" + str ( auth_key ) + '\"' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) else : local_path = model_path + \"/imputation_sklearn.sav\" os . makedirs ( os . path . dirname ( local_path ), exist_ok = True ) pickle . dump ( imputer , open ( local_path , \"wb\" )) imputer = pickle . load ( open ( model_path + \"/imputation_sklearn.sav\" , \"rb\" ) ) @F . pandas_udf ( returnType = T . ArrayType ( T . DoubleType ())) def prediction ( * cols ): input_pdf = pd . concat ( cols , axis = 1 ) return pd . Series ( row . tolist () for row in imputer . transform ( input_pdf )) result_df = idf . withColumn ( \"features\" , prediction ( * list_of_cols )) if persist : result_df = result_df . persist ( persist_option ) odf_schema = result_df . schema for i in list_of_cols : odf_schema = odf_schema . add ( T . StructField ( i + \"_imputed\" , T . FloatType ())) odf = ( result_df . rdd . map ( lambda x : ( * x , * x [ \"features\" ])) . toDF ( schema = odf_schema ) . drop ( \"features\" ) ) output_cols = [] for i in list_of_cols : if output_mode == \"append\" : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) else : output_cols . append ( i + \"_imputed\" ) else : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) odf = odf . select ( idf . columns + output_cols ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) if persist : idf . unpersist () if not pre_existing_model : idf_model . unpersist () result_df . unpersist () return odf def imputation_matrixFactorization ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], id_col = \"\" , output_mode = \"replace\" , stats_missing = {}, print_impact = False , ): \"\"\" imputation_matrixFactorization uses collaborative filtering technique to impute missing values. Collaborative filtering is commonly used in recommender systems to fill the missing user-item entries and PySpark provides an implementation using alternating least squares (ALS) algorithm, which is used in this function. To fit our problem into the ALS model, each attribute is treated as an item and an id column needs to be specified by the user to generate the user-item pairs. In the case, ID column doesn't exist in the dataset and proxu ID column is implicitly generated by the function. Subsequently, all user-item pairs with known values will be used to train the ALS model and the trained model can be used to predict the user-item pairs with missing values. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , num_cols ) else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( num_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns dropped from the imputation as all values are null: \" + \",\" . join ( empty_cols ) ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . where ( F . col ( \"missing_pct\" ) < 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = num_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ( [ e for e in list_of_cols if ( e not in drop_cols ) & ( e != id_col ) & ( e not in empty_cols ) ] ) ) if ( len ( list_of_cols ) == 0 ) | ( len ([ e for e in list_of_cols if e in missing_cols ]) == 0 ): warnings . warn ( \"No Imputation Performed - No Column(s) to Impute\" ) return idf if len ( list_of_cols ) == 1 : warnings . warn ( \"No Imputation Performed - Needs more than 1 column for matrix factorization\" ) return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) remove_id = False if id_col == \"\" : idf = idf . withColumn ( \"id\" , F . monotonically_increasing_id ()) . withColumn ( \"id\" , F . row_number () . over ( Window . orderBy ( \"id\" )) ) id_col = \"id\" remove_id = True key_and_val = F . create_map ( list ( chain . from_iterable ([[ F . lit ( c ), F . col ( c )] for c in list_of_cols ])) ) df_flatten = idf . select ( id_col , F . explode ( key_and_val )) . withColumn ( \"key\" , F . concat ( F . col ( \"key\" ), F . lit ( \"_imputed\" )) ) id_type = get_dtype ( idf , id_col ) if id_type == \"string\" : id_indexer = StringIndexer () . setInputCol ( id_col ) . setOutputCol ( \"IDLabel\" ) id_indexer_model = id_indexer . fit ( df_flatten ) df_flatten = id_indexer_model . transform ( df_flatten ) . drop ( id_col ) else : df_flatten = df_flatten . withColumnRenamed ( id_col , \"IDLabel\" ) indexer = StringIndexer () . setInputCol ( \"key\" ) . setOutputCol ( \"keyLabel\" ) indexer_model = indexer . fit ( df_flatten ) df_encoded = indexer_model . transform ( df_flatten ) . drop ( \"key\" ) df_model = df_encoded . where ( F . col ( \"value\" ) . isNotNull ()) df_test = df_encoded . where ( F . col ( \"value\" ) . isNull ()) if ( df_model . select ( \"IDLabel\" ) . distinct () . count () < df_encoded . select ( \"IDLabel\" ) . distinct () . count () ): warnings . warn ( \"The returned odf may not be fully imputed because values for all list_of_cols are null for some IDs\" ) als = ALS ( maxIter = 20 , regParam = 0.01 , userCol = \"IDLabel\" , itemCol = \"keyLabel\" , ratingCol = \"value\" , coldStartStrategy = \"drop\" , ) model = als . fit ( df_model ) df_pred = ( model . transform ( df_test ) . drop ( \"value\" ) . withColumnRenamed ( \"prediction\" , \"value\" ) ) df_encoded_pred = df_model . union ( df_pred . select ( df_model . columns )) if id_type == \"string\" : IDlabelReverse = IndexToString () . setInputCol ( \"IDLabel\" ) . setOutputCol ( id_col ) df_encoded_pred = IDlabelReverse . transform ( df_encoded_pred ) else : df_encoded_pred = df_encoded_pred . withColumnRenamed ( \"IDLabel\" , id_col ) keylabelReverse = IndexToString () . setInputCol ( \"keyLabel\" ) . setOutputCol ( \"key\" ) odf_imputed = ( keylabelReverse . transform ( df_encoded_pred ) . groupBy ( id_col ) . pivot ( \"key\" ) . agg ( F . first ( \"value\" )) . select ( [ id_col ] + [( i + \"_imputed\" ) for i in list_of_cols if i in missing_cols ] ) ) odf = idf . join ( odf_imputed , id_col , \"left_outer\" ) for i in list_of_cols : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) elif output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) if remove_id : odf = odf . drop ( \"id\" ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : output_cols = [ ( i + \"_imputed\" ) for i in [ e for e in list_of_cols if e in missing_cols ] ] odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def auto_imputation ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], id_col = \"\" , null_pct = 0.1 , stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , root_path = \"\" , auth_key = \"NA\" , print_impact = True , ): \"\"\" auto_imputation tests for 5 imputation methods using the other imputation functions provided in this module and returns the one with the best performance. The 5 methods are: (1) imputation_MMM with method_type=\"mean\" (2) imputation_MMM with method_type=\"median\" (3) imputation_sklearn with method_type=\"KNN\" (4) imputation_sklearn with method_type=\"regression\" (5) imputation_matrixFactorization Samples without missing values in attributes to impute are used for testing by removing some % of values and impute them again using the above 5 methods. RMSE/attribute_mean is used as the evaluation metric for each attribute to reduce the effect of unit difference among attributes. The final error of a method is calculated by the sum of ( RMSE/attribute_mean) for all numerical attributes to impute and the method with the least error will be selected. The above testing is only applicable for numerical attributes. If categorical attributes are included, they will be automatically imputed using imputation_MMM. In addition, if there is only one numerical attribute to impute, only method (1) and (2) will be tested because the rest of the methods require more than one column. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") null_pct proportion of the valid input data to be replaced by None to form the test data (Default value = 0.1) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") root_path This argument takes in a base folder path for writing out intermediate_data/ folder to. Default value is \"\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. It also print the name of best performing imputation method along with RMSE details. Returns ------- DataFrame Imputed Dataframe \"\"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf ) missing_df . write . parquet ( root_path + \"intermediate_data/imputation_comparison/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/imputation_comparison/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns have all null values: \" + \",\" . join ( empty_cols )) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = idf . columns if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e != id_col )]) ) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) del_cols = [ e for e in list_of_cols if e in empty_cols ] odf_del = idf . drop ( * del_cols ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] num_cols , cat_cols , other_cols = attributeType_segregation ( odf_del . select ( list_of_cols ) ) missing_catcols = [ e for e in cat_cols if e in missing_cols ] missing_numcols = [ e for e in num_cols if e in missing_cols ] if missing_catcols : odf_imputed_cat = imputation_MMM ( spark , odf_del , list_of_cols = missing_catcols , stats_missing = stats_missing ) else : odf_imputed_cat = odf_del if len ( missing_numcols ) == 0 : warnings . warn ( \"No Imputation Performed for numerical columns - No Column(s) to Impute\" ) return odf_imputed_cat idf_test = ( odf_imputed_cat . dropna ( subset = missing_numcols ) . withColumn ( \"index\" , F . monotonically_increasing_id ()) . withColumn ( \"index\" , F . row_number () . over ( Window . orderBy ( \"index\" ))) ) null_count = int ( null_pct * idf_test . count ()) idf_null = idf_test for i in missing_numcols : null_index = random . sample ( range ( idf_test . count ()), null_count ) idf_null = idf_null . withColumn ( i , F . when ( F . col ( \"index\" ) . isin ( null_index ), None ) . otherwise ( F . col ( i )) ) idf_null . write . parquet ( root_path + \"intermediate_data/imputation_comparison/test_dataset\" , mode = \"overwrite\" , ) idf_null = spark . read . parquet ( root_path + \"intermediate_data/imputation_comparison/test_dataset\" ) method1 = imputation_MMM ( spark , idf_null , list_of_cols = missing_numcols , method_type = \"mean\" , stats_missing = stats_missing , output_mode = output_mode , ) method2 = imputation_MMM ( spark , idf_null , list_of_cols = missing_numcols , method_type = \"median\" , stats_missing = stats_missing , output_mode = output_mode , ) valid_methods = [ method1 , method2 ] if len ( num_cols ) > 1 : method3 = imputation_sklearn ( spark , idf_null , list_of_cols = num_cols , method_type = \"KNN\" , stats_missing = stats_missing , output_mode = output_mode , auth_key = auth_key , ) method4 = imputation_sklearn ( spark , idf_null , list_of_cols = num_cols , method_type = \"regression\" , stats_missing = stats_missing , output_mode = output_mode , auth_key = auth_key , ) method5 = imputation_matrixFactorization ( spark , idf_null , list_of_cols = num_cols , id_col = id_col , stats_missing = stats_missing , output_mode = output_mode , ) valid_methods = [ method1 , method2 , method3 , method4 , method5 ] nrmse_all = [] method_all = [ \"MMM-mean\" , \"MMM-median\" , \"KNN\" , \"regression\" , \"matrix_factorization\" ] for index , method in enumerate ( valid_methods ): nrmse = 0 for i in missing_numcols : pred_col = ( i + \"_imputed\" ) if output_mode == \"append\" else i idf_joined = ( idf_test . select ( \"index\" , F . col ( i ) . alias ( \"val\" )) . join ( method . select ( \"index\" , F . col ( pred_col ) . alias ( \"pred\" )), \"index\" , \"left_outer\" , ) . dropna () ) idf_joined = recast_column ( idf = idf_joined , list_of_cols = [ \"val\" , \"pred\" ], list_of_dtypes = [ \"double\" , \"double\" ], ) pred_mean = float ( method . select ( F . mean ( pred_col )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) i_nrmse = ( RegressionEvaluator ( metricName = \"rmse\" , labelCol = \"val\" , predictionCol = \"pred\" ) . evaluate ( idf_joined ) ) / abs ( pred_mean ) nrmse += i_nrmse nrmse_all . append ( nrmse ) min_index = nrmse_all . index ( np . min ( nrmse_all )) best_method = method_all [ min_index ] odf = valid_methods [ min_index ] if print_impact : print ( list ( zip ( method_all , nrmse_all ))) print ( \"Best Imputation Method: \" , best_method ) return odf def autoencoder_latentFeatures ( spark , idf , list_of_cols = \"all\" , drop_cols = [], reduction_params = 0.5 , sample_size = 500000 , epochs = 100 , batch_size = 256 , pre_existing_model = False , model_path = \"NA\" , standardization = True , standardization_configs = { \"pre_existing_model\" : False , \"model_path\" : \"NA\" }, imputation = False , imputation_configs = { \"imputation_function\" : \"imputation_MMM\" }, stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , root_path = \"\" , auth_key = \"NA\" , print_impact = False , ): \"\"\" Many machine learning models suffer from \"the curse of dimensionality\" when the number of features is too large. autoencoder_latentFeatures is able to reduce the dimensionality by compressing input attributes to a smaller number of latent features. To be more specific, it trains a neural network model using TensorFlow library. The neural network contains an encoder and a decoder, where the encoder learns to represent the input using smaller number of latent features controlled by the input *reduction_params* and the decoder learns to reproduce the input using the latent features. In the end, only the encoder will be kept and the latent features generated by the encoder will be added to the output dataframe. However, the neural network model is not trained in a scalable manner, which might not be able to handle large input dataframe. Thus, an input *sample_size* (the default value is 500,000) can be set to control the number of samples to be used to train the model. If the total number of samples exceeds *sample_size*, the rest of the samples will be predicted using the fitted encoder. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise, the model might not converge smoothly. Inputs *standardization* and *standardization_configs* can be set accordingly to perform standardization within the function. In addition, if a sample contains missing values in the model input, the output values for its latent features will all be None. Thus data imputation is also recommended if missing values exist, which can be done within the function by setting inputs *imputation* and *imputation_configs*. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) reduction_params Determines the number of encoded features in the result. If reduction_params < 1, int(reduction_params * <number of columns>) columns will be generated. Else, reduction_params columns will be generated. (Default value = 0.5) sample_size Maximum rows for training the autoencoder model using tensorflow. (Default value = 500000) epochs Number of epochs to train the tensorflow model. (Default value = 100) batch_size Number of samples per gradient update when fitting the tensorflow model. (Default value = 256) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_<col_index>. \u201cappend\u201d option append transformed columns with format latent_<col_index> to the input dataset, e.g. latent_0, latent_1 will be appended if reduction_params=2. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") root_path This argument takes in a base folder path for writing out intermediate_data/ folder to. Default value is \"\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns ------- DataFrame Dataframe with Latent Features \"\"\" if \"arm64\" in platform . version () . lower (): warnings . warn ( \"This function is currently not supported for ARM64 - Mac M1 Machine\" ) return idf num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Latent Features Generated - No Column(s) to Transform\" ) return idf if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , list_of_cols ) missing_df . write . parquet ( root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from dimensionality reduction as all values are null: \" + \",\" . join ( empty_cols ) ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] if standardization : idf_standardized = z_standardization ( spark , idf , list_of_cols = list_of_cols , output_mode = \"append\" , ** standardization_configs , ) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols if ( i + \"_scaled\" ) in idf_standardized . columns ] else : idf_standardized = idf for i in list_of_cols : idf_standardized = idf_standardized . withColumn ( i + \"_scaled\" , F . col ( i )) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols ] if imputation : all_functions = globals () . copy () all_functions . update ( locals ()) f = all_functions . get ( imputation_configs [ \"imputation_function\" ]) args = copy . deepcopy ( imputation_configs ) args . pop ( \"imputation_function\" , None ) missing_df_scaled = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . withColumn ( \"attribute\" , F . concat ( F . col ( \"attribute\" ), F . lit ( \"_scaled\" ))) ) missing_df_scaled . write . parquet ( root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation_scaled\" , mode = \"overwrite\" , ) stats_missing_scaled = { \"file_path\" : root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation_scaled\" , \"file_type\" : \"parquet\" , } idf_imputed = f ( spark , idf_standardized , list_of_cols_scaled , stats_missing = stats_missing_scaled , ** args , ) else : idf_imputed = idf_standardized . dropna ( subset = list_of_cols_scaled ) n_inputs = len ( list_of_cols_scaled ) if reduction_params < 1 : n_bottleneck = int ( reduction_params * n_inputs ) else : n_bottleneck = int ( reduction_params ) if pre_existing_model : if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + model_path + \"/autoencoders_latentFeatures/encoder.h5 .\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( \"aws s3 cp \" + model_path + \"/autoencoders_latentFeatures/model.h5 .\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) encoder = load_model ( \"encoder.h5\" ) model = load_model ( \"model.h5\" ) elif run_type == \"ak8s\" : bash_cmd = ( 'azcopy cp \"' + model_path + \"/autoencoders_latentFeatures/encoder.h5\" + str ( auth_key ) + '\" .' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( 'azcopy cp \"' + model_path + \"/autoencoders_latentFeatures/model.h5\" + str ( auth_key ) + '\" .' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) encoder = load_model ( \"encoder.h5\" ) model = load_model ( \"model.h5\" ) else : encoder = load_model ( model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) model = load_model ( model_path + \"/autoencoders_latentFeatures/model.h5\" ) else : idf_valid = idf_imputed . select ( list_of_cols_scaled ) idf_model = idf_valid . sample ( False , min ( 1.0 , float ( sample_size ) / idf_valid . count ()), 0 ) idf_train = idf_model . sample ( False , 0.8 , 0 ) idf_test = idf_model . subtract ( idf_train ) X_train = idf_train . toPandas () X_test = idf_test . toPandas () visible = Input ( shape = ( n_inputs ,)) e = Dense ( n_inputs * 2 )( visible ) e = BatchNormalization ()( e ) e = LeakyReLU ()( e ) e = Dense ( n_inputs )( e ) e = BatchNormalization ()( e ) e = LeakyReLU ()( e ) bottleneck = Dense ( n_bottleneck )( e ) d = Dense ( n_inputs )( bottleneck ) d = BatchNormalization ()( d ) d = LeakyReLU ()( d ) d = Dense ( n_inputs * 2 )( d ) d = BatchNormalization ()( d ) d = LeakyReLU ()( d ) output = Dense ( n_inputs , activation = \"linear\" )( d ) model = Model ( inputs = visible , outputs = output ) encoder = Model ( inputs = visible , outputs = bottleneck ) model . compile ( optimizer = \"adam\" , loss = \"mse\" ) history = model . fit ( X_train , X_train , epochs = int ( epochs ), batch_size = int ( batch_size ), verbose = 2 , validation_data = ( X_test , X_test ), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): if run_type == \"emr\" : encoder . save ( \"encoder.h5\" ) model . save ( \"model.h5\" ) bash_cmd = ( \"aws s3 cp encoder.h5 \" + model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( \"aws s3 cp model.h5 \" + model_path + \"/autoencoders_latentFeatures/model.h5\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) elif run_type == \"ak8s\" : encoder . save ( \"encoder.h5\" ) model . save ( \"model.h5\" ) bash_cmd = ( 'azcopy cp \"encoder.h5\" \"' + model_path + \"/autoencoders_latentFeatures/encoder.h5\" + str ( auth_key ) + '\" ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( 'azcopy cp \"model.h5\" \"' + model_path + \"/autoencoders_latentFeatures/model.h5\" + str ( auth_key ) + '\" ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) else : if not os . path . exists ( model_path + \"/autoencoders_latentFeatures/\" ): os . makedirs ( model_path + \"/autoencoders_latentFeatures/\" ) encoder . save ( model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) model . save ( model_path + \"/autoencoders_latentFeatures/model.h5\" ) class ModelWrapperPickable : def __init__ ( self , model ): self . model = model def __getstate__ ( self ): model_str = \"\" with tempfile . NamedTemporaryFile ( suffix = \".hdf5\" , delete = True ) as fd : tensorflow . keras . models . save_model ( self . model , fd . name , overwrite = True ) model_str = fd . read () d = { \"model_str\" : model_str } return d def __setstate__ ( self , state ): with tempfile . NamedTemporaryFile ( suffix = \".hdf5\" , delete = True ) as fd : fd . write ( state [ \"model_str\" ]) fd . flush () self . model = tensorflow . keras . models . load_model ( fd . name ) model_wrapper = ModelWrapperPickable ( encoder ) def compute_output_pandas_udf ( model_wrapper ): @F . pandas_udf ( returnType = T . ArrayType ( T . DoubleType ())) def predict_pandas_udf ( * cols ): X = pd . concat ( cols , axis = 1 ) return pd . Series ( row . tolist () for row in model_wrapper . model . predict ( X )) return predict_pandas_udf odf = idf_imputed . withColumn ( \"predicted_output\" , compute_output_pandas_udf ( model_wrapper )( * list_of_cols_scaled ), ) odf_schema = odf . schema for i in range ( 0 , n_bottleneck ): odf_schema = odf_schema . add ( T . StructField ( \"latent_\" + str ( i ), T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"predicted_output\" ])) . toDF ( schema = odf_schema ) . drop ( \"predicted_output\" ) ) odf = odf . drop ( * list_of_cols_scaled ) if output_mode == \"replace\" : odf = odf . drop ( * list_of_cols ) if print_impact : output_cols = [ \"latent_\" + str ( j ) for j in range ( 0 , n_bottleneck )] odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def PCA_latentFeatures ( spark , idf , list_of_cols = \"all\" , drop_cols = [], explained_variance_cutoff = 0.95 , pre_existing_model = False , model_path = \"NA\" , standardization = True , standardization_configs = { \"pre_existing_model\" : False , \"model_path\" : \"NA\" }, imputation = False , imputation_configs = { \"imputation_function\" : \"imputation_MMM\" }, stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , root_path = \"\" , auth_key = \"NA\" , print_impact = False , ): \"\"\" Similar to autoencoder_latentFeatures, PCA_latentFeatures also generates latent features which reduces the dimensionality of the input dataframe but through a different technique: Principal Component Analysis (PCA). PCA algorithm produces principal components such that it can describe most of the remaining variance and all the principal components are orthogonal to each other. The final number of generated principal components is controlled by the input *explained_variance_cutoff*. In other words, the number of selected principal components, k, is the minimum value such that top k components (latent features) can explain at least *explained_variance_cutoff* of the total variance. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise the generated latent features might be dominated by the attributes with larger variance. Inputs *standardization* and *standardization_configs* can be set accordingly to perform standardization within the function. In addition, data imputation is also recommended if missing values exist, which can be done within the function by setting inputs *imputation* and *imputation_configs*. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) explained_variance_cutoff Determines the number of latent columns in the output. If N is the smallest integer such that top N latent columns explain more than explained_variance_cutoff variance, these N columns will be selected. (Default value = 0.95) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_<col_index>. \u201cappend\u201d option append transformed columns with format latent_<col_index> to the input dataset, e.g. latent_0, latent_1. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") root_path This argument takes in a base folder path for writing out intermediate_data/ folder to. Default value is \"\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns ------- DataFrame Dataframe with Latent Features \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Latent Features Generated - No Column(s) to Transform\" ) return idf if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , list_of_cols ) missing_df . write . parquet ( root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from dimensionality reduction as all values are null: \" + \",\" . join ( empty_cols ) ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] if standardization : idf_standardized = z_standardization ( spark , idf , list_of_cols = list_of_cols , output_mode = \"append\" , ** standardization_configs , ) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols if ( i + \"_scaled\" ) in idf_standardized . columns ] else : idf_standardized = idf for i in list_of_cols : idf_standardized = idf_standardized . withColumn ( i + \"_scaled\" , F . col ( i )) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols ] if imputation : all_functions = globals () . copy () all_functions . update ( locals ()) f = all_functions . get ( imputation_configs [ \"imputation_function\" ]) args = copy . deepcopy ( imputation_configs ) args . pop ( \"imputation_function\" , None ) missing_df_scaled = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . withColumn ( \"attribute\" , F . concat ( F . col ( \"attribute\" ), F . lit ( \"_scaled\" ))) ) missing_df_scaled . write . parquet ( root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation_scaled\" , mode = \"overwrite\" , ) stats_missing_scaled = { \"file_path\" : root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation_scaled\" , \"file_type\" : \"parquet\" , } idf_imputed = f ( spark , idf_standardized , list_of_cols_scaled , stats_missing = stats_missing_scaled , ** args , ) else : idf_imputed = idf_standardized . dropna ( subset = list_of_cols_scaled ) idf_imputed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () assembler = VectorAssembler ( inputCols = list_of_cols_scaled , outputCol = \"features\" ) assembled_data = assembler . transform ( idf_imputed ) . drop ( * list_of_cols_scaled ) if pre_existing_model : pca = PCA . load ( model_path + \"/PCA_latentFeatures/pca_path\" ) pcaModel = PCAModel . load ( model_path + \"/PCA_latentFeatures/pcaModel_path\" ) n = pca . getK () else : pca = PCA ( k = len ( list_of_cols_scaled ), inputCol = \"features\" , outputCol = \"features_pca\" ) pcaModel = pca . fit ( assembled_data ) explained_variance = 0 for n in range ( 1 , len ( list_of_cols ) + 1 ): explained_variance += pcaModel . explainedVariance [ n - 1 ] if explained_variance > explained_variance_cutoff : break pca = PCA ( k = n , inputCol = \"features\" , outputCol = \"features_pca\" ) pcaModel = pca . fit ( assembled_data ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): pcaModel . write () . overwrite () . save ( model_path + \"/PCA_latentFeatures/pcaModel_path\" ) pca . write () . overwrite () . save ( model_path + \"/PCA_latentFeatures/pca_path\" ) def vector_to_array ( v ): return v . toArray () . tolist () f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . FloatType ())) odf = ( pcaModel . transform ( assembled_data ) . withColumn ( \"features_pca_array\" , f_vector_to_array ( \"features_pca\" )) . drop ( * [ \"features\" , \"features_pca\" ]) ) odf_schema = odf . schema for i in range ( 0 , n ): odf_schema = odf_schema . add ( T . StructField ( \"latent_\" + str ( i ), T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"features_pca_array\" ])) . toDF ( schema = odf_schema ) . drop ( \"features_pca_array\" ) . replace ( float ( \"nan\" ), None , subset = [ \"latent_\" + str ( j ) for j in range ( 0 , n )]) ) if output_mode == \"replace\" : odf = odf . drop ( * list_of_cols ) if print_impact : print ( \"Explained Variance: \" , round ( np . sum ( pcaModel . explainedVariance [ 0 : n ]), 4 )) output_cols = [ \"latent_\" + str ( j ) for j in range ( 0 , n )] odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def feature_transformation ( idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"sqrt\" , N = None , output_mode = \"replace\" , print_impact = False , ): \"\"\" As the name indicates, feature_transformation performs mathematical transformation over selected attributes. The following methods are supported for an input attribute x: ln(x), log10(x), log2(x), e^x, 2^x, 10^x, N^x, square and cube root of x, x^2, x^3, x^N, trigonometric transformations of x (sin, cos, tan, asin, acos, atan), radians, x%N, x!, 1/x, floor and ceiling of x and x rounded to N decimal places. Some transformations only work with positive or non-negative input values such as log and square root and an error will be returned if violated. Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"ln\", \"log10\", \"log2\", \"exp\", \"powOf2\" (2^x), \"powOf10\" (10^x), \"powOfN\" (N^x), \"sqrt\" (square root), \"cbrt\" (cube root), \"sq\" (square), \"cb\" (cube), \"toPowerN\" (x^N), \"sin\", \"cos\", \"tan\", \"asin\", \"acos\", \"atan\", \"radians\", \"remainderDivByN\" (x%N), \"factorial\" (x!), \"mul_inv\" (1/x), \"floor\", \"ceil\", \"roundN\" (round to N decimal places) (Default value = \"sqrt\") N None by default. If method_type is \"powOfN\", \"toPowerN\", \"remainderDivByN\" or \"roundN\", N will be used as the required constant. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix (E.g. \"_ln\", \"_powOf<N>\") to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if ( len ( list_of_cols ) == 0 ) | ( any ( x not in num_cols for x in list_of_cols )): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in [ \"ln\" , \"log10\" , \"log2\" , \"exp\" , \"powOf2\" , \"powOf10\" , \"powOfN\" , \"sqrt\" , \"cbrt\" , \"sq\" , \"cb\" , \"toPowerN\" , \"sin\" , \"cos\" , \"tan\" , \"asin\" , \"acos\" , \"atan\" , \"radians\" , \"remainderDivByN\" , \"factorial\" , \"mul_inv\" , \"floor\" , \"ceil\" , \"roundN\" , ]: raise TypeError ( \"Invalid input method_type\" ) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] list_of_cols = num_cols odf = idf transformation_function = { \"ln\" : F . log , \"log10\" : F . log10 , \"log2\" : F . log2 , \"exp\" : F . exp , \"powOf2\" : ( lambda x : F . pow ( 2.0 , x )), \"powOf10\" : ( lambda x : F . pow ( 10.0 , x )), \"powOfN\" : ( lambda x : F . pow ( N , x )), \"sqrt\" : F . sqrt , \"cbrt\" : F . cbrt , \"sq\" : ( lambda x : x ** 2 ), \"cb\" : ( lambda x : x ** 3 ), \"toPowerN\" : ( lambda x : x ** N ), \"sin\" : F . sin , \"cos\" : F . cos , \"tan\" : F . tan , \"asin\" : F . asin , \"acos\" : F . acos , \"atan\" : F . atan , \"radians\" : F . radians , \"remainderDivByN\" : ( lambda x : x % F . lit ( N )), \"factorial\" : F . factorial , \"mul_inv\" : ( lambda x : F . lit ( 1 ) / x ), \"floor\" : F . floor , \"ceil\" : F . ceil , \"roundN\" : ( lambda x : F . round ( x , N )), } def get_col_name ( i ): if output_mode == \"replace\" : return i else : if method_type in [ \"powOfN\" , \"toPowerN\" , \"remainderDivByN\" , \"roundN\" ]: return i + \"_\" + method_type [: - 1 ] + str ( N ) else : return i + \"_\" + method_type output_cols = [] for i in list_of_cols : modify_col = get_col_name ( i ) odf = odf . withColumn ( modify_col , transformation_function [ method_type ]( F . col ( i ))) output_cols . append ( modify_col ) if print_impact : print ( \"Before:\" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After:\" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def boxcox_transformation ( idf , list_of_cols = \"all\" , drop_cols = [], boxcox_lambda = None , output_mode = \"replace\" , print_impact = False , ): \"\"\" Some machine learning algorithms require the input data to follow normal distributions. Thus, when the input data is too skewed, boxcox_transformation can be used to transform it into a more normal-like distribution. The transformed value of a sample x depends on a coefficient lambda: (1) if lambda = 0, x is transformed into log(x); (2) if lambda !=0, x is transformed into (x^lambda-1)/lambda. The value of lambda can be either specified by the user or automatically selected within the function. If lambda needs to be selected within the function, a range of values (0, 1, -1, 0.5, -0.5, 2, -2, 0.25, -0.25, 3, -3, 4, -4, 5, -5) will be tested and the lambda, which optimizes the KolmogorovSmirnovTest by PySpark with the theoretical distribution being normal, will be used to perform the transformation. Different lambda values can be assigned to different attributes but one attribute can only be assigned one lambda value. Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) boxcox_lambda Lambda value for box_cox transormation. If boxcox_lambda is not None, it will be directly used for the transformation. It can be a (1) list: each element represents a lambda value for an attribute and the length of the list must be the same as the number of columns to transform. (2) int/float: all attributes will be assigned the same lambda value. Else, search for the best lambda among [1,-1,0.5,-0.5,2,-2,0.25,-0.25,3,-3,4,-4,5,-5] for each column and apply the transformation (Default value = None) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix \"_bxcx_<lambda>\" to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if ( len ( list_of_cols ) == 0 ) | ( any ( x not in num_cols for x in list_of_cols )): raise TypeError ( \"Invalid input for Column(s)\" ) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] list_of_cols = num_cols odf = idf col_mins = idf . select ([ F . min ( i ) for i in list_of_cols ]) if any ([ i <= 0 for i in col_mins . rdd . flatMap ( lambda x : x ) . collect ()]): col_mins . show ( 1 , False ) raise ValueError ( \"Data must be positive\" ) if boxcox_lambda is not None : if isinstance ( boxcox_lambda , ( list , tuple )): if len ( boxcox_lambda ) != len ( list_of_cols ): raise TypeError ( \"Invalid input for boxcox_lambda\" ) elif not all ([ isinstance ( l , ( float , int )) for l in boxcox_lambda ]): raise TypeError ( \"Invalid input for boxcox_lambda\" ) else : boxcox_lambda_list = list ( boxcox_lambda ) elif isinstance ( boxcox_lambda , ( float , int )): boxcox_lambda_list = [ boxcox_lambda ] * len ( list_of_cols ) else : raise TypeError ( \"Invalid input for boxcox_lambda\" ) else : boxcox_lambda_list = [] for i in list_of_cols : lambdaVal = [ 1 , - 1 , 0.5 , - 0.5 , 2 , - 2 , 0.25 , - 0.25 , 3 , - 3 , 4 , - 4 , 5 , - 5 ] best_pVal = 0 for j in lambdaVal : pVal = Statistics . kolmogorovSmirnovTest ( odf . select ( F . pow ( F . col ( i ), j )) . rdd . flatMap ( lambda x : x ), \"norm\" ) . pValue if pVal > best_pVal : best_pVal = pVal best_lambdaVal = j pVal = Statistics . kolmogorovSmirnovTest ( odf . select ( F . log ( F . col ( i ))) . rdd . flatMap ( lambda x : x ), \"norm\" ) . pValue if pVal > best_pVal : best_pVal = pVal best_lambdaVal = 0 boxcox_lambda_list . append ( best_lambdaVal ) output_cols = [] for i , curr_lambdaVal in zip ( list_of_cols , boxcox_lambda_list ): if curr_lambdaVal != 1 : modify_col = ( ( i + \"_bxcx_\" + str ( curr_lambdaVal )) if output_mode == \"append\" else i ) output_cols . append ( modify_col ) if curr_lambdaVal == 0 : odf = odf . withColumn ( modify_col , F . log ( F . col ( i ))) else : odf = odf . withColumn ( modify_col , F . pow ( F . col ( i ), curr_lambdaVal )) if len ( output_cols ) == 0 : warnings . warn ( \"lambdaVal for all columns are 1 so no transformation is performed and idf is returned\" ) return idf if print_impact : print ( \"Transformed Columns: \" , list_of_cols ) print ( \"Best BoxCox Parameter(s): \" , boxcox_lambda_list ) print ( \"Before:\" ) idf . select ( list_of_cols ) . describe () . unionByName ( idf . select ([ F . skewness ( i ) . alias ( i ) for i in list_of_cols ]) . withColumn ( \"summary\" , F . lit ( \"skewness\" ) ) ) . show ( 6 , False ) print ( \"After:\" ) if output_mode == \"replace\" : odf . select ( list_of_cols ) . describe () . unionByName ( odf . select ([ F . skewness ( i ) . alias ( i ) for i in list_of_cols ]) . withColumn ( \"summary\" , F . lit ( \"skewness\" ) ) ) . show ( 6 , False ) else : output_cols = [( \"`\" + i + \"`\" ) for i in output_cols ] odf . select ( output_cols ) . describe () . unionByName ( odf . select ( [ F . skewness ( i ) . alias ( i [ 1 : - 1 ]) for i in output_cols ] ) . withColumn ( \"summary\" , F . lit ( \"skewness\" )) ) . show ( 6 , False ) return odf def outlier_categories ( spark , idf , list_of_cols = \"all\" , drop_cols = [], coverage = 1.0 , max_category = 50 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" This function replaces less frequently seen values (called as outlier values in the current context) in a categorical column by 'outlier_categories'. Outlier values can be defined in two ways \u2013 a) Max N categories, where N is user defined value. In this method, top N-1 frequently seen categories are considered and rest are clubbed under single category 'outlier_categories'. or Alternatively, b) Coverage \u2013 top frequently seen categories are considered till it covers minimum N% of rows and rest lesser seen values are mapped to 'outlier_categories'. Even if the Coverage is less, maximum category constraint is given priority. Further, there is a caveat that when multiple categories have same rank. Then, number of categorical values can be more than max_category defined by the user. This function performs better when distinct values, in any column, are not more than 100. It is recommended that to drop those columns from the computation which has more than 100 distinct values, to get better performance out of this function. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) coverage Defines the minimum % of rows that will be mapped to actual category name and the rest to be mapped to 'outlier_categories' and takes value between 0 to 1. Coverage of 0.8 can be interpreted as top frequently seen categories are considered till it covers minimum 80% of rows and rest lesser seen values are mapped to 'outlier_categories'. (Default value = 1.0) max_category Even if coverage is less, only (max_category - 1) categories will be mapped to actual name and rest to 'outlier_categories'. Caveat is when multiple categories have same rank, then #categories can be more than max_category. (Default value = 50) pre_existing_model Boolean argument \u2013 True or False. True if the model with the outlier/other values for each attribute exists already to be used, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") print_impact True, False This argument is to print before and after unique count of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Outlier Categories Computation - No categorical column(s) to transform\" ) return idf if ( coverage <= 0 ) | ( coverage > 1 ): raise TypeError ( \"Invalid input for Coverage Value\" ) if max_category < 2 : raise TypeError ( \"Invalid input for Maximum No. of Categories Allowed\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) idf = idf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) if pre_existing_model : df_model = spark . read . csv ( model_path + \"/outlier_categories\" , header = True , inferSchema = True ) else : for index , i in enumerate ( list_of_cols ): window = Window . partitionBy () . orderBy ( F . desc ( \"count_pct\" )) df_cats = ( idf . select ( i ) . groupBy ( i ) . count () . dropna () . withColumn ( \"count_pct\" , F . col ( \"count\" ) / F . sum ( \"count\" ) . over ( Window . partitionBy ()), ) . withColumn ( \"rank\" , F . rank () . over ( window )) . withColumn ( \"cumu\" , F . sum ( \"count_pct\" ) . over ( window . rowsBetween ( Window . unboundedPreceding , 0 ) ), ) . withColumn ( \"lag_cumu\" , F . lag ( \"cumu\" ) . over ( window )) . fillna ( 0 ) . where ( ~ (( F . col ( \"cumu\" ) >= coverage ) & ( F . col ( \"lag_cumu\" ) >= coverage ))) . where ( F . col ( \"rank\" ) <= ( max_category - 1 )) . select ( F . lit ( i ) . alias ( \"attribute\" ), F . col ( i ) . alias ( \"parameters\" )) ) if index == 0 : df_model = df_cats else : df_model = df_model . union ( df_cats ) df_params = df_model . rdd . groupByKey () . mapValues ( list ) . collect () dict_params = dict ( df_params ) broadcast_params = spark . sparkContext . broadcast ( dict_params ) def get_params ( key ): parameters = list () dict_params = broadcast_params . value params = dict_params . get ( key ) if params : parameters = params return parameters odf = idf for i in list_of_cols : parameters = get_params ( i ) if output_mode == \"replace\" : odf = odf . withColumn ( i , F . when ( ( F . col ( i ) . isin ( parameters )) | ( F . col ( i ) . isNull ()), F . col ( i ) ) . otherwise ( \"outlier_categories\" ), ) else : odf = odf . withColumn ( i + \"_outliered\" , F . when ( ( F . col ( i ) . isin ( parameters )) | ( F . col ( i ) . isNull ()), F . col ( i ) ) . otherwise ( \"outlier_categories\" ), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model . repartition ( 1 ) . write . csv ( model_path + \"/outlier_categories\" , header = True , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_outliered\" ) for i in list_of_cols ] uniqueCount_computation ( spark , idf , list_of_cols ) . select ( \"attribute\" , F . col ( \"unique_values\" ) . alias ( \"uniqueValues_before\" ) ) . show ( len ( list_of_cols ), False ) uniqueCount_computation ( spark , odf , output_cols ) . select ( \"attribute\" , F . col ( \"unique_values\" ) . alias ( \"uniqueValues_after\" ) ) . show ( len ( list_of_cols ), False ) idf . unpersist () return odf def expression_parser ( idf , list_of_expr , postfix = \"\" , print_impact = False ): \"\"\" expression_parser can be used to evaluate a list of SQL expressions and output the result as new features. It is able to handle column names containing special characters such as \u201c.\u201d, \u201c-\u201d, \u201c@\u201d, \u201c^\u201d, etc, by converting them to \u201c_\u201d first before the evaluation and convert them back to the original names before returning the output dataframe. Parameters ---------- idf Input Dataframe list_of_expr List of expressions to evaluate as new features e.g., [\"expr1\",\"expr2\"]. Alternatively, expressions can be specified in a string format, where different expressions are separated by pipe delimiter \u201c|\u201d e.g., \"expr1|expr2\". postfix postfix for new feature name.Naming convention \"f\" + expression_index + postfix e.g. with postfix of \"new\", new added features are named as f0new, f1new etc. (Default value = \"\") print_impact True, False This argument is to print the descriptive statistics of the parsed features (Default value = False) Returns ------- DataFrame Parsed Dataframe \"\"\" if isinstance ( list_of_expr , str ): list_of_expr = [ x . strip () for x in list_of_expr . split ( \"|\" )] special_chars = [ \"&\" , \"$\" , \";\" , \":\" , \",\" , \"*\" , \"#\" , \"@\" , \"?\" , \"%\" , \"!\" , \"^\" , \"(\" , \")\" , \"-\" , \"/\" , \"'\" , \".\" , '\"' , ] rename_cols = [] replace_chars = {} for char in special_chars : for col in idf . columns : if char in col : rename_cols . append ( col ) if col in replace_chars . keys (): ( replace_chars [ col ]) . append ( char ) else : replace_chars [ col ] = [ char ] rename_mapping_to_new , rename_mapping_to_old = {}, {} idf_renamed = idf for col in rename_cols : new_col = col for char in replace_chars [ col ]: new_col = new_col . replace ( char , \"_\" ) rename_mapping_to_old [ new_col ] = col rename_mapping_to_new [ col ] = new_col idf_renamed = idf_renamed . withColumnRenamed ( col , new_col ) list_of_expr_ = [] for expr in list_of_expr : new_expr = expr for col in rename_cols : if col in expr : new_expr = new_expr . replace ( col , rename_mapping_to_new [ col ]) list_of_expr_ . append ( new_expr ) list_of_expr = list_of_expr_ odf = idf_renamed new_cols = [] for index , exp in enumerate ( list_of_expr ): odf = odf . withColumn ( \"f\" + str ( index ) + postfix , F . expr ( exp )) new_cols . append ( \"f\" + str ( index ) + postfix ) for new_col , col in rename_mapping_to_old . items (): odf = odf . withColumnRenamed ( new_col , col ) if print_impact : print ( \"Columns Added: \" , new_cols ) odf . select ( new_cols ) . describe () . show ( 5 , False ) return odf Functions def IQR_standardization ( spark, idf, list_of_cols='all', drop_cols=[], pre_existing_model=False, model_path='NA', output_mode='replace', print_impact=False) Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (25/50/75 percentile for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns DataFrame Rescaled Dataframe Expand source code def IQR_standardization ( spark , idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (25/50/75 percentile for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Standardization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/IQR_standardization\" ) parameters = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"feature\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : parameters = idf . approxQuantile ( list_of_cols , [ 0.25 , 0.5 , 0.75 ], 0.01 ) excluded_cols = [] for i , param in zip ( list_of_cols , parameters ): if len ( param ) > 0 : if round ( param [ 0 ], 5 ) == round ( param [ 2 ], 5 ): excluded_cols . append ( i ) else : excluded_cols . append ( i ) if len ( excluded_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from standardization because the 75th and 25th percentiles are the same:\" + str ( excluded_cols ) ) odf = idf for index , i in enumerate ( list_of_cols ): if i not in excluded_cols : modify_col = ( i + \"_scaled\" ) if ( output_mode == \"append\" ) else i odf = odf . withColumn ( modify_col , ( F . col ( i ) - parameters [ index ][ 1 ]) / ( parameters [ index ][ 2 ] - parameters [ index ][ 0 ]), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( list_of_cols , parameters ), schema = [ \"feature\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/IQR_standardization\" , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [ ( i + \"_scaled\" ) for i in list_of_cols if i not in excluded_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def PCA_latentFeatures ( spark, idf, list_of_cols='all', drop_cols=[], explained_variance_cutoff=0.95, pre_existing_model=False, model_path='NA', standardization=True, standardization_configs={'pre_existing_model': False, 'model_path': 'NA'}, imputation=False, imputation_configs={'imputation_function': 'imputation_MMM'}, stats_missing={}, output_mode='replace', run_type='local', root_path='', auth_key='NA', print_impact=False) Similar to autoencoder_latentFeatures, PCA_latentFeatures also generates latent features which reduces the dimensionality of the input dataframe but through a different technique: Principal Component Analysis (PCA). PCA algorithm produces principal components such that it can describe most of the remaining variance and all the principal components are orthogonal to each other. The final number of generated principal components is controlled by the input explained_variance_cutoff . In other words, the number of selected principal components, k, is the minimum value such that top k components (latent features) can explain at least explained_variance_cutoff of the total variance. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise the generated latent features might be dominated by the attributes with larger variance. Inputs standardization and standardization_configs can be set accordingly to perform standardization within the function. In addition, data imputation is also recommended if missing values exist, which can be done within the function by setting inputs imputation and imputation_configs . Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) explained_variance_cutoff Determines the number of latent columns in the output. If N is the smallest integer such that top N latent columns explain more than explained_variance_cutoff variance, these N columns will be selected. (Default value = 0.95) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_ . \u201cappend\u201d option append transformed columns with format latent_ to the input dataset, e.g. latent_0, latent_1. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") root_path This argument takes in a base folder path for writing out intermediate_data/ folder to. Default value is \"\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns DataFrame Dataframe with Latent Features Expand source code def PCA_latentFeatures ( spark , idf , list_of_cols = \"all\" , drop_cols = [], explained_variance_cutoff = 0.95 , pre_existing_model = False , model_path = \"NA\" , standardization = True , standardization_configs = { \"pre_existing_model\" : False , \"model_path\" : \"NA\" }, imputation = False , imputation_configs = { \"imputation_function\" : \"imputation_MMM\" }, stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , root_path = \"\" , auth_key = \"NA\" , print_impact = False , ): \"\"\" Similar to autoencoder_latentFeatures, PCA_latentFeatures also generates latent features which reduces the dimensionality of the input dataframe but through a different technique: Principal Component Analysis (PCA). PCA algorithm produces principal components such that it can describe most of the remaining variance and all the principal components are orthogonal to each other. The final number of generated principal components is controlled by the input *explained_variance_cutoff*. In other words, the number of selected principal components, k, is the minimum value such that top k components (latent features) can explain at least *explained_variance_cutoff* of the total variance. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise the generated latent features might be dominated by the attributes with larger variance. Inputs *standardization* and *standardization_configs* can be set accordingly to perform standardization within the function. In addition, data imputation is also recommended if missing values exist, which can be done within the function by setting inputs *imputation* and *imputation_configs*. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) explained_variance_cutoff Determines the number of latent columns in the output. If N is the smallest integer such that top N latent columns explain more than explained_variance_cutoff variance, these N columns will be selected. (Default value = 0.95) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_<col_index>. \u201cappend\u201d option append transformed columns with format latent_<col_index> to the input dataset, e.g. latent_0, latent_1. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") root_path This argument takes in a base folder path for writing out intermediate_data/ folder to. Default value is \"\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns ------- DataFrame Dataframe with Latent Features \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Latent Features Generated - No Column(s) to Transform\" ) return idf if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , list_of_cols ) missing_df . write . parquet ( root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from dimensionality reduction as all values are null: \" + \",\" . join ( empty_cols ) ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] if standardization : idf_standardized = z_standardization ( spark , idf , list_of_cols = list_of_cols , output_mode = \"append\" , ** standardization_configs , ) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols if ( i + \"_scaled\" ) in idf_standardized . columns ] else : idf_standardized = idf for i in list_of_cols : idf_standardized = idf_standardized . withColumn ( i + \"_scaled\" , F . col ( i )) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols ] if imputation : all_functions = globals () . copy () all_functions . update ( locals ()) f = all_functions . get ( imputation_configs [ \"imputation_function\" ]) args = copy . deepcopy ( imputation_configs ) args . pop ( \"imputation_function\" , None ) missing_df_scaled = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . withColumn ( \"attribute\" , F . concat ( F . col ( \"attribute\" ), F . lit ( \"_scaled\" ))) ) missing_df_scaled . write . parquet ( root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation_scaled\" , mode = \"overwrite\" , ) stats_missing_scaled = { \"file_path\" : root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation_scaled\" , \"file_type\" : \"parquet\" , } idf_imputed = f ( spark , idf_standardized , list_of_cols_scaled , stats_missing = stats_missing_scaled , ** args , ) else : idf_imputed = idf_standardized . dropna ( subset = list_of_cols_scaled ) idf_imputed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () assembler = VectorAssembler ( inputCols = list_of_cols_scaled , outputCol = \"features\" ) assembled_data = assembler . transform ( idf_imputed ) . drop ( * list_of_cols_scaled ) if pre_existing_model : pca = PCA . load ( model_path + \"/PCA_latentFeatures/pca_path\" ) pcaModel = PCAModel . load ( model_path + \"/PCA_latentFeatures/pcaModel_path\" ) n = pca . getK () else : pca = PCA ( k = len ( list_of_cols_scaled ), inputCol = \"features\" , outputCol = \"features_pca\" ) pcaModel = pca . fit ( assembled_data ) explained_variance = 0 for n in range ( 1 , len ( list_of_cols ) + 1 ): explained_variance += pcaModel . explainedVariance [ n - 1 ] if explained_variance > explained_variance_cutoff : break pca = PCA ( k = n , inputCol = \"features\" , outputCol = \"features_pca\" ) pcaModel = pca . fit ( assembled_data ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): pcaModel . write () . overwrite () . save ( model_path + \"/PCA_latentFeatures/pcaModel_path\" ) pca . write () . overwrite () . save ( model_path + \"/PCA_latentFeatures/pca_path\" ) def vector_to_array ( v ): return v . toArray () . tolist () f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . FloatType ())) odf = ( pcaModel . transform ( assembled_data ) . withColumn ( \"features_pca_array\" , f_vector_to_array ( \"features_pca\" )) . drop ( * [ \"features\" , \"features_pca\" ]) ) odf_schema = odf . schema for i in range ( 0 , n ): odf_schema = odf_schema . add ( T . StructField ( \"latent_\" + str ( i ), T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"features_pca_array\" ])) . toDF ( schema = odf_schema ) . drop ( \"features_pca_array\" ) . replace ( float ( \"nan\" ), None , subset = [ \"latent_\" + str ( j ) for j in range ( 0 , n )]) ) if output_mode == \"replace\" : odf = odf . drop ( * list_of_cols ) if print_impact : print ( \"Explained Variance: \" , round ( np . sum ( pcaModel . explainedVariance [ 0 : n ]), 4 )) output_cols = [ \"latent_\" + str ( j ) for j in range ( 0 , n )] odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def attribute_binning ( spark, idf, list_of_cols='all', drop_cols=[], method_type='equal_range', bin_size=10, bin_dtype='numerical', pre_existing_model=False, model_path='NA', output_mode='replace', print_impact=False) Attribute binning (or discretization) is a method of numerical attribute into discrete (integer or categorical values) using pre-defined number of bins. This data pre-processing technique is used to reduce the effects of minor observation errors. Also, Binning introduces non-linearity and tends to improve the performance of the model. In this function, we are focussing on unsupervised way of binning i.e. without taking the target variable into account - Equal Range Binning, Equal Frequency Binning. In Equal Range method, each bin is of equal size/width and computed as: w = max- min / no. of bins bins cutoff=[min, min+w,min+2w\u2026..,max-w,max] whereas in Equal Frequency binning method, bins are created in such a way that each bin has equal no. of rows, though the width of bins may vary from each other. w = 1 / no. of bins bins cutoff=[min, wthpctile, 2wthpctile\u2026.,max ] Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Number of bins. (Default value = 10) bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") pre_existing_model Boolean argument \u2013 True or False. True if binning model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print the number of categories generated for each attribute (may or may be not same as bin_size) Returns DataFrame Binned Dataframe Expand source code def attribute_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"equal_range\" , bin_size = 10 , bin_dtype = \"numerical\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Attribute binning (or discretization) is a method of numerical attribute into discrete (integer or categorical values) using pre-defined number of bins. This data pre-processing technique is used to reduce the effects of minor observation errors. Also, Binning introduces non-linearity and tends to improve the performance of the model. In this function, we are focussing on unsupervised way of binning i.e. without taking the target variable into account - Equal Range Binning, Equal Frequency Binning. In Equal Range method, each bin is of equal size/width and computed as: w = max- min / no. of bins *bins cutoff=[min, min+w,min+2w\u2026..,max-w,max]* whereas in Equal Frequency binning method, bins are created in such a way that each bin has equal no. of rows, though the width of bins may vary from each other. w = 1 / no. of bins *bins cutoff=[min, wthpctile, 2wthpctile\u2026.,max ]* Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Number of bins. (Default value = 10) bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") pre_existing_model Boolean argument \u2013 True or False. True if binning model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print the number of categories generated for each attribute (may or may be not same as bin_size) Returns ------- DataFrame Binned Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Binning Performed - No numerical column(s) to transform\" ) return idf if method_type not in ( \"equal_frequency\" , \"equal_range\" ): raise TypeError ( \"Invalid input for method_type\" ) if bin_size < 2 : raise TypeError ( \"Invalid input for bin_size\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/attribute_binning\" ) bin_cutoffs = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) bin_cutoffs . append ( mapped_value ) else : if method_type == \"equal_frequency\" : pctile_width = 1 / bin_size pctile_cutoff = [] for j in range ( 1 , bin_size ): pctile_cutoff . append ( j * pctile_width ) bin_cutoffs = idf . approxQuantile ( list_of_cols , pctile_cutoff , 0.01 ) else : funs = [ F . max , F . min ] exprs = [ f ( F . col ( c )) for f in funs for c in list_of_cols ] list_result = idf . groupby () . agg ( * exprs ) . rdd . flatMap ( lambda x : x ) . collect () bin_cutoffs = [] drop_col_process = [] for i in range ( int ( len ( list_result ) / 2 )): bin_cutoff = [] max_val = list_result [ i ] min_val = list_result [ i + int ( len ( list_result ) / 2 )] if not max_val and max_val != 0 : drop_col_process . append ( list_of_cols [ i ]) continue bin_width = ( max_val - min_val ) / bin_size for j in range ( 1 , bin_size ): bin_cutoff . append ( min_val + j * bin_width ) bin_cutoffs . append ( bin_cutoff ) if drop_col_process : warnings . warn ( \"Columns contains too much null values. Dropping \" + \", \" . join ( drop_col_process ) ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_col_process ]) ) if model_path != \"NA\" : df_model = spark . createDataFrame ( zip ( list_of_cols , bin_cutoffs ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . write . parquet ( model_path + \"/attribute_binning\" , mode = \"overwrite\" ) def bucket_label ( value , index ): if value is None : return None for i in range ( 0 , len ( bin_cutoffs [ index ])): if value <= bin_cutoffs [ index ][ i ]: if bin_dtype == \"numerical\" : return i + 1 else : if i == 0 : return \"<= \" + str ( round ( bin_cutoffs [ index ][ i ], 4 )) else : return ( str ( round ( bin_cutoffs [ index ][ i - 1 ], 4 )) + \"-\" + str ( round ( bin_cutoffs [ index ][ i ], 4 )) ) else : next if bin_dtype == \"numerical\" : return len ( bin_cutoffs [ 0 ]) + 1 else : return \"> \" + str ( round ( bin_cutoffs [ index ][ len ( bin_cutoffs [ 0 ]) - 1 ], 4 )) if bin_dtype == \"numerical\" : f_bucket_label = F . udf ( bucket_label , T . IntegerType ()) else : f_bucket_label = F . udf ( bucket_label , T . StringType ()) odf = idf for idx , i in enumerate ( list_of_cols ): odf = odf . withColumn ( i + \"_binned\" , f_bucket_label ( F . col ( i ), F . lit ( idx ))) if output_mode == \"replace\" : for col in list_of_cols : odf = odf . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_binned\" ) for i in list_of_cols ] uniqueCount_computation ( spark , odf , output_cols ) . show ( len ( output_cols ), False ) return odf def auto_imputation ( spark, idf, list_of_cols='missing', drop_cols=[], id_col='', null_pct=0.1, stats_missing={}, output_mode='replace', run_type='local', root_path='', auth_key='NA', print_impact=True) auto_imputation tests for 5 imputation methods using the other imputation functions provided in this module and returns the one with the best performance. The 5 methods are: (1) imputation_MMM with method_type=\"mean\" (2) imputation_MMM with method_type=\"median\" (3) imputation_sklearn with method_type=\"KNN\" (4) imputation_sklearn with method_type=\"regression\" (5) imputation_matrixFactorization Samples without missing values in attributes to impute are used for testing by removing some % of values and impute them again using the above 5 methods. RMSE/attribute_mean is used as the evaluation metric for each attribute to reduce the effect of unit difference among attributes. The final error of a method is calculated by the sum of ( RMSE/attribute_mean) for all numerical attributes to impute and the method with the least error will be selected. The above testing is only applicable for numerical attributes. If categorical attributes are included, they will be automatically imputed using imputation_MMM. In addition, if there is only one numerical attribute to impute, only method (1) and (2) will be tested because the rest of the methods require more than one column. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") null_pct proportion of the valid input data to be replaced by None to form the test data (Default value = 0.1) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") root_path This argument takes in a base folder path for writing out intermediate_data/ folder to. Default value is \"\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. It also print the name of best performing imputation method along with RMSE details. Returns DataFrame Imputed Dataframe Expand source code def auto_imputation ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], id_col = \"\" , null_pct = 0.1 , stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , root_path = \"\" , auth_key = \"NA\" , print_impact = True , ): \"\"\" auto_imputation tests for 5 imputation methods using the other imputation functions provided in this module and returns the one with the best performance. The 5 methods are: (1) imputation_MMM with method_type=\"mean\" (2) imputation_MMM with method_type=\"median\" (3) imputation_sklearn with method_type=\"KNN\" (4) imputation_sklearn with method_type=\"regression\" (5) imputation_matrixFactorization Samples without missing values in attributes to impute are used for testing by removing some % of values and impute them again using the above 5 methods. RMSE/attribute_mean is used as the evaluation metric for each attribute to reduce the effect of unit difference among attributes. The final error of a method is calculated by the sum of ( RMSE/attribute_mean) for all numerical attributes to impute and the method with the least error will be selected. The above testing is only applicable for numerical attributes. If categorical attributes are included, they will be automatically imputed using imputation_MMM. In addition, if there is only one numerical attribute to impute, only method (1) and (2) will be tested because the rest of the methods require more than one column. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") null_pct proportion of the valid input data to be replaced by None to form the test data (Default value = 0.1) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") root_path This argument takes in a base folder path for writing out intermediate_data/ folder to. Default value is \"\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. It also print the name of best performing imputation method along with RMSE details. Returns ------- DataFrame Imputed Dataframe \"\"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf ) missing_df . write . parquet ( root_path + \"intermediate_data/imputation_comparison/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/imputation_comparison/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns have all null values: \" + \",\" . join ( empty_cols )) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = idf . columns if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e != id_col )]) ) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) del_cols = [ e for e in list_of_cols if e in empty_cols ] odf_del = idf . drop ( * del_cols ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] num_cols , cat_cols , other_cols = attributeType_segregation ( odf_del . select ( list_of_cols ) ) missing_catcols = [ e for e in cat_cols if e in missing_cols ] missing_numcols = [ e for e in num_cols if e in missing_cols ] if missing_catcols : odf_imputed_cat = imputation_MMM ( spark , odf_del , list_of_cols = missing_catcols , stats_missing = stats_missing ) else : odf_imputed_cat = odf_del if len ( missing_numcols ) == 0 : warnings . warn ( \"No Imputation Performed for numerical columns - No Column(s) to Impute\" ) return odf_imputed_cat idf_test = ( odf_imputed_cat . dropna ( subset = missing_numcols ) . withColumn ( \"index\" , F . monotonically_increasing_id ()) . withColumn ( \"index\" , F . row_number () . over ( Window . orderBy ( \"index\" ))) ) null_count = int ( null_pct * idf_test . count ()) idf_null = idf_test for i in missing_numcols : null_index = random . sample ( range ( idf_test . count ()), null_count ) idf_null = idf_null . withColumn ( i , F . when ( F . col ( \"index\" ) . isin ( null_index ), None ) . otherwise ( F . col ( i )) ) idf_null . write . parquet ( root_path + \"intermediate_data/imputation_comparison/test_dataset\" , mode = \"overwrite\" , ) idf_null = spark . read . parquet ( root_path + \"intermediate_data/imputation_comparison/test_dataset\" ) method1 = imputation_MMM ( spark , idf_null , list_of_cols = missing_numcols , method_type = \"mean\" , stats_missing = stats_missing , output_mode = output_mode , ) method2 = imputation_MMM ( spark , idf_null , list_of_cols = missing_numcols , method_type = \"median\" , stats_missing = stats_missing , output_mode = output_mode , ) valid_methods = [ method1 , method2 ] if len ( num_cols ) > 1 : method3 = imputation_sklearn ( spark , idf_null , list_of_cols = num_cols , method_type = \"KNN\" , stats_missing = stats_missing , output_mode = output_mode , auth_key = auth_key , ) method4 = imputation_sklearn ( spark , idf_null , list_of_cols = num_cols , method_type = \"regression\" , stats_missing = stats_missing , output_mode = output_mode , auth_key = auth_key , ) method5 = imputation_matrixFactorization ( spark , idf_null , list_of_cols = num_cols , id_col = id_col , stats_missing = stats_missing , output_mode = output_mode , ) valid_methods = [ method1 , method2 , method3 , method4 , method5 ] nrmse_all = [] method_all = [ \"MMM-mean\" , \"MMM-median\" , \"KNN\" , \"regression\" , \"matrix_factorization\" ] for index , method in enumerate ( valid_methods ): nrmse = 0 for i in missing_numcols : pred_col = ( i + \"_imputed\" ) if output_mode == \"append\" else i idf_joined = ( idf_test . select ( \"index\" , F . col ( i ) . alias ( \"val\" )) . join ( method . select ( \"index\" , F . col ( pred_col ) . alias ( \"pred\" )), \"index\" , \"left_outer\" , ) . dropna () ) idf_joined = recast_column ( idf = idf_joined , list_of_cols = [ \"val\" , \"pred\" ], list_of_dtypes = [ \"double\" , \"double\" ], ) pred_mean = float ( method . select ( F . mean ( pred_col )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) i_nrmse = ( RegressionEvaluator ( metricName = \"rmse\" , labelCol = \"val\" , predictionCol = \"pred\" ) . evaluate ( idf_joined ) ) / abs ( pred_mean ) nrmse += i_nrmse nrmse_all . append ( nrmse ) min_index = nrmse_all . index ( np . min ( nrmse_all )) best_method = method_all [ min_index ] odf = valid_methods [ min_index ] if print_impact : print ( list ( zip ( method_all , nrmse_all ))) print ( \"Best Imputation Method: \" , best_method ) return odf def autoencoder_latentFeatures ( spark, idf, list_of_cols='all', drop_cols=[], reduction_params=0.5, sample_size=500000, epochs=100, batch_size=256, pre_existing_model=False, model_path='NA', standardization=True, standardization_configs={'pre_existing_model': False, 'model_path': 'NA'}, imputation=False, imputation_configs={'imputation_function': 'imputation_MMM'}, stats_missing={}, output_mode='replace', run_type='local', root_path='', auth_key='NA', print_impact=False) Many machine learning models suffer from \"the curse of dimensionality\" when the number of features is too large. autoencoder_latentFeatures is able to reduce the dimensionality by compressing input attributes to a smaller number of latent features. To be more specific, it trains a neural network model using TensorFlow library. The neural network contains an encoder and a decoder, where the encoder learns to represent the input using smaller number of latent features controlled by the input reduction_params and the decoder learns to reproduce the input using the latent features. In the end, only the encoder will be kept and the latent features generated by the encoder will be added to the output dataframe. However, the neural network model is not trained in a scalable manner, which might not be able to handle large input dataframe. Thus, an input sample_size (the default value is 500,000) can be set to control the number of samples to be used to train the model. If the total number of samples exceeds sample_size , the rest of the samples will be predicted using the fitted encoder. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise, the model might not converge smoothly. Inputs standardization and standardization_configs can be set accordingly to perform standardization within the function. In addition, if a sample contains missing values in the model input, the output values for its latent features will all be None. Thus data imputation is also recommended if missing values exist, which can be done within the function by setting inputs imputation and imputation_configs . Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) reduction_params Determines the number of encoded features in the result. If reduction_params < 1, int(reduction_params * ) columns will be generated. Else, reduction_params columns will be generated. (Default value = 0.5) sample_size Maximum rows for training the autoencoder model using tensorflow. (Default value = 500000) epochs Number of epochs to train the tensorflow model. (Default value = 100) batch_size Number of samples per gradient update when fitting the tensorflow model. (Default value = 256) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_ . \u201cappend\u201d option append transformed columns with format latent_ to the input dataset, e.g. latent_0, latent_1 will be appended if reduction_params=2. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") root_path This argument takes in a base folder path for writing out intermediate_data/ folder to. Default value is \"\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns DataFrame Dataframe with Latent Features Expand source code def autoencoder_latentFeatures ( spark , idf , list_of_cols = \"all\" , drop_cols = [], reduction_params = 0.5 , sample_size = 500000 , epochs = 100 , batch_size = 256 , pre_existing_model = False , model_path = \"NA\" , standardization = True , standardization_configs = { \"pre_existing_model\" : False , \"model_path\" : \"NA\" }, imputation = False , imputation_configs = { \"imputation_function\" : \"imputation_MMM\" }, stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , root_path = \"\" , auth_key = \"NA\" , print_impact = False , ): \"\"\" Many machine learning models suffer from \"the curse of dimensionality\" when the number of features is too large. autoencoder_latentFeatures is able to reduce the dimensionality by compressing input attributes to a smaller number of latent features. To be more specific, it trains a neural network model using TensorFlow library. The neural network contains an encoder and a decoder, where the encoder learns to represent the input using smaller number of latent features controlled by the input *reduction_params* and the decoder learns to reproduce the input using the latent features. In the end, only the encoder will be kept and the latent features generated by the encoder will be added to the output dataframe. However, the neural network model is not trained in a scalable manner, which might not be able to handle large input dataframe. Thus, an input *sample_size* (the default value is 500,000) can be set to control the number of samples to be used to train the model. If the total number of samples exceeds *sample_size*, the rest of the samples will be predicted using the fitted encoder. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise, the model might not converge smoothly. Inputs *standardization* and *standardization_configs* can be set accordingly to perform standardization within the function. In addition, if a sample contains missing values in the model input, the output values for its latent features will all be None. Thus data imputation is also recommended if missing values exist, which can be done within the function by setting inputs *imputation* and *imputation_configs*. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) reduction_params Determines the number of encoded features in the result. If reduction_params < 1, int(reduction_params * <number of columns>) columns will be generated. Else, reduction_params columns will be generated. (Default value = 0.5) sample_size Maximum rows for training the autoencoder model using tensorflow. (Default value = 500000) epochs Number of epochs to train the tensorflow model. (Default value = 100) batch_size Number of samples per gradient update when fitting the tensorflow model. (Default value = 256) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_<col_index>. \u201cappend\u201d option append transformed columns with format latent_<col_index> to the input dataset, e.g. latent_0, latent_1 will be appended if reduction_params=2. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") root_path This argument takes in a base folder path for writing out intermediate_data/ folder to. Default value is \"\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns ------- DataFrame Dataframe with Latent Features \"\"\" if \"arm64\" in platform . version () . lower (): warnings . warn ( \"This function is currently not supported for ARM64 - Mac M1 Machine\" ) return idf num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Latent Features Generated - No Column(s) to Transform\" ) return idf if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , list_of_cols ) missing_df . write . parquet ( root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from dimensionality reduction as all values are null: \" + \",\" . join ( empty_cols ) ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] if standardization : idf_standardized = z_standardization ( spark , idf , list_of_cols = list_of_cols , output_mode = \"append\" , ** standardization_configs , ) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols if ( i + \"_scaled\" ) in idf_standardized . columns ] else : idf_standardized = idf for i in list_of_cols : idf_standardized = idf_standardized . withColumn ( i + \"_scaled\" , F . col ( i )) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols ] if imputation : all_functions = globals () . copy () all_functions . update ( locals ()) f = all_functions . get ( imputation_configs [ \"imputation_function\" ]) args = copy . deepcopy ( imputation_configs ) args . pop ( \"imputation_function\" , None ) missing_df_scaled = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . withColumn ( \"attribute\" , F . concat ( F . col ( \"attribute\" ), F . lit ( \"_scaled\" ))) ) missing_df_scaled . write . parquet ( root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation_scaled\" , mode = \"overwrite\" , ) stats_missing_scaled = { \"file_path\" : root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation_scaled\" , \"file_type\" : \"parquet\" , } idf_imputed = f ( spark , idf_standardized , list_of_cols_scaled , stats_missing = stats_missing_scaled , ** args , ) else : idf_imputed = idf_standardized . dropna ( subset = list_of_cols_scaled ) n_inputs = len ( list_of_cols_scaled ) if reduction_params < 1 : n_bottleneck = int ( reduction_params * n_inputs ) else : n_bottleneck = int ( reduction_params ) if pre_existing_model : if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + model_path + \"/autoencoders_latentFeatures/encoder.h5 .\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( \"aws s3 cp \" + model_path + \"/autoencoders_latentFeatures/model.h5 .\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) encoder = load_model ( \"encoder.h5\" ) model = load_model ( \"model.h5\" ) elif run_type == \"ak8s\" : bash_cmd = ( 'azcopy cp \"' + model_path + \"/autoencoders_latentFeatures/encoder.h5\" + str ( auth_key ) + '\" .' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( 'azcopy cp \"' + model_path + \"/autoencoders_latentFeatures/model.h5\" + str ( auth_key ) + '\" .' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) encoder = load_model ( \"encoder.h5\" ) model = load_model ( \"model.h5\" ) else : encoder = load_model ( model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) model = load_model ( model_path + \"/autoencoders_latentFeatures/model.h5\" ) else : idf_valid = idf_imputed . select ( list_of_cols_scaled ) idf_model = idf_valid . sample ( False , min ( 1.0 , float ( sample_size ) / idf_valid . count ()), 0 ) idf_train = idf_model . sample ( False , 0.8 , 0 ) idf_test = idf_model . subtract ( idf_train ) X_train = idf_train . toPandas () X_test = idf_test . toPandas () visible = Input ( shape = ( n_inputs ,)) e = Dense ( n_inputs * 2 )( visible ) e = BatchNormalization ()( e ) e = LeakyReLU ()( e ) e = Dense ( n_inputs )( e ) e = BatchNormalization ()( e ) e = LeakyReLU ()( e ) bottleneck = Dense ( n_bottleneck )( e ) d = Dense ( n_inputs )( bottleneck ) d = BatchNormalization ()( d ) d = LeakyReLU ()( d ) d = Dense ( n_inputs * 2 )( d ) d = BatchNormalization ()( d ) d = LeakyReLU ()( d ) output = Dense ( n_inputs , activation = \"linear\" )( d ) model = Model ( inputs = visible , outputs = output ) encoder = Model ( inputs = visible , outputs = bottleneck ) model . compile ( optimizer = \"adam\" , loss = \"mse\" ) history = model . fit ( X_train , X_train , epochs = int ( epochs ), batch_size = int ( batch_size ), verbose = 2 , validation_data = ( X_test , X_test ), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): if run_type == \"emr\" : encoder . save ( \"encoder.h5\" ) model . save ( \"model.h5\" ) bash_cmd = ( \"aws s3 cp encoder.h5 \" + model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( \"aws s3 cp model.h5 \" + model_path + \"/autoencoders_latentFeatures/model.h5\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) elif run_type == \"ak8s\" : encoder . save ( \"encoder.h5\" ) model . save ( \"model.h5\" ) bash_cmd = ( 'azcopy cp \"encoder.h5\" \"' + model_path + \"/autoencoders_latentFeatures/encoder.h5\" + str ( auth_key ) + '\" ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( 'azcopy cp \"model.h5\" \"' + model_path + \"/autoencoders_latentFeatures/model.h5\" + str ( auth_key ) + '\" ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) else : if not os . path . exists ( model_path + \"/autoencoders_latentFeatures/\" ): os . makedirs ( model_path + \"/autoencoders_latentFeatures/\" ) encoder . save ( model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) model . save ( model_path + \"/autoencoders_latentFeatures/model.h5\" ) class ModelWrapperPickable : def __init__ ( self , model ): self . model = model def __getstate__ ( self ): model_str = \"\" with tempfile . NamedTemporaryFile ( suffix = \".hdf5\" , delete = True ) as fd : tensorflow . keras . models . save_model ( self . model , fd . name , overwrite = True ) model_str = fd . read () d = { \"model_str\" : model_str } return d def __setstate__ ( self , state ): with tempfile . NamedTemporaryFile ( suffix = \".hdf5\" , delete = True ) as fd : fd . write ( state [ \"model_str\" ]) fd . flush () self . model = tensorflow . keras . models . load_model ( fd . name ) model_wrapper = ModelWrapperPickable ( encoder ) def compute_output_pandas_udf ( model_wrapper ): @F . pandas_udf ( returnType = T . ArrayType ( T . DoubleType ())) def predict_pandas_udf ( * cols ): X = pd . concat ( cols , axis = 1 ) return pd . Series ( row . tolist () for row in model_wrapper . model . predict ( X )) return predict_pandas_udf odf = idf_imputed . withColumn ( \"predicted_output\" , compute_output_pandas_udf ( model_wrapper )( * list_of_cols_scaled ), ) odf_schema = odf . schema for i in range ( 0 , n_bottleneck ): odf_schema = odf_schema . add ( T . StructField ( \"latent_\" + str ( i ), T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"predicted_output\" ])) . toDF ( schema = odf_schema ) . drop ( \"predicted_output\" ) ) odf = odf . drop ( * list_of_cols_scaled ) if output_mode == \"replace\" : odf = odf . drop ( * list_of_cols ) if print_impact : output_cols = [ \"latent_\" + str ( j ) for j in range ( 0 , n_bottleneck )] odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def boxcox_transformation ( idf, list_of_cols='all', drop_cols=[], boxcox_lambda=None, output_mode='replace', print_impact=False) Some machine learning algorithms require the input data to follow normal distributions. Thus, when the input data is too skewed, boxcox_transformation can be used to transform it into a more normal-like distribution. The transformed value of a sample x depends on a coefficient lambda: (1) if lambda = 0, x is transformed into log(x); (2) if lambda !=0, x is transformed into (x^lambda-1)/lambda. The value of lambda can be either specified by the user or automatically selected within the function. If lambda needs to be selected within the function, a range of values (0, 1, -1, 0.5, -0.5, 2, -2, 0.25, -0.25, 3, -3, 4, -4, 5, -5) will be tested and the lambda, which optimizes the KolmogorovSmirnovTest by PySpark with the theoretical distribution being normal, will be used to perform the transformation. Different lambda values can be assigned to different attributes but one attribute can only be assigned one lambda value. Parameters idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) boxcox_lambda Lambda value for box_cox transormation. If boxcox_lambda is not None, it will be directly used for the transformation. It can be a (1) list: each element represents a lambda value for an attribute and the length of the list must be the same as the number of columns to transform. (2) int/float: all attributes will be assigned the same lambda value. Else, search for the best lambda among [1,-1,0.5,-0.5,2,-2,0.25,-0.25,3,-3,4,-4,5,-5] for each column and apply the transformation (Default value = None) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix \" bxcx \" to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns DataFrame Transformed Dataframe Expand source code def boxcox_transformation ( idf , list_of_cols = \"all\" , drop_cols = [], boxcox_lambda = None , output_mode = \"replace\" , print_impact = False , ): \"\"\" Some machine learning algorithms require the input data to follow normal distributions. Thus, when the input data is too skewed, boxcox_transformation can be used to transform it into a more normal-like distribution. The transformed value of a sample x depends on a coefficient lambda: (1) if lambda = 0, x is transformed into log(x); (2) if lambda !=0, x is transformed into (x^lambda-1)/lambda. The value of lambda can be either specified by the user or automatically selected within the function. If lambda needs to be selected within the function, a range of values (0, 1, -1, 0.5, -0.5, 2, -2, 0.25, -0.25, 3, -3, 4, -4, 5, -5) will be tested and the lambda, which optimizes the KolmogorovSmirnovTest by PySpark with the theoretical distribution being normal, will be used to perform the transformation. Different lambda values can be assigned to different attributes but one attribute can only be assigned one lambda value. Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) boxcox_lambda Lambda value for box_cox transormation. If boxcox_lambda is not None, it will be directly used for the transformation. It can be a (1) list: each element represents a lambda value for an attribute and the length of the list must be the same as the number of columns to transform. (2) int/float: all attributes will be assigned the same lambda value. Else, search for the best lambda among [1,-1,0.5,-0.5,2,-2,0.25,-0.25,3,-3,4,-4,5,-5] for each column and apply the transformation (Default value = None) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix \"_bxcx_<lambda>\" to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if ( len ( list_of_cols ) == 0 ) | ( any ( x not in num_cols for x in list_of_cols )): raise TypeError ( \"Invalid input for Column(s)\" ) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] list_of_cols = num_cols odf = idf col_mins = idf . select ([ F . min ( i ) for i in list_of_cols ]) if any ([ i <= 0 for i in col_mins . rdd . flatMap ( lambda x : x ) . collect ()]): col_mins . show ( 1 , False ) raise ValueError ( \"Data must be positive\" ) if boxcox_lambda is not None : if isinstance ( boxcox_lambda , ( list , tuple )): if len ( boxcox_lambda ) != len ( list_of_cols ): raise TypeError ( \"Invalid input for boxcox_lambda\" ) elif not all ([ isinstance ( l , ( float , int )) for l in boxcox_lambda ]): raise TypeError ( \"Invalid input for boxcox_lambda\" ) else : boxcox_lambda_list = list ( boxcox_lambda ) elif isinstance ( boxcox_lambda , ( float , int )): boxcox_lambda_list = [ boxcox_lambda ] * len ( list_of_cols ) else : raise TypeError ( \"Invalid input for boxcox_lambda\" ) else : boxcox_lambda_list = [] for i in list_of_cols : lambdaVal = [ 1 , - 1 , 0.5 , - 0.5 , 2 , - 2 , 0.25 , - 0.25 , 3 , - 3 , 4 , - 4 , 5 , - 5 ] best_pVal = 0 for j in lambdaVal : pVal = Statistics . kolmogorovSmirnovTest ( odf . select ( F . pow ( F . col ( i ), j )) . rdd . flatMap ( lambda x : x ), \"norm\" ) . pValue if pVal > best_pVal : best_pVal = pVal best_lambdaVal = j pVal = Statistics . kolmogorovSmirnovTest ( odf . select ( F . log ( F . col ( i ))) . rdd . flatMap ( lambda x : x ), \"norm\" ) . pValue if pVal > best_pVal : best_pVal = pVal best_lambdaVal = 0 boxcox_lambda_list . append ( best_lambdaVal ) output_cols = [] for i , curr_lambdaVal in zip ( list_of_cols , boxcox_lambda_list ): if curr_lambdaVal != 1 : modify_col = ( ( i + \"_bxcx_\" + str ( curr_lambdaVal )) if output_mode == \"append\" else i ) output_cols . append ( modify_col ) if curr_lambdaVal == 0 : odf = odf . withColumn ( modify_col , F . log ( F . col ( i ))) else : odf = odf . withColumn ( modify_col , F . pow ( F . col ( i ), curr_lambdaVal )) if len ( output_cols ) == 0 : warnings . warn ( \"lambdaVal for all columns are 1 so no transformation is performed and idf is returned\" ) return idf if print_impact : print ( \"Transformed Columns: \" , list_of_cols ) print ( \"Best BoxCox Parameter(s): \" , boxcox_lambda_list ) print ( \"Before:\" ) idf . select ( list_of_cols ) . describe () . unionByName ( idf . select ([ F . skewness ( i ) . alias ( i ) for i in list_of_cols ]) . withColumn ( \"summary\" , F . lit ( \"skewness\" ) ) ) . show ( 6 , False ) print ( \"After:\" ) if output_mode == \"replace\" : odf . select ( list_of_cols ) . describe () . unionByName ( odf . select ([ F . skewness ( i ) . alias ( i ) for i in list_of_cols ]) . withColumn ( \"summary\" , F . lit ( \"skewness\" ) ) ) . show ( 6 , False ) else : output_cols = [( \"`\" + i + \"`\" ) for i in output_cols ] odf . select ( output_cols ) . describe () . unionByName ( odf . select ( [ F . skewness ( i ) . alias ( i [ 1 : - 1 ]) for i in output_cols ] ) . withColumn ( \"summary\" , F . lit ( \"skewness\" )) ) . show ( 6 , False ) return odf def cat_to_num_supervised ( spark, idf, list_of_cols='all', drop_cols=[], label_col='label', event_label=1, pre_existing_model=False, model_path='NA', output_mode='replace', persist=False, persist_option=StorageLevel(True, True, False, False, 1), print_impact=False) This is a supervised method to convert a categorical attribute into a numerical attribute. It takes a label/target column to indicate whether the event is positive or negative. For each column, the positive event rate for each categorical value is used as the encoded numerical value. For example, there are 3 distinct values in a categorical attribute X: X1, X2 and X3. Within the input dataframe, there are - 15 positive events and 5 negative events with X==X1; - 10 positive events and 40 negative events with X==X2; - 20 positive events and 20 negative events with X==X3. Thus, value X1 is mapped to 15/(15+5) = 0.75, value X2 is mapped to 10/(10+40) = 0.2 and value X3 is mapped to 20/(20+20) = 0.5. This mapping will be applied to all values in attribute X. This encoding method can be helpful in avoiding creating too many dummy variables which may cause dimensionality issue and it also works with categorical attributes without an order or rank. Parameters spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) pre_existing_model Boolean argument \u2013 True or False. True if model (original and mapped numerical value for each column) exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" is used to save the model in \"intermediate_data/\" folder for optimization purpose. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_encoded\" e.g. column X is appended as X_encoded. (Default value = \"replace\") persist Boolean argument - True or False. This parameter is for optimization purpose. If True, repeatedly used dataframe will be persisted (StorageLevel can be specified in persist_option). We recommend setting this parameter as True if at least one of the following criteria is True: (1) The underlying data source is in csv format (2) The transformation will be applicable to most columns. (Default value = False) persist_option A pyspark.StorageLevel instance. This parameter is useful only when persist is True. (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) print_impact True, False (Default value = False) This argument is to print out the descriptive statistics of encoded columns. Returns DataFrame Encoded Dataframe Expand source code def cat_to_num_supervised ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , persist = False , persist_option = pyspark . StorageLevel . MEMORY_AND_DISK , print_impact = False , ): \"\"\" This is a supervised method to convert a categorical attribute into a numerical attribute. It takes a label/target column to indicate whether the event is positive or negative. For each column, the positive event rate for each categorical value is used as the encoded numerical value. For example, there are 3 distinct values in a categorical attribute X: X1, X2 and X3. Within the input dataframe, there are - 15 positive events and 5 negative events with X==X1; - 10 positive events and 40 negative events with X==X2; - 20 positive events and 20 negative events with X==X3. Thus, value X1 is mapped to 15/(15+5) = 0.75, value X2 is mapped to 10/(10+40) = 0.2 and value X3 is mapped to 20/(20+20) = 0.5. This mapping will be applied to all values in attribute X. This encoding method can be helpful in avoiding creating too many dummy variables which may cause dimensionality issue and it also works with categorical attributes without an order or rank. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) pre_existing_model Boolean argument \u2013 True or False. True if model (original and mapped numerical value for each column) exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" is used to save the model in \"intermediate_data/\" folder for optimization purpose. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_encoded\" e.g. column X is appended as X_encoded. (Default value = \"replace\") persist Boolean argument - True or False. This parameter is for optimization purpose. If True, repeatedly used dataframe will be persisted (StorageLevel can be specified in persist_option). We recommend setting this parameter as True if at least one of the following criteria is True: (1) The underlying data source is in csv format (2) The transformation will be applicable to most columns. (Default value = False) persist_option A pyspark.StorageLevel instance. This parameter is useful only when persist is True. (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) print_impact True, False (Default value = False) This argument is to print out the descriptive statistics of encoded columns. Returns ------- DataFrame Encoded Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols elif isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e != label_col )]) ) if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Categorical Encoding - No categorical column(s) to transform\" ) return idf if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) odf = idf label_col_bool = label_col + \"_cat_to_num_sup_temp\" idf = idf . withColumn ( label_col_bool , F . when ( F . col ( label_col ) == event_label , \"1\" ) . otherwise ( \"0\" ), ) if model_path == \"NA\" : skip_if_error = True model_path = \"intermediate_data\" else : skip_if_error = False save_model = True if persist : idf = idf . persist ( persist_option ) for index , i in enumerate ( list_of_cols ): if pre_existing_model : df_tmp = spark . read . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , inferSchema = True , ) else : df_tmp = ( idf . select ( i , label_col_bool ) . groupBy ( i ) . pivot ( label_col_bool ) . count () . fillna ( 0 ) . withColumn ( i + \"_encoded\" , F . round ( F . col ( \"1\" ) / ( F . col ( \"1\" ) + F . col ( \"0\" )), 4 ) ) . drop ( * [ \"1\" , \"0\" ]) ) if save_model : try : df_tmp . coalesce ( 1 ) . write . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , mode = \"overwrite\" , ignoreLeadingWhiteSpace = False , ignoreTrailingWhiteSpace = False , ) df_tmp = spark . read . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , inferSchema = True , ) except Exception as error : if skip_if_error : warnings . warn ( \"For optimization purpose, we recommend specifying a valid model_path value to save the intermediate data. Saving to the default path - '\" + model_path + \"/cat_to_num_supervised/\" + i + \"' faced an error.\" ) save_model = False else : raise error if df_tmp . count () > 1 : odf = odf . join ( df_tmp , i , \"left_outer\" ) else : odf = odf . crossJoin ( df_tmp ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_encoded\" , i ) odf = odf . select ([ i for i in idf . columns if i != label_col_bool ]) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_encoded\" ) for i in list_of_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . summary ( \"count\" , \"min\" , \"max\" ) . show ( 3 , False ) print ( \"After: \" ) odf . select ( output_cols ) . summary ( \"count\" , \"min\" , \"max\" ) . show ( 3 , False ) if persist : idf . unpersist () return odf def cat_to_num_transformer ( spark, idf, list_of_cols, drop_cols, method_type, encoding, label_col, event_label) This is method which helps converting a categorical attribute into numerical attribute(s) based on the analysis dataset. If there's a presence of label column then the relevant processing would happen through cat_to_num_supervised. However, for unsupervised scenario, the processing would happen through cat_to_num_unsupervised. Computation details can be referred from the respective functions. Parameters spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. method_type Depending upon the use case the method type can be either Supervised or Unsupervised. For Supervised use case, label_col is mandatory. encoding \"label_encoding\" or \"onehot_encoding\" In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available that can be selected by index_order argument). In one-hot encoding, every unique value in the column will be added in a form of dummy/binary column. label_col Label/Target column event_label Value of (positive) event Expand source code def cat_to_num_transformer ( spark , idf , list_of_cols , drop_cols , method_type , encoding , label_col , event_label ): \"\"\" This is method which helps converting a categorical attribute into numerical attribute(s) based on the analysis dataset. If there's a presence of label column then the relevant processing would happen through cat_to_num_supervised. However, for unsupervised scenario, the processing would happen through cat_to_num_unsupervised. Computation details can be referred from the respective functions. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. method_type Depending upon the use case the method type can be either Supervised or Unsupervised. For Supervised use case, label_col is mandatory. encoding \"label_encoding\" or \"onehot_encoding\" In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available that can be selected by index_order argument). In one-hot encoding, every unique value in the column will be added in a form of dummy/binary column. label_col Label/Target column event_label Value of (positive) event \"\"\" num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if len ( cat_cols ) > 0 : if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if ( method_type == \"supervised\" ) & ( label_col is not None ): odf = cat_to_num_supervised ( spark , idf , label_col = label_col , event_label = event_label ) odf = odf . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , F . lit ( 1 )) . otherwise ( F . lit ( 0 )), ) return odf elif ( method_type == \"unsupervised\" ) & ( label_col is None ): odf = cat_to_num_unsupervised ( spark , idf , method_type = encoding , index_order = \"frequencyDesc\" , ) return odf else : return idf def cat_to_num_unsupervised ( spark, idf, list_of_cols='all', drop_cols=[], method_type='label_encoding', index_order='frequencyDesc', cardinality_threshold=50, pre_existing_model=False, model_path='NA', stats_unique={}, output_mode='replace', print_impact=False) This is unsupervised method of converting a categorical attribute into numerical attribute(s). This is among the most important transformations required for any modelling exercise, as most of the machine learning algorithms cannot process categorical values. It covers two popular encoding techniques \u2013 label encoding & one-hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available \u2013 can be selected by index_order argument). One of the pitfalls of using this technique is that the model may learn some spurious relationship, which doesn't exist or might not make any logical sense in the real world settings. In one-hot encoding, every unique value in the attribute will be added as a feature in a form of dummy/binary attribute. However, using this method on high cardinality attributes can further aggravate the dimensionality issue. Parameters spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"label_encoding\" or \"onehot_encoding\" In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available that can be selected by index_order argument). In one-hot encoding, every unique value in the column will be added in a form of dummy/binary column. (Default value = 1) index_order \"frequencyDesc\", \"frequencyAsc\", \"alphabetDesc\", \"alphabetAsc\". Valid only for Label Encoding method_type. (Default value = \"frequencyDesc\") cardinality_threshold Defines threshold to skip columns with higher cardinality values from encoding - a warning is issued. (Default value = 50) pre_existing_model Boolean argument - True or False. True if encoding model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre existing model nor there is a need to save one. stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \" index\" for label encoding e.g. column X is appended as X_index, or a postfix \" \" for one hot encoding, n varies from 0 to unique value count e.g. column X is appended as X_0, X_1, X_2 (n = 3 i.e. no. of unique values for X). (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the change in schema (one hot encoding) or descriptive statistics (label encoding) Returns DataFrame Encoded Dataframe Expand source code def cat_to_num_unsupervised ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"label_encoding\" , index_order = \"frequencyDesc\" , cardinality_threshold = 50 , pre_existing_model = False , model_path = \"NA\" , stats_unique = {}, output_mode = \"replace\" , print_impact = False , ): \"\"\" This is unsupervised method of converting a categorical attribute into numerical attribute(s). This is among the most important transformations required for any modelling exercise, as most of the machine learning algorithms cannot process categorical values. It covers two popular encoding techniques \u2013 label encoding & one-hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available \u2013 can be selected by index_order argument). One of the pitfalls of using this technique is that the model may learn some spurious relationship, which doesn't exist or might not make any logical sense in the real world settings. In one-hot encoding, every unique value in the attribute will be added as a feature in a form of dummy/binary attribute. However, using this method on high cardinality attributes can further aggravate the dimensionality issue. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"label_encoding\" or \"onehot_encoding\" In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available that can be selected by index_order argument). In one-hot encoding, every unique value in the column will be added in a form of dummy/binary column. (Default value = 1) index_order \"frequencyDesc\", \"frequencyAsc\", \"alphabetDesc\", \"alphabetAsc\". Valid only for Label Encoding method_type. (Default value = \"frequencyDesc\") cardinality_threshold Defines threshold to skip columns with higher cardinality values from encoding - a warning is issued. (Default value = 50) pre_existing_model Boolean argument - True or False. True if encoding model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre existing model nor there is a need to save one. stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_index\" for label encoding e.g. column X is appended as X_index, or a postfix \"_{n}\" for one hot encoding, n varies from 0 to unique value count e.g. column X is appended as X_0, X_1, X_2 (n = 3 i.e. no. of unique values for X). (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the change in schema (one hot encoding) or descriptive statistics (label encoding) Returns ------- DataFrame Encoded Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"onehot_encoding\" , \"label_encoding\" ): raise TypeError ( \"Invalid input for method_type\" ) if index_order not in ( \"frequencyDesc\" , \"frequencyAsc\" , \"alphabetDesc\" , \"alphabetAsc\" , ): raise TypeError ( \"Invalid input for Encoding Index Order\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if stats_unique == {}: skip_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) > cardinality_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : skip_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) > cardinality_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) skip_cols = list ( set ([ e for e in skip_cols if e in list_of_cols and e not in drop_cols ]) ) if skip_cols : warnings . warn ( \"Columns dropped from encoding due to high cardinality: \" + \",\" . join ( skip_cols ) ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols + skip_cols ]) ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Encoding Computation - No categorical column(s) to transform\" ) return idf list_of_cols_vec = [] list_of_cols_idx = [] for i in list_of_cols : list_of_cols_vec . append ( i + \"_vec\" ) list_of_cols_idx . append ( i + \"_index\" ) odf_indexed = idf if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): for idx , i in enumerate ( list_of_cols ): if pre_existing_model : indexerModel = StringIndexerModel . load ( model_path + \"/cat_to_num_unsupervised/indexer-model/\" + i ) else : stringIndexer = StringIndexer ( inputCol = i , outputCol = i + \"_index\" , stringOrderType = index_order , handleInvalid = \"keep\" , ) indexerModel = stringIndexer . fit ( idf . select ( i )) if model_path != \"NA\" : indexerModel . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/indexer-model/\" + i ) odf_indexed = indexerModel . transform ( odf_indexed ) else : if pre_existing_model : indexerModel = StringIndexerModel . load ( model_path + \"/cat_to_num_unsupervised/indexer\" ) else : stringIndexer = StringIndexer ( inputCols = list_of_cols , outputCols = list_of_cols_idx , stringOrderType = index_order , handleInvalid = \"keep\" , ) indexerModel = stringIndexer . fit ( odf_indexed ) if model_path != \"NA\" : indexerModel . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/indexer\" ) odf_indexed = indexerModel . transform ( odf_indexed ) if method_type == \"onehot_encoding\" : if pre_existing_model : encoder = OneHotEncoder . load ( model_path + \"/cat_to_num_unsupervised/encoder\" ) else : encoder = OneHotEncoder ( inputCols = list_of_cols_idx , outputCols = list_of_cols_vec , handleInvalid = \"keep\" , ) if model_path != \"NA\" : encoder . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/encoder\" ) odf = encoder . fit ( odf_indexed ) . transform ( odf_indexed ) new_cols = [] odf_sample = odf . take ( 1 ) for i in list_of_cols : odf_schema = odf . schema uniq_cats = odf_sample [ 0 ] . asDict ()[ i + \"_vec\" ] . size for j in range ( 0 , uniq_cats ): odf_schema = odf_schema . add ( T . StructField ( i + \"_\" + str ( j ), T . IntegerType ()) ) new_cols . append ( i + \"_\" + str ( j )) odf = odf . rdd . map ( lambda x : ( * x , * ( DenseVector ( x [ i + \"_vec\" ]) . toArray () . astype ( int ) . tolist ()), ) ) . toDF ( schema = odf_schema ) if output_mode == \"replace\" : odf = odf . drop ( i , i + \"_vec\" , i + \"_index\" ) else : odf = odf . drop ( i + \"_vec\" , i + \"_index\" ) else : odf = odf_indexed for i in list_of_cols : odf = odf . withColumn ( i + \"_index\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . col ( i + \"_index\" ) . cast ( T . IntegerType ()) ), ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_index\" , i ) odf = odf . select ( idf . columns ) if print_impact : if method_type == \"label_encoding\" : if output_mode == \"append\" : new_cols = [ i + \"_index\" for i in list_of_cols ] else : new_cols = list_of_cols print ( \"Before\" ) idf . select ( list_of_cols ) . summary ( \"count\" , \"min\" , \"max\" ) . show ( 3 , False ) print ( \"After\" ) odf . select ( new_cols ) . summary ( \"count\" , \"min\" , \"max\" ) . show ( 3 , False ) if method_type == \"onehot_encoding\" : print ( \"Before\" ) idf . select ( list_of_cols ) . printSchema () print ( \"After\" ) if output_mode == \"append\" : odf . select ( list_of_cols + new_cols ) . printSchema () else : odf . select ( new_cols ) . printSchema () if skip_cols : print ( \"Columns dropped from encoding due to high cardinality: \" + \",\" . join ( skip_cols ) ) return odf def expression_parser ( idf, list_of_expr, postfix='', print_impact=False) expression_parser can be used to evaluate a list of SQL expressions and output the result as new features. It is able to handle column names containing special characters such as \u201c.\u201d, \u201c-\u201d, \u201c@\u201d, \u201c^\u201d, etc, by converting them to \u201c_\u201d first before the evaluation and convert them back to the original names before returning the output dataframe. Parameters idf Input Dataframe list_of_expr List of expressions to evaluate as new features e.g., [\"expr1\",\"expr2\"]. Alternatively, expressions can be specified in a string format, where different expressions are separated by pipe delimiter \u201c|\u201d e.g., \"expr1|expr2\". postfix postfix for new feature name.Naming convention \"f\" + expression_index + postfix e.g. with postfix of \"new\", new added features are named as f0new, f1new etc. (Default value = \"\") print_impact True, False This argument is to print the descriptive statistics of the parsed features (Default value = False) Returns DataFrame Parsed Dataframe Expand source code def expression_parser ( idf , list_of_expr , postfix = \"\" , print_impact = False ): \"\"\" expression_parser can be used to evaluate a list of SQL expressions and output the result as new features. It is able to handle column names containing special characters such as \u201c.\u201d, \u201c-\u201d, \u201c@\u201d, \u201c^\u201d, etc, by converting them to \u201c_\u201d first before the evaluation and convert them back to the original names before returning the output dataframe. Parameters ---------- idf Input Dataframe list_of_expr List of expressions to evaluate as new features e.g., [\"expr1\",\"expr2\"]. Alternatively, expressions can be specified in a string format, where different expressions are separated by pipe delimiter \u201c|\u201d e.g., \"expr1|expr2\". postfix postfix for new feature name.Naming convention \"f\" + expression_index + postfix e.g. with postfix of \"new\", new added features are named as f0new, f1new etc. (Default value = \"\") print_impact True, False This argument is to print the descriptive statistics of the parsed features (Default value = False) Returns ------- DataFrame Parsed Dataframe \"\"\" if isinstance ( list_of_expr , str ): list_of_expr = [ x . strip () for x in list_of_expr . split ( \"|\" )] special_chars = [ \"&\" , \"$\" , \";\" , \":\" , \",\" , \"*\" , \"#\" , \"@\" , \"?\" , \"%\" , \"!\" , \"^\" , \"(\" , \")\" , \"-\" , \"/\" , \"'\" , \".\" , '\"' , ] rename_cols = [] replace_chars = {} for char in special_chars : for col in idf . columns : if char in col : rename_cols . append ( col ) if col in replace_chars . keys (): ( replace_chars [ col ]) . append ( char ) else : replace_chars [ col ] = [ char ] rename_mapping_to_new , rename_mapping_to_old = {}, {} idf_renamed = idf for col in rename_cols : new_col = col for char in replace_chars [ col ]: new_col = new_col . replace ( char , \"_\" ) rename_mapping_to_old [ new_col ] = col rename_mapping_to_new [ col ] = new_col idf_renamed = idf_renamed . withColumnRenamed ( col , new_col ) list_of_expr_ = [] for expr in list_of_expr : new_expr = expr for col in rename_cols : if col in expr : new_expr = new_expr . replace ( col , rename_mapping_to_new [ col ]) list_of_expr_ . append ( new_expr ) list_of_expr = list_of_expr_ odf = idf_renamed new_cols = [] for index , exp in enumerate ( list_of_expr ): odf = odf . withColumn ( \"f\" + str ( index ) + postfix , F . expr ( exp )) new_cols . append ( \"f\" + str ( index ) + postfix ) for new_col , col in rename_mapping_to_old . items (): odf = odf . withColumnRenamed ( new_col , col ) if print_impact : print ( \"Columns Added: \" , new_cols ) odf . select ( new_cols ) . describe () . show ( 5 , False ) return odf def feature_transformation ( idf, list_of_cols='all', drop_cols=[], method_type='sqrt', N=None, output_mode='replace', print_impact=False) As the name indicates, feature_transformation performs mathematical transformation over selected attributes. The following methods are supported for an input attribute x: ln(x), log10(x), log2(x), e^x, 2^x, 10^x, N^x, square and cube root of x, x^2, x^3, x^N, trigonometric transformations of x (sin, cos, tan, asin, acos, atan), radians, x%N, x!, 1/x, floor and ceiling of x and x rounded to N decimal places. Some transformations only work with positive or non-negative input values such as log and square root and an error will be returned if violated. Parameters idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"ln\", \"log10\", \"log2\", \"exp\", \"powOf2\" (2^x), \"powOf10\" (10^x), \"powOfN\" (N^x), \"sqrt\" (square root), \"cbrt\" (cube root), \"sq\" (square), \"cb\" (cube), \"toPowerN\" (x^N), \"sin\", \"cos\", \"tan\", \"asin\", \"acos\", \"atan\", \"radians\", \"remainderDivByN\" (x%N), \"factorial\" (x!), \"mul_inv\" (1/x), \"floor\", \"ceil\", \"roundN\" (round to N decimal places) (Default value = \"sqrt\") N None by default. If method_type is \"powOfN\", \"toPowerN\", \"remainderDivByN\" or \"roundN\", N will be used as the required constant. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix (E.g. \"_ln\", \"_powOf \") to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns DataFrame Transformed Dataframe Expand source code def feature_transformation ( idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"sqrt\" , N = None , output_mode = \"replace\" , print_impact = False , ): \"\"\" As the name indicates, feature_transformation performs mathematical transformation over selected attributes. The following methods are supported for an input attribute x: ln(x), log10(x), log2(x), e^x, 2^x, 10^x, N^x, square and cube root of x, x^2, x^3, x^N, trigonometric transformations of x (sin, cos, tan, asin, acos, atan), radians, x%N, x!, 1/x, floor and ceiling of x and x rounded to N decimal places. Some transformations only work with positive or non-negative input values such as log and square root and an error will be returned if violated. Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"ln\", \"log10\", \"log2\", \"exp\", \"powOf2\" (2^x), \"powOf10\" (10^x), \"powOfN\" (N^x), \"sqrt\" (square root), \"cbrt\" (cube root), \"sq\" (square), \"cb\" (cube), \"toPowerN\" (x^N), \"sin\", \"cos\", \"tan\", \"asin\", \"acos\", \"atan\", \"radians\", \"remainderDivByN\" (x%N), \"factorial\" (x!), \"mul_inv\" (1/x), \"floor\", \"ceil\", \"roundN\" (round to N decimal places) (Default value = \"sqrt\") N None by default. If method_type is \"powOfN\", \"toPowerN\", \"remainderDivByN\" or \"roundN\", N will be used as the required constant. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix (E.g. \"_ln\", \"_powOf<N>\") to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if ( len ( list_of_cols ) == 0 ) | ( any ( x not in num_cols for x in list_of_cols )): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in [ \"ln\" , \"log10\" , \"log2\" , \"exp\" , \"powOf2\" , \"powOf10\" , \"powOfN\" , \"sqrt\" , \"cbrt\" , \"sq\" , \"cb\" , \"toPowerN\" , \"sin\" , \"cos\" , \"tan\" , \"asin\" , \"acos\" , \"atan\" , \"radians\" , \"remainderDivByN\" , \"factorial\" , \"mul_inv\" , \"floor\" , \"ceil\" , \"roundN\" , ]: raise TypeError ( \"Invalid input method_type\" ) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] list_of_cols = num_cols odf = idf transformation_function = { \"ln\" : F . log , \"log10\" : F . log10 , \"log2\" : F . log2 , \"exp\" : F . exp , \"powOf2\" : ( lambda x : F . pow ( 2.0 , x )), \"powOf10\" : ( lambda x : F . pow ( 10.0 , x )), \"powOfN\" : ( lambda x : F . pow ( N , x )), \"sqrt\" : F . sqrt , \"cbrt\" : F . cbrt , \"sq\" : ( lambda x : x ** 2 ), \"cb\" : ( lambda x : x ** 3 ), \"toPowerN\" : ( lambda x : x ** N ), \"sin\" : F . sin , \"cos\" : F . cos , \"tan\" : F . tan , \"asin\" : F . asin , \"acos\" : F . acos , \"atan\" : F . atan , \"radians\" : F . radians , \"remainderDivByN\" : ( lambda x : x % F . lit ( N )), \"factorial\" : F . factorial , \"mul_inv\" : ( lambda x : F . lit ( 1 ) / x ), \"floor\" : F . floor , \"ceil\" : F . ceil , \"roundN\" : ( lambda x : F . round ( x , N )), } def get_col_name ( i ): if output_mode == \"replace\" : return i else : if method_type in [ \"powOfN\" , \"toPowerN\" , \"remainderDivByN\" , \"roundN\" ]: return i + \"_\" + method_type [: - 1 ] + str ( N ) else : return i + \"_\" + method_type output_cols = [] for i in list_of_cols : modify_col = get_col_name ( i ) odf = odf . withColumn ( modify_col , transformation_function [ method_type ]( F . col ( i ))) output_cols . append ( modify_col ) if print_impact : print ( \"Before:\" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After:\" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def imputation_MMM ( spark, idf, list_of_cols='missing', drop_cols=[], method_type='median', pre_existing_model=False, model_path='NA', output_mode='replace', stats_missing={}, stats_mode={}, print_impact=False) This function handles missing value related issues by substituting null values by the measure of central tendency (mode for categorical features and mean/median for numerical features). For numerical attributes, it leverages Imputer functionality of Spark MLlib. Though, Imputer can be used for categorical attributes but this feature is available only in Spark3.x, therefore for categorical features, we compute mode or leverage mode computation from Measures of Central Tendency. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"median\", \"mean\" (valid only for for numerical columns attributes). Mode is only option for categorical columns. (Default value = \"median\") pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns DataFrame Imputed Dataframe Expand source code def imputation_MMM ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], method_type = \"median\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_missing = {}, stats_mode = {}, print_impact = False , ): \"\"\" This function handles missing value related issues by substituting null values by the measure of central tendency (mode for categorical features and mean/median for numerical features). For numerical attributes, it leverages [Imputer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer .html) functionality of Spark MLlib. Though, Imputer can be used for categorical attributes but this feature is available only in Spark3.x, therefore for categorical features, we compute mode or leverage mode computation from Measures of Central Tendency. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"median\", \"mean\" (valid only for for numerical columns attributes). Mode is only option for categorical columns. (Default value = \"median\") pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf ) else : missing_df = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) if ( len ( missing_cols ) == 0 ) & ( not pre_existing_model ) & ( model_path == \"NA\" ): return idf num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols + cat_cols if list_of_cols == \"missing\" : list_of_cols = [ x for x in missing_cols if x in num_cols + cat_cols ] if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Imputation performed- No column(s) to impute\" ) return idf if any ( x not in num_cols + cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"mode\" , \"mean\" , \"median\" ): raise TypeError ( \"Invalid input for method_type\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) odf = idf if len ( num_cols ) > 0 : recast_cols = [] recast_type = [] for i in num_cols : if get_dtype ( idf , i ) not in ( \"float\" , \"double\" ): odf = odf . withColumn ( i , F . col ( i ) . cast ( T . DoubleType ())) recast_cols . append ( i + \"_imputed\" ) recast_type . append ( get_dtype ( idf , i )) # For mode imputation if method_type == \"mode\" : if stats_mode == {}: parameters = [ str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) for i in num_cols ] else : mode_df = read_dataset ( spark , ** stats_mode ) . replace ( \"None\" , None ) mode_df_cols = list ( mode_df . select ( \"attribute\" ) . toPandas ()[ \"attribute\" ]) parameters = [] for i in num_cols : if i not in mode_df_cols : parameters . append ( str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) ) else : parameters . append ( mode_df . where ( F . col ( \"attribute\" ) == i ) . select ( \"mode\" ) . rdd . flatMap ( list ) . collect ()[ 0 ] ) for index , i in enumerate ( num_cols ): odf = odf . withColumn ( i + \"_imputed\" , F . when ( F . col ( i ) . isNull (), parameters [ index ]) . otherwise ( F . col ( i )), ) else : # For mean, median imputation # Building new imputer model or uploading the existing model if pre_existing_model : imputerModel = ImputerModel . load ( model_path + \"/imputation_MMM/num_imputer-model\" ) else : imputer = Imputer ( strategy = method_type , inputCols = num_cols , outputCols = [( e + \"_imputed\" ) for e in num_cols ], ) imputerModel = imputer . fit ( odf ) # Applying model # odf = recast_column(imputerModel.transform(odf), recast_cols, recast_type) odf = imputerModel . transform ( odf ) for i , j in zip ( recast_cols , recast_type ): odf = odf . withColumn ( i , F . col ( i ) . cast ( j )) # Saving model if required if ( not pre_existing_model ) & ( model_path != \"NA\" ): imputerModel . write () . overwrite () . save ( model_path + \"/imputation_MMM/num_imputer-model\" ) if len ( cat_cols ) > 0 : if pre_existing_model : df_model = spark . read . csv ( model_path + \"/imputation_MMM/cat_imputer\" , header = True , inferSchema = True , ) parameters = [] for i in cat_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : if stats_mode == {}: parameters = [ str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) for i in cat_cols ] else : mode_df = read_dataset ( spark , ** stats_mode ) . replace ( \"None\" , None ) parameters = [ mode_df . where ( F . col ( \"attribute\" ) == i ) . select ( \"mode\" ) . rdd . flatMap ( list ) . collect ()[ 0 ] for i in cat_cols ] for index , i in enumerate ( cat_cols ): odf = odf . withColumn ( i + \"_imputed\" , F . when ( F . col ( i ) . isNull (), parameters [ index ]) . otherwise ( F . col ( i )), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( cat_cols , parameters ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . repartition ( 1 ) . write . csv ( model_path + \"/imputation_MMM/cat_imputer\" , header = True , mode = \"overwrite\" , ) for i in num_cols + cat_cols : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) elif output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : output_cols = [ ( i + \"_imputed\" ) for i in [ e for e in ( num_cols + cat_cols ) if e in missing_cols ] ] odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def imputation_matrixFactorization ( spark, idf, list_of_cols='missing', drop_cols=[], id_col='', output_mode='replace', stats_missing={}, print_impact=False) imputation_matrixFactorization uses collaborative filtering technique to impute missing values. Collaborative filtering is commonly used in recommender systems to fill the missing user-item entries and PySpark provides an implementation using alternating least squares (ALS) algorithm, which is used in this function. To fit our problem into the ALS model, each attribute is treated as an item and an id column needs to be specified by the user to generate the user-item pairs. In the case, ID column doesn't exist in the dataset and proxu ID column is implicitly generated by the function. Subsequently, all user-item pairs with known values will be used to train the ALS model and the trained model can be used to predict the user-item pairs with missing values. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns DataFrame Imputed Dataframe Expand source code def imputation_matrixFactorization ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], id_col = \"\" , output_mode = \"replace\" , stats_missing = {}, print_impact = False , ): \"\"\" imputation_matrixFactorization uses collaborative filtering technique to impute missing values. Collaborative filtering is commonly used in recommender systems to fill the missing user-item entries and PySpark provides an implementation using alternating least squares (ALS) algorithm, which is used in this function. To fit our problem into the ALS model, each attribute is treated as an item and an id column needs to be specified by the user to generate the user-item pairs. In the case, ID column doesn't exist in the dataset and proxu ID column is implicitly generated by the function. Subsequently, all user-item pairs with known values will be used to train the ALS model and the trained model can be used to predict the user-item pairs with missing values. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , num_cols ) else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( num_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns dropped from the imputation as all values are null: \" + \",\" . join ( empty_cols ) ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . where ( F . col ( \"missing_pct\" ) < 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = num_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ( [ e for e in list_of_cols if ( e not in drop_cols ) & ( e != id_col ) & ( e not in empty_cols ) ] ) ) if ( len ( list_of_cols ) == 0 ) | ( len ([ e for e in list_of_cols if e in missing_cols ]) == 0 ): warnings . warn ( \"No Imputation Performed - No Column(s) to Impute\" ) return idf if len ( list_of_cols ) == 1 : warnings . warn ( \"No Imputation Performed - Needs more than 1 column for matrix factorization\" ) return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) remove_id = False if id_col == \"\" : idf = idf . withColumn ( \"id\" , F . monotonically_increasing_id ()) . withColumn ( \"id\" , F . row_number () . over ( Window . orderBy ( \"id\" )) ) id_col = \"id\" remove_id = True key_and_val = F . create_map ( list ( chain . from_iterable ([[ F . lit ( c ), F . col ( c )] for c in list_of_cols ])) ) df_flatten = idf . select ( id_col , F . explode ( key_and_val )) . withColumn ( \"key\" , F . concat ( F . col ( \"key\" ), F . lit ( \"_imputed\" )) ) id_type = get_dtype ( idf , id_col ) if id_type == \"string\" : id_indexer = StringIndexer () . setInputCol ( id_col ) . setOutputCol ( \"IDLabel\" ) id_indexer_model = id_indexer . fit ( df_flatten ) df_flatten = id_indexer_model . transform ( df_flatten ) . drop ( id_col ) else : df_flatten = df_flatten . withColumnRenamed ( id_col , \"IDLabel\" ) indexer = StringIndexer () . setInputCol ( \"key\" ) . setOutputCol ( \"keyLabel\" ) indexer_model = indexer . fit ( df_flatten ) df_encoded = indexer_model . transform ( df_flatten ) . drop ( \"key\" ) df_model = df_encoded . where ( F . col ( \"value\" ) . isNotNull ()) df_test = df_encoded . where ( F . col ( \"value\" ) . isNull ()) if ( df_model . select ( \"IDLabel\" ) . distinct () . count () < df_encoded . select ( \"IDLabel\" ) . distinct () . count () ): warnings . warn ( \"The returned odf may not be fully imputed because values for all list_of_cols are null for some IDs\" ) als = ALS ( maxIter = 20 , regParam = 0.01 , userCol = \"IDLabel\" , itemCol = \"keyLabel\" , ratingCol = \"value\" , coldStartStrategy = \"drop\" , ) model = als . fit ( df_model ) df_pred = ( model . transform ( df_test ) . drop ( \"value\" ) . withColumnRenamed ( \"prediction\" , \"value\" ) ) df_encoded_pred = df_model . union ( df_pred . select ( df_model . columns )) if id_type == \"string\" : IDlabelReverse = IndexToString () . setInputCol ( \"IDLabel\" ) . setOutputCol ( id_col ) df_encoded_pred = IDlabelReverse . transform ( df_encoded_pred ) else : df_encoded_pred = df_encoded_pred . withColumnRenamed ( \"IDLabel\" , id_col ) keylabelReverse = IndexToString () . setInputCol ( \"keyLabel\" ) . setOutputCol ( \"key\" ) odf_imputed = ( keylabelReverse . transform ( df_encoded_pred ) . groupBy ( id_col ) . pivot ( \"key\" ) . agg ( F . first ( \"value\" )) . select ( [ id_col ] + [( i + \"_imputed\" ) for i in list_of_cols if i in missing_cols ] ) ) odf = idf . join ( odf_imputed , id_col , \"left_outer\" ) for i in list_of_cols : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) elif output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) if remove_id : odf = odf . drop ( \"id\" ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : output_cols = [ ( i + \"_imputed\" ) for i in [ e for e in list_of_cols if e in missing_cols ] ] odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def imputation_sklearn ( spark, idf, list_of_cols='missing', drop_cols=[], missing_threshold=1.0, method_type='regression', use_sampling=True, sample_method='random', strata_cols='all', stratified_type='population', sample_size=10000, sample_seed=42, persist=True, persist_option=StorageLevel(True, True, False, False, 1), pre_existing_model=False, model_path='NA', output_mode='replace', stats_missing={}, run_type='local', auth_key='NA', print_impact=False) The function \"imputation_sklearn\" leverages sklearn imputer algorithms. Two methods are supported via this function: \u201cKNN\u201d and \u201cregression\u201d. \u201cKNN\u201d option trains a sklearn.impute.KNNImputer which is based on k-Nearest Neighbors algorithm. The missing values of a sample are imputed using the mean of its 5 nearest neighbors in the training set. \u201cregression\u201d option trains a sklearn.impute.IterativeImputer which models attribute to impute as a function of rest of the attributes and imputes using the estimation. Imputation is performed in an iterative way from attributes with fewest number of missing values to most. All the hyperparameters used in the above mentioned imputers are their default values. However, sklearn imputers are not scalable, which might be slow if the size of the input dataframe is large. In fact, if the input dataframe size exceeds 10 GigaBytes, the model fitting step powered by sklearn might fail. Thus, an input sample_size (the default value is 10,000) can be set to control the number of samples to be used to train the imputer. If the total number of input dataset exceeds sample_size, the rest of the samples will be imputed using the trained imputer in a scalable manner. This is one of the way to demonstrate how Anovos has been designed as a scalable feature engineering library. Parameters spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) missing_threshold Float argument - If list_of_cols is \"missing\", this argument is used to determined the missing threshold for every column. The column that has more (count of missing value/ count of total value) >= missing_threshold will be excluded from the list of columns to be imputed. (Default value = 1.0) method_type \"KNN\", \"regression\". \"KNN\" option trains a sklearn.impute.KNNImputer. \"regression\" option trains a sklearn.impute.IterativeImputer (Default value = \"regression\") use_sampling Boolean argument - True or False. This argument is used to determine whether to use sampling on source and target dataset, True will enable the use of sample method, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) sample_method If use_sampling is True, this argument is used to determine the sampling method. \"stratified\" for Stratified sampling, \"random\" for Random Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"random\") strata_cols If use_sampling is True and sample_method is \"stratified\", this argument is used to determine the list of columns used to be treated as strata. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"all\") stratified_type If use_sampling is True and sample_method is \"stratified\", this argument is used to determine the stratified sampling method. \"population\" stands for Proportionate Stratified Sampling, \"balanced\" stands for Optimum Stratified Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"population\") sample_size If use_sampling is True, this argument is used to determine maximum rows for training the sklearn imputer (Default value = 10000) sample_seed If use_sampling is True, this argument is used to determine the seed of sampling method. (Default value = 42) persist Boolean argument - True or False. This argument is used to determine whether to persist on binning result of source and target dataset, True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) persist_option If persist is True, this argument is used to determine the type of persist. For all the pyspark.StorageLevel option available in persist, please refer to https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns DataFrame Imputed Dataframe Expand source code def imputation_sklearn ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], missing_threshold = 1.0 , method_type = \"regression\" , use_sampling = True , sample_method = \"random\" , strata_cols = \"all\" , stratified_type = \"population\" , sample_size = 10000 , sample_seed = 42 , persist = True , persist_option = pyspark . StorageLevel . MEMORY_AND_DISK , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_missing = {}, run_type = \"local\" , auth_key = \"NA\" , print_impact = False , ): \"\"\" The function \"imputation_sklearn\" leverages sklearn imputer algorithms. Two methods are supported via this function: \u201cKNN\u201d and \u201cregression\u201d. \u201cKNN\u201d option trains a sklearn.impute.KNNImputer which is based on k-Nearest Neighbors algorithm. The missing values of a sample are imputed using the mean of its 5 nearest neighbors in the training set. \u201cregression\u201d option trains a sklearn.impute.IterativeImputer which models attribute to impute as a function of rest of the attributes and imputes using the estimation. Imputation is performed in an iterative way from attributes with fewest number of missing values to most. All the hyperparameters used in the above mentioned imputers are their default values. However, sklearn imputers are not scalable, which might be slow if the size of the input dataframe is large. In fact, if the input dataframe size exceeds 10 GigaBytes, the model fitting step powered by sklearn might fail. Thus, an input sample_size (the default value is 10,000) can be set to control the number of samples to be used to train the imputer. If the total number of input dataset exceeds sample_size, the rest of the samples will be imputed using the trained imputer in a scalable manner. This is one of the way to demonstrate how Anovos has been designed as a scalable feature engineering library. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) missing_threshold Float argument - If list_of_cols is \"missing\", this argument is used to determined the missing threshold for every column. The column that has more (count of missing value/ count of total value) >= missing_threshold will be excluded from the list of columns to be imputed. (Default value = 1.0) method_type \"KNN\", \"regression\". \"KNN\" option trains a sklearn.impute.KNNImputer. \"regression\" option trains a sklearn.impute.IterativeImputer (Default value = \"regression\") use_sampling Boolean argument - True or False. This argument is used to determine whether to use sampling on source and target dataset, True will enable the use of sample method, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) sample_method If use_sampling is True, this argument is used to determine the sampling method. \"stratified\" for Stratified sampling, \"random\" for Random Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"random\") strata_cols If use_sampling is True and sample_method is \"stratified\", this argument is used to determine the list of columns used to be treated as strata. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"all\") stratified_type If use_sampling is True and sample_method is \"stratified\", this argument is used to determine the stratified sampling method. \"population\" stands for Proportionate Stratified Sampling, \"balanced\" stands for Optimum Stratified Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"population\") sample_size If use_sampling is True, this argument is used to determine maximum rows for training the sklearn imputer (Default value = 10000) sample_seed If use_sampling is True, this argument is used to determine the seed of sampling method. (Default value = 42) persist Boolean argument - True or False. This argument is used to determine whether to persist on binning result of source and target dataset, True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) persist_option If persist is True, this argument is used to determine the type of persist. For all the pyspark.StorageLevel option available in persist, please refer to https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" if persist : idf = idf . persist ( persist_option ) num_cols = attributeType_segregation ( idf )[ 0 ] if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , num_cols ) else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( num_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns dropped from the imputation as all values are null: \" + \",\" . join ( empty_cols ) ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . where ( F . col ( \"missing_pct\" ) < missing_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = num_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e not in empty_cols )]) ) if len ( list_of_cols ) <= 1 : warnings . warn ( \"No Imputation Performed - No Column(s) or Insufficient Column(s) to Impute\" ) return idf if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) if ( ( len ([ e for e in list_of_cols if e in missing_cols ]) == 0 ) & ( not pre_existing_model ) & ( model_path == \"NA\" ) ): warnings . warn ( \"No Imputation Performed - No Column(s) to Impute and No Imputation Model to be saved\" ) return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"KNN\" , \"regression\" ): raise TypeError ( \"Invalid input for method_type\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : if run_type == \"emr\" : bash_cmd = \"aws s3 cp \" + model_path + \"/imputation_sklearn.sav .\" output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) elif run_type == \"ak8s\" : bash_cmd = ( 'azcopy cp \"' + model_path + \"/imputation_sklearn.sav\" + str ( auth_key ) + '\" .' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) else : imputer = pickle . load ( open ( model_path + \"/imputation_sklearn.sav\" , \"rb\" )) else : if use_sampling : count_idf = idf . count () if count_idf > sample_size : idf_model = data_sample ( idf , strata_cols = strata_cols , fraction = sample_size / count_idf , method_type = sample_method , stratified_type = stratified_type , seed_value = sample_seed , ) else : idf_model = idf else : idf_model = idf if persist : idf_model = idf_model . persist ( persist_option ) idf_pd = idf_model . select ( list_of_cols ) . toPandas () if method_type == \"KNN\" : imputer = KNNImputer ( n_neighbors = 5 , weights = \"uniform\" , metric = \"nan_euclidean\" ) if method_type == \"regression\" : imputer = IterativeImputer () imputer . fit ( idf_pd ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): if run_type == \"emr\" : pickle . dump ( imputer , open ( \"imputation_sklearn.sav\" , \"wb\" )) bash_cmd = ( \"aws s3 cp imputation_sklearn.sav \" + model_path + \"/imputation_sklearn.sav\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) elif run_type == \"ak8s\" : pickle . dump ( imputer , open ( \"imputation_sklearn.sav\" , \"wb\" )) bash_cmd = ( 'azcopy cp \"imputation_sklearn.sav\" \"' + ends_with ( model_path ) + \"imputation_sklearn.sav\" + str ( auth_key ) + '\"' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) else : local_path = model_path + \"/imputation_sklearn.sav\" os . makedirs ( os . path . dirname ( local_path ), exist_ok = True ) pickle . dump ( imputer , open ( local_path , \"wb\" )) imputer = pickle . load ( open ( model_path + \"/imputation_sklearn.sav\" , \"rb\" ) ) @F . pandas_udf ( returnType = T . ArrayType ( T . DoubleType ())) def prediction ( * cols ): input_pdf = pd . concat ( cols , axis = 1 ) return pd . Series ( row . tolist () for row in imputer . transform ( input_pdf )) result_df = idf . withColumn ( \"features\" , prediction ( * list_of_cols )) if persist : result_df = result_df . persist ( persist_option ) odf_schema = result_df . schema for i in list_of_cols : odf_schema = odf_schema . add ( T . StructField ( i + \"_imputed\" , T . FloatType ())) odf = ( result_df . rdd . map ( lambda x : ( * x , * x [ \"features\" ])) . toDF ( schema = odf_schema ) . drop ( \"features\" ) ) output_cols = [] for i in list_of_cols : if output_mode == \"append\" : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) else : output_cols . append ( i + \"_imputed\" ) else : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) odf = odf . select ( idf . columns + output_cols ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) if persist : idf . unpersist () if not pre_existing_model : idf_model . unpersist () result_df . unpersist () return odf def monotonic_binning ( spark, idf, list_of_cols='all', drop_cols=[], label_col='label', event_label=1, bin_method='equal_range', bin_size=10, bin_dtype='numerical', output_mode='replace') This function constitutes supervised way of binning the numerical attribute into discrete (integer or categorical values) attribute. Instead of pre-defined fixed number of bins, number of bins are dynamically computed to ensure the monotonic nature of bins i.e. % event should increase or decrease with the bin. Monotonic nature of bins is evaluated by looking at spearman rank correlation, which should be either +1 or -1, between the bin index and % event. In case, the monotonic nature is not attained, user defined fixed number of bins are used for the binning. Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) bin_method \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Default number of bins in case monotonicity is not achieved. bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") Returns DataFrame Binned Dataframe Expand source code def monotonic_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , bin_method = \"equal_range\" , bin_size = 10 , bin_dtype = \"numerical\" , output_mode = \"replace\" , ): \"\"\" This function constitutes supervised way of binning the numerical attribute into discrete (integer or categorical values) attribute. Instead of pre-defined fixed number of bins, number of bins are dynamically computed to ensure the monotonic nature of bins i.e. % event should increase or decrease with the bin. Monotonic nature of bins is evaluated by looking at spearman rank correlation, which should be either +1 or -1, between the bin index and % event. In case, the monotonic nature is not attained, user defined fixed number of bins are used for the binning. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) bin_method \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Default number of bins in case monotonicity is not achieved. bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") Returns ------- DataFrame Binned Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) odf = idf for col in list_of_cols : n = 20 r = 0 while n > 2 : tmp = ( attribute_binning ( spark , idf , [ col ], drop_cols = [], method_type = bin_method , bin_size = n , output_mode = \"append\" , ) . select ( label_col , col , col + \"_binned\" ) . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col + \"_binned\" ) . agg ( F . avg ( col ) . alias ( \"mean_val\" ), F . avg ( label_col ) . alias ( \"mean_label\" )) . dropna () ) r , p = stats . spearmanr ( tmp . toPandas ()[[ \"mean_val\" ]], tmp . toPandas ()[[ \"mean_label\" ]] ) if r == 1.0 : odf = attribute_binning ( spark , odf , [ col ], drop_cols = [], method_type = bin_method , bin_size = n , bin_dtype = bin_dtype , output_mode = output_mode , ) break n = n - 1 r = 0 if r < 1.0 : odf = attribute_binning ( spark , odf , [ col ], drop_cols = [], method_type = bin_method , bin_size = bin_size , bin_dtype = bin_dtype , output_mode = output_mode , ) return odf def normalization ( idf, list_of_cols='all', drop_cols=[], pre_existing_model=False, model_path='NA', output_mode='replace', print_impact=False) Parameters idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if normalization/scalar model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out before and after descriptive statistics of rescaled columns. Returns DataFrame Rescaled Dataframe Expand source code def normalization ( idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if normalization/scalar model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Normalization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) idf_id = idf . withColumn ( \"tempID\" , F . monotonically_increasing_id ()) idf_partial = idf_id . select ([ \"tempID\" ] + list_of_cols ) assembler = VectorAssembler ( inputCols = list_of_cols , outputCol = \"list_of_cols_vector\" , handleInvalid = \"keep\" ) assembled_data = assembler . transform ( idf_partial ) if pre_existing_model : scalerModel = MinMaxScalerModel . load ( model_path + \"/normalization\" ) else : scaler = MinMaxScaler ( inputCol = \"list_of_cols_vector\" , outputCol = \"list_of_cols_scaled\" ) scalerModel = scaler . fit ( assembled_data ) if model_path != \"NA\" : scalerModel . write () . overwrite () . save ( model_path + \"/normalization\" ) scaledData = scalerModel . transform ( assembled_data ) def vector_to_array ( v ): return v . toArray () . tolist () f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . FloatType ())) odf_partial = scaledData . withColumn ( \"list_of_cols_array\" , f_vector_to_array ( \"list_of_cols_scaled\" ) ) . drop ( * [ \"list_of_cols_scaled\" , \"list_of_cols_vector\" ]) odf_schema = odf_partial . schema for i in list_of_cols : odf_schema = odf_schema . add ( T . StructField ( i + \"_scaled\" , T . FloatType ())) odf_partial = ( odf_partial . rdd . map ( lambda x : ( * x , * x [ \"list_of_cols_array\" ])) . toDF ( schema = odf_schema ) . drop ( \"list_of_cols_array\" ) ) odf = idf_id . join ( odf_partial . drop ( * list_of_cols ), \"tempID\" , \"left_outer\" ) . select ( idf . columns + [ ( F . when ( F . isnan ( F . col ( i + \"_scaled\" )), None ) . otherwise ( F . col ( i + \"_scaled\" ) ) ) . alias ( i + \"_scaled\" ) for i in list_of_cols ] ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_scaled\" , i ) odf = odf . select ( idf . columns ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_scaled\" ) for i in list_of_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def outlier_categories ( spark, idf, list_of_cols='all', drop_cols=[], coverage=1.0, max_category=50, pre_existing_model=False, model_path='NA', output_mode='replace', print_impact=False) This function replaces less frequently seen values (called as outlier values in the current context) in a categorical column by 'outlier_categories'. Outlier values can be defined in two ways \u2013 a) Max N categories, where N is user defined value. In this method, top N-1 frequently seen categories are considered and rest are clubbed under single category 'outlier_categories'. or Alternatively, b) Coverage \u2013 top frequently seen categories are considered till it covers minimum N% of rows and rest lesser seen values are mapped to 'outlier_categories'. Even if the Coverage is less, maximum category constraint is given priority. Further, there is a caveat that when multiple categories have same rank. Then, number of categorical values can be more than max_category defined by the user. This function performs better when distinct values, in any column, are not more than 100. It is recommended that to drop those columns from the computation which has more than 100 distinct values, to get better performance out of this function. Parameters spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) coverage Defines the minimum % of rows that will be mapped to actual category name and the rest to be mapped to 'outlier_categories' and takes value between 0 to 1. Coverage of 0.8 can be interpreted as top frequently seen categories are considered till it covers minimum 80% of rows and rest lesser seen values are mapped to 'outlier_categories'. (Default value = 1.0) max_category Even if coverage is less, only (max_category - 1) categories will be mapped to actual name and rest to 'outlier_categories'. Caveat is when multiple categories have same rank, then #categories can be more than max_category. (Default value = 50) pre_existing_model Boolean argument \u2013 True or False. True if the model with the outlier/other values for each attribute exists already to be used, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") print_impact True, False This argument is to print before and after unique count of the transformed features (Default value = False) Returns DataFrame Transformed Dataframe Expand source code def outlier_categories ( spark , idf , list_of_cols = \"all\" , drop_cols = [], coverage = 1.0 , max_category = 50 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" This function replaces less frequently seen values (called as outlier values in the current context) in a categorical column by 'outlier_categories'. Outlier values can be defined in two ways \u2013 a) Max N categories, where N is user defined value. In this method, top N-1 frequently seen categories are considered and rest are clubbed under single category 'outlier_categories'. or Alternatively, b) Coverage \u2013 top frequently seen categories are considered till it covers minimum N% of rows and rest lesser seen values are mapped to 'outlier_categories'. Even if the Coverage is less, maximum category constraint is given priority. Further, there is a caveat that when multiple categories have same rank. Then, number of categorical values can be more than max_category defined by the user. This function performs better when distinct values, in any column, are not more than 100. It is recommended that to drop those columns from the computation which has more than 100 distinct values, to get better performance out of this function. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) coverage Defines the minimum % of rows that will be mapped to actual category name and the rest to be mapped to 'outlier_categories' and takes value between 0 to 1. Coverage of 0.8 can be interpreted as top frequently seen categories are considered till it covers minimum 80% of rows and rest lesser seen values are mapped to 'outlier_categories'. (Default value = 1.0) max_category Even if coverage is less, only (max_category - 1) categories will be mapped to actual name and rest to 'outlier_categories'. Caveat is when multiple categories have same rank, then #categories can be more than max_category. (Default value = 50) pre_existing_model Boolean argument \u2013 True or False. True if the model with the outlier/other values for each attribute exists already to be used, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") print_impact True, False This argument is to print before and after unique count of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Outlier Categories Computation - No categorical column(s) to transform\" ) return idf if ( coverage <= 0 ) | ( coverage > 1 ): raise TypeError ( \"Invalid input for Coverage Value\" ) if max_category < 2 : raise TypeError ( \"Invalid input for Maximum No. of Categories Allowed\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) idf = idf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) if pre_existing_model : df_model = spark . read . csv ( model_path + \"/outlier_categories\" , header = True , inferSchema = True ) else : for index , i in enumerate ( list_of_cols ): window = Window . partitionBy () . orderBy ( F . desc ( \"count_pct\" )) df_cats = ( idf . select ( i ) . groupBy ( i ) . count () . dropna () . withColumn ( \"count_pct\" , F . col ( \"count\" ) / F . sum ( \"count\" ) . over ( Window . partitionBy ()), ) . withColumn ( \"rank\" , F . rank () . over ( window )) . withColumn ( \"cumu\" , F . sum ( \"count_pct\" ) . over ( window . rowsBetween ( Window . unboundedPreceding , 0 ) ), ) . withColumn ( \"lag_cumu\" , F . lag ( \"cumu\" ) . over ( window )) . fillna ( 0 ) . where ( ~ (( F . col ( \"cumu\" ) >= coverage ) & ( F . col ( \"lag_cumu\" ) >= coverage ))) . where ( F . col ( \"rank\" ) <= ( max_category - 1 )) . select ( F . lit ( i ) . alias ( \"attribute\" ), F . col ( i ) . alias ( \"parameters\" )) ) if index == 0 : df_model = df_cats else : df_model = df_model . union ( df_cats ) df_params = df_model . rdd . groupByKey () . mapValues ( list ) . collect () dict_params = dict ( df_params ) broadcast_params = spark . sparkContext . broadcast ( dict_params ) def get_params ( key ): parameters = list () dict_params = broadcast_params . value params = dict_params . get ( key ) if params : parameters = params return parameters odf = idf for i in list_of_cols : parameters = get_params ( i ) if output_mode == \"replace\" : odf = odf . withColumn ( i , F . when ( ( F . col ( i ) . isin ( parameters )) | ( F . col ( i ) . isNull ()), F . col ( i ) ) . otherwise ( \"outlier_categories\" ), ) else : odf = odf . withColumn ( i + \"_outliered\" , F . when ( ( F . col ( i ) . isin ( parameters )) | ( F . col ( i ) . isNull ()), F . col ( i ) ) . otherwise ( \"outlier_categories\" ), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model . repartition ( 1 ) . write . csv ( model_path + \"/outlier_categories\" , header = True , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_outliered\" ) for i in list_of_cols ] uniqueCount_computation ( spark , idf , list_of_cols ) . select ( \"attribute\" , F . col ( \"unique_values\" ) . alias ( \"uniqueValues_before\" ) ) . show ( len ( list_of_cols ), False ) uniqueCount_computation ( spark , odf , output_cols ) . select ( \"attribute\" , F . col ( \"unique_values\" ) . alias ( \"uniqueValues_after\" ) ) . show ( len ( list_of_cols ), False ) idf . unpersist () return odf def z_standardization ( spark, idf, list_of_cols='all', drop_cols=[], pre_existing_model=False, model_path='NA', output_mode='replace', print_impact=False) Standardization is commonly used in data pre-processing process. z_standardization standardizes the selected attributes of an input dataframe by normalizing each attribute to have standard deviation of 1 and mean of 0. For each attribute, the standard deviation (s) and mean (u) are calculated and a sample x will be standardized into ( x-u)/s. If the standard deviation of an attribute is 0, it will be excluded in standardization and a warning will be shown. None values will be kept as None in the output dataframe. Parameters spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (Mean/stddev for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns DataFrame Rescaled Dataframe Expand source code def z_standardization ( spark , idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Standardization is commonly used in data pre-processing process. z_standardization standardizes the selected attributes of an input dataframe by normalizing each attribute to have standard deviation of 1 and mean of 0. For each attribute, the standard deviation (s) and mean (u) are calculated and a sample x will be standardized into ( x-u)/s. If the standard deviation of an attribute is 0, it will be excluded in standardization and a warning will be shown. None values will be kept as None in the output dataframe. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (Mean/stddev for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Standardization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) parameters = [] excluded_cols = [] if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/z_standardization\" ) for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"feature\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : for i in list_of_cols : mean , stddev = idf . select ( F . mean ( i ), F . stddev ( i )) . first () parameters . append ( [ float ( mean ) if mean else None , float ( stddev ) if stddev else None ] ) if stddev : if round ( stddev , 5 ) == 0.0 : excluded_cols . append ( i ) else : excluded_cols . append ( i ) if len ( excluded_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from standardization because the standard deviation is zero:\" + str ( excluded_cols ) ) odf = idf for index , i in enumerate ( list_of_cols ): if i not in excluded_cols : modify_col = ( i + \"_scaled\" ) if ( output_mode == \"append\" ) else i odf = odf . withColumn ( modify_col , ( F . col ( i ) - parameters [ index ][ 0 ]) / parameters [ index ][ 1 ] ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( list_of_cols , parameters ), schema = [ \"feature\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/z_standardization\" , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [ ( i + \"_scaled\" ) for i in list_of_cols if i not in excluded_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf","title":"<code>transformers</code>"},{"location":"api/data_transformer/transformers.html#transformers","text":"The data transformer module supports selected pre-processing & transformation functions, such as binning, encoding, scaling, imputation, to name a few, which are required for statistics generation and quality checks. Functions supported through this module are listed below: attribute_binning monotonic_binning cat_to_num_transformer cat_to_num_unsupervised cat_to_num_supervised z_standardization IQR_standardization normalization imputation_MMM imputation_sklearn imputation_matrixFactorization auto_imputation autoencoder_latentFeatures PCA_latentFeatures feature_transformation boxcox_transformation outlier_categories expression_parser Expand source code # coding=utf-8 \"\"\" The data transformer module supports selected pre-processing & transformation functions, such as binning, encoding, scaling, imputation, to name a few, which are required for statistics generation and quality checks. Functions supported through this module are listed below: - attribute_binning - monotonic_binning - cat_to_num_transformer - cat_to_num_unsupervised - cat_to_num_supervised - z_standardization - IQR_standardization - normalization - imputation_MMM - imputation_sklearn - imputation_matrixFactorization - auto_imputation - autoencoder_latentFeatures - PCA_latentFeatures - feature_transformation - boxcox_transformation - outlier_categories - expression_parser \"\"\" import copy import os import pickle import random import platform import subprocess import tempfile import warnings from itertools import chain import numpy as np import pandas as pd import pyspark from packaging import version from scipy import stats if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): from pyspark.ml.feature import OneHotEncoderEstimator as OneHotEncoder else : from pyspark.ml.feature import OneHotEncoder from pyspark.ml.evaluation import RegressionEvaluator from pyspark.ml.feature import ( PCA , Imputer , ImputerModel , IndexToString , MinMaxScaler , MinMaxScalerModel , PCAModel , StringIndexer , StringIndexerModel , VectorAssembler , ) from pyspark.ml.linalg import DenseVector from pyspark.ml.recommendation import ALS from pyspark.mllib.stat import Statistics from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql.window import Window from anovos.data_analyzer.stats_generator import ( missingCount_computation , uniqueCount_computation , ) from anovos.data_ingest.data_ingest import read_dataset , recast_column from anovos.data_ingest.data_sampling import data_sample from anovos.shared.utils import ends_with , attributeType_segregation , get_dtype # enable_iterative_imputer is prequisite for importing IterativeImputer # check the following issue for more details https://github.com/scikit-learn/scikit-learn/issues/16833 from sklearn.experimental import enable_iterative_imputer # noqa from sklearn.impute import KNNImputer , IterativeImputer if \"arm64\" not in platform . version () . lower (): import tensorflow from tensorflow.keras.models import load_model , Model from tensorflow.keras.layers import Dense , Input , BatchNormalization , LeakyReLU def attribute_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"equal_range\" , bin_size = 10 , bin_dtype = \"numerical\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Attribute binning (or discretization) is a method of numerical attribute into discrete (integer or categorical values) using pre-defined number of bins. This data pre-processing technique is used to reduce the effects of minor observation errors. Also, Binning introduces non-linearity and tends to improve the performance of the model. In this function, we are focussing on unsupervised way of binning i.e. without taking the target variable into account - Equal Range Binning, Equal Frequency Binning. In Equal Range method, each bin is of equal size/width and computed as: w = max- min / no. of bins *bins cutoff=[min, min+w,min+2w\u2026..,max-w,max]* whereas in Equal Frequency binning method, bins are created in such a way that each bin has equal no. of rows, though the width of bins may vary from each other. w = 1 / no. of bins *bins cutoff=[min, wthpctile, 2wthpctile\u2026.,max ]* Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Number of bins. (Default value = 10) bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") pre_existing_model Boolean argument \u2013 True or False. True if binning model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print the number of categories generated for each attribute (may or may be not same as bin_size) Returns ------- DataFrame Binned Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Binning Performed - No numerical column(s) to transform\" ) return idf if method_type not in ( \"equal_frequency\" , \"equal_range\" ): raise TypeError ( \"Invalid input for method_type\" ) if bin_size < 2 : raise TypeError ( \"Invalid input for bin_size\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/attribute_binning\" ) bin_cutoffs = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) bin_cutoffs . append ( mapped_value ) else : if method_type == \"equal_frequency\" : pctile_width = 1 / bin_size pctile_cutoff = [] for j in range ( 1 , bin_size ): pctile_cutoff . append ( j * pctile_width ) bin_cutoffs = idf . approxQuantile ( list_of_cols , pctile_cutoff , 0.01 ) else : funs = [ F . max , F . min ] exprs = [ f ( F . col ( c )) for f in funs for c in list_of_cols ] list_result = idf . groupby () . agg ( * exprs ) . rdd . flatMap ( lambda x : x ) . collect () bin_cutoffs = [] drop_col_process = [] for i in range ( int ( len ( list_result ) / 2 )): bin_cutoff = [] max_val = list_result [ i ] min_val = list_result [ i + int ( len ( list_result ) / 2 )] if not max_val and max_val != 0 : drop_col_process . append ( list_of_cols [ i ]) continue bin_width = ( max_val - min_val ) / bin_size for j in range ( 1 , bin_size ): bin_cutoff . append ( min_val + j * bin_width ) bin_cutoffs . append ( bin_cutoff ) if drop_col_process : warnings . warn ( \"Columns contains too much null values. Dropping \" + \", \" . join ( drop_col_process ) ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_col_process ]) ) if model_path != \"NA\" : df_model = spark . createDataFrame ( zip ( list_of_cols , bin_cutoffs ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . write . parquet ( model_path + \"/attribute_binning\" , mode = \"overwrite\" ) def bucket_label ( value , index ): if value is None : return None for i in range ( 0 , len ( bin_cutoffs [ index ])): if value <= bin_cutoffs [ index ][ i ]: if bin_dtype == \"numerical\" : return i + 1 else : if i == 0 : return \"<= \" + str ( round ( bin_cutoffs [ index ][ i ], 4 )) else : return ( str ( round ( bin_cutoffs [ index ][ i - 1 ], 4 )) + \"-\" + str ( round ( bin_cutoffs [ index ][ i ], 4 )) ) else : next if bin_dtype == \"numerical\" : return len ( bin_cutoffs [ 0 ]) + 1 else : return \"> \" + str ( round ( bin_cutoffs [ index ][ len ( bin_cutoffs [ 0 ]) - 1 ], 4 )) if bin_dtype == \"numerical\" : f_bucket_label = F . udf ( bucket_label , T . IntegerType ()) else : f_bucket_label = F . udf ( bucket_label , T . StringType ()) odf = idf for idx , i in enumerate ( list_of_cols ): odf = odf . withColumn ( i + \"_binned\" , f_bucket_label ( F . col ( i ), F . lit ( idx ))) if output_mode == \"replace\" : for col in list_of_cols : odf = odf . drop ( col ) . withColumnRenamed ( col + \"_binned\" , col ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_binned\" ) for i in list_of_cols ] uniqueCount_computation ( spark , odf , output_cols ) . show ( len ( output_cols ), False ) return odf def monotonic_binning ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , bin_method = \"equal_range\" , bin_size = 10 , bin_dtype = \"numerical\" , output_mode = \"replace\" , ): \"\"\" This function constitutes supervised way of binning the numerical attribute into discrete (integer or categorical values) attribute. Instead of pre-defined fixed number of bins, number of bins are dynamically computed to ensure the monotonic nature of bins i.e. % event should increase or decrease with the bin. Monotonic nature of bins is evaluated by looking at spearman rank correlation, which should be either +1 or -1, between the bin index and % event. In case, the monotonic nature is not attained, user defined fixed number of bins are used for the binning. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) bin_method \"equal_frequency\", \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Default number of bins in case monotonicity is not achieved. bin_dtype \"numerical\", \"categorical\". With \"numerical\" option, original value is replaced with an Integer (1,2,\u2026) and with \"categorical\" option, original replaced with a string describing min and max value allowed in the bin (\"minval-maxval\"). (Default value = \"numerical\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_binned\" e.g. column X is appended as X_binned. (Default value = \"replace\") Returns ------- DataFrame Binned Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in ( drop_cols + [ label_col ])]) ) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) odf = idf for col in list_of_cols : n = 20 r = 0 while n > 2 : tmp = ( attribute_binning ( spark , idf , [ col ], drop_cols = [], method_type = bin_method , bin_size = n , output_mode = \"append\" , ) . select ( label_col , col , col + \"_binned\" ) . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , 1 ) . otherwise ( 0 ) ) . groupBy ( col + \"_binned\" ) . agg ( F . avg ( col ) . alias ( \"mean_val\" ), F . avg ( label_col ) . alias ( \"mean_label\" )) . dropna () ) r , p = stats . spearmanr ( tmp . toPandas ()[[ \"mean_val\" ]], tmp . toPandas ()[[ \"mean_label\" ]] ) if r == 1.0 : odf = attribute_binning ( spark , odf , [ col ], drop_cols = [], method_type = bin_method , bin_size = n , bin_dtype = bin_dtype , output_mode = output_mode , ) break n = n - 1 r = 0 if r < 1.0 : odf = attribute_binning ( spark , odf , [ col ], drop_cols = [], method_type = bin_method , bin_size = bin_size , bin_dtype = bin_dtype , output_mode = output_mode , ) return odf def cat_to_num_transformer ( spark , idf , list_of_cols , drop_cols , method_type , encoding , label_col , event_label ): \"\"\" This is method which helps converting a categorical attribute into numerical attribute(s) based on the analysis dataset. If there's a presence of label column then the relevant processing would happen through cat_to_num_supervised. However, for unsupervised scenario, the processing would happen through cat_to_num_unsupervised. Computation details can be referred from the respective functions. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. method_type Depending upon the use case the method type can be either Supervised or Unsupervised. For Supervised use case, label_col is mandatory. encoding \"label_encoding\" or \"onehot_encoding\" In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available that can be selected by index_order argument). In one-hot encoding, every unique value in the column will be added in a form of dummy/binary column. label_col Label/Target column event_label Value of (positive) event \"\"\" num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if len ( cat_cols ) > 0 : if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if ( method_type == \"supervised\" ) & ( label_col is not None ): odf = cat_to_num_supervised ( spark , idf , label_col = label_col , event_label = event_label ) odf = odf . withColumn ( label_col , F . when ( F . col ( label_col ) == event_label , F . lit ( 1 )) . otherwise ( F . lit ( 0 )), ) return odf elif ( method_type == \"unsupervised\" ) & ( label_col is None ): odf = cat_to_num_unsupervised ( spark , idf , method_type = encoding , index_order = \"frequencyDesc\" , ) return odf else : return idf def cat_to_num_unsupervised ( spark , idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"label_encoding\" , index_order = \"frequencyDesc\" , cardinality_threshold = 50 , pre_existing_model = False , model_path = \"NA\" , stats_unique = {}, output_mode = \"replace\" , print_impact = False , ): \"\"\" This is unsupervised method of converting a categorical attribute into numerical attribute(s). This is among the most important transformations required for any modelling exercise, as most of the machine learning algorithms cannot process categorical values. It covers two popular encoding techniques \u2013 label encoding & one-hot encoding. In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available \u2013 can be selected by index_order argument). One of the pitfalls of using this technique is that the model may learn some spurious relationship, which doesn't exist or might not make any logical sense in the real world settings. In one-hot encoding, every unique value in the attribute will be added as a feature in a form of dummy/binary attribute. However, using this method on high cardinality attributes can further aggravate the dimensionality issue. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"label_encoding\" or \"onehot_encoding\" In label encoding, each categorical value is assigned a unique integer based on alphabetical or frequency ordering (both ascending & descending options are available that can be selected by index_order argument). In one-hot encoding, every unique value in the column will be added in a form of dummy/binary column. (Default value = 1) index_order \"frequencyDesc\", \"frequencyAsc\", \"alphabetDesc\", \"alphabetAsc\". Valid only for Label Encoding method_type. (Default value = \"frequencyDesc\") cardinality_threshold Defines threshold to skip columns with higher cardinality values from encoding - a warning is issued. (Default value = 50) pre_existing_model Boolean argument - True or False. True if encoding model exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre existing model nor there is a need to save one. stats_unique Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_index\" for label encoding e.g. column X is appended as X_index, or a postfix \"_{n}\" for one hot encoding, n varies from 0 to unique value count e.g. column X is appended as X_0, X_1, X_2 (n = 3 i.e. no. of unique values for X). (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the change in schema (one hot encoding) or descriptive statistics (label encoding) Returns ------- DataFrame Encoded Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"onehot_encoding\" , \"label_encoding\" ): raise TypeError ( \"Invalid input for method_type\" ) if index_order not in ( \"frequencyDesc\" , \"frequencyAsc\" , \"alphabetDesc\" , \"alphabetAsc\" , ): raise TypeError ( \"Invalid input for Encoding Index Order\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if stats_unique == {}: skip_cols = ( uniqueCount_computation ( spark , idf , list_of_cols ) . where ( F . col ( \"unique_values\" ) > cardinality_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) else : skip_cols = ( read_dataset ( spark , ** stats_unique ) . where ( F . col ( \"unique_values\" ) > cardinality_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) skip_cols = list ( set ([ e for e in skip_cols if e in list_of_cols and e not in drop_cols ]) ) if skip_cols : warnings . warn ( \"Columns dropped from encoding due to high cardinality: \" + \",\" . join ( skip_cols ) ) list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols + skip_cols ]) ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Encoding Computation - No categorical column(s) to transform\" ) return idf list_of_cols_vec = [] list_of_cols_idx = [] for i in list_of_cols : list_of_cols_vec . append ( i + \"_vec\" ) list_of_cols_idx . append ( i + \"_index\" ) odf_indexed = idf if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): for idx , i in enumerate ( list_of_cols ): if pre_existing_model : indexerModel = StringIndexerModel . load ( model_path + \"/cat_to_num_unsupervised/indexer-model/\" + i ) else : stringIndexer = StringIndexer ( inputCol = i , outputCol = i + \"_index\" , stringOrderType = index_order , handleInvalid = \"keep\" , ) indexerModel = stringIndexer . fit ( idf . select ( i )) if model_path != \"NA\" : indexerModel . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/indexer-model/\" + i ) odf_indexed = indexerModel . transform ( odf_indexed ) else : if pre_existing_model : indexerModel = StringIndexerModel . load ( model_path + \"/cat_to_num_unsupervised/indexer\" ) else : stringIndexer = StringIndexer ( inputCols = list_of_cols , outputCols = list_of_cols_idx , stringOrderType = index_order , handleInvalid = \"keep\" , ) indexerModel = stringIndexer . fit ( odf_indexed ) if model_path != \"NA\" : indexerModel . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/indexer\" ) odf_indexed = indexerModel . transform ( odf_indexed ) if method_type == \"onehot_encoding\" : if pre_existing_model : encoder = OneHotEncoder . load ( model_path + \"/cat_to_num_unsupervised/encoder\" ) else : encoder = OneHotEncoder ( inputCols = list_of_cols_idx , outputCols = list_of_cols_vec , handleInvalid = \"keep\" , ) if model_path != \"NA\" : encoder . write () . overwrite () . save ( model_path + \"/cat_to_num_unsupervised/encoder\" ) odf = encoder . fit ( odf_indexed ) . transform ( odf_indexed ) new_cols = [] odf_sample = odf . take ( 1 ) for i in list_of_cols : odf_schema = odf . schema uniq_cats = odf_sample [ 0 ] . asDict ()[ i + \"_vec\" ] . size for j in range ( 0 , uniq_cats ): odf_schema = odf_schema . add ( T . StructField ( i + \"_\" + str ( j ), T . IntegerType ()) ) new_cols . append ( i + \"_\" + str ( j )) odf = odf . rdd . map ( lambda x : ( * x , * ( DenseVector ( x [ i + \"_vec\" ]) . toArray () . astype ( int ) . tolist ()), ) ) . toDF ( schema = odf_schema ) if output_mode == \"replace\" : odf = odf . drop ( i , i + \"_vec\" , i + \"_index\" ) else : odf = odf . drop ( i + \"_vec\" , i + \"_index\" ) else : odf = odf_indexed for i in list_of_cols : odf = odf . withColumn ( i + \"_index\" , F . when ( F . col ( i ) . isNull (), None ) . otherwise ( F . col ( i + \"_index\" ) . cast ( T . IntegerType ()) ), ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_index\" , i ) odf = odf . select ( idf . columns ) if print_impact : if method_type == \"label_encoding\" : if output_mode == \"append\" : new_cols = [ i + \"_index\" for i in list_of_cols ] else : new_cols = list_of_cols print ( \"Before\" ) idf . select ( list_of_cols ) . summary ( \"count\" , \"min\" , \"max\" ) . show ( 3 , False ) print ( \"After\" ) odf . select ( new_cols ) . summary ( \"count\" , \"min\" , \"max\" ) . show ( 3 , False ) if method_type == \"onehot_encoding\" : print ( \"Before\" ) idf . select ( list_of_cols ) . printSchema () print ( \"After\" ) if output_mode == \"append\" : odf . select ( list_of_cols + new_cols ) . printSchema () else : odf . select ( new_cols ) . printSchema () if skip_cols : print ( \"Columns dropped from encoding due to high cardinality: \" + \",\" . join ( skip_cols ) ) return odf def cat_to_num_supervised ( spark , idf , list_of_cols = \"all\" , drop_cols = [], label_col = \"label\" , event_label = 1 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , persist = False , persist_option = pyspark . StorageLevel . MEMORY_AND_DISK , print_impact = False , ): \"\"\" This is a supervised method to convert a categorical attribute into a numerical attribute. It takes a label/target column to indicate whether the event is positive or negative. For each column, the positive event rate for each categorical value is used as the encoded numerical value. For example, there are 3 distinct values in a categorical attribute X: X1, X2 and X3. Within the input dataframe, there are - 15 positive events and 5 negative events with X==X1; - 10 positive events and 40 negative events with X==X2; - 20 positive events and 20 negative events with X==X3. Thus, value X1 is mapped to 15/(15+5) = 0.75, value X2 is mapped to 10/(10+40) = 0.2 and value X3 is mapped to 20/(20+20) = 0.5. This mapping will be applied to all values in attribute X. This encoding method can be helpful in avoiding creating too many dummy variables which may cause dimensionality issue and it also works with categorical attributes without an order or rank. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) label_col Label/Target column (Default value = \"label\") event_label Value of (positive) event (i.e label 1) (Default value = 1) pre_existing_model Boolean argument \u2013 True or False. True if model (original and mapped numerical value for each column) exists already, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" is used to save the model in \"intermediate_data/\" folder for optimization purpose. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_encoded\" e.g. column X is appended as X_encoded. (Default value = \"replace\") persist Boolean argument - True or False. This parameter is for optimization purpose. If True, repeatedly used dataframe will be persisted (StorageLevel can be specified in persist_option). We recommend setting this parameter as True if at least one of the following criteria is True: (1) The underlying data source is in csv format (2) The transformation will be applicable to most columns. (Default value = False) persist_option A pyspark.StorageLevel instance. This parameter is useful only when persist is True. (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) print_impact True, False (Default value = False) This argument is to print out the descriptive statistics of encoded columns. Returns ------- DataFrame Encoded Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols elif isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e != label_col )]) ) if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Categorical Encoding - No categorical column(s) to transform\" ) return idf if label_col not in idf . columns : raise TypeError ( \"Invalid input for Label Column\" ) odf = idf label_col_bool = label_col + \"_cat_to_num_sup_temp\" idf = idf . withColumn ( label_col_bool , F . when ( F . col ( label_col ) == event_label , \"1\" ) . otherwise ( \"0\" ), ) if model_path == \"NA\" : skip_if_error = True model_path = \"intermediate_data\" else : skip_if_error = False save_model = True if persist : idf = idf . persist ( persist_option ) for index , i in enumerate ( list_of_cols ): if pre_existing_model : df_tmp = spark . read . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , inferSchema = True , ) else : df_tmp = ( idf . select ( i , label_col_bool ) . groupBy ( i ) . pivot ( label_col_bool ) . count () . fillna ( 0 ) . withColumn ( i + \"_encoded\" , F . round ( F . col ( \"1\" ) / ( F . col ( \"1\" ) + F . col ( \"0\" )), 4 ) ) . drop ( * [ \"1\" , \"0\" ]) ) if save_model : try : df_tmp . coalesce ( 1 ) . write . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , mode = \"overwrite\" , ignoreLeadingWhiteSpace = False , ignoreTrailingWhiteSpace = False , ) df_tmp = spark . read . csv ( model_path + \"/cat_to_num_supervised/\" + i , header = True , inferSchema = True , ) except Exception as error : if skip_if_error : warnings . warn ( \"For optimization purpose, we recommend specifying a valid model_path value to save the intermediate data. Saving to the default path - '\" + model_path + \"/cat_to_num_supervised/\" + i + \"' faced an error.\" ) save_model = False else : raise error if df_tmp . count () > 1 : odf = odf . join ( df_tmp , i , \"left_outer\" ) else : odf = odf . crossJoin ( df_tmp ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_encoded\" , i ) odf = odf . select ([ i for i in idf . columns if i != label_col_bool ]) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_encoded\" ) for i in list_of_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . summary ( \"count\" , \"min\" , \"max\" ) . show ( 3 , False ) print ( \"After: \" ) odf . select ( output_cols ) . summary ( \"count\" , \"min\" , \"max\" ) . show ( 3 , False ) if persist : idf . unpersist () return odf def z_standardization ( spark , idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Standardization is commonly used in data pre-processing process. z_standardization standardizes the selected attributes of an input dataframe by normalizing each attribute to have standard deviation of 1 and mean of 0. For each attribute, the standard deviation (s) and mean (u) are calculated and a sample x will be standardized into ( x-u)/s. If the standard deviation of an attribute is 0, it will be excluded in standardization and a warning will be shown. None values will be kept as None in the output dataframe. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (Mean/stddev for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Standardization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) parameters = [] excluded_cols = [] if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/z_standardization\" ) for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"feature\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : for i in list_of_cols : mean , stddev = idf . select ( F . mean ( i ), F . stddev ( i )) . first () parameters . append ( [ float ( mean ) if mean else None , float ( stddev ) if stddev else None ] ) if stddev : if round ( stddev , 5 ) == 0.0 : excluded_cols . append ( i ) else : excluded_cols . append ( i ) if len ( excluded_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from standardization because the standard deviation is zero:\" + str ( excluded_cols ) ) odf = idf for index , i in enumerate ( list_of_cols ): if i not in excluded_cols : modify_col = ( i + \"_scaled\" ) if ( output_mode == \"append\" ) else i odf = odf . withColumn ( modify_col , ( F . col ( i ) - parameters [ index ][ 0 ]) / parameters [ index ][ 1 ] ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( list_of_cols , parameters ), schema = [ \"feature\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/z_standardization\" , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [ ( i + \"_scaled\" ) for i in list_of_cols if i not in excluded_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def IQR_standardization ( spark , idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if model files (25/50/75 percentile for each feature) exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out the before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Standardization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : df_model = spark . read . parquet ( model_path + \"/IQR_standardization\" ) parameters = [] for i in list_of_cols : mapped_value = ( df_model . where ( F . col ( \"feature\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : parameters = idf . approxQuantile ( list_of_cols , [ 0.25 , 0.5 , 0.75 ], 0.01 ) excluded_cols = [] for i , param in zip ( list_of_cols , parameters ): if len ( param ) > 0 : if round ( param [ 0 ], 5 ) == round ( param [ 2 ], 5 ): excluded_cols . append ( i ) else : excluded_cols . append ( i ) if len ( excluded_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from standardization because the 75th and 25th percentiles are the same:\" + str ( excluded_cols ) ) odf = idf for index , i in enumerate ( list_of_cols ): if i not in excluded_cols : modify_col = ( i + \"_scaled\" ) if ( output_mode == \"append\" ) else i odf = odf . withColumn ( modify_col , ( F . col ( i ) - parameters [ index ][ 1 ]) / ( parameters [ index ][ 2 ] - parameters [ index ][ 0 ]), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( list_of_cols , parameters ), schema = [ \"feature\" , \"parameters\" ] ) df_model . coalesce ( 1 ) . write . parquet ( model_path + \"/IQR_standardization\" , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [ ( i + \"_scaled\" ) for i in list_of_cols if i not in excluded_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def normalization ( idf , list_of_cols = \"all\" , drop_cols = [], pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) pre_existing_model Boolean argument \u2013 True or False. True if normalization/scalar model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_scaled\" e.g. column X is appended as X_scaled. (Default value = \"replace\") print_impact True, False (Default value = False) This argument is to print out before and after descriptive statistics of rescaled columns. Returns ------- DataFrame Rescaled Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Normalization Performed - No numerical column(s) to transform\" ) return idf if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) idf_id = idf . withColumn ( \"tempID\" , F . monotonically_increasing_id ()) idf_partial = idf_id . select ([ \"tempID\" ] + list_of_cols ) assembler = VectorAssembler ( inputCols = list_of_cols , outputCol = \"list_of_cols_vector\" , handleInvalid = \"keep\" ) assembled_data = assembler . transform ( idf_partial ) if pre_existing_model : scalerModel = MinMaxScalerModel . load ( model_path + \"/normalization\" ) else : scaler = MinMaxScaler ( inputCol = \"list_of_cols_vector\" , outputCol = \"list_of_cols_scaled\" ) scalerModel = scaler . fit ( assembled_data ) if model_path != \"NA\" : scalerModel . write () . overwrite () . save ( model_path + \"/normalization\" ) scaledData = scalerModel . transform ( assembled_data ) def vector_to_array ( v ): return v . toArray () . tolist () f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . FloatType ())) odf_partial = scaledData . withColumn ( \"list_of_cols_array\" , f_vector_to_array ( \"list_of_cols_scaled\" ) ) . drop ( * [ \"list_of_cols_scaled\" , \"list_of_cols_vector\" ]) odf_schema = odf_partial . schema for i in list_of_cols : odf_schema = odf_schema . add ( T . StructField ( i + \"_scaled\" , T . FloatType ())) odf_partial = ( odf_partial . rdd . map ( lambda x : ( * x , * x [ \"list_of_cols_array\" ])) . toDF ( schema = odf_schema ) . drop ( \"list_of_cols_array\" ) ) odf = idf_id . join ( odf_partial . drop ( * list_of_cols ), \"tempID\" , \"left_outer\" ) . select ( idf . columns + [ ( F . when ( F . isnan ( F . col ( i + \"_scaled\" )), None ) . otherwise ( F . col ( i + \"_scaled\" ) ) ) . alias ( i + \"_scaled\" ) for i in list_of_cols ] ) if output_mode == \"replace\" : for i in list_of_cols : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_scaled\" , i ) odf = odf . select ( idf . columns ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_scaled\" ) for i in list_of_cols ] print ( \"Before: \" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After: \" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def imputation_MMM ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], method_type = \"median\" , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_missing = {}, stats_mode = {}, print_impact = False , ): \"\"\" This function handles missing value related issues by substituting null values by the measure of central tendency (mode for categorical features and mean/median for numerical features). For numerical attributes, it leverages [Imputer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer .html) functionality of Spark MLlib. Though, Imputer can be used for categorical attributes but this feature is available only in Spark3.x, therefore for categorical features, we compute mode or leverage mode computation from Measures of Central Tendency. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"median\", \"mean\" (valid only for for numerical columns attributes). Mode is only option for categorical columns. (Default value = \"median\") pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) stats_mode Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on most frequently seen values i.e. if measures_of_centralTendency or mode_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf ) else : missing_df = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) if ( len ( missing_cols ) == 0 ) & ( not pre_existing_model ) & ( model_path == \"NA\" ): return idf num_cols , cat_cols , other_cols = attributeType_segregation ( idf ) if list_of_cols == \"all\" : list_of_cols = num_cols + cat_cols if list_of_cols == \"missing\" : list_of_cols = [ x for x in missing_cols if x in num_cols + cat_cols ] if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Imputation performed- No column(s) to impute\" ) return idf if any ( x not in num_cols + cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"mode\" , \"mean\" , \"median\" ): raise TypeError ( \"Invalid input for method_type\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) num_cols , cat_cols , other_cols = attributeType_segregation ( idf . select ( list_of_cols )) odf = idf if len ( num_cols ) > 0 : recast_cols = [] recast_type = [] for i in num_cols : if get_dtype ( idf , i ) not in ( \"float\" , \"double\" ): odf = odf . withColumn ( i , F . col ( i ) . cast ( T . DoubleType ())) recast_cols . append ( i + \"_imputed\" ) recast_type . append ( get_dtype ( idf , i )) # For mode imputation if method_type == \"mode\" : if stats_mode == {}: parameters = [ str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) for i in num_cols ] else : mode_df = read_dataset ( spark , ** stats_mode ) . replace ( \"None\" , None ) mode_df_cols = list ( mode_df . select ( \"attribute\" ) . toPandas ()[ \"attribute\" ]) parameters = [] for i in num_cols : if i not in mode_df_cols : parameters . append ( str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) ) else : parameters . append ( mode_df . where ( F . col ( \"attribute\" ) == i ) . select ( \"mode\" ) . rdd . flatMap ( list ) . collect ()[ 0 ] ) for index , i in enumerate ( num_cols ): odf = odf . withColumn ( i + \"_imputed\" , F . when ( F . col ( i ) . isNull (), parameters [ index ]) . otherwise ( F . col ( i )), ) else : # For mean, median imputation # Building new imputer model or uploading the existing model if pre_existing_model : imputerModel = ImputerModel . load ( model_path + \"/imputation_MMM/num_imputer-model\" ) else : imputer = Imputer ( strategy = method_type , inputCols = num_cols , outputCols = [( e + \"_imputed\" ) for e in num_cols ], ) imputerModel = imputer . fit ( odf ) # Applying model # odf = recast_column(imputerModel.transform(odf), recast_cols, recast_type) odf = imputerModel . transform ( odf ) for i , j in zip ( recast_cols , recast_type ): odf = odf . withColumn ( i , F . col ( i ) . cast ( j )) # Saving model if required if ( not pre_existing_model ) & ( model_path != \"NA\" ): imputerModel . write () . overwrite () . save ( model_path + \"/imputation_MMM/num_imputer-model\" ) if len ( cat_cols ) > 0 : if pre_existing_model : df_model = spark . read . csv ( model_path + \"/imputation_MMM/cat_imputer\" , header = True , inferSchema = True , ) parameters = [] for i in cat_cols : mapped_value = ( df_model . where ( F . col ( \"attribute\" ) == i ) . select ( \"parameters\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) parameters . append ( mapped_value ) else : if stats_mode == {}: parameters = [ str ( ( idf . select ( i ) . dropna () . groupby ( i ) . count () . orderBy ( \"count\" , ascending = False ) . first () or [ None ] )[ 0 ] ) for i in cat_cols ] else : mode_df = read_dataset ( spark , ** stats_mode ) . replace ( \"None\" , None ) parameters = [ mode_df . where ( F . col ( \"attribute\" ) == i ) . select ( \"mode\" ) . rdd . flatMap ( list ) . collect ()[ 0 ] for i in cat_cols ] for index , i in enumerate ( cat_cols ): odf = odf . withColumn ( i + \"_imputed\" , F . when ( F . col ( i ) . isNull (), parameters [ index ]) . otherwise ( F . col ( i )), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model = spark . createDataFrame ( zip ( cat_cols , parameters ), schema = [ \"attribute\" , \"parameters\" ] ) df_model . repartition ( 1 ) . write . csv ( model_path + \"/imputation_MMM/cat_imputer\" , header = True , mode = \"overwrite\" , ) for i in num_cols + cat_cols : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) elif output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : output_cols = [ ( i + \"_imputed\" ) for i in [ e for e in ( num_cols + cat_cols ) if e in missing_cols ] ] odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def imputation_sklearn ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], missing_threshold = 1.0 , method_type = \"regression\" , use_sampling = True , sample_method = \"random\" , strata_cols = \"all\" , stratified_type = \"population\" , sample_size = 10000 , sample_seed = 42 , persist = True , persist_option = pyspark . StorageLevel . MEMORY_AND_DISK , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , stats_missing = {}, run_type = \"local\" , auth_key = \"NA\" , print_impact = False , ): \"\"\" The function \"imputation_sklearn\" leverages sklearn imputer algorithms. Two methods are supported via this function: \u201cKNN\u201d and \u201cregression\u201d. \u201cKNN\u201d option trains a sklearn.impute.KNNImputer which is based on k-Nearest Neighbors algorithm. The missing values of a sample are imputed using the mean of its 5 nearest neighbors in the training set. \u201cregression\u201d option trains a sklearn.impute.IterativeImputer which models attribute to impute as a function of rest of the attributes and imputes using the estimation. Imputation is performed in an iterative way from attributes with fewest number of missing values to most. All the hyperparameters used in the above mentioned imputers are their default values. However, sklearn imputers are not scalable, which might be slow if the size of the input dataframe is large. In fact, if the input dataframe size exceeds 10 GigaBytes, the model fitting step powered by sklearn might fail. Thus, an input sample_size (the default value is 10,000) can be set to control the number of samples to be used to train the imputer. If the total number of input dataset exceeds sample_size, the rest of the samples will be imputed using the trained imputer in a scalable manner. This is one of the way to demonstrate how Anovos has been designed as a scalable feature engineering library. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) missing_threshold Float argument - If list_of_cols is \"missing\", this argument is used to determined the missing threshold for every column. The column that has more (count of missing value/ count of total value) >= missing_threshold will be excluded from the list of columns to be imputed. (Default value = 1.0) method_type \"KNN\", \"regression\". \"KNN\" option trains a sklearn.impute.KNNImputer. \"regression\" option trains a sklearn.impute.IterativeImputer (Default value = \"regression\") use_sampling Boolean argument - True or False. This argument is used to determine whether to use sampling on source and target dataset, True will enable the use of sample method, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) sample_method If use_sampling is True, this argument is used to determine the sampling method. \"stratified\" for Stratified sampling, \"random\" for Random Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"random\") strata_cols If use_sampling is True and sample_method is \"stratified\", this argument is used to determine the list of columns used to be treated as strata. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"all\") stratified_type If use_sampling is True and sample_method is \"stratified\", this argument is used to determine the stratified sampling method. \"population\" stands for Proportionate Stratified Sampling, \"balanced\" stands for Optimum Stratified Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"population\") sample_size If use_sampling is True, this argument is used to determine maximum rows for training the sklearn imputer (Default value = 10000) sample_seed If use_sampling is True, this argument is used to determine the seed of sampling method. (Default value = 42) persist Boolean argument - True or False. This argument is used to determine whether to persist on binning result of source and target dataset, True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) persist_option If persist is True, this argument is used to determine the type of persist. For all the pyspark.StorageLevel option available in persist, please refer to https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.StorageLevel.html (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) pre_existing_model Boolean argument \u2013 True or False. True if imputation model exists already, False otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" if persist : idf = idf . persist ( persist_option ) num_cols = attributeType_segregation ( idf )[ 0 ] if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , num_cols ) else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( num_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns dropped from the imputation as all values are null: \" + \",\" . join ( empty_cols ) ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . where ( F . col ( \"missing_pct\" ) < missing_threshold ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = num_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e not in empty_cols )]) ) if len ( list_of_cols ) <= 1 : warnings . warn ( \"No Imputation Performed - No Column(s) or Insufficient Column(s) to Impute\" ) return idf if str ( pre_existing_model ) . lower () == \"true\" : pre_existing_model = True elif str ( pre_existing_model ) . lower () == \"false\" : pre_existing_model = False else : raise TypeError ( \"Non-Boolean input for pre_existing_model\" ) if ( ( len ([ e for e in list_of_cols if e in missing_cols ]) == 0 ) & ( not pre_existing_model ) & ( model_path == \"NA\" ) ): warnings . warn ( \"No Imputation Performed - No Column(s) to Impute and No Imputation Model to be saved\" ) return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in ( \"KNN\" , \"regression\" ): raise TypeError ( \"Invalid input for method_type\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) if pre_existing_model : if run_type == \"emr\" : bash_cmd = \"aws s3 cp \" + model_path + \"/imputation_sklearn.sav .\" output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) elif run_type == \"ak8s\" : bash_cmd = ( 'azcopy cp \"' + model_path + \"/imputation_sklearn.sav\" + str ( auth_key ) + '\" .' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) else : imputer = pickle . load ( open ( model_path + \"/imputation_sklearn.sav\" , \"rb\" )) else : if use_sampling : count_idf = idf . count () if count_idf > sample_size : idf_model = data_sample ( idf , strata_cols = strata_cols , fraction = sample_size / count_idf , method_type = sample_method , stratified_type = stratified_type , seed_value = sample_seed , ) else : idf_model = idf else : idf_model = idf if persist : idf_model = idf_model . persist ( persist_option ) idf_pd = idf_model . select ( list_of_cols ) . toPandas () if method_type == \"KNN\" : imputer = KNNImputer ( n_neighbors = 5 , weights = \"uniform\" , metric = \"nan_euclidean\" ) if method_type == \"regression\" : imputer = IterativeImputer () imputer . fit ( idf_pd ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): if run_type == \"emr\" : pickle . dump ( imputer , open ( \"imputation_sklearn.sav\" , \"wb\" )) bash_cmd = ( \"aws s3 cp imputation_sklearn.sav \" + model_path + \"/imputation_sklearn.sav\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) elif run_type == \"ak8s\" : pickle . dump ( imputer , open ( \"imputation_sklearn.sav\" , \"wb\" )) bash_cmd = ( 'azcopy cp \"imputation_sklearn.sav\" \"' + ends_with ( model_path ) + \"imputation_sklearn.sav\" + str ( auth_key ) + '\"' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) imputer = pickle . load ( open ( \"imputation_sklearn.sav\" , \"rb\" )) else : local_path = model_path + \"/imputation_sklearn.sav\" os . makedirs ( os . path . dirname ( local_path ), exist_ok = True ) pickle . dump ( imputer , open ( local_path , \"wb\" )) imputer = pickle . load ( open ( model_path + \"/imputation_sklearn.sav\" , \"rb\" ) ) @F . pandas_udf ( returnType = T . ArrayType ( T . DoubleType ())) def prediction ( * cols ): input_pdf = pd . concat ( cols , axis = 1 ) return pd . Series ( row . tolist () for row in imputer . transform ( input_pdf )) result_df = idf . withColumn ( \"features\" , prediction ( * list_of_cols )) if persist : result_df = result_df . persist ( persist_option ) odf_schema = result_df . schema for i in list_of_cols : odf_schema = odf_schema . add ( T . StructField ( i + \"_imputed\" , T . FloatType ())) odf = ( result_df . rdd . map ( lambda x : ( * x , * x [ \"features\" ])) . toDF ( schema = odf_schema ) . drop ( \"features\" ) ) output_cols = [] for i in list_of_cols : if output_mode == \"append\" : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) else : output_cols . append ( i + \"_imputed\" ) else : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) odf = odf . select ( idf . columns + output_cols ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) if persist : idf . unpersist () if not pre_existing_model : idf_model . unpersist () result_df . unpersist () return odf def imputation_matrixFactorization ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], id_col = \"\" , output_mode = \"replace\" , stats_missing = {}, print_impact = False , ): \"\"\" imputation_matrixFactorization uses collaborative filtering technique to impute missing values. Collaborative filtering is commonly used in recommender systems to fill the missing user-item entries and PySpark provides an implementation using alternating least squares (ALS) algorithm, which is used in this function. To fit our problem into the ALS model, each attribute is treated as an item and an id column needs to be specified by the user to generate the user-item pairs. In the case, ID column doesn't exist in the dataset and proxu ID column is implicitly generated by the function. Subsequently, all user-item pairs with known values will be used to train the ALS model and the trained model can be used to predict the user-item pairs with missing values. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. Returns ------- DataFrame Imputed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , num_cols ) else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( num_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns dropped from the imputation as all values are null: \" + \",\" . join ( empty_cols ) ) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . where ( F . col ( \"missing_pct\" ) < 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = num_cols if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ( [ e for e in list_of_cols if ( e not in drop_cols ) & ( e != id_col ) & ( e not in empty_cols ) ] ) ) if ( len ( list_of_cols ) == 0 ) | ( len ([ e for e in list_of_cols if e in missing_cols ]) == 0 ): warnings . warn ( \"No Imputation Performed - No Column(s) to Impute\" ) return idf if len ( list_of_cols ) == 1 : warnings . warn ( \"No Imputation Performed - Needs more than 1 column for matrix factorization\" ) return idf if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) remove_id = False if id_col == \"\" : idf = idf . withColumn ( \"id\" , F . monotonically_increasing_id ()) . withColumn ( \"id\" , F . row_number () . over ( Window . orderBy ( \"id\" )) ) id_col = \"id\" remove_id = True key_and_val = F . create_map ( list ( chain . from_iterable ([[ F . lit ( c ), F . col ( c )] for c in list_of_cols ])) ) df_flatten = idf . select ( id_col , F . explode ( key_and_val )) . withColumn ( \"key\" , F . concat ( F . col ( \"key\" ), F . lit ( \"_imputed\" )) ) id_type = get_dtype ( idf , id_col ) if id_type == \"string\" : id_indexer = StringIndexer () . setInputCol ( id_col ) . setOutputCol ( \"IDLabel\" ) id_indexer_model = id_indexer . fit ( df_flatten ) df_flatten = id_indexer_model . transform ( df_flatten ) . drop ( id_col ) else : df_flatten = df_flatten . withColumnRenamed ( id_col , \"IDLabel\" ) indexer = StringIndexer () . setInputCol ( \"key\" ) . setOutputCol ( \"keyLabel\" ) indexer_model = indexer . fit ( df_flatten ) df_encoded = indexer_model . transform ( df_flatten ) . drop ( \"key\" ) df_model = df_encoded . where ( F . col ( \"value\" ) . isNotNull ()) df_test = df_encoded . where ( F . col ( \"value\" ) . isNull ()) if ( df_model . select ( \"IDLabel\" ) . distinct () . count () < df_encoded . select ( \"IDLabel\" ) . distinct () . count () ): warnings . warn ( \"The returned odf may not be fully imputed because values for all list_of_cols are null for some IDs\" ) als = ALS ( maxIter = 20 , regParam = 0.01 , userCol = \"IDLabel\" , itemCol = \"keyLabel\" , ratingCol = \"value\" , coldStartStrategy = \"drop\" , ) model = als . fit ( df_model ) df_pred = ( model . transform ( df_test ) . drop ( \"value\" ) . withColumnRenamed ( \"prediction\" , \"value\" ) ) df_encoded_pred = df_model . union ( df_pred . select ( df_model . columns )) if id_type == \"string\" : IDlabelReverse = IndexToString () . setInputCol ( \"IDLabel\" ) . setOutputCol ( id_col ) df_encoded_pred = IDlabelReverse . transform ( df_encoded_pred ) else : df_encoded_pred = df_encoded_pred . withColumnRenamed ( \"IDLabel\" , id_col ) keylabelReverse = IndexToString () . setInputCol ( \"keyLabel\" ) . setOutputCol ( \"key\" ) odf_imputed = ( keylabelReverse . transform ( df_encoded_pred ) . groupBy ( id_col ) . pivot ( \"key\" ) . agg ( F . first ( \"value\" )) . select ( [ id_col ] + [( i + \"_imputed\" ) for i in list_of_cols if i in missing_cols ] ) ) odf = idf . join ( odf_imputed , id_col , \"left_outer\" ) for i in list_of_cols : if i not in missing_cols : odf = odf . drop ( i + \"_imputed\" ) elif output_mode == \"replace\" : odf = odf . drop ( i ) . withColumnRenamed ( i + \"_imputed\" , i ) if remove_id : odf = odf . drop ( \"id\" ) if print_impact : if output_mode == \"replace\" : odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , list_of_cols ) . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_after\" ) ), \"attribute\" , \"inner\" , ) else : output_cols = [ ( i + \"_imputed\" ) for i in [ e for e in list_of_cols if e in missing_cols ] ] odf_print = missing_df . select ( \"attribute\" , F . col ( \"missing_count\" ) . alias ( \"missingCount_before\" ) ) . join ( missingCount_computation ( spark , odf , output_cols ) . withColumnRenamed ( \"attribute\" , \"attribute_after\" ) . withColumn ( \"attribute\" , F . expr ( \"substring(attribute_after, 1, length(attribute_after)-8)\" ), ) . drop ( \"missing_pct\" ), \"attribute\" , \"inner\" , ) odf_print . show ( len ( list_of_cols ), False ) return odf def auto_imputation ( spark , idf , list_of_cols = \"missing\" , drop_cols = [], id_col = \"\" , null_pct = 0.1 , stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , root_path = \"\" , auth_key = \"NA\" , print_impact = True , ): \"\"\" auto_imputation tests for 5 imputation methods using the other imputation functions provided in this module and returns the one with the best performance. The 5 methods are: (1) imputation_MMM with method_type=\"mean\" (2) imputation_MMM with method_type=\"median\" (3) imputation_sklearn with method_type=\"KNN\" (4) imputation_sklearn with method_type=\"regression\" (5) imputation_matrixFactorization Samples without missing values in attributes to impute are used for testing by removing some % of values and impute them again using the above 5 methods. RMSE/attribute_mean is used as the evaluation metric for each attribute to reduce the effect of unit difference among attributes. The final error of a method is calculated by the sum of ( RMSE/attribute_mean) for all numerical attributes to impute and the method with the least error will be selected. The above testing is only applicable for numerical attributes. If categorical attributes are included, they will be automatically imputed using imputation_MMM. In addition, if there is only one numerical attribute to impute, only method (1) and (2) will be tested because the rest of the methods require more than one column. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of columns to impute e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. This is super useful instead of specifying all column names manually. \"missing\" (default) can be passed to include only those columns with missing values. One of the usecases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for the future use e.g. a column may not have missing value in the training dataset but missing values may possibly appear in the prediction dataset. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) id_col ID column (Default value = \"\") null_pct proportion of the valid input data to be replaced by None to form the test data (Default value = 0.1) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_imputed\" e.g. column X is appended as X_imputed. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") root_path This argument takes in a base folder path for writing out intermediate_data/ folder to. Default value is \"\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False (Default value = False) This argument is to print out before and after missing counts of imputed columns. It also print the name of best performing imputation method along with RMSE details. Returns ------- DataFrame Imputed Dataframe \"\"\" if stats_missing == {}: missing_df = missingCount_computation ( spark , idf ) missing_df . write . parquet ( root_path + \"intermediate_data/imputation_comparison/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/imputation_comparison/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"Following columns have all null values: \" + \",\" . join ( empty_cols )) missing_cols = ( missing_df . where ( F . col ( \"missing_count\" ) > 0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if list_of_cols == \"all\" : list_of_cols = idf . columns if list_of_cols == \"missing\" : list_of_cols = missing_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if ( e not in drop_cols ) & ( e != id_col )]) ) if any ( x not in idf . columns for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) del_cols = [ e for e in list_of_cols if e in empty_cols ] odf_del = idf . drop ( * del_cols ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] num_cols , cat_cols , other_cols = attributeType_segregation ( odf_del . select ( list_of_cols ) ) missing_catcols = [ e for e in cat_cols if e in missing_cols ] missing_numcols = [ e for e in num_cols if e in missing_cols ] if missing_catcols : odf_imputed_cat = imputation_MMM ( spark , odf_del , list_of_cols = missing_catcols , stats_missing = stats_missing ) else : odf_imputed_cat = odf_del if len ( missing_numcols ) == 0 : warnings . warn ( \"No Imputation Performed for numerical columns - No Column(s) to Impute\" ) return odf_imputed_cat idf_test = ( odf_imputed_cat . dropna ( subset = missing_numcols ) . withColumn ( \"index\" , F . monotonically_increasing_id ()) . withColumn ( \"index\" , F . row_number () . over ( Window . orderBy ( \"index\" ))) ) null_count = int ( null_pct * idf_test . count ()) idf_null = idf_test for i in missing_numcols : null_index = random . sample ( range ( idf_test . count ()), null_count ) idf_null = idf_null . withColumn ( i , F . when ( F . col ( \"index\" ) . isin ( null_index ), None ) . otherwise ( F . col ( i )) ) idf_null . write . parquet ( root_path + \"intermediate_data/imputation_comparison/test_dataset\" , mode = \"overwrite\" , ) idf_null = spark . read . parquet ( root_path + \"intermediate_data/imputation_comparison/test_dataset\" ) method1 = imputation_MMM ( spark , idf_null , list_of_cols = missing_numcols , method_type = \"mean\" , stats_missing = stats_missing , output_mode = output_mode , ) method2 = imputation_MMM ( spark , idf_null , list_of_cols = missing_numcols , method_type = \"median\" , stats_missing = stats_missing , output_mode = output_mode , ) valid_methods = [ method1 , method2 ] if len ( num_cols ) > 1 : method3 = imputation_sklearn ( spark , idf_null , list_of_cols = num_cols , method_type = \"KNN\" , stats_missing = stats_missing , output_mode = output_mode , auth_key = auth_key , ) method4 = imputation_sklearn ( spark , idf_null , list_of_cols = num_cols , method_type = \"regression\" , stats_missing = stats_missing , output_mode = output_mode , auth_key = auth_key , ) method5 = imputation_matrixFactorization ( spark , idf_null , list_of_cols = num_cols , id_col = id_col , stats_missing = stats_missing , output_mode = output_mode , ) valid_methods = [ method1 , method2 , method3 , method4 , method5 ] nrmse_all = [] method_all = [ \"MMM-mean\" , \"MMM-median\" , \"KNN\" , \"regression\" , \"matrix_factorization\" ] for index , method in enumerate ( valid_methods ): nrmse = 0 for i in missing_numcols : pred_col = ( i + \"_imputed\" ) if output_mode == \"append\" else i idf_joined = ( idf_test . select ( \"index\" , F . col ( i ) . alias ( \"val\" )) . join ( method . select ( \"index\" , F . col ( pred_col ) . alias ( \"pred\" )), \"index\" , \"left_outer\" , ) . dropna () ) idf_joined = recast_column ( idf = idf_joined , list_of_cols = [ \"val\" , \"pred\" ], list_of_dtypes = [ \"double\" , \"double\" ], ) pred_mean = float ( method . select ( F . mean ( pred_col )) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) i_nrmse = ( RegressionEvaluator ( metricName = \"rmse\" , labelCol = \"val\" , predictionCol = \"pred\" ) . evaluate ( idf_joined ) ) / abs ( pred_mean ) nrmse += i_nrmse nrmse_all . append ( nrmse ) min_index = nrmse_all . index ( np . min ( nrmse_all )) best_method = method_all [ min_index ] odf = valid_methods [ min_index ] if print_impact : print ( list ( zip ( method_all , nrmse_all ))) print ( \"Best Imputation Method: \" , best_method ) return odf def autoencoder_latentFeatures ( spark , idf , list_of_cols = \"all\" , drop_cols = [], reduction_params = 0.5 , sample_size = 500000 , epochs = 100 , batch_size = 256 , pre_existing_model = False , model_path = \"NA\" , standardization = True , standardization_configs = { \"pre_existing_model\" : False , \"model_path\" : \"NA\" }, imputation = False , imputation_configs = { \"imputation_function\" : \"imputation_MMM\" }, stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , root_path = \"\" , auth_key = \"NA\" , print_impact = False , ): \"\"\" Many machine learning models suffer from \"the curse of dimensionality\" when the number of features is too large. autoencoder_latentFeatures is able to reduce the dimensionality by compressing input attributes to a smaller number of latent features. To be more specific, it trains a neural network model using TensorFlow library. The neural network contains an encoder and a decoder, where the encoder learns to represent the input using smaller number of latent features controlled by the input *reduction_params* and the decoder learns to reproduce the input using the latent features. In the end, only the encoder will be kept and the latent features generated by the encoder will be added to the output dataframe. However, the neural network model is not trained in a scalable manner, which might not be able to handle large input dataframe. Thus, an input *sample_size* (the default value is 500,000) can be set to control the number of samples to be used to train the model. If the total number of samples exceeds *sample_size*, the rest of the samples will be predicted using the fitted encoder. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise, the model might not converge smoothly. Inputs *standardization* and *standardization_configs* can be set accordingly to perform standardization within the function. In addition, if a sample contains missing values in the model input, the output values for its latent features will all be None. Thus data imputation is also recommended if missing values exist, which can be done within the function by setting inputs *imputation* and *imputation_configs*. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) reduction_params Determines the number of encoded features in the result. If reduction_params < 1, int(reduction_params * <number of columns>) columns will be generated. Else, reduction_params columns will be generated. (Default value = 0.5) sample_size Maximum rows for training the autoencoder model using tensorflow. (Default value = 500000) epochs Number of epochs to train the tensorflow model. (Default value = 100) batch_size Number of samples per gradient update when fitting the tensorflow model. (Default value = 256) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_<col_index>. \u201cappend\u201d option append transformed columns with format latent_<col_index> to the input dataset, e.g. latent_0, latent_1 will be appended if reduction_params=2. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") root_path This argument takes in a base folder path for writing out intermediate_data/ folder to. Default value is \"\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns ------- DataFrame Dataframe with Latent Features \"\"\" if \"arm64\" in platform . version () . lower (): warnings . warn ( \"This function is currently not supported for ARM64 - Mac M1 Machine\" ) return idf num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Latent Features Generated - No Column(s) to Transform\" ) return idf if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , list_of_cols ) missing_df . write . parquet ( root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from dimensionality reduction as all values are null: \" + \",\" . join ( empty_cols ) ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] if standardization : idf_standardized = z_standardization ( spark , idf , list_of_cols = list_of_cols , output_mode = \"append\" , ** standardization_configs , ) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols if ( i + \"_scaled\" ) in idf_standardized . columns ] else : idf_standardized = idf for i in list_of_cols : idf_standardized = idf_standardized . withColumn ( i + \"_scaled\" , F . col ( i )) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols ] if imputation : all_functions = globals () . copy () all_functions . update ( locals ()) f = all_functions . get ( imputation_configs [ \"imputation_function\" ]) args = copy . deepcopy ( imputation_configs ) args . pop ( \"imputation_function\" , None ) missing_df_scaled = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . withColumn ( \"attribute\" , F . concat ( F . col ( \"attribute\" ), F . lit ( \"_scaled\" ))) ) missing_df_scaled . write . parquet ( root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation_scaled\" , mode = \"overwrite\" , ) stats_missing_scaled = { \"file_path\" : root_path + \"intermediate_data/autoencoder_latentFeatures/missingCount_computation_scaled\" , \"file_type\" : \"parquet\" , } idf_imputed = f ( spark , idf_standardized , list_of_cols_scaled , stats_missing = stats_missing_scaled , ** args , ) else : idf_imputed = idf_standardized . dropna ( subset = list_of_cols_scaled ) n_inputs = len ( list_of_cols_scaled ) if reduction_params < 1 : n_bottleneck = int ( reduction_params * n_inputs ) else : n_bottleneck = int ( reduction_params ) if pre_existing_model : if run_type == \"emr\" : bash_cmd = ( \"aws s3 cp \" + model_path + \"/autoencoders_latentFeatures/encoder.h5 .\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( \"aws s3 cp \" + model_path + \"/autoencoders_latentFeatures/model.h5 .\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) encoder = load_model ( \"encoder.h5\" ) model = load_model ( \"model.h5\" ) elif run_type == \"ak8s\" : bash_cmd = ( 'azcopy cp \"' + model_path + \"/autoencoders_latentFeatures/encoder.h5\" + str ( auth_key ) + '\" .' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( 'azcopy cp \"' + model_path + \"/autoencoders_latentFeatures/model.h5\" + str ( auth_key ) + '\" .' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) encoder = load_model ( \"encoder.h5\" ) model = load_model ( \"model.h5\" ) else : encoder = load_model ( model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) model = load_model ( model_path + \"/autoencoders_latentFeatures/model.h5\" ) else : idf_valid = idf_imputed . select ( list_of_cols_scaled ) idf_model = idf_valid . sample ( False , min ( 1.0 , float ( sample_size ) / idf_valid . count ()), 0 ) idf_train = idf_model . sample ( False , 0.8 , 0 ) idf_test = idf_model . subtract ( idf_train ) X_train = idf_train . toPandas () X_test = idf_test . toPandas () visible = Input ( shape = ( n_inputs ,)) e = Dense ( n_inputs * 2 )( visible ) e = BatchNormalization ()( e ) e = LeakyReLU ()( e ) e = Dense ( n_inputs )( e ) e = BatchNormalization ()( e ) e = LeakyReLU ()( e ) bottleneck = Dense ( n_bottleneck )( e ) d = Dense ( n_inputs )( bottleneck ) d = BatchNormalization ()( d ) d = LeakyReLU ()( d ) d = Dense ( n_inputs * 2 )( d ) d = BatchNormalization ()( d ) d = LeakyReLU ()( d ) output = Dense ( n_inputs , activation = \"linear\" )( d ) model = Model ( inputs = visible , outputs = output ) encoder = Model ( inputs = visible , outputs = bottleneck ) model . compile ( optimizer = \"adam\" , loss = \"mse\" ) history = model . fit ( X_train , X_train , epochs = int ( epochs ), batch_size = int ( batch_size ), verbose = 2 , validation_data = ( X_test , X_test ), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): if run_type == \"emr\" : encoder . save ( \"encoder.h5\" ) model . save ( \"model.h5\" ) bash_cmd = ( \"aws s3 cp encoder.h5 \" + model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( \"aws s3 cp model.h5 \" + model_path + \"/autoencoders_latentFeatures/model.h5\" ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) elif run_type == \"ak8s\" : encoder . save ( \"encoder.h5\" ) model . save ( \"model.h5\" ) bash_cmd = ( 'azcopy cp \"encoder.h5\" \"' + model_path + \"/autoencoders_latentFeatures/encoder.h5\" + str ( auth_key ) + '\" ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) bash_cmd = ( 'azcopy cp \"model.h5\" \"' + model_path + \"/autoencoders_latentFeatures/model.h5\" + str ( auth_key ) + '\" ' ) output = subprocess . check_output ([ \"bash\" , \"-c\" , bash_cmd ]) else : if not os . path . exists ( model_path + \"/autoencoders_latentFeatures/\" ): os . makedirs ( model_path + \"/autoencoders_latentFeatures/\" ) encoder . save ( model_path + \"/autoencoders_latentFeatures/encoder.h5\" ) model . save ( model_path + \"/autoencoders_latentFeatures/model.h5\" ) class ModelWrapperPickable : def __init__ ( self , model ): self . model = model def __getstate__ ( self ): model_str = \"\" with tempfile . NamedTemporaryFile ( suffix = \".hdf5\" , delete = True ) as fd : tensorflow . keras . models . save_model ( self . model , fd . name , overwrite = True ) model_str = fd . read () d = { \"model_str\" : model_str } return d def __setstate__ ( self , state ): with tempfile . NamedTemporaryFile ( suffix = \".hdf5\" , delete = True ) as fd : fd . write ( state [ \"model_str\" ]) fd . flush () self . model = tensorflow . keras . models . load_model ( fd . name ) model_wrapper = ModelWrapperPickable ( encoder ) def compute_output_pandas_udf ( model_wrapper ): @F . pandas_udf ( returnType = T . ArrayType ( T . DoubleType ())) def predict_pandas_udf ( * cols ): X = pd . concat ( cols , axis = 1 ) return pd . Series ( row . tolist () for row in model_wrapper . model . predict ( X )) return predict_pandas_udf odf = idf_imputed . withColumn ( \"predicted_output\" , compute_output_pandas_udf ( model_wrapper )( * list_of_cols_scaled ), ) odf_schema = odf . schema for i in range ( 0 , n_bottleneck ): odf_schema = odf_schema . add ( T . StructField ( \"latent_\" + str ( i ), T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"predicted_output\" ])) . toDF ( schema = odf_schema ) . drop ( \"predicted_output\" ) ) odf = odf . drop ( * list_of_cols_scaled ) if output_mode == \"replace\" : odf = odf . drop ( * list_of_cols ) if print_impact : output_cols = [ \"latent_\" + str ( j ) for j in range ( 0 , n_bottleneck )] odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def PCA_latentFeatures ( spark , idf , list_of_cols = \"all\" , drop_cols = [], explained_variance_cutoff = 0.95 , pre_existing_model = False , model_path = \"NA\" , standardization = True , standardization_configs = { \"pre_existing_model\" : False , \"model_path\" : \"NA\" }, imputation = False , imputation_configs = { \"imputation_function\" : \"imputation_MMM\" }, stats_missing = {}, output_mode = \"replace\" , run_type = \"local\" , root_path = \"\" , auth_key = \"NA\" , print_impact = False , ): \"\"\" Similar to autoencoder_latentFeatures, PCA_latentFeatures also generates latent features which reduces the dimensionality of the input dataframe but through a different technique: Principal Component Analysis (PCA). PCA algorithm produces principal components such that it can describe most of the remaining variance and all the principal components are orthogonal to each other. The final number of generated principal components is controlled by the input *explained_variance_cutoff*. In other words, the number of selected principal components, k, is the minimum value such that top k components (latent features) can explain at least *explained_variance_cutoff* of the total variance. Standardization is highly recommended if the input attributes are not of the same scale. Otherwise the generated latent features might be dominated by the attributes with larger variance. Inputs *standardization* and *standardization_configs* can be set accordingly to perform standardization within the function. In addition, data imputation is also recommended if missing values exist, which can be done within the function by setting inputs *imputation* and *imputation_configs*. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of numerical columns to encode e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) explained_variance_cutoff Determines the number of latent columns in the output. If N is the smallest integer such that top N latent columns explain more than explained_variance_cutoff variance, these N columns will be selected. (Default value = 0.95) pre_existing_model Boolean argument \u2013 True or False. True if model exists already, False Otherwise (Default value = False) model_path If pre_existing_model is True, this argument is path for referring the pre-saved model. If pre_existing_model is False, this argument can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. standardization Boolean argument \u2013 True or False. True, if the standardization required. (Default value = True) standardization_configs z_standardization function arguments in dictionary format. (Default value = {\"pre_existing_model\": False) imputation Boolean argument \u2013 True or False. True, if the imputation required. (Default value = False) imputation_configs Takes input in dictionary format. Imputation function name is provided with key \"imputation_name\". optional arguments pertaining to that imputation function can be provided with argument name as key. (Default value = {\"imputation_function\": \"imputation_MMM\"}) stats_missing Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on missing count/pct i.e. if measures_of_counts or missingCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns: latent_<col_index>. \u201cappend\u201d option append transformed columns with format latent_<col_index> to the input dataset, e.g. latent_0, latent_1. (Default value = \"replace\") run_type \"local\", \"emr\", \"databricks\", \"ak8s\" (Default value = \"local\") root_path This argument takes in a base folder path for writing out intermediate_data/ folder to. Default value is \"\" auth_key Option to pass an authorization key to write to filesystems. Currently applicable only for ak8s run_type. Default value is kept as \"NA\" print_impact True, False This argument is to print descriptive statistics of the latest features (Default value = False) Returns ------- DataFrame Dataframe with Latent Features \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Latent Features Generated - No Column(s) to Transform\" ) return idf if stats_missing == {}: missing_df = missingCount_computation ( spark , idf , list_of_cols ) missing_df . write . parquet ( root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation\" , mode = \"overwrite\" , ) stats_missing = { \"file_path\" : root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation\" , \"file_type\" : \"parquet\" , } else : missing_df = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . where ( F . col ( \"attribute\" ) . isin ( list_of_cols )) ) empty_cols = ( missing_df . where ( F . col ( \"missing_pct\" ) == 1.0 ) . select ( \"attribute\" ) . rdd . flatMap ( lambda x : x ) . collect () ) if len ( empty_cols ) > 0 : warnings . warn ( \"The following column(s) are excluded from dimensionality reduction as all values are null: \" + \",\" . join ( empty_cols ) ) list_of_cols = [ e for e in list_of_cols if e not in empty_cols ] if standardization : idf_standardized = z_standardization ( spark , idf , list_of_cols = list_of_cols , output_mode = \"append\" , ** standardization_configs , ) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols if ( i + \"_scaled\" ) in idf_standardized . columns ] else : idf_standardized = idf for i in list_of_cols : idf_standardized = idf_standardized . withColumn ( i + \"_scaled\" , F . col ( i )) list_of_cols_scaled = [ i + \"_scaled\" for i in list_of_cols ] if imputation : all_functions = globals () . copy () all_functions . update ( locals ()) f = all_functions . get ( imputation_configs [ \"imputation_function\" ]) args = copy . deepcopy ( imputation_configs ) args . pop ( \"imputation_function\" , None ) missing_df_scaled = ( read_dataset ( spark , ** stats_missing ) . select ( \"attribute\" , \"missing_count\" , \"missing_pct\" ) . withColumn ( \"attribute\" , F . concat ( F . col ( \"attribute\" ), F . lit ( \"_scaled\" ))) ) missing_df_scaled . write . parquet ( root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation_scaled\" , mode = \"overwrite\" , ) stats_missing_scaled = { \"file_path\" : root_path + \"intermediate_data/PCA_latentFeatures/missingCount_computation_scaled\" , \"file_type\" : \"parquet\" , } idf_imputed = f ( spark , idf_standardized , list_of_cols_scaled , stats_missing = stats_missing_scaled , ** args , ) else : idf_imputed = idf_standardized . dropna ( subset = list_of_cols_scaled ) idf_imputed . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) . count () assembler = VectorAssembler ( inputCols = list_of_cols_scaled , outputCol = \"features\" ) assembled_data = assembler . transform ( idf_imputed ) . drop ( * list_of_cols_scaled ) if pre_existing_model : pca = PCA . load ( model_path + \"/PCA_latentFeatures/pca_path\" ) pcaModel = PCAModel . load ( model_path + \"/PCA_latentFeatures/pcaModel_path\" ) n = pca . getK () else : pca = PCA ( k = len ( list_of_cols_scaled ), inputCol = \"features\" , outputCol = \"features_pca\" ) pcaModel = pca . fit ( assembled_data ) explained_variance = 0 for n in range ( 1 , len ( list_of_cols ) + 1 ): explained_variance += pcaModel . explainedVariance [ n - 1 ] if explained_variance > explained_variance_cutoff : break pca = PCA ( k = n , inputCol = \"features\" , outputCol = \"features_pca\" ) pcaModel = pca . fit ( assembled_data ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): pcaModel . write () . overwrite () . save ( model_path + \"/PCA_latentFeatures/pcaModel_path\" ) pca . write () . overwrite () . save ( model_path + \"/PCA_latentFeatures/pca_path\" ) def vector_to_array ( v ): return v . toArray () . tolist () f_vector_to_array = F . udf ( vector_to_array , T . ArrayType ( T . FloatType ())) odf = ( pcaModel . transform ( assembled_data ) . withColumn ( \"features_pca_array\" , f_vector_to_array ( \"features_pca\" )) . drop ( * [ \"features\" , \"features_pca\" ]) ) odf_schema = odf . schema for i in range ( 0 , n ): odf_schema = odf_schema . add ( T . StructField ( \"latent_\" + str ( i ), T . FloatType ())) odf = ( odf . rdd . map ( lambda x : ( * x , * x [ \"features_pca_array\" ])) . toDF ( schema = odf_schema ) . drop ( \"features_pca_array\" ) . replace ( float ( \"nan\" ), None , subset = [ \"latent_\" + str ( j ) for j in range ( 0 , n )]) ) if output_mode == \"replace\" : odf = odf . drop ( * list_of_cols ) if print_impact : print ( \"Explained Variance: \" , round ( np . sum ( pcaModel . explainedVariance [ 0 : n ]), 4 )) output_cols = [ \"latent_\" + str ( j ) for j in range ( 0 , n )] odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def feature_transformation ( idf , list_of_cols = \"all\" , drop_cols = [], method_type = \"sqrt\" , N = None , output_mode = \"replace\" , print_impact = False , ): \"\"\" As the name indicates, feature_transformation performs mathematical transformation over selected attributes. The following methods are supported for an input attribute x: ln(x), log10(x), log2(x), e^x, 2^x, 10^x, N^x, square and cube root of x, x^2, x^3, x^N, trigonometric transformations of x (sin, cos, tan, asin, acos, atan), radians, x%N, x!, 1/x, floor and ceiling of x and x rounded to N decimal places. Some transformations only work with positive or non-negative input values such as log and square root and an error will be returned if violated. Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) method_type \"ln\", \"log10\", \"log2\", \"exp\", \"powOf2\" (2^x), \"powOf10\" (10^x), \"powOfN\" (N^x), \"sqrt\" (square root), \"cbrt\" (cube root), \"sq\" (square), \"cb\" (cube), \"toPowerN\" (x^N), \"sin\", \"cos\", \"tan\", \"asin\", \"acos\", \"atan\", \"radians\", \"remainderDivByN\" (x%N), \"factorial\" (x!), \"mul_inv\" (1/x), \"floor\", \"ceil\", \"roundN\" (round to N decimal places) (Default value = \"sqrt\") N None by default. If method_type is \"powOfN\", \"toPowerN\", \"remainderDivByN\" or \"roundN\", N will be used as the required constant. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix (E.g. \"_ln\", \"_powOf<N>\") to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if ( len ( list_of_cols ) == 0 ) | ( any ( x not in num_cols for x in list_of_cols )): raise TypeError ( \"Invalid input for Column(s)\" ) if method_type not in [ \"ln\" , \"log10\" , \"log2\" , \"exp\" , \"powOf2\" , \"powOf10\" , \"powOfN\" , \"sqrt\" , \"cbrt\" , \"sq\" , \"cb\" , \"toPowerN\" , \"sin\" , \"cos\" , \"tan\" , \"asin\" , \"acos\" , \"atan\" , \"radians\" , \"remainderDivByN\" , \"factorial\" , \"mul_inv\" , \"floor\" , \"ceil\" , \"roundN\" , ]: raise TypeError ( \"Invalid input method_type\" ) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] list_of_cols = num_cols odf = idf transformation_function = { \"ln\" : F . log , \"log10\" : F . log10 , \"log2\" : F . log2 , \"exp\" : F . exp , \"powOf2\" : ( lambda x : F . pow ( 2.0 , x )), \"powOf10\" : ( lambda x : F . pow ( 10.0 , x )), \"powOfN\" : ( lambda x : F . pow ( N , x )), \"sqrt\" : F . sqrt , \"cbrt\" : F . cbrt , \"sq\" : ( lambda x : x ** 2 ), \"cb\" : ( lambda x : x ** 3 ), \"toPowerN\" : ( lambda x : x ** N ), \"sin\" : F . sin , \"cos\" : F . cos , \"tan\" : F . tan , \"asin\" : F . asin , \"acos\" : F . acos , \"atan\" : F . atan , \"radians\" : F . radians , \"remainderDivByN\" : ( lambda x : x % F . lit ( N )), \"factorial\" : F . factorial , \"mul_inv\" : ( lambda x : F . lit ( 1 ) / x ), \"floor\" : F . floor , \"ceil\" : F . ceil , \"roundN\" : ( lambda x : F . round ( x , N )), } def get_col_name ( i ): if output_mode == \"replace\" : return i else : if method_type in [ \"powOfN\" , \"toPowerN\" , \"remainderDivByN\" , \"roundN\" ]: return i + \"_\" + method_type [: - 1 ] + str ( N ) else : return i + \"_\" + method_type output_cols = [] for i in list_of_cols : modify_col = get_col_name ( i ) odf = odf . withColumn ( modify_col , transformation_function [ method_type ]( F . col ( i ))) output_cols . append ( modify_col ) if print_impact : print ( \"Before:\" ) idf . select ( list_of_cols ) . describe () . show ( 5 , False ) print ( \"After:\" ) odf . select ( output_cols ) . describe () . show ( 5 , False ) return odf def boxcox_transformation ( idf , list_of_cols = \"all\" , drop_cols = [], boxcox_lambda = None , output_mode = \"replace\" , print_impact = False , ): \"\"\" Some machine learning algorithms require the input data to follow normal distributions. Thus, when the input data is too skewed, boxcox_transformation can be used to transform it into a more normal-like distribution. The transformed value of a sample x depends on a coefficient lambda: (1) if lambda = 0, x is transformed into log(x); (2) if lambda !=0, x is transformed into (x^lambda-1)/lambda. The value of lambda can be either specified by the user or automatically selected within the function. If lambda needs to be selected within the function, a range of values (0, 1, -1, 0.5, -0.5, 2, -2, 0.25, -0.25, 3, -3, 4, -4, 5, -5) will be tested and the lambda, which optimizes the KolmogorovSmirnovTest by PySpark with the theoretical distribution being normal, will be used to perform the transformation. Different lambda values can be assigned to different attributes but one attribute can only be assigned one lambda value. Parameters ---------- idf Input Dataframe list_of_cols List of numerical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) boxcox_lambda Lambda value for box_cox transormation. If boxcox_lambda is not None, it will be directly used for the transformation. It can be a (1) list: each element represents a lambda value for an attribute and the length of the list must be the same as the number of columns to transform. (2) int/float: all attributes will be assigned the same lambda value. Else, search for the best lambda among [1,-1,0.5,-0.5,2,-2,0.25,-0.25,3,-3,4,-4,5,-5] for each column and apply the transformation (Default value = None) output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed columns. \u201cappend\u201d option append transformed columns with a postfix \"_bxcx_<lambda>\" to the input dataset. (Default value = \"replace\") print_impact True, False This argument is to print before and after descriptive statistics of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" num_cols = attributeType_segregation ( idf )[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if ( len ( list_of_cols ) == 0 ) | ( any ( x not in num_cols for x in list_of_cols )): raise TypeError ( \"Invalid input for Column(s)\" ) num_cols = attributeType_segregation ( idf . select ( list_of_cols ))[ 0 ] list_of_cols = num_cols odf = idf col_mins = idf . select ([ F . min ( i ) for i in list_of_cols ]) if any ([ i <= 0 for i in col_mins . rdd . flatMap ( lambda x : x ) . collect ()]): col_mins . show ( 1 , False ) raise ValueError ( \"Data must be positive\" ) if boxcox_lambda is not None : if isinstance ( boxcox_lambda , ( list , tuple )): if len ( boxcox_lambda ) != len ( list_of_cols ): raise TypeError ( \"Invalid input for boxcox_lambda\" ) elif not all ([ isinstance ( l , ( float , int )) for l in boxcox_lambda ]): raise TypeError ( \"Invalid input for boxcox_lambda\" ) else : boxcox_lambda_list = list ( boxcox_lambda ) elif isinstance ( boxcox_lambda , ( float , int )): boxcox_lambda_list = [ boxcox_lambda ] * len ( list_of_cols ) else : raise TypeError ( \"Invalid input for boxcox_lambda\" ) else : boxcox_lambda_list = [] for i in list_of_cols : lambdaVal = [ 1 , - 1 , 0.5 , - 0.5 , 2 , - 2 , 0.25 , - 0.25 , 3 , - 3 , 4 , - 4 , 5 , - 5 ] best_pVal = 0 for j in lambdaVal : pVal = Statistics . kolmogorovSmirnovTest ( odf . select ( F . pow ( F . col ( i ), j )) . rdd . flatMap ( lambda x : x ), \"norm\" ) . pValue if pVal > best_pVal : best_pVal = pVal best_lambdaVal = j pVal = Statistics . kolmogorovSmirnovTest ( odf . select ( F . log ( F . col ( i ))) . rdd . flatMap ( lambda x : x ), \"norm\" ) . pValue if pVal > best_pVal : best_pVal = pVal best_lambdaVal = 0 boxcox_lambda_list . append ( best_lambdaVal ) output_cols = [] for i , curr_lambdaVal in zip ( list_of_cols , boxcox_lambda_list ): if curr_lambdaVal != 1 : modify_col = ( ( i + \"_bxcx_\" + str ( curr_lambdaVal )) if output_mode == \"append\" else i ) output_cols . append ( modify_col ) if curr_lambdaVal == 0 : odf = odf . withColumn ( modify_col , F . log ( F . col ( i ))) else : odf = odf . withColumn ( modify_col , F . pow ( F . col ( i ), curr_lambdaVal )) if len ( output_cols ) == 0 : warnings . warn ( \"lambdaVal for all columns are 1 so no transformation is performed and idf is returned\" ) return idf if print_impact : print ( \"Transformed Columns: \" , list_of_cols ) print ( \"Best BoxCox Parameter(s): \" , boxcox_lambda_list ) print ( \"Before:\" ) idf . select ( list_of_cols ) . describe () . unionByName ( idf . select ([ F . skewness ( i ) . alias ( i ) for i in list_of_cols ]) . withColumn ( \"summary\" , F . lit ( \"skewness\" ) ) ) . show ( 6 , False ) print ( \"After:\" ) if output_mode == \"replace\" : odf . select ( list_of_cols ) . describe () . unionByName ( odf . select ([ F . skewness ( i ) . alias ( i ) for i in list_of_cols ]) . withColumn ( \"summary\" , F . lit ( \"skewness\" ) ) ) . show ( 6 , False ) else : output_cols = [( \"`\" + i + \"`\" ) for i in output_cols ] odf . select ( output_cols ) . describe () . unionByName ( odf . select ( [ F . skewness ( i ) . alias ( i [ 1 : - 1 ]) for i in output_cols ] ) . withColumn ( \"summary\" , F . lit ( \"skewness\" )) ) . show ( 6 , False ) return odf def outlier_categories ( spark , idf , list_of_cols = \"all\" , drop_cols = [], coverage = 1.0 , max_category = 50 , pre_existing_model = False , model_path = \"NA\" , output_mode = \"replace\" , print_impact = False , ): \"\"\" This function replaces less frequently seen values (called as outlier values in the current context) in a categorical column by 'outlier_categories'. Outlier values can be defined in two ways \u2013 a) Max N categories, where N is user defined value. In this method, top N-1 frequently seen categories are considered and rest are clubbed under single category 'outlier_categories'. or Alternatively, b) Coverage \u2013 top frequently seen categories are considered till it covers minimum N% of rows and rest lesser seen values are mapped to 'outlier_categories'. Even if the Coverage is less, maximum category constraint is given priority. Further, there is a caveat that when multiple categories have same rank. Then, number of categorical values can be more than max_category defined by the user. This function performs better when distinct values, in any column, are not more than 100. It is recommended that to drop those columns from the computation which has more than 100 distinct values, to get better performance out of this function. Parameters ---------- spark Spark Session idf Input Dataframe list_of_cols List of categorical columns to transform e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all categorical columns for analysis. This is super useful instead of specifying all column names manually. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". It is most useful when coupled with the \u201call\u201d value of list_of_cols, when we need to consider all columns except a few handful of them. (Default value = []) coverage Defines the minimum % of rows that will be mapped to actual category name and the rest to be mapped to 'outlier_categories' and takes value between 0 to 1. Coverage of 0.8 can be interpreted as top frequently seen categories are considered till it covers minimum 80% of rows and rest lesser seen values are mapped to 'outlier_categories'. (Default value = 1.0) max_category Even if coverage is less, only (max_category - 1) categories will be mapped to actual name and rest to 'outlier_categories'. Caveat is when multiple categories have same rank, then #categories can be more than max_category. (Default value = 50) pre_existing_model Boolean argument \u2013 True or False. True if the model with the outlier/other values for each attribute exists already to be used, False Otherwise. (Default value = False) model_path If pre_existing_model is True, this argument is path for the pre-saved model. If pre_existing_model is False, this field can be used for saving the model. Default \"NA\" means there is neither pre-existing model nor there is a need to save one. output_mode \"replace\", \"append\". \u201creplace\u201d option replaces original columns with transformed column. \u201cappend\u201d option append transformed column to the input dataset with a postfix \"_outliered\" e.g. column X is appended as X_outliered. (Default value = \"replace\") print_impact True, False This argument is to print before and after unique count of the transformed features (Default value = False) Returns ------- DataFrame Transformed Dataframe \"\"\" cat_cols = attributeType_segregation ( idf )[ 1 ] if list_of_cols == \"all\" : list_of_cols = cat_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in cat_cols for x in list_of_cols ): raise TypeError ( \"Invalid input for Column(s)\" ) if len ( list_of_cols ) == 0 : warnings . warn ( \"No Outlier Categories Computation - No categorical column(s) to transform\" ) return idf if ( coverage <= 0 ) | ( coverage > 1 ): raise TypeError ( \"Invalid input for Coverage Value\" ) if max_category < 2 : raise TypeError ( \"Invalid input for Maximum No. of Categories Allowed\" ) if output_mode not in ( \"replace\" , \"append\" ): raise TypeError ( \"Invalid input for output_mode\" ) idf = idf . persist ( pyspark . StorageLevel . MEMORY_AND_DISK ) if pre_existing_model : df_model = spark . read . csv ( model_path + \"/outlier_categories\" , header = True , inferSchema = True ) else : for index , i in enumerate ( list_of_cols ): window = Window . partitionBy () . orderBy ( F . desc ( \"count_pct\" )) df_cats = ( idf . select ( i ) . groupBy ( i ) . count () . dropna () . withColumn ( \"count_pct\" , F . col ( \"count\" ) / F . sum ( \"count\" ) . over ( Window . partitionBy ()), ) . withColumn ( \"rank\" , F . rank () . over ( window )) . withColumn ( \"cumu\" , F . sum ( \"count_pct\" ) . over ( window . rowsBetween ( Window . unboundedPreceding , 0 ) ), ) . withColumn ( \"lag_cumu\" , F . lag ( \"cumu\" ) . over ( window )) . fillna ( 0 ) . where ( ~ (( F . col ( \"cumu\" ) >= coverage ) & ( F . col ( \"lag_cumu\" ) >= coverage ))) . where ( F . col ( \"rank\" ) <= ( max_category - 1 )) . select ( F . lit ( i ) . alias ( \"attribute\" ), F . col ( i ) . alias ( \"parameters\" )) ) if index == 0 : df_model = df_cats else : df_model = df_model . union ( df_cats ) df_params = df_model . rdd . groupByKey () . mapValues ( list ) . collect () dict_params = dict ( df_params ) broadcast_params = spark . sparkContext . broadcast ( dict_params ) def get_params ( key ): parameters = list () dict_params = broadcast_params . value params = dict_params . get ( key ) if params : parameters = params return parameters odf = idf for i in list_of_cols : parameters = get_params ( i ) if output_mode == \"replace\" : odf = odf . withColumn ( i , F . when ( ( F . col ( i ) . isin ( parameters )) | ( F . col ( i ) . isNull ()), F . col ( i ) ) . otherwise ( \"outlier_categories\" ), ) else : odf = odf . withColumn ( i + \"_outliered\" , F . when ( ( F . col ( i ) . isin ( parameters )) | ( F . col ( i ) . isNull ()), F . col ( i ) ) . otherwise ( \"outlier_categories\" ), ) if ( not pre_existing_model ) & ( model_path != \"NA\" ): df_model . repartition ( 1 ) . write . csv ( model_path + \"/outlier_categories\" , header = True , mode = \"overwrite\" ) if print_impact : if output_mode == \"replace\" : output_cols = list_of_cols else : output_cols = [( i + \"_outliered\" ) for i in list_of_cols ] uniqueCount_computation ( spark , idf , list_of_cols ) . select ( \"attribute\" , F . col ( \"unique_values\" ) . alias ( \"uniqueValues_before\" ) ) . show ( len ( list_of_cols ), False ) uniqueCount_computation ( spark , odf , output_cols ) . select ( \"attribute\" , F . col ( \"unique_values\" ) . alias ( \"uniqueValues_after\" ) ) . show ( len ( list_of_cols ), False ) idf . unpersist () return odf def expression_parser ( idf , list_of_expr , postfix = \"\" , print_impact = False ): \"\"\" expression_parser can be used to evaluate a list of SQL expressions and output the result as new features. It is able to handle column names containing special characters such as \u201c.\u201d, \u201c-\u201d, \u201c@\u201d, \u201c^\u201d, etc, by converting them to \u201c_\u201d first before the evaluation and convert them back to the original names before returning the output dataframe. Parameters ---------- idf Input Dataframe list_of_expr List of expressions to evaluate as new features e.g., [\"expr1\",\"expr2\"]. Alternatively, expressions can be specified in a string format, where different expressions are separated by pipe delimiter \u201c|\u201d e.g., \"expr1|expr2\". postfix postfix for new feature name.Naming convention \"f\" + expression_index + postfix e.g. with postfix of \"new\", new added features are named as f0new, f1new etc. (Default value = \"\") print_impact True, False This argument is to print the descriptive statistics of the parsed features (Default value = False) Returns ------- DataFrame Parsed Dataframe \"\"\" if isinstance ( list_of_expr , str ): list_of_expr = [ x . strip () for x in list_of_expr . split ( \"|\" )] special_chars = [ \"&\" , \"$\" , \";\" , \":\" , \",\" , \"*\" , \"#\" , \"@\" , \"?\" , \"%\" , \"!\" , \"^\" , \"(\" , \")\" , \"-\" , \"/\" , \"'\" , \".\" , '\"' , ] rename_cols = [] replace_chars = {} for char in special_chars : for col in idf . columns : if char in col : rename_cols . append ( col ) if col in replace_chars . keys (): ( replace_chars [ col ]) . append ( char ) else : replace_chars [ col ] = [ char ] rename_mapping_to_new , rename_mapping_to_old = {}, {} idf_renamed = idf for col in rename_cols : new_col = col for char in replace_chars [ col ]: new_col = new_col . replace ( char , \"_\" ) rename_mapping_to_old [ new_col ] = col rename_mapping_to_new [ col ] = new_col idf_renamed = idf_renamed . withColumnRenamed ( col , new_col ) list_of_expr_ = [] for expr in list_of_expr : new_expr = expr for col in rename_cols : if col in expr : new_expr = new_expr . replace ( col , rename_mapping_to_new [ col ]) list_of_expr_ . append ( new_expr ) list_of_expr = list_of_expr_ odf = idf_renamed new_cols = [] for index , exp in enumerate ( list_of_expr ): odf = odf . withColumn ( \"f\" + str ( index ) + postfix , F . expr ( exp )) new_cols . append ( \"f\" + str ( index ) + postfix ) for new_col , col in rename_mapping_to_old . items (): odf = odf . withColumnRenamed ( new_col , col ) if print_impact : print ( \"Columns Added: \" , new_cols ) odf . select ( new_cols ) . describe () . show ( 5 , False ) return odf","title":"transformers"},{"location":"api/data_transformer/transformers.html#functions","text":"def IQR_standardization ( spark, idf, list_of_cols='all', drop_cols=[], pre_existing_model=False, model_path='NA', output_mode='replace', print_impact=False)","title":"Functions"},{"location":"api/drift_stability/_index.html","text":"Overview Sub-modules anovos.drift_stability.drift_detector anovos.drift_stability.stability anovos.drift_stability.validations","title":"Overview"},{"location":"api/drift_stability/_index.html#overview","text":"","title":"Overview"},{"location":"api/drift_stability/_index.html#sub-modules","text":"anovos.drift_stability.drift_detector anovos.drift_stability.stability anovos.drift_stability.validations","title":"Sub-modules"},{"location":"api/drift_stability/drift_detector.html","text":"drift_detector Expand source code import sys import operator import functools import pyspark from loguru import logger from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql.window import Window from anovos.data_ingest.data_sampling import data_sample from anovos.data_transformer.transformers import attribute_binning from anovos.shared.utils import attributeType_segregation from .validations import check_distance_method , check_list_of_columns @check_distance_method @check_list_of_columns def statistics ( spark , idf_target , idf_source , list_of_cols = \"all\" , drop_cols = None , method_type = \"PSI\" , bin_method = \"equal_range\" , bin_size = 10 , threshold = 0.1 , use_sampling = True , sample_method = \"random\" , strata_cols = \"all\" , stratified_type = \"population\" , sample_size = 100000 , sample_seed = 42 , persist = True , persist_option = pyspark . StorageLevel . MEMORY_AND_DISK , pre_existing_source = False , source_save = True , source_path = \"NA\" , model_directory = \"drift_statistics\" , print_impact = False , ): \"\"\" When the performance of a deployed machine learning model degrades in production, one potential reason is that the data used in training and prediction are not following the same distribution. Data drift mainly includes the following manifestations: - Covariate shift: training and test data follow different distributions. For example, An algorithm predicting income that is trained on younger population but tested on older population. - Prior probability shift: change of prior probability. For example in a spam classification problem, the proportion of spam emails changes from 0.2 in training data to 0.6 in testing data. - Concept shift: the distribution of the target variable changes given fixed input values. For example in the same spam classification problem, emails tagged as spam in training data are more likely to be tagged as non-spam in testing data. In our module, we mainly focus on covariate shift detection. In summary, given 2 datasets, source and target datasets, we would like to quantify the drift of some numerical attributes from source to target datasets. The whole process can be broken down into 2 steps: (1) convert each attribute of interest in source and target datasets into source and target probability distributions. (2) calculate the statistical distance between source and target distributions for each attribute. In the first step, attribute_binning is firstly performed to bin the numerical attributes of the source dataset, which requires two input variables: bin_method and bin_size. The same binning method is applied on the target dataset to align two results. The probability distributions are computed by dividing the frequency of each bin by the total frequency. In the second step, 4 choices of statistical metrics are provided to measure the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). They are calculated as below: For two discrete probability distributions *P=(p_1,\u2026,p_k)* and *Q=(q_1,\u2026,q_k),* ![https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/drift_stats_formulae.png](https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/drift_stats_formulae.png) A threshold can be set to flag out drifted attributes. If multiple statistical metrics have been calculated, an attribute will be marked as drifted if any of its statistical metric is larger than the threshold. This function can be used in many scenarios. For example: 1. Attribute level data drift can be analysed together with the attribute importance of a machine learning model. The more important an attribute is, the more attention it needs to be given if drift presents. 2. To analyse data drift over time, one can treat one dataset as the source / baseline dataset and multiple datasets as the target datasets. Drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. Parameters ---------- spark Spark Session idf_target Input Dataframe idf_source Baseline/Source Dataframe. This argument is ignored if pre_existing_source is True. list_of_cols List of columns to check drift e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = None) method_type \"PSI\", \"JSD\", \"HD\", \"KS\",\"all\". \"all\" can be passed to calculate all drift metrics. One or more methods can be passed in a form of list or string where different metrics are separated by pipe delimiter \u201c|\u201d e.g. [\"PSI\", \"JSD\"] or \"PSI|JSD\". (Default value = \"PSI\") bin_method String argument - \"equal_frequency\" or \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Integer argument - Number of bins for creating histogram. (Default value = 10) threshold Float argument - A column is flagged if any drift metric is above the threshold. (Default value = 0.1) use_sampling Boolean argument - True or False. This argument is used to determine whether to use random sample method on source and target dataset, True will enable the use of sample method, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) sample_method String argument - \"random\" or \"stratified\". If use_sampling is True, this argument is used to determine the sampling method. \"stratified\" for Stratified sampling, \"random\" for Random Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"random\") strata_cols If use_sampling is True and sample_method is \"stratified\", this argument is used to determine the list of columns used to be treated as strata. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"all\") stratified_type String argument - \"population\" or \"balanced\". If use_sampling is True and sample_method is \"stratified\", this string argument is used to determine the stratified sampling method. \"population\" stands for Proportionate Stratified Sampling, \"balanced\" stands for Optimum Stratified Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"population\") sample_size Integer argument - If use_sampling is True, this argument is used to determine the sample size of sampling method. (Default value = 100000) sample_seed Integer argument - If use_sampling is True, this argument is used to determine the seed of sampling method. (Default value = 42) persist Boolean argument - True or False. This argument is used to determine whether to persist on binning result of source and target dataset, True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) persist_option If persist is True, this argument is used to determine the type of persist. (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) pre_existing_source Boolean argument \u2013 True or False. True if the drift_statistics folder (binning model & frequency counts for each attribute) exists already, False Otherwise. (Default value = False) source_save Boolean argument - True or False. This argument will determine whether or not to save the source to source_path. (Default value = False) source_path If pre_existing_source is False, this argument can be used for saving the drift_statistics folder. The drift_statistics folder will have attribute_binning (binning model) & frequency_counts sub-folders. If pre_existing_source is True, this argument is path for referring the drift_statistics folder. Default \"NA\" for temporarily saving data in \"intermediate_data/\" folder. (Default value = \"NA\") model_directory If pre_existing_source is False, this argument can be used for saving the drift stats to folder. The default drift statics directory is drift_statistics folder will have attribute_binning If pre_existing_source is True, this argument is model_directory for referring the drift statistics dir. Default \"drift_statistics\" for temporarily saving source dataset attribute_binning folder. (Default value = \"drift_statistics\") print_impact Boolean argument - True or False. This argument is to print out the drift statistics of all attributes and attributes meeting the threshold. (Default value = False) Returns ------- DataFrame [attribute, *metric, flagged] Number of columns will be dependent on method argument. There will be one column for each drift method/metric. \"\"\" drop_cols = drop_cols or [] num_cols = attributeType_segregation ( idf_target . select ( list_of_cols ))[ 0 ] count_target = idf_target . count () count_source = idf_source . count () if use_sampling : if count_target > sample_size : idf_target = data_sample ( idf_target , strata_cols = strata_cols , fraction = sample_size / count_target , method_type = sample_method , stratified_type = stratified_type , seed_value = sample_seed , ) if persist : idf_target = idf_target . persist ( persist_option ) count_target = idf_target . count () if count_source > sample_size : idf_source = data_sample ( idf_source , strata_cols = strata_cols , fraction = sample_size / count_source , method_type = sample_method , stratified_type = stratified_type , seed_value = sample_seed , ) if persist : idf_source = idf_source . persist ( persist_option ) count_source = idf_source . count () if source_path == \"NA\" : source_path = \"intermediate_data\" if not pre_existing_source : source_bin = attribute_binning ( spark , idf_source , list_of_cols = num_cols , method_type = bin_method , bin_size = bin_size , pre_existing_model = False , model_path = source_path + \"/\" + model_directory , ) if persist : source_bin = source_bin . persist ( persist_option ) target_bin = attribute_binning ( spark , idf_target , list_of_cols = num_cols , method_type = bin_method , bin_size = bin_size , pre_existing_model = True , model_path = source_path + \"/\" + model_directory , ) if persist : target_bin = target_bin . persist ( persist_option ) temp_list = [] for i in list_of_cols : temp_method_join_list = [] if pre_existing_source : x = spark . read . csv ( source_path + \"/\" + model_directory + \"/frequency_counts/\" + i , header = True , inferSchema = True , ) else : x = ( source_bin . groupBy ( i ) . agg (( F . count ( i ) / count_source ) . alias ( \"p\" )) . fillna ( - 1 ) ) if source_save : x . coalesce ( 1 ) . write . csv ( source_path + \"/\" + model_directory + \"/frequency_counts/\" + i , header = True , mode = \"overwrite\" , ) y = target_bin . groupBy ( i ) . agg (( F . count ( i ) / count_target ) . alias ( \"q\" )) . fillna ( - 1 ) xy = ( x . join ( y , i , \"full_outer\" ) . fillna ( 0.0001 , subset = [ \"p\" , \"q\" ]) . replace ( 0 , 0.0001 ) . orderBy ( i ) ) if \"PSI\" in method_type : xy_psi = ( xy . withColumn ( \"deduct_ln_mul\" , (( F . col ( \"p\" ) - F . col ( \"q\" )) * ( F . log ( F . col ( \"p\" ) / F . col ( \"q\" )))), ) . select ( F . sum ( F . col ( \"deduct_ln_mul\" )) . alias ( \"PSI\" )) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"PSI\" ) ) temp_method_join_list . append ( xy_psi ) if \"HD\" in method_type : xy_hd = ( xy . withColumn ( \"pow\" , F . pow (( F . sqrt ( F . col ( \"p\" )) - F . sqrt ( F . col ( \"q\" ))), 2 ), ) . select ( F . sqrt ( F . sum ( F . col ( \"pow\" )) / 2 ) . alias ( \"HD\" )) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"HD\" ) ) temp_method_join_list . append ( xy_hd ) if \"JSD\" in method_type : xy_jsd = ( xy . withColumn ( \"m\" , (( F . col ( \"p\" ) + F . col ( \"q\" )) / 2 )) . withColumn ( \"log_pm\" , ( F . col ( \"p\" ) * F . log ( F . col ( \"p\" ) / F . col ( \"m\" )))) . withColumn ( \"log_qm\" , ( F . col ( \"q\" ) * F . log ( F . col ( \"q\" ) / F . col ( \"m\" )))) . select ( F . sum ( F . col ( \"log_pm\" )) . alias ( \"pm\" ), F . sum ( F . col ( \"log_qm\" )) . alias ( \"qm\" ), ) . select ((( F . col ( \"pm\" ) + F . col ( \"qm\" )) / 2 ) . alias ( \"JSD\" )) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"JSD\" ) ) temp_method_join_list . append ( xy_jsd ) if \"KS\" in method_type : xy_ks = ( xy . withColumn ( \"cum_sum_p\" , F . sum ( F . col ( \"p\" )) . over ( Window . partitionBy () . orderBy () . rowsBetween ( - sys . maxsize , 0 ) ), ) . withColumn ( \"cum_sum_q\" , F . sum ( F . col ( \"q\" )) . over ( Window . partitionBy () . orderBy () . rowsBetween ( - sys . maxsize , 0 ) ), ) . withColumn ( \"deduct_abs\" , F . abs ( F . col ( \"cum_sum_p\" ) - F . col ( \"cum_sum_q\" )) ) . select ( F . max ( F . col ( \"deduct_abs\" )) . alias ( \"KS\" ), ) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"KS\" ) ) temp_method_join_list . append ( xy_ks ) xy_temp = temp_method_join_list [ 0 ] if len ( temp_method_join_list ) > 1 : for count in range ( 1 , len ( temp_method_join_list )): xy_temp = xy_temp . join ( temp_method_join_list [ count ], \"attribute\" , \"inner\" ) temp_list . append ( xy_temp ) def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) odf_union = unionAll ( temp_list ) cond_expr = functools . reduce ( operator . or_ , [( F . col ( c ) > threshold ) for c in odf_union . columns [ 1 :]] ) odf = odf_union . withColumn ( \"flagged\" , F . when ( cond_expr , 1 ) . otherwise ( 0 )) if print_impact : logger . info ( \"All Attributes:\" ) odf . show ( len ( list_of_cols )) logger . info ( \"Attributes meeting Data Drift threshold:\" ) drift = odf . where ( F . col ( \"flagged\" ) == 1 ) drift . show ( drift . count ()) if persist : idf_target . unpersist () idf_source . unpersist () if not pre_existing_source : source_bin . unpersist () target_bin . unpersist () return odf Functions def statistics ( spark, idf_target, idf_source, list_of_cols='all', drop_cols=None, method_type='PSI', bin_method='equal_range', bin_size=10, threshold=0.1, use_sampling=True, sample_method='random', strata_cols='all', stratified_type='population', sample_size=100000, sample_seed=42, persist=True, persist_option=StorageLevel(True, True, False, False, 1), pre_existing_source=False, source_save=True, source_path='NA', model_directory='drift_statistics', print_impact=False) When the performance of a deployed machine learning model degrades in production, one potential reason is that the data used in training and prediction are not following the same distribution. Data drift mainly includes the following manifestations: Covariate shift: training and test data follow different distributions. For example, An algorithm predicting income that is trained on younger population but tested on older population. Prior probability shift: change of prior probability. For example in a spam classification problem, the proportion of spam emails changes from 0.2 in training data to 0.6 in testing data. Concept shift: the distribution of the target variable changes given fixed input values. For example in the same spam classification problem, emails tagged as spam in training data are more likely to be tagged as non-spam in testing data. In our module, we mainly focus on covariate shift detection. In summary, given 2 datasets, source and target datasets, we would like to quantify the drift of some numerical attributes from source to target datasets. The whole process can be broken down into 2 steps: (1) convert each attribute of interest in source and target datasets into source and target probability distributions. (2) calculate the statistical distance between source and target distributions for each attribute. In the first step, attribute_binning is firstly performed to bin the numerical attributes of the source dataset, which requires two input variables: bin_method and bin_size. The same binning method is applied on the target dataset to align two results. The probability distributions are computed by dividing the frequency of each bin by the total frequency. In the second step, 4 choices of statistical metrics are provided to measure the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). They are calculated as below: For two discrete probability distributions P=(p_1,\u2026,p_k) and Q=(q_1,\u2026,q_k), A threshold can be set to flag out drifted attributes. If multiple statistical metrics have been calculated, an attribute will be marked as drifted if any of its statistical metric is larger than the threshold. This function can be used in many scenarios. For example: Attribute level data drift can be analysed together with the attribute importance of a machine learning model. The more important an attribute is, the more attention it needs to be given if drift presents. To analyse data drift over time, one can treat one dataset as the source / baseline dataset and multiple datasets as the target datasets. Drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. Parameters spark Spark Session idf_target Input Dataframe idf_source Baseline/Source Dataframe. This argument is ignored if pre_existing_source is True. list_of_cols List of columns to check drift e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = None) method_type \"PSI\", \"JSD\", \"HD\", \"KS\",\"all\". \"all\" can be passed to calculate all drift metrics. One or more methods can be passed in a form of list or string where different metrics are separated by pipe delimiter \u201c|\u201d e.g. [\"PSI\", \"JSD\"] or \"PSI|JSD\". (Default value = \"PSI\") bin_method String argument - \"equal_frequency\" or \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Integer argument - Number of bins for creating histogram. (Default value = 10) threshold Float argument - A column is flagged if any drift metric is above the threshold. (Default value = 0.1) use_sampling Boolean argument - True or False. This argument is used to determine whether to use random sample method on source and target dataset, True will enable the use of sample method, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) sample_method String argument - \"random\" or \"stratified\". If use_sampling is True, this argument is used to determine the sampling method. \"stratified\" for Stratified sampling, \"random\" for Random Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"random\") strata_cols If use_sampling is True and sample_method is \"stratified\", this argument is used to determine the list of columns used to be treated as strata. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"all\") stratified_type String argument - \"population\" or \"balanced\". If use_sampling is True and sample_method is \"stratified\", this string argument is used to determine the stratified sampling method. \"population\" stands for Proportionate Stratified Sampling, \"balanced\" stands for Optimum Stratified Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"population\") sample_size Integer argument - If use_sampling is True, this argument is used to determine the sample size of sampling method. (Default value = 100000) sample_seed Integer argument - If use_sampling is True, this argument is used to determine the seed of sampling method. (Default value = 42) persist Boolean argument - True or False. This argument is used to determine whether to persist on binning result of source and target dataset, True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) persist_option If persist is True, this argument is used to determine the type of persist. (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) pre_existing_source Boolean argument \u2013 True or False. True if the drift_statistics folder (binning model & frequency counts for each attribute) exists already, False Otherwise. (Default value = False) source_save Boolean argument - True or False. This argument will determine whether or not to save the source to source_path. (Default value = False) source_path If pre_existing_source is False, this argument can be used for saving the drift_statistics folder. The drift_statistics folder will have attribute_binning (binning model) & frequency_counts sub-folders. If pre_existing_source is True, this argument is path for referring the drift_statistics folder. Default \"NA\" for temporarily saving data in \"intermediate_data/\" folder. (Default value = \"NA\") model_directory If pre_existing_source is False, this argument can be used for saving the drift stats to folder. The default drift statics directory is drift_statistics folder will have attribute_binning If pre_existing_source is True, this argument is model_directory for referring the drift statistics dir. Default \"drift_statistics\" for temporarily saving source dataset attribute_binning folder. (Default value = \"drift_statistics\") print_impact Boolean argument - True or False. This argument is to print out the drift statistics of all attributes and attributes meeting the threshold. (Default value = False) Returns DataFrame [attribute, *metric, flagged] Number of columns will be dependent on method argument. There will be one column for each drift method/metric. Expand source code @check_distance_method @check_list_of_columns def statistics ( spark , idf_target , idf_source , list_of_cols = \"all\" , drop_cols = None , method_type = \"PSI\" , bin_method = \"equal_range\" , bin_size = 10 , threshold = 0.1 , use_sampling = True , sample_method = \"random\" , strata_cols = \"all\" , stratified_type = \"population\" , sample_size = 100000 , sample_seed = 42 , persist = True , persist_option = pyspark . StorageLevel . MEMORY_AND_DISK , pre_existing_source = False , source_save = True , source_path = \"NA\" , model_directory = \"drift_statistics\" , print_impact = False , ): \"\"\" When the performance of a deployed machine learning model degrades in production, one potential reason is that the data used in training and prediction are not following the same distribution. Data drift mainly includes the following manifestations: - Covariate shift: training and test data follow different distributions. For example, An algorithm predicting income that is trained on younger population but tested on older population. - Prior probability shift: change of prior probability. For example in a spam classification problem, the proportion of spam emails changes from 0.2 in training data to 0.6 in testing data. - Concept shift: the distribution of the target variable changes given fixed input values. For example in the same spam classification problem, emails tagged as spam in training data are more likely to be tagged as non-spam in testing data. In our module, we mainly focus on covariate shift detection. In summary, given 2 datasets, source and target datasets, we would like to quantify the drift of some numerical attributes from source to target datasets. The whole process can be broken down into 2 steps: (1) convert each attribute of interest in source and target datasets into source and target probability distributions. (2) calculate the statistical distance between source and target distributions for each attribute. In the first step, attribute_binning is firstly performed to bin the numerical attributes of the source dataset, which requires two input variables: bin_method and bin_size. The same binning method is applied on the target dataset to align two results. The probability distributions are computed by dividing the frequency of each bin by the total frequency. In the second step, 4 choices of statistical metrics are provided to measure the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). They are calculated as below: For two discrete probability distributions *P=(p_1,\u2026,p_k)* and *Q=(q_1,\u2026,q_k),* ![https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/drift_stats_formulae.png](https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/drift_stats_formulae.png) A threshold can be set to flag out drifted attributes. If multiple statistical metrics have been calculated, an attribute will be marked as drifted if any of its statistical metric is larger than the threshold. This function can be used in many scenarios. For example: 1. Attribute level data drift can be analysed together with the attribute importance of a machine learning model. The more important an attribute is, the more attention it needs to be given if drift presents. 2. To analyse data drift over time, one can treat one dataset as the source / baseline dataset and multiple datasets as the target datasets. Drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. Parameters ---------- spark Spark Session idf_target Input Dataframe idf_source Baseline/Source Dataframe. This argument is ignored if pre_existing_source is True. list_of_cols List of columns to check drift e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = None) method_type \"PSI\", \"JSD\", \"HD\", \"KS\",\"all\". \"all\" can be passed to calculate all drift metrics. One or more methods can be passed in a form of list or string where different metrics are separated by pipe delimiter \u201c|\u201d e.g. [\"PSI\", \"JSD\"] or \"PSI|JSD\". (Default value = \"PSI\") bin_method String argument - \"equal_frequency\" or \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Integer argument - Number of bins for creating histogram. (Default value = 10) threshold Float argument - A column is flagged if any drift metric is above the threshold. (Default value = 0.1) use_sampling Boolean argument - True or False. This argument is used to determine whether to use random sample method on source and target dataset, True will enable the use of sample method, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) sample_method String argument - \"random\" or \"stratified\". If use_sampling is True, this argument is used to determine the sampling method. \"stratified\" for Stratified sampling, \"random\" for Random Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"random\") strata_cols If use_sampling is True and sample_method is \"stratified\", this argument is used to determine the list of columns used to be treated as strata. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"all\") stratified_type String argument - \"population\" or \"balanced\". If use_sampling is True and sample_method is \"stratified\", this string argument is used to determine the stratified sampling method. \"population\" stands for Proportionate Stratified Sampling, \"balanced\" stands for Optimum Stratified Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"population\") sample_size Integer argument - If use_sampling is True, this argument is used to determine the sample size of sampling method. (Default value = 100000) sample_seed Integer argument - If use_sampling is True, this argument is used to determine the seed of sampling method. (Default value = 42) persist Boolean argument - True or False. This argument is used to determine whether to persist on binning result of source and target dataset, True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) persist_option If persist is True, this argument is used to determine the type of persist. (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) pre_existing_source Boolean argument \u2013 True or False. True if the drift_statistics folder (binning model & frequency counts for each attribute) exists already, False Otherwise. (Default value = False) source_save Boolean argument - True or False. This argument will determine whether or not to save the source to source_path. (Default value = False) source_path If pre_existing_source is False, this argument can be used for saving the drift_statistics folder. The drift_statistics folder will have attribute_binning (binning model) & frequency_counts sub-folders. If pre_existing_source is True, this argument is path for referring the drift_statistics folder. Default \"NA\" for temporarily saving data in \"intermediate_data/\" folder. (Default value = \"NA\") model_directory If pre_existing_source is False, this argument can be used for saving the drift stats to folder. The default drift statics directory is drift_statistics folder will have attribute_binning If pre_existing_source is True, this argument is model_directory for referring the drift statistics dir. Default \"drift_statistics\" for temporarily saving source dataset attribute_binning folder. (Default value = \"drift_statistics\") print_impact Boolean argument - True or False. This argument is to print out the drift statistics of all attributes and attributes meeting the threshold. (Default value = False) Returns ------- DataFrame [attribute, *metric, flagged] Number of columns will be dependent on method argument. There will be one column for each drift method/metric. \"\"\" drop_cols = drop_cols or [] num_cols = attributeType_segregation ( idf_target . select ( list_of_cols ))[ 0 ] count_target = idf_target . count () count_source = idf_source . count () if use_sampling : if count_target > sample_size : idf_target = data_sample ( idf_target , strata_cols = strata_cols , fraction = sample_size / count_target , method_type = sample_method , stratified_type = stratified_type , seed_value = sample_seed , ) if persist : idf_target = idf_target . persist ( persist_option ) count_target = idf_target . count () if count_source > sample_size : idf_source = data_sample ( idf_source , strata_cols = strata_cols , fraction = sample_size / count_source , method_type = sample_method , stratified_type = stratified_type , seed_value = sample_seed , ) if persist : idf_source = idf_source . persist ( persist_option ) count_source = idf_source . count () if source_path == \"NA\" : source_path = \"intermediate_data\" if not pre_existing_source : source_bin = attribute_binning ( spark , idf_source , list_of_cols = num_cols , method_type = bin_method , bin_size = bin_size , pre_existing_model = False , model_path = source_path + \"/\" + model_directory , ) if persist : source_bin = source_bin . persist ( persist_option ) target_bin = attribute_binning ( spark , idf_target , list_of_cols = num_cols , method_type = bin_method , bin_size = bin_size , pre_existing_model = True , model_path = source_path + \"/\" + model_directory , ) if persist : target_bin = target_bin . persist ( persist_option ) temp_list = [] for i in list_of_cols : temp_method_join_list = [] if pre_existing_source : x = spark . read . csv ( source_path + \"/\" + model_directory + \"/frequency_counts/\" + i , header = True , inferSchema = True , ) else : x = ( source_bin . groupBy ( i ) . agg (( F . count ( i ) / count_source ) . alias ( \"p\" )) . fillna ( - 1 ) ) if source_save : x . coalesce ( 1 ) . write . csv ( source_path + \"/\" + model_directory + \"/frequency_counts/\" + i , header = True , mode = \"overwrite\" , ) y = target_bin . groupBy ( i ) . agg (( F . count ( i ) / count_target ) . alias ( \"q\" )) . fillna ( - 1 ) xy = ( x . join ( y , i , \"full_outer\" ) . fillna ( 0.0001 , subset = [ \"p\" , \"q\" ]) . replace ( 0 , 0.0001 ) . orderBy ( i ) ) if \"PSI\" in method_type : xy_psi = ( xy . withColumn ( \"deduct_ln_mul\" , (( F . col ( \"p\" ) - F . col ( \"q\" )) * ( F . log ( F . col ( \"p\" ) / F . col ( \"q\" )))), ) . select ( F . sum ( F . col ( \"deduct_ln_mul\" )) . alias ( \"PSI\" )) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"PSI\" ) ) temp_method_join_list . append ( xy_psi ) if \"HD\" in method_type : xy_hd = ( xy . withColumn ( \"pow\" , F . pow (( F . sqrt ( F . col ( \"p\" )) - F . sqrt ( F . col ( \"q\" ))), 2 ), ) . select ( F . sqrt ( F . sum ( F . col ( \"pow\" )) / 2 ) . alias ( \"HD\" )) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"HD\" ) ) temp_method_join_list . append ( xy_hd ) if \"JSD\" in method_type : xy_jsd = ( xy . withColumn ( \"m\" , (( F . col ( \"p\" ) + F . col ( \"q\" )) / 2 )) . withColumn ( \"log_pm\" , ( F . col ( \"p\" ) * F . log ( F . col ( \"p\" ) / F . col ( \"m\" )))) . withColumn ( \"log_qm\" , ( F . col ( \"q\" ) * F . log ( F . col ( \"q\" ) / F . col ( \"m\" )))) . select ( F . sum ( F . col ( \"log_pm\" )) . alias ( \"pm\" ), F . sum ( F . col ( \"log_qm\" )) . alias ( \"qm\" ), ) . select ((( F . col ( \"pm\" ) + F . col ( \"qm\" )) / 2 ) . alias ( \"JSD\" )) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"JSD\" ) ) temp_method_join_list . append ( xy_jsd ) if \"KS\" in method_type : xy_ks = ( xy . withColumn ( \"cum_sum_p\" , F . sum ( F . col ( \"p\" )) . over ( Window . partitionBy () . orderBy () . rowsBetween ( - sys . maxsize , 0 ) ), ) . withColumn ( \"cum_sum_q\" , F . sum ( F . col ( \"q\" )) . over ( Window . partitionBy () . orderBy () . rowsBetween ( - sys . maxsize , 0 ) ), ) . withColumn ( \"deduct_abs\" , F . abs ( F . col ( \"cum_sum_p\" ) - F . col ( \"cum_sum_q\" )) ) . select ( F . max ( F . col ( \"deduct_abs\" )) . alias ( \"KS\" ), ) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"KS\" ) ) temp_method_join_list . append ( xy_ks ) xy_temp = temp_method_join_list [ 0 ] if len ( temp_method_join_list ) > 1 : for count in range ( 1 , len ( temp_method_join_list )): xy_temp = xy_temp . join ( temp_method_join_list [ count ], \"attribute\" , \"inner\" ) temp_list . append ( xy_temp ) def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) odf_union = unionAll ( temp_list ) cond_expr = functools . reduce ( operator . or_ , [( F . col ( c ) > threshold ) for c in odf_union . columns [ 1 :]] ) odf = odf_union . withColumn ( \"flagged\" , F . when ( cond_expr , 1 ) . otherwise ( 0 )) if print_impact : logger . info ( \"All Attributes:\" ) odf . show ( len ( list_of_cols )) logger . info ( \"Attributes meeting Data Drift threshold:\" ) drift = odf . where ( F . col ( \"flagged\" ) == 1 ) drift . show ( drift . count ()) if persist : idf_target . unpersist () idf_source . unpersist () if not pre_existing_source : source_bin . unpersist () target_bin . unpersist () return odf","title":"<code>drift_detector</code>"},{"location":"api/drift_stability/drift_detector.html#drift_detector","text":"Expand source code import sys import operator import functools import pyspark from loguru import logger from pyspark.sql import functions as F from pyspark.sql import types as T from pyspark.sql.window import Window from anovos.data_ingest.data_sampling import data_sample from anovos.data_transformer.transformers import attribute_binning from anovos.shared.utils import attributeType_segregation from .validations import check_distance_method , check_list_of_columns @check_distance_method @check_list_of_columns def statistics ( spark , idf_target , idf_source , list_of_cols = \"all\" , drop_cols = None , method_type = \"PSI\" , bin_method = \"equal_range\" , bin_size = 10 , threshold = 0.1 , use_sampling = True , sample_method = \"random\" , strata_cols = \"all\" , stratified_type = \"population\" , sample_size = 100000 , sample_seed = 42 , persist = True , persist_option = pyspark . StorageLevel . MEMORY_AND_DISK , pre_existing_source = False , source_save = True , source_path = \"NA\" , model_directory = \"drift_statistics\" , print_impact = False , ): \"\"\" When the performance of a deployed machine learning model degrades in production, one potential reason is that the data used in training and prediction are not following the same distribution. Data drift mainly includes the following manifestations: - Covariate shift: training and test data follow different distributions. For example, An algorithm predicting income that is trained on younger population but tested on older population. - Prior probability shift: change of prior probability. For example in a spam classification problem, the proportion of spam emails changes from 0.2 in training data to 0.6 in testing data. - Concept shift: the distribution of the target variable changes given fixed input values. For example in the same spam classification problem, emails tagged as spam in training data are more likely to be tagged as non-spam in testing data. In our module, we mainly focus on covariate shift detection. In summary, given 2 datasets, source and target datasets, we would like to quantify the drift of some numerical attributes from source to target datasets. The whole process can be broken down into 2 steps: (1) convert each attribute of interest in source and target datasets into source and target probability distributions. (2) calculate the statistical distance between source and target distributions for each attribute. In the first step, attribute_binning is firstly performed to bin the numerical attributes of the source dataset, which requires two input variables: bin_method and bin_size. The same binning method is applied on the target dataset to align two results. The probability distributions are computed by dividing the frequency of each bin by the total frequency. In the second step, 4 choices of statistical metrics are provided to measure the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). They are calculated as below: For two discrete probability distributions *P=(p_1,\u2026,p_k)* and *Q=(q_1,\u2026,q_k),* ![https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/drift_stats_formulae.png](https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/drift_stats_formulae.png) A threshold can be set to flag out drifted attributes. If multiple statistical metrics have been calculated, an attribute will be marked as drifted if any of its statistical metric is larger than the threshold. This function can be used in many scenarios. For example: 1. Attribute level data drift can be analysed together with the attribute importance of a machine learning model. The more important an attribute is, the more attention it needs to be given if drift presents. 2. To analyse data drift over time, one can treat one dataset as the source / baseline dataset and multiple datasets as the target datasets. Drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. Parameters ---------- spark Spark Session idf_target Input Dataframe idf_source Baseline/Source Dataframe. This argument is ignored if pre_existing_source is True. list_of_cols List of columns to check drift e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all (non-array) columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = None) method_type \"PSI\", \"JSD\", \"HD\", \"KS\",\"all\". \"all\" can be passed to calculate all drift metrics. One or more methods can be passed in a form of list or string where different metrics are separated by pipe delimiter \u201c|\u201d e.g. [\"PSI\", \"JSD\"] or \"PSI|JSD\". (Default value = \"PSI\") bin_method String argument - \"equal_frequency\" or \"equal_range\". In \"equal_range\" method, each bin is of equal size/width and in \"equal_frequency\", each bin has equal no. of rows, though the width of bins may vary. (Default value = \"equal_range\") bin_size Integer argument - Number of bins for creating histogram. (Default value = 10) threshold Float argument - A column is flagged if any drift metric is above the threshold. (Default value = 0.1) use_sampling Boolean argument - True or False. This argument is used to determine whether to use random sample method on source and target dataset, True will enable the use of sample method, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) sample_method String argument - \"random\" or \"stratified\". If use_sampling is True, this argument is used to determine the sampling method. \"stratified\" for Stratified sampling, \"random\" for Random Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"random\") strata_cols If use_sampling is True and sample_method is \"stratified\", this argument is used to determine the list of columns used to be treated as strata. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"all\") stratified_type String argument - \"population\" or \"balanced\". If use_sampling is True and sample_method is \"stratified\", this string argument is used to determine the stratified sampling method. \"population\" stands for Proportionate Stratified Sampling, \"balanced\" stands for Optimum Stratified Sampling. For more details, please refer to https://docs.anovos.ai/api/data_ingest/data_sampling.html. (Default value = \"population\") sample_size Integer argument - If use_sampling is True, this argument is used to determine the sample size of sampling method. (Default value = 100000) sample_seed Integer argument - If use_sampling is True, this argument is used to determine the seed of sampling method. (Default value = 42) persist Boolean argument - True or False. This argument is used to determine whether to persist on binning result of source and target dataset, True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) persist_option If persist is True, this argument is used to determine the type of persist. (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) pre_existing_source Boolean argument \u2013 True or False. True if the drift_statistics folder (binning model & frequency counts for each attribute) exists already, False Otherwise. (Default value = False) source_save Boolean argument - True or False. This argument will determine whether or not to save the source to source_path. (Default value = False) source_path If pre_existing_source is False, this argument can be used for saving the drift_statistics folder. The drift_statistics folder will have attribute_binning (binning model) & frequency_counts sub-folders. If pre_existing_source is True, this argument is path for referring the drift_statistics folder. Default \"NA\" for temporarily saving data in \"intermediate_data/\" folder. (Default value = \"NA\") model_directory If pre_existing_source is False, this argument can be used for saving the drift stats to folder. The default drift statics directory is drift_statistics folder will have attribute_binning If pre_existing_source is True, this argument is model_directory for referring the drift statistics dir. Default \"drift_statistics\" for temporarily saving source dataset attribute_binning folder. (Default value = \"drift_statistics\") print_impact Boolean argument - True or False. This argument is to print out the drift statistics of all attributes and attributes meeting the threshold. (Default value = False) Returns ------- DataFrame [attribute, *metric, flagged] Number of columns will be dependent on method argument. There will be one column for each drift method/metric. \"\"\" drop_cols = drop_cols or [] num_cols = attributeType_segregation ( idf_target . select ( list_of_cols ))[ 0 ] count_target = idf_target . count () count_source = idf_source . count () if use_sampling : if count_target > sample_size : idf_target = data_sample ( idf_target , strata_cols = strata_cols , fraction = sample_size / count_target , method_type = sample_method , stratified_type = stratified_type , seed_value = sample_seed , ) if persist : idf_target = idf_target . persist ( persist_option ) count_target = idf_target . count () if count_source > sample_size : idf_source = data_sample ( idf_source , strata_cols = strata_cols , fraction = sample_size / count_source , method_type = sample_method , stratified_type = stratified_type , seed_value = sample_seed , ) if persist : idf_source = idf_source . persist ( persist_option ) count_source = idf_source . count () if source_path == \"NA\" : source_path = \"intermediate_data\" if not pre_existing_source : source_bin = attribute_binning ( spark , idf_source , list_of_cols = num_cols , method_type = bin_method , bin_size = bin_size , pre_existing_model = False , model_path = source_path + \"/\" + model_directory , ) if persist : source_bin = source_bin . persist ( persist_option ) target_bin = attribute_binning ( spark , idf_target , list_of_cols = num_cols , method_type = bin_method , bin_size = bin_size , pre_existing_model = True , model_path = source_path + \"/\" + model_directory , ) if persist : target_bin = target_bin . persist ( persist_option ) temp_list = [] for i in list_of_cols : temp_method_join_list = [] if pre_existing_source : x = spark . read . csv ( source_path + \"/\" + model_directory + \"/frequency_counts/\" + i , header = True , inferSchema = True , ) else : x = ( source_bin . groupBy ( i ) . agg (( F . count ( i ) / count_source ) . alias ( \"p\" )) . fillna ( - 1 ) ) if source_save : x . coalesce ( 1 ) . write . csv ( source_path + \"/\" + model_directory + \"/frequency_counts/\" + i , header = True , mode = \"overwrite\" , ) y = target_bin . groupBy ( i ) . agg (( F . count ( i ) / count_target ) . alias ( \"q\" )) . fillna ( - 1 ) xy = ( x . join ( y , i , \"full_outer\" ) . fillna ( 0.0001 , subset = [ \"p\" , \"q\" ]) . replace ( 0 , 0.0001 ) . orderBy ( i ) ) if \"PSI\" in method_type : xy_psi = ( xy . withColumn ( \"deduct_ln_mul\" , (( F . col ( \"p\" ) - F . col ( \"q\" )) * ( F . log ( F . col ( \"p\" ) / F . col ( \"q\" )))), ) . select ( F . sum ( F . col ( \"deduct_ln_mul\" )) . alias ( \"PSI\" )) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"PSI\" ) ) temp_method_join_list . append ( xy_psi ) if \"HD\" in method_type : xy_hd = ( xy . withColumn ( \"pow\" , F . pow (( F . sqrt ( F . col ( \"p\" )) - F . sqrt ( F . col ( \"q\" ))), 2 ), ) . select ( F . sqrt ( F . sum ( F . col ( \"pow\" )) / 2 ) . alias ( \"HD\" )) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"HD\" ) ) temp_method_join_list . append ( xy_hd ) if \"JSD\" in method_type : xy_jsd = ( xy . withColumn ( \"m\" , (( F . col ( \"p\" ) + F . col ( \"q\" )) / 2 )) . withColumn ( \"log_pm\" , ( F . col ( \"p\" ) * F . log ( F . col ( \"p\" ) / F . col ( \"m\" )))) . withColumn ( \"log_qm\" , ( F . col ( \"q\" ) * F . log ( F . col ( \"q\" ) / F . col ( \"m\" )))) . select ( F . sum ( F . col ( \"log_pm\" )) . alias ( \"pm\" ), F . sum ( F . col ( \"log_qm\" )) . alias ( \"qm\" ), ) . select ((( F . col ( \"pm\" ) + F . col ( \"qm\" )) / 2 ) . alias ( \"JSD\" )) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"JSD\" ) ) temp_method_join_list . append ( xy_jsd ) if \"KS\" in method_type : xy_ks = ( xy . withColumn ( \"cum_sum_p\" , F . sum ( F . col ( \"p\" )) . over ( Window . partitionBy () . orderBy () . rowsBetween ( - sys . maxsize , 0 ) ), ) . withColumn ( \"cum_sum_q\" , F . sum ( F . col ( \"q\" )) . over ( Window . partitionBy () . orderBy () . rowsBetween ( - sys . maxsize , 0 ) ), ) . withColumn ( \"deduct_abs\" , F . abs ( F . col ( \"cum_sum_p\" ) - F . col ( \"cum_sum_q\" )) ) . select ( F . max ( F . col ( \"deduct_abs\" )) . alias ( \"KS\" ), ) . withColumn ( \"attribute\" , F . lit ( str ( i ))) . select ( \"attribute\" , \"KS\" ) ) temp_method_join_list . append ( xy_ks ) xy_temp = temp_method_join_list [ 0 ] if len ( temp_method_join_list ) > 1 : for count in range ( 1 , len ( temp_method_join_list )): xy_temp = xy_temp . join ( temp_method_join_list [ count ], \"attribute\" , \"inner\" ) temp_list . append ( xy_temp ) def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) odf_union = unionAll ( temp_list ) cond_expr = functools . reduce ( operator . or_ , [( F . col ( c ) > threshold ) for c in odf_union . columns [ 1 :]] ) odf = odf_union . withColumn ( \"flagged\" , F . when ( cond_expr , 1 ) . otherwise ( 0 )) if print_impact : logger . info ( \"All Attributes:\" ) odf . show ( len ( list_of_cols )) logger . info ( \"Attributes meeting Data Drift threshold:\" ) drift = odf . where ( F . col ( \"flagged\" ) == 1 ) drift . show ( drift . count ()) if persist : idf_target . unpersist () idf_source . unpersist () if not pre_existing_source : source_bin . unpersist () target_bin . unpersist () return odf","title":"drift_detector"},{"location":"api/drift_stability/drift_detector.html#functions","text":"def statistics ( spark, idf_target, idf_source, list_of_cols='all', drop_cols=None, method_type='PSI', bin_method='equal_range', bin_size=10, threshold=0.1, use_sampling=True, sample_method='random', strata_cols='all', stratified_type='population', sample_size=100000, sample_seed=42, persist=True, persist_option=StorageLevel(True, True, False, False, 1), pre_existing_source=False, source_save=True, source_path='NA', model_directory='drift_statistics', print_impact=False) When the performance of a deployed machine learning model degrades in production, one potential reason is that the data used in training and prediction are not following the same distribution. Data drift mainly includes the following manifestations: Covariate shift: training and test data follow different distributions. For example, An algorithm predicting income that is trained on younger population but tested on older population. Prior probability shift: change of prior probability. For example in a spam classification problem, the proportion of spam emails changes from 0.2 in training data to 0.6 in testing data. Concept shift: the distribution of the target variable changes given fixed input values. For example in the same spam classification problem, emails tagged as spam in training data are more likely to be tagged as non-spam in testing data. In our module, we mainly focus on covariate shift detection. In summary, given 2 datasets, source and target datasets, we would like to quantify the drift of some numerical attributes from source to target datasets. The whole process can be broken down into 2 steps: (1) convert each attribute of interest in source and target datasets into source and target probability distributions. (2) calculate the statistical distance between source and target distributions for each attribute. In the first step, attribute_binning is firstly performed to bin the numerical attributes of the source dataset, which requires two input variables: bin_method and bin_size. The same binning method is applied on the target dataset to align two results. The probability distributions are computed by dividing the frequency of each bin by the total frequency. In the second step, 4 choices of statistical metrics are provided to measure the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). They are calculated as below: For two discrete probability distributions P=(p_1,\u2026,p_k) and Q=(q_1,\u2026,q_k), A threshold can be set to flag out drifted attributes. If multiple statistical metrics have been calculated, an attribute will be marked as drifted if any of its statistical metric is larger than the threshold. This function can be used in many scenarios. For example: Attribute level data drift can be analysed together with the attribute importance of a machine learning model. The more important an attribute is, the more attention it needs to be given if drift presents. To analyse data drift over time, one can treat one dataset as the source / baseline dataset and multiple datasets as the target datasets. Drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time.","title":"Functions"},{"location":"api/drift_stability/stability.html","text":"stability Expand source code import numpy as np import pyspark import sympy as sp from loguru import logger from pyspark.sql import DataFrame from pyspark.sql import functions as F from pyspark.sql import types as T from scipy.stats import variation from anovos.data_transformer.transformers import attribute_binning from anovos.shared.utils import attributeType_segregation from .validations import compute_si , check_metric_weightages , check_threshold def stability_index_computation ( spark , idfs , list_of_cols = \"all\" , drop_cols = [], metric_weightages = { \"mean\" : 0.5 , \"stddev\" : 0.3 , \"kurtosis\" : 0.2 }, binary_cols = [], existing_metric_path = \"\" , appended_metric_path = \"\" , persist : bool = True , persist_option = pyspark . StorageLevel . MEMORY_AND_DISK , threshold = 1 , print_impact = False , ): \"\"\" The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 6 datasets collected in 6 consecutive time periods (D1, D2, \u2026, D6), data stability index of an attribute measures how stable the attribute is from D1 to D6. The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. In summary, given N datasets representing different time periods, we would like to measure the stability of some numerical attributes from the first to the N-th dataset. The whole process can be broken down into 2 steps: (1) Choose a few statistical metrics to describe the distribution of each attribute at each time period. (2) Compute attribute level stability by combining the stability of each statistical metric over time periods. In the first step, we choose mean, standard deviation and kurtosis as the statistical metrics in our implementation. Intuitively, they represent different aspects of a distribution: mean measures central tendency, standard deviation measures dispersion and kurtosis measures shape of a distribution. Reasons of selecting those 3 metrics will be explained in a later section. With mean, standard deviation and kurtosis computed for each attribute at each time interval, we can form 3 arrays of size N for each attribute. In the second step, Coefficient of Variation (CV) is used to measure the stability of each metric. CV represents the ratio of the standard deviation to the mean, which is a unitless statistic to compare the relative variation from one array to another. Considering the wide potential range of CV, the absolute value of CV is then mapped to an integer between 0 and 4 according to the table below, where 0 indicates highly unstable and 4 indicates highly stable. We call this integer a metric stability index. | abs(CV) Interval | Metric Stability Index | |------------------|------------------------| | [0, 0.03) | 4 | | [0.03, 0.1) | 3 | | [0.1, 0.2) | 2 | | [0.2, 0.5) | 1 | | [0.5, +inf) | 0 | Finally, the attribute stability index (SI) is a weighted sum of 3 metric stability indexes, where we assign 50% for mean, 30% for standard deviation and 20% for kurtosis by default. The final output is a float between 0 and 4 and an attribute can be classified as one of the following categories: very unstable (0\u2264SI<1), unstable (1\u2264SI<2), marginally stable (2\u2264SI<3), stable (3\u2264SI<3.5) and very stable (3.5\u2264SI\u22644). For example, there are 6 samples of attribute X from T1 to T6. For each sample, we have computed the statistical metrics of X from T1 to T6: | idx | Mean | Standard deviation | Kurtosis | |-----|------|--------------------|----------| | 1 | 11 | 2 | 3.9 | | 2 | 12 | 1 | 4.2 | | 3 | 15 | 3 | 4.0 | | 4 | 10 | 2 | 4.1 | | 5 | 11 | 1 | 4.2 | | 6 | 13 | 0.5 | 4.0 | Then we calculate the Coefficient of Variation for each array: - CV of mean = CV([11, 12, 15, 10, 11, 13]) = 0.136 - CV of standard deviation = CV([2, 1, 3, 2, 1, 0.5]) = 0.529 - CV of kurtosis = CV([3.9, 4.2, 4.0, 4.1, 4.2, 4.0]) = 0.027 Metric stability indexes are then computed by mapping each CV value to an integer accordingly. As a result, metric stability index is 2 for mean, 0 for standard deviation and 4 for kurtosis. Why mean is chosen over median? - Dummy variables which take only the value 0 or 1 are frequently seen in machine learning features. Mean of a dummy variable represents the proportion of value 1 and median of a dummy variable is either 0 or 1 whichever is more frequent. However, CV may not work well when 0 appears in the array or the array contains both positive and negative values. For example, intuitively [0,0,0,0,0,1,0,0,0] is a stable array but its CV is 2.83 which is extremely high, but cv of [0.45,0.44,0.48,0.49,0.42,0.52,0.49,0.47,0.48] is 0.06 which is much more reasonable. Thus we decided to use mean instead of median. Although median is considered as a more robust choice, outlier treatment can be applied prior to data stability analysis to handle this issue. Why kurtosis is chosen over skewness? - Kurtosis is a positive value (note that we use kurtosis instead of excess kurtosis which) but skewness can range from \u2013inf to +inf. Usually, if skewness is between -0.5 and 0.5, the distribution is approximately symmetric. Thus, if the skewness fluctuates around 0, the CV is highly likely to be high or invalid because the mean will be close to 0. Stability index is preferred in the following scenario: - Pairwise drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. However this can be time-consuming especially when the number of target dataset is large. In this case, measuring data stability instead of data drift would be a much faster alternative and the source/baseline dataset is not required as well Troubleshooting - If the attribute stability index appears to be nan, it may due to one of the following reasons: - One metric ( likely to be kurtosis) is nan. For example, the kurtosis of a sample is nan If its standard deviation is 0. - The mean of a metric from the first to the N-th dataset is zero, causing the denominator of CV to be 0. For example, when mean of attribute X is always zero for all datasets, its stability index would be nan. Limitation - Limitation of CV: CV may not work well when 0 appears in the array or the array contains both positive and negative values. Parameters ---------- spark Spark Session idfs Variable number of input dataframes list_of_cols List of numerical columns to check stability e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = []) metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) binary_cols List of numerical columns to be treated as binary columns e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d. For the specified binary columns, only the mean value will be used as the statistical metric (standard deviation and kurtosis will be skipped). In addition, standard deviation will be used to measure the stability of mean instead of CV. (Default value = []) existing_metric_path This argument is path for referring pre-existing metrics of historical datasets and is of schema [idx, attribute, mean, stdev, kurtosis]. idx is index number of historical datasets assigned in chronological order. (Default value = \"\") appended_metric_path This argument is path for saving input dataframes metrics after appending to the historical datasets' metrics. (Default value = \"\") persist Boolean argument - True or False. This argument is used to determine whether to persist on binning result of source and target dataset, True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) persist_option If persist is True, this argument is used to determine the type of persist. (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) threshold A column is flagged if the stability index is below the threshold, which varies between 0 to 4. The following criteria can be used to classifiy stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all attributes and potential unstable attributes. Returns ------- DataFrame [attribute, mean_stddev, mean_si, stddev_si, kurtosis_si, mean_cv, stddev_cv, kurtosis_cv, stability_index]. *_cv is coefficient of variation for each metric. *_si is stability index for each metric. stability_index is net weighted stability index based on the individual metrics' stability index. \"\"\" num_cols = attributeType_segregation ( idfs [ 0 ])[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if isinstance ( binary_cols , str ): binary_cols = [ x . strip () for x in binary_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if any ( x not in list_of_cols for x in binary_cols ): raise TypeError ( \"Invalid input for Binary Column(s)\" ) check_metric_weightages ( metric_weightages ) check_threshold ( threshold ) if existing_metric_path : existing_metric_df = spark . read . csv ( existing_metric_path , header = True , inferSchema = True ) dfs_count = int ( existing_metric_df . select ( F . max ( F . col ( \"idx\" ))) . first ()[ 0 ]) + 1 else : existing_metric_df = None dfs_count = 1 def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) if persist : for i in range ( len ( idfs )): idfs [ i ] = idfs [ i ] . select ( list_of_cols ) idfs [ i ] . persist ( persist_option ) list_temp_all_col = [] if appended_metric_path : list_append_all = [] for i in list_of_cols : if i in binary_cols : col_type = \"Binary\" else : col_type = \"Numerical\" count_idf = dfs_count list_temp_col_in_idf = [] for idf in idfs : df_stat_each = idf . select ( F . mean ( i ) . alias ( \"mean\" ), F . stddev ( i ) . alias ( \"stddev\" ), ( F . kurtosis ( i ) + F . lit ( 3 )) . alias ( \"kurtosis\" ), ) list_temp_col_in_idf . append ( df_stat_each ) if appended_metric_path : df_append_single = df_stat_each . select ( F . lit ( str ( count_idf )) . alias ( \"idx\" ), F . lit ( str ( i )) . alias ( \"attribute\" ), F . lit ( col_type ) . alias ( \"type\" ), \"mean\" , \"stddev\" , \"kurtosis\" , ) list_append_all . append ( df_append_single ) count_idf += 1 if existing_metric_df : existing_df_for_single_col = existing_metric_df . where ( F . col ( \"attribute\" ) == str ( i ) ) . select ( \"mean\" , \"stddev\" , \"kurtosis\" ) if existing_df_for_single_col . count () > 0 : list_temp_col_in_idf . append ( existing_df_for_single_col ) df_stat_col = ( unionAll ( list_temp_col_in_idf ) . select ( F . stddev ( \"mean\" ) . alias ( \"std_of_mean\" ), F . mean ( \"mean\" ) . alias ( \"mean_of_mean\" ), F . stddev ( \"stddev\" ) . alias ( \"std_of_stddev\" ), F . mean ( \"stddev\" ) . alias ( \"mean_of_stddev\" ), F . stddev ( \"kurtosis\" ) . alias ( \"std_of_kurtosis\" ), F . mean ( \"kurtosis\" ) . alias ( \"mean_of_kurtosis\" ), ) . select ( F . lit ( str ( i )) . alias ( \"attribute\" ), F . lit ( col_type ) . alias ( \"type\" ), F . col ( \"std_of_mean\" ) . alias ( \"mean_stddev\" ), ( F . col ( \"std_of_mean\" ) / F . col ( \"mean_of_mean\" )) . alias ( \"mean_cv\" ), ( F . col ( \"std_of_stddev\" ) / F . col ( \"mean_of_stddev\" )) . alias ( \"stddev_cv\" ), ( F . col ( \"std_of_kurtosis\" ) / F . col ( \"mean_of_kurtosis\" )) . alias ( \"kurtosis_cv\" ), ) ) list_temp_all_col . append ( df_stat_col ) odf = unionAll ( list_temp_all_col ) if appended_metric_path : if existing_metric_df : list_append_all . append ( existing_metric_df ) df_append = unionAll ( list_append_all ) . orderBy ( F . col ( \"idx\" )) df_append . coalesce ( 1 ) . write . csv ( appended_metric_path , header = True , mode = \"overwrite\" ) f_compute_si = F . udf ( compute_si ( metric_weightages ), T . ArrayType ( T . FloatType ())) odf = ( odf . replace ( np . nan , None ) . withColumn ( \"si_array\" , f_compute_si ( \"type\" , \"mean_stddev\" , \"mean_cv\" , \"stddev_cv\" , \"kurtosis_cv\" ), ) . withColumn ( \"mean_si\" , F . col ( \"si_array\" ) . getItem ( 0 )) . withColumn ( \"stddev_si\" , F . col ( \"si_array\" ) . getItem ( 1 )) . withColumn ( \"kurtosis_si\" , F . col ( \"si_array\" ) . getItem ( 2 )) . withColumn ( \"stability_index\" , F . col ( \"si_array\" ) . getItem ( 3 )) . withColumn ( \"flagged\" , F . when ( ( F . col ( \"stability_index\" ) < threshold ) | ( F . col ( \"stability_index\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) . withColumn ( \"mean_stddev\" , F . round ( F . col ( \"mean_stddev\" ), 4 )) . withColumn ( \"mean_cv\" , F . round ( F . col ( \"mean_cv\" ), 4 )) . withColumn ( \"stddev_cv\" , F . round ( F . col ( \"stddev_cv\" ), 4 )) . withColumn ( \"kurtosis_cv\" , F . round ( F . col ( \"kurtosis_cv\" ), 4 )) . drop ( \"si_array\" ) ) if print_impact : logger . info ( \"All Attributes:\" ) odf . show ( len ( list_of_cols )) logger . info ( \"Potential Unstable Attributes:\" ) unstable = odf . where ( F . col ( \"flagged\" ) == 1 ) unstable . show ( unstable . count ()) if persist : for i in range ( len ( idfs )): idfs [ i ] . unpersist () return odf def feature_stability_estimation ( spark , attribute_stats , attribute_transformation , metric_weightages = { \"mean\" : 0.5 , \"stddev\" : 0.3 , \"kurtosis\" : 0.2 }, threshold = 1 , print_impact = False , ): \"\"\" This function is able to estimate the stability index of a new feature composed of certain attributes whose stability metrics are known. For example, the new feature F can be expressed as F = g(X1, X2, \u2026, Xn), where X1, X2, \u2026, Xn represent different attributes and g represents the transformation function. The most straightforward way is to generate the new feature for all periods and calculate its stability index. However, it requires reading all historical data again which can be unrealistic for large datasets. Thus, the objective of this function is to estimate feature stability index without reading historical data. One example can be the following scenario: we have attributes A and B, we have their respective stability statistics from T1 to T7. At T7 we realise we need to generate a new feature: A/B, but we don\u2019t have statistics metrics of A/B from T1 to T6 and this is where this function can be applied to generate an estimation without reading datasets from T1 to T6. The estimation can be broken down into 3 steps. 1. Estimate mean and stddev for the new feature based on attribute metrics (no existing resource found to estimate Feature kurtosis). Estimated mean and stddev are generated for each time period using the formula below according to [1]: ![https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/feature_stability_formulae.png](https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/feature_stability_formulae.png) 2. Calculate Coefficient of variation (CV) for estimated feature mean and stddev. Each CV can be then mapped to an integer between 0 and 4 to generate the metric stability index. 3. Similar to the attribute stability index, each metric is assigned a weightage between 0 and 1, where the default values are 50 for mean, 30% for standard deviation and 20% for kurtosis. Because we are unable to generate kurtosis stability index, its minimum and maximum possible values (0 and 4) are used to output a range for global stability index (GSI): * Lower bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfce * Upper bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfd2 [1] Benaroya, H., Han, S. M., & Nagurka, M. (2005). Probability models in engineering and science (Vol. 192, pp. 168-169). CRC press. Parameters ---------- spark Spark Session attribute_stats Spark dataframe. The intermediate dataframe saved by running function stabilityIndex_computation with schema [idx, attribute, mean, stddev, kurtosis]. It should contain all the attributes used in argument attribute_transformation. attribute_transformation Takes input in dictionary format: each key-value combination represents one new feature. Each key is a string containing all the attributes involved in the new feature seperated by '|'. Each value is the transformation of the attributes in string. For example, {'X|Y|Z': 'X**2+Y/Z', 'A': 'log(A)'} metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) threshold A column is flagged if the stability index is below the threshold, which varies between 0 and 4. The following criteria can be used to classify stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all newly generated features and potential unstable features. Returns ------- DataFrame [feature_formula, mean_cv, stddev_cv, mean_si, stddev_si, stability_index_lower_bound, stability_index_upper_bound, flagged_lower, flagged_upper]. *_cv is coefficient of variation for each metric. *_si is stability index for each metric. stability_index_lower_bound and stability_index_upper_bound form a range for estimated stability index. flagged_lower and flagged_upper indicate whether the feature is potentially unstable based on the lower and upper bounds for stability index. \"\"\" if ( round ( metric_weightages . get ( \"mean\" , 0 ) + metric_weightages . get ( \"stddev\" , 0 ) + metric_weightages . get ( \"kurtosis\" , 0 ), 3 , ) != 1 ): raise ValueError ( \"Invalid input for metric weightages. Either metric name is incorrect or sum of metric weightages is not 1.0.\" ) def stats_estimation ( attributes , transformation , mean , stddev ): attribute_means = list ( zip ( attributes , mean )) first_dev = [] second_dev = [] est_mean = 0 est_var = 0 for attr , s in zip ( attributes , stddev ): first_dev = sp . diff ( transformation , attr ) second_dev = sp . diff ( transformation , attr , 2 ) est_mean += s ** 2 * second_dev . subs ( attribute_means ) / 2 est_var += s ** 2 * ( first_dev . subs ( attribute_means )) ** 2 transformation = sp . parse_expr ( transformation ) est_mean += transformation . subs ( attribute_means ) return [ float ( est_mean ), float ( est_var )] f_stats_estimation = F . udf ( stats_estimation , T . ArrayType ( T . FloatType ())) index = ( attribute_stats . select ( \"idx\" ) . distinct () . orderBy ( \"idx\" ) . rdd . flatMap ( list ) . collect () ) attribute_names = list ( attribute_transformation . keys ()) transformations = list ( attribute_transformation . values ()) feature_metric = [] for attributes , transformation in zip ( attribute_names , transformations ): attributes = [ x . strip () for x in attributes . split ( \"|\" )] for idx in index : attr_mean_list , attr_stddev_list = [], [] for attr in attributes : df_temp = attribute_stats . where ( ( F . col ( \"idx\" ) == idx ) & ( F . col ( \"attribute\" ) == attr ) ) if df_temp . count () == 0 : raise TypeError ( \"Invalid input for attribute_stats: all involved attributes must have available statistics across all time periods (idx)\" ) attr_mean_list . append ( df_temp . select ( \"mean\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) attr_stddev_list . append ( df_temp . select ( \"stddev\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) feature_metric . append ( [ idx , transformation , attributes , attr_mean_list , attr_stddev_list ] ) schema = T . StructType ( [ T . StructField ( \"idx\" , T . IntegerType (), True ), T . StructField ( \"transformation\" , T . StringType (), True ), T . StructField ( \"attributes\" , T . ArrayType ( T . StringType ()), True ), T . StructField ( \"attr_mean_list\" , T . ArrayType ( T . FloatType ()), True ), T . StructField ( \"attr_stddev_list\" , T . ArrayType ( T . FloatType ()), True ), ] ) df_feature_metric = ( spark . createDataFrame ( feature_metric , schema = schema ) . withColumn ( \"est_feature_stats\" , f_stats_estimation ( \"attributes\" , \"transformation\" , \"attr_mean_list\" , \"attr_stddev_list\" ), ) . withColumn ( \"est_feature_mean\" , F . col ( \"est_feature_stats\" )[ 0 ]) . withColumn ( \"est_feature_stddev\" , F . sqrt ( F . col ( \"est_feature_stats\" )[ 1 ])) . select ( \"idx\" , \"attributes\" , \"transformation\" , \"est_feature_mean\" , \"est_feature_stddev\" , ) ) output = [] for idx , i in enumerate ( transformations ): i_output = [ i ] for metric in [ \"est_feature_mean\" , \"est_feature_stddev\" ]: metric_stats = ( df_feature_metric . where ( F . col ( \"transformation\" ) == i ) . orderBy ( \"idx\" ) . select ( metric ) . fillna ( np . nan ) . rdd . flatMap ( list ) . collect () ) metric_cv = round ( float ( variation ([ a for a in metric_stats ])), 4 ) i_output . append ( metric_cv ) output . append ( i_output ) schema = T . StructType ( [ T . StructField ( \"feature_formula\" , T . StringType (), True ), T . StructField ( \"mean_cv\" , T . FloatType (), True ), T . StructField ( \"stddev_cv\" , T . FloatType (), True ), ] ) odf = spark . createDataFrame ( output , schema = schema ) def score_cv ( cv , thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): if cv is None : return None else : cv = abs ( cv ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( thresholds ): if cv < thresh : return stability_index [ i ] return stability_index [ - 1 ] f_score_cv = F . udf ( score_cv , T . IntegerType ()) odf = ( odf . replace ( np . nan , None ) . withColumn ( \"mean_si\" , f_score_cv ( F . col ( \"mean_cv\" ))) . withColumn ( \"stddev_si\" , f_score_cv ( F . col ( \"stddev_cv\" ))) . withColumn ( \"stability_index_lower_bound\" , F . round ( F . col ( \"mean_si\" ) * metric_weightages . get ( \"mean\" , 0 ) + F . col ( \"stddev_si\" ) * metric_weightages . get ( \"stddev\" , 0 ), 4 , ), ) . withColumn ( \"stability_index_upper_bound\" , F . round ( F . col ( \"stability_index_lower_bound\" ) + 4 * metric_weightages . get ( \"kurtosis\" , 0 ), 4 , ), ) . withColumn ( \"flagged_lower\" , F . when ( ( F . col ( \"stability_index_lower_bound\" ) < threshold ) | ( F . col ( \"stability_index_lower_bound\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) . withColumn ( \"flagged_upper\" , F . when ( ( F . col ( \"stability_index_upper_bound\" ) < threshold ) | ( F . col ( \"stability_index_upper_bound\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) ) if print_impact : logger . info ( \"All Features:\" ) odf . show ( len ( attribute_names ), False ) logger . info ( \"Potential Unstable Features Identified by Both Lower and Upper Bounds:\" ) unstable = odf . where ( F . col ( \"flagged_upper\" ) == 1 ) unstable . show ( unstable . count ()) return odf Functions def feature_stability_estimation ( spark, attribute_stats, attribute_transformation, metric_weightages={'mean': 0.5, 'stddev': 0.3, 'kurtosis': 0.2}, threshold=1, print_impact=False) This function is able to estimate the stability index of a new feature composed of certain attributes whose stability metrics are known. For example, the new feature F can be expressed as F = g(X1, X2, \u2026, Xn), where X1, X2, \u2026, Xn represent different attributes and g represents the transformation function. The most straightforward way is to generate the new feature for all periods and calculate its stability index. However, it requires reading all historical data again which can be unrealistic for large datasets. Thus, the objective of this function is to estimate feature stability index without reading historical data. One example can be the following scenario: we have attributes A and B, we have their respective stability statistics from T1 to T7. At T7 we realise we need to generate a new feature: A/B, but we don\u2019t have statistics metrics of A/B from T1 to T6 and this is where this function can be applied to generate an estimation without reading datasets from T1 to T6. The estimation can be broken down into 3 steps. 1. Estimate mean and stddev for the new feature based on attribute metrics (no existing resource found to estimate Feature kurtosis). Estimated mean and stddev are generated for each time period using the formula below according to [1]: 2. Calculate Coefficient of variation (CV) for estimated feature mean and stddev. Each CV can be then mapped to an integer between 0 and 4 to generate the metric stability index. 3. Similar to the attribute stability index, each metric is assigned a weightage between 0 and 1, where the default values are 50 for mean, 30% for standard deviation and 20% for kurtosis. Because we are unable to generate kurtosis stability index, its minimum and maximum possible values (0 and 4) are used to output a range for global stability index (GSI): * Lower bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfce * Upper bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfd2 [1] Benaroya, H., Han, S. M., & Nagurka, M. (2005). Probability models in engineering and science (Vol. 192, pp. 168-169). CRC press. Parameters spark Spark Session attribute_stats Spark dataframe. The intermediate dataframe saved by running function stabilityIndex_computation with schema [idx, attribute, mean, stddev, kurtosis]. It should contain all the attributes used in argument attribute_transformation. attribute_transformation Takes input in dictionary format: each key-value combination represents one new feature. Each key is a string containing all the attributes involved in the new feature seperated by '|'. Each value is the transformation of the attributes in string. For example, {'X|Y|Z': 'X**2+Y/Z', 'A': 'log(A)'} metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) threshold A column is flagged if the stability index is below the threshold, which varies between 0 and 4. The following criteria can be used to classify stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all newly generated features and potential unstable features. Returns DataFrame [feature_formula, mean_cv, stddev_cv, mean_si, stddev_si, stability_index_lower_bound, stability_index_upper_bound, flagged_lower, flagged_upper]. _cv is coefficient of variation for each metric. _si is stability index for each metric. stability_index_lower_bound and stability_index_upper_bound form a range for estimated stability index. flagged_lower and flagged_upper indicate whether the feature is potentially unstable based on the lower and upper bounds for stability index. Expand source code def feature_stability_estimation ( spark , attribute_stats , attribute_transformation , metric_weightages = { \"mean\" : 0.5 , \"stddev\" : 0.3 , \"kurtosis\" : 0.2 }, threshold = 1 , print_impact = False , ): \"\"\" This function is able to estimate the stability index of a new feature composed of certain attributes whose stability metrics are known. For example, the new feature F can be expressed as F = g(X1, X2, \u2026, Xn), where X1, X2, \u2026, Xn represent different attributes and g represents the transformation function. The most straightforward way is to generate the new feature for all periods and calculate its stability index. However, it requires reading all historical data again which can be unrealistic for large datasets. Thus, the objective of this function is to estimate feature stability index without reading historical data. One example can be the following scenario: we have attributes A and B, we have their respective stability statistics from T1 to T7. At T7 we realise we need to generate a new feature: A/B, but we don\u2019t have statistics metrics of A/B from T1 to T6 and this is where this function can be applied to generate an estimation without reading datasets from T1 to T6. The estimation can be broken down into 3 steps. 1. Estimate mean and stddev for the new feature based on attribute metrics (no existing resource found to estimate Feature kurtosis). Estimated mean and stddev are generated for each time period using the formula below according to [1]: ![https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/feature_stability_formulae.png](https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/feature_stability_formulae.png) 2. Calculate Coefficient of variation (CV) for estimated feature mean and stddev. Each CV can be then mapped to an integer between 0 and 4 to generate the metric stability index. 3. Similar to the attribute stability index, each metric is assigned a weightage between 0 and 1, where the default values are 50 for mean, 30% for standard deviation and 20% for kurtosis. Because we are unable to generate kurtosis stability index, its minimum and maximum possible values (0 and 4) are used to output a range for global stability index (GSI): * Lower bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfce * Upper bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfd2 [1] Benaroya, H., Han, S. M., & Nagurka, M. (2005). Probability models in engineering and science (Vol. 192, pp. 168-169). CRC press. Parameters ---------- spark Spark Session attribute_stats Spark dataframe. The intermediate dataframe saved by running function stabilityIndex_computation with schema [idx, attribute, mean, stddev, kurtosis]. It should contain all the attributes used in argument attribute_transformation. attribute_transformation Takes input in dictionary format: each key-value combination represents one new feature. Each key is a string containing all the attributes involved in the new feature seperated by '|'. Each value is the transformation of the attributes in string. For example, {'X|Y|Z': 'X**2+Y/Z', 'A': 'log(A)'} metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) threshold A column is flagged if the stability index is below the threshold, which varies between 0 and 4. The following criteria can be used to classify stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all newly generated features and potential unstable features. Returns ------- DataFrame [feature_formula, mean_cv, stddev_cv, mean_si, stddev_si, stability_index_lower_bound, stability_index_upper_bound, flagged_lower, flagged_upper]. *_cv is coefficient of variation for each metric. *_si is stability index for each metric. stability_index_lower_bound and stability_index_upper_bound form a range for estimated stability index. flagged_lower and flagged_upper indicate whether the feature is potentially unstable based on the lower and upper bounds for stability index. \"\"\" if ( round ( metric_weightages . get ( \"mean\" , 0 ) + metric_weightages . get ( \"stddev\" , 0 ) + metric_weightages . get ( \"kurtosis\" , 0 ), 3 , ) != 1 ): raise ValueError ( \"Invalid input for metric weightages. Either metric name is incorrect or sum of metric weightages is not 1.0.\" ) def stats_estimation ( attributes , transformation , mean , stddev ): attribute_means = list ( zip ( attributes , mean )) first_dev = [] second_dev = [] est_mean = 0 est_var = 0 for attr , s in zip ( attributes , stddev ): first_dev = sp . diff ( transformation , attr ) second_dev = sp . diff ( transformation , attr , 2 ) est_mean += s ** 2 * second_dev . subs ( attribute_means ) / 2 est_var += s ** 2 * ( first_dev . subs ( attribute_means )) ** 2 transformation = sp . parse_expr ( transformation ) est_mean += transformation . subs ( attribute_means ) return [ float ( est_mean ), float ( est_var )] f_stats_estimation = F . udf ( stats_estimation , T . ArrayType ( T . FloatType ())) index = ( attribute_stats . select ( \"idx\" ) . distinct () . orderBy ( \"idx\" ) . rdd . flatMap ( list ) . collect () ) attribute_names = list ( attribute_transformation . keys ()) transformations = list ( attribute_transformation . values ()) feature_metric = [] for attributes , transformation in zip ( attribute_names , transformations ): attributes = [ x . strip () for x in attributes . split ( \"|\" )] for idx in index : attr_mean_list , attr_stddev_list = [], [] for attr in attributes : df_temp = attribute_stats . where ( ( F . col ( \"idx\" ) == idx ) & ( F . col ( \"attribute\" ) == attr ) ) if df_temp . count () == 0 : raise TypeError ( \"Invalid input for attribute_stats: all involved attributes must have available statistics across all time periods (idx)\" ) attr_mean_list . append ( df_temp . select ( \"mean\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) attr_stddev_list . append ( df_temp . select ( \"stddev\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) feature_metric . append ( [ idx , transformation , attributes , attr_mean_list , attr_stddev_list ] ) schema = T . StructType ( [ T . StructField ( \"idx\" , T . IntegerType (), True ), T . StructField ( \"transformation\" , T . StringType (), True ), T . StructField ( \"attributes\" , T . ArrayType ( T . StringType ()), True ), T . StructField ( \"attr_mean_list\" , T . ArrayType ( T . FloatType ()), True ), T . StructField ( \"attr_stddev_list\" , T . ArrayType ( T . FloatType ()), True ), ] ) df_feature_metric = ( spark . createDataFrame ( feature_metric , schema = schema ) . withColumn ( \"est_feature_stats\" , f_stats_estimation ( \"attributes\" , \"transformation\" , \"attr_mean_list\" , \"attr_stddev_list\" ), ) . withColumn ( \"est_feature_mean\" , F . col ( \"est_feature_stats\" )[ 0 ]) . withColumn ( \"est_feature_stddev\" , F . sqrt ( F . col ( \"est_feature_stats\" )[ 1 ])) . select ( \"idx\" , \"attributes\" , \"transformation\" , \"est_feature_mean\" , \"est_feature_stddev\" , ) ) output = [] for idx , i in enumerate ( transformations ): i_output = [ i ] for metric in [ \"est_feature_mean\" , \"est_feature_stddev\" ]: metric_stats = ( df_feature_metric . where ( F . col ( \"transformation\" ) == i ) . orderBy ( \"idx\" ) . select ( metric ) . fillna ( np . nan ) . rdd . flatMap ( list ) . collect () ) metric_cv = round ( float ( variation ([ a for a in metric_stats ])), 4 ) i_output . append ( metric_cv ) output . append ( i_output ) schema = T . StructType ( [ T . StructField ( \"feature_formula\" , T . StringType (), True ), T . StructField ( \"mean_cv\" , T . FloatType (), True ), T . StructField ( \"stddev_cv\" , T . FloatType (), True ), ] ) odf = spark . createDataFrame ( output , schema = schema ) def score_cv ( cv , thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): if cv is None : return None else : cv = abs ( cv ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( thresholds ): if cv < thresh : return stability_index [ i ] return stability_index [ - 1 ] f_score_cv = F . udf ( score_cv , T . IntegerType ()) odf = ( odf . replace ( np . nan , None ) . withColumn ( \"mean_si\" , f_score_cv ( F . col ( \"mean_cv\" ))) . withColumn ( \"stddev_si\" , f_score_cv ( F . col ( \"stddev_cv\" ))) . withColumn ( \"stability_index_lower_bound\" , F . round ( F . col ( \"mean_si\" ) * metric_weightages . get ( \"mean\" , 0 ) + F . col ( \"stddev_si\" ) * metric_weightages . get ( \"stddev\" , 0 ), 4 , ), ) . withColumn ( \"stability_index_upper_bound\" , F . round ( F . col ( \"stability_index_lower_bound\" ) + 4 * metric_weightages . get ( \"kurtosis\" , 0 ), 4 , ), ) . withColumn ( \"flagged_lower\" , F . when ( ( F . col ( \"stability_index_lower_bound\" ) < threshold ) | ( F . col ( \"stability_index_lower_bound\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) . withColumn ( \"flagged_upper\" , F . when ( ( F . col ( \"stability_index_upper_bound\" ) < threshold ) | ( F . col ( \"stability_index_upper_bound\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) ) if print_impact : logger . info ( \"All Features:\" ) odf . show ( len ( attribute_names ), False ) logger . info ( \"Potential Unstable Features Identified by Both Lower and Upper Bounds:\" ) unstable = odf . where ( F . col ( \"flagged_upper\" ) == 1 ) unstable . show ( unstable . count ()) return odf def stability_index_computation ( spark, idfs, list_of_cols='all', drop_cols=[], metric_weightages={'mean': 0.5, 'stddev': 0.3, 'kurtosis': 0.2}, binary_cols=[], existing_metric_path='', appended_metric_path='', persist: bool = True, persist_option=StorageLevel(True, True, False, False, 1), threshold=1, print_impact=False) The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 6 datasets collected in 6 consecutive time periods (D1, D2, \u2026, D6), data stability index of an attribute measures how stable the attribute is from D1 to D6. The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. In summary, given N datasets representing different time periods, we would like to measure the stability of some numerical attributes from the first to the N-th dataset. The whole process can be broken down into 2 steps: (1) Choose a few statistical metrics to describe the distribution of each attribute at each time period. (2) Compute attribute level stability by combining the stability of each statistical metric over time periods. In the first step, we choose mean, standard deviation and kurtosis as the statistical metrics in our implementation. Intuitively, they represent different aspects of a distribution: mean measures central tendency, standard deviation measures dispersion and kurtosis measures shape of a distribution. Reasons of selecting those 3 metrics will be explained in a later section. With mean, standard deviation and kurtosis computed for each attribute at each time interval, we can form 3 arrays of size N for each attribute. In the second step, Coefficient of Variation (CV) is used to measure the stability of each metric. CV represents the ratio of the standard deviation to the mean, which is a unitless statistic to compare the relative variation from one array to another. Considering the wide potential range of CV, the absolute value of CV is then mapped to an integer between 0 and 4 according to the table below, where 0 indicates highly unstable and 4 indicates highly stable. We call this integer a metric stability index. abs(CV) Interval Metric Stability Index [0, 0.03) 4 [0.03, 0.1) 3 [0.1, 0.2) 2 [0.2, 0.5) 1 [0.5, +inf) 0 Finally, the attribute stability index (SI) is a weighted sum of 3 metric stability indexes, where we assign 50% for mean, 30% for standard deviation and 20% for kurtosis by default. The final output is a float between 0 and 4 and an attribute can be classified as one of the following categories: very unstable (0\u2264SI<1), unstable (1\u2264SI<2), marginally stable (2\u2264SI<3), stable (3\u2264SI<3.5) and very stable (3.5\u2264SI\u22644). For example, there are 6 samples of attribute X from T1 to T6. For each sample, we have computed the statistical metrics of X from T1 to T6: idx Mean Standard deviation Kurtosis 1 11 2 3.9 2 12 1 4.2 3 15 3 4.0 4 10 2 4.1 5 11 1 4.2 6 13 0.5 4.0 Then we calculate the Coefficient of Variation for each array: CV of mean = CV([11, 12, 15, 10, 11, 13]) = 0.136 CV of standard deviation = CV([2, 1, 3, 2, 1, 0.5]) = 0.529 CV of kurtosis = CV([3.9, 4.2, 4.0, 4.1, 4.2, 4.0]) = 0.027 Metric stability indexes are then computed by mapping each CV value to an integer accordingly. As a result, metric stability index is 2 for mean, 0 for standard deviation and 4 for kurtosis. Why mean is chosen over median? Dummy variables which take only the value 0 or 1 are frequently seen in machine learning features. Mean of a dummy variable represents the proportion of value 1 and median of a dummy variable is either 0 or 1 whichever is more frequent. However, CV may not work well when 0 appears in the array or the array contains both positive and negative values. For example, intuitively [0,0,0,0,0,1,0,0,0] is a stable array but its CV is 2.83 which is extremely high, but cv of [0.45,0.44,0.48,0.49,0.42,0.52,0.49,0.47,0.48] is 0.06 which is much more reasonable. Thus we decided to use mean instead of median. Although median is considered as a more robust choice, outlier treatment can be applied prior to data stability analysis to handle this issue. Why kurtosis is chosen over skewness? Kurtosis is a positive value (note that we use kurtosis instead of excess kurtosis which) but skewness can range from \u2013inf to +inf. Usually, if skewness is between -0.5 and 0.5, the distribution is approximately symmetric. Thus, if the skewness fluctuates around 0, the CV is highly likely to be high or invalid because the mean will be close to 0. Stability index is preferred in the following scenario: Pairwise drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. However this can be time-consuming especially when the number of target dataset is large. In this case, measuring data stability instead of data drift would be a much faster alternative and the source/baseline dataset is not required as well Troubleshooting If the attribute stability index appears to be nan, it may due to one of the following reasons: - One metric ( likely to be kurtosis) is nan. For example, the kurtosis of a sample is nan If its standard deviation is 0. - The mean of a metric from the first to the N-th dataset is zero, causing the denominator of CV to be 0. For example, when mean of attribute X is always zero for all datasets, its stability index would be nan. Limitation Limitation of CV: CV may not work well when 0 appears in the array or the array contains both positive and negative values. Parameters spark Spark Session idfs Variable number of input dataframes list_of_cols List of numerical columns to check stability e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = []) metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) binary_cols List of numerical columns to be treated as binary columns e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d. For the specified binary columns, only the mean value will be used as the statistical metric (standard deviation and kurtosis will be skipped). In addition, standard deviation will be used to measure the stability of mean instead of CV. (Default value = []) existing_metric_path This argument is path for referring pre-existing metrics of historical datasets and is of schema [idx, attribute, mean, stdev, kurtosis]. idx is index number of historical datasets assigned in chronological order. (Default value = \"\") appended_metric_path This argument is path for saving input dataframes metrics after appending to the historical datasets' metrics. (Default value = \"\") persist Boolean argument - True or False. This argument is used to determine whether to persist on binning result of source and target dataset, True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) persist_option If persist is True, this argument is used to determine the type of persist. (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) threshold A column is flagged if the stability index is below the threshold, which varies between 0 to 4. The following criteria can be used to classifiy stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all attributes and potential unstable attributes. Returns DataFrame [attribute, mean_stddev, mean_si, stddev_si, kurtosis_si, mean_cv, stddev_cv, kurtosis_cv, stability_index]. _cv is coefficient of variation for each metric. _si is stability index for each metric. stability_index is net weighted stability index based on the individual metrics' stability index. Expand source code def stability_index_computation ( spark , idfs , list_of_cols = \"all\" , drop_cols = [], metric_weightages = { \"mean\" : 0.5 , \"stddev\" : 0.3 , \"kurtosis\" : 0.2 }, binary_cols = [], existing_metric_path = \"\" , appended_metric_path = \"\" , persist : bool = True , persist_option = pyspark . StorageLevel . MEMORY_AND_DISK , threshold = 1 , print_impact = False , ): \"\"\" The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 6 datasets collected in 6 consecutive time periods (D1, D2, \u2026, D6), data stability index of an attribute measures how stable the attribute is from D1 to D6. The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. In summary, given N datasets representing different time periods, we would like to measure the stability of some numerical attributes from the first to the N-th dataset. The whole process can be broken down into 2 steps: (1) Choose a few statistical metrics to describe the distribution of each attribute at each time period. (2) Compute attribute level stability by combining the stability of each statistical metric over time periods. In the first step, we choose mean, standard deviation and kurtosis as the statistical metrics in our implementation. Intuitively, they represent different aspects of a distribution: mean measures central tendency, standard deviation measures dispersion and kurtosis measures shape of a distribution. Reasons of selecting those 3 metrics will be explained in a later section. With mean, standard deviation and kurtosis computed for each attribute at each time interval, we can form 3 arrays of size N for each attribute. In the second step, Coefficient of Variation (CV) is used to measure the stability of each metric. CV represents the ratio of the standard deviation to the mean, which is a unitless statistic to compare the relative variation from one array to another. Considering the wide potential range of CV, the absolute value of CV is then mapped to an integer between 0 and 4 according to the table below, where 0 indicates highly unstable and 4 indicates highly stable. We call this integer a metric stability index. | abs(CV) Interval | Metric Stability Index | |------------------|------------------------| | [0, 0.03) | 4 | | [0.03, 0.1) | 3 | | [0.1, 0.2) | 2 | | [0.2, 0.5) | 1 | | [0.5, +inf) | 0 | Finally, the attribute stability index (SI) is a weighted sum of 3 metric stability indexes, where we assign 50% for mean, 30% for standard deviation and 20% for kurtosis by default. The final output is a float between 0 and 4 and an attribute can be classified as one of the following categories: very unstable (0\u2264SI<1), unstable (1\u2264SI<2), marginally stable (2\u2264SI<3), stable (3\u2264SI<3.5) and very stable (3.5\u2264SI\u22644). For example, there are 6 samples of attribute X from T1 to T6. For each sample, we have computed the statistical metrics of X from T1 to T6: | idx | Mean | Standard deviation | Kurtosis | |-----|------|--------------------|----------| | 1 | 11 | 2 | 3.9 | | 2 | 12 | 1 | 4.2 | | 3 | 15 | 3 | 4.0 | | 4 | 10 | 2 | 4.1 | | 5 | 11 | 1 | 4.2 | | 6 | 13 | 0.5 | 4.0 | Then we calculate the Coefficient of Variation for each array: - CV of mean = CV([11, 12, 15, 10, 11, 13]) = 0.136 - CV of standard deviation = CV([2, 1, 3, 2, 1, 0.5]) = 0.529 - CV of kurtosis = CV([3.9, 4.2, 4.0, 4.1, 4.2, 4.0]) = 0.027 Metric stability indexes are then computed by mapping each CV value to an integer accordingly. As a result, metric stability index is 2 for mean, 0 for standard deviation and 4 for kurtosis. Why mean is chosen over median? - Dummy variables which take only the value 0 or 1 are frequently seen in machine learning features. Mean of a dummy variable represents the proportion of value 1 and median of a dummy variable is either 0 or 1 whichever is more frequent. However, CV may not work well when 0 appears in the array or the array contains both positive and negative values. For example, intuitively [0,0,0,0,0,1,0,0,0] is a stable array but its CV is 2.83 which is extremely high, but cv of [0.45,0.44,0.48,0.49,0.42,0.52,0.49,0.47,0.48] is 0.06 which is much more reasonable. Thus we decided to use mean instead of median. Although median is considered as a more robust choice, outlier treatment can be applied prior to data stability analysis to handle this issue. Why kurtosis is chosen over skewness? - Kurtosis is a positive value (note that we use kurtosis instead of excess kurtosis which) but skewness can range from \u2013inf to +inf. Usually, if skewness is between -0.5 and 0.5, the distribution is approximately symmetric. Thus, if the skewness fluctuates around 0, the CV is highly likely to be high or invalid because the mean will be close to 0. Stability index is preferred in the following scenario: - Pairwise drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. However this can be time-consuming especially when the number of target dataset is large. In this case, measuring data stability instead of data drift would be a much faster alternative and the source/baseline dataset is not required as well Troubleshooting - If the attribute stability index appears to be nan, it may due to one of the following reasons: - One metric ( likely to be kurtosis) is nan. For example, the kurtosis of a sample is nan If its standard deviation is 0. - The mean of a metric from the first to the N-th dataset is zero, causing the denominator of CV to be 0. For example, when mean of attribute X is always zero for all datasets, its stability index would be nan. Limitation - Limitation of CV: CV may not work well when 0 appears in the array or the array contains both positive and negative values. Parameters ---------- spark Spark Session idfs Variable number of input dataframes list_of_cols List of numerical columns to check stability e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = []) metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) binary_cols List of numerical columns to be treated as binary columns e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d. For the specified binary columns, only the mean value will be used as the statistical metric (standard deviation and kurtosis will be skipped). In addition, standard deviation will be used to measure the stability of mean instead of CV. (Default value = []) existing_metric_path This argument is path for referring pre-existing metrics of historical datasets and is of schema [idx, attribute, mean, stdev, kurtosis]. idx is index number of historical datasets assigned in chronological order. (Default value = \"\") appended_metric_path This argument is path for saving input dataframes metrics after appending to the historical datasets' metrics. (Default value = \"\") persist Boolean argument - True or False. This argument is used to determine whether to persist on binning result of source and target dataset, True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) persist_option If persist is True, this argument is used to determine the type of persist. (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) threshold A column is flagged if the stability index is below the threshold, which varies between 0 to 4. The following criteria can be used to classifiy stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all attributes and potential unstable attributes. Returns ------- DataFrame [attribute, mean_stddev, mean_si, stddev_si, kurtosis_si, mean_cv, stddev_cv, kurtosis_cv, stability_index]. *_cv is coefficient of variation for each metric. *_si is stability index for each metric. stability_index is net weighted stability index based on the individual metrics' stability index. \"\"\" num_cols = attributeType_segregation ( idfs [ 0 ])[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if isinstance ( binary_cols , str ): binary_cols = [ x . strip () for x in binary_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if any ( x not in list_of_cols for x in binary_cols ): raise TypeError ( \"Invalid input for Binary Column(s)\" ) check_metric_weightages ( metric_weightages ) check_threshold ( threshold ) if existing_metric_path : existing_metric_df = spark . read . csv ( existing_metric_path , header = True , inferSchema = True ) dfs_count = int ( existing_metric_df . select ( F . max ( F . col ( \"idx\" ))) . first ()[ 0 ]) + 1 else : existing_metric_df = None dfs_count = 1 def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) if persist : for i in range ( len ( idfs )): idfs [ i ] = idfs [ i ] . select ( list_of_cols ) idfs [ i ] . persist ( persist_option ) list_temp_all_col = [] if appended_metric_path : list_append_all = [] for i in list_of_cols : if i in binary_cols : col_type = \"Binary\" else : col_type = \"Numerical\" count_idf = dfs_count list_temp_col_in_idf = [] for idf in idfs : df_stat_each = idf . select ( F . mean ( i ) . alias ( \"mean\" ), F . stddev ( i ) . alias ( \"stddev\" ), ( F . kurtosis ( i ) + F . lit ( 3 )) . alias ( \"kurtosis\" ), ) list_temp_col_in_idf . append ( df_stat_each ) if appended_metric_path : df_append_single = df_stat_each . select ( F . lit ( str ( count_idf )) . alias ( \"idx\" ), F . lit ( str ( i )) . alias ( \"attribute\" ), F . lit ( col_type ) . alias ( \"type\" ), \"mean\" , \"stddev\" , \"kurtosis\" , ) list_append_all . append ( df_append_single ) count_idf += 1 if existing_metric_df : existing_df_for_single_col = existing_metric_df . where ( F . col ( \"attribute\" ) == str ( i ) ) . select ( \"mean\" , \"stddev\" , \"kurtosis\" ) if existing_df_for_single_col . count () > 0 : list_temp_col_in_idf . append ( existing_df_for_single_col ) df_stat_col = ( unionAll ( list_temp_col_in_idf ) . select ( F . stddev ( \"mean\" ) . alias ( \"std_of_mean\" ), F . mean ( \"mean\" ) . alias ( \"mean_of_mean\" ), F . stddev ( \"stddev\" ) . alias ( \"std_of_stddev\" ), F . mean ( \"stddev\" ) . alias ( \"mean_of_stddev\" ), F . stddev ( \"kurtosis\" ) . alias ( \"std_of_kurtosis\" ), F . mean ( \"kurtosis\" ) . alias ( \"mean_of_kurtosis\" ), ) . select ( F . lit ( str ( i )) . alias ( \"attribute\" ), F . lit ( col_type ) . alias ( \"type\" ), F . col ( \"std_of_mean\" ) . alias ( \"mean_stddev\" ), ( F . col ( \"std_of_mean\" ) / F . col ( \"mean_of_mean\" )) . alias ( \"mean_cv\" ), ( F . col ( \"std_of_stddev\" ) / F . col ( \"mean_of_stddev\" )) . alias ( \"stddev_cv\" ), ( F . col ( \"std_of_kurtosis\" ) / F . col ( \"mean_of_kurtosis\" )) . alias ( \"kurtosis_cv\" ), ) ) list_temp_all_col . append ( df_stat_col ) odf = unionAll ( list_temp_all_col ) if appended_metric_path : if existing_metric_df : list_append_all . append ( existing_metric_df ) df_append = unionAll ( list_append_all ) . orderBy ( F . col ( \"idx\" )) df_append . coalesce ( 1 ) . write . csv ( appended_metric_path , header = True , mode = \"overwrite\" ) f_compute_si = F . udf ( compute_si ( metric_weightages ), T . ArrayType ( T . FloatType ())) odf = ( odf . replace ( np . nan , None ) . withColumn ( \"si_array\" , f_compute_si ( \"type\" , \"mean_stddev\" , \"mean_cv\" , \"stddev_cv\" , \"kurtosis_cv\" ), ) . withColumn ( \"mean_si\" , F . col ( \"si_array\" ) . getItem ( 0 )) . withColumn ( \"stddev_si\" , F . col ( \"si_array\" ) . getItem ( 1 )) . withColumn ( \"kurtosis_si\" , F . col ( \"si_array\" ) . getItem ( 2 )) . withColumn ( \"stability_index\" , F . col ( \"si_array\" ) . getItem ( 3 )) . withColumn ( \"flagged\" , F . when ( ( F . col ( \"stability_index\" ) < threshold ) | ( F . col ( \"stability_index\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) . withColumn ( \"mean_stddev\" , F . round ( F . col ( \"mean_stddev\" ), 4 )) . withColumn ( \"mean_cv\" , F . round ( F . col ( \"mean_cv\" ), 4 )) . withColumn ( \"stddev_cv\" , F . round ( F . col ( \"stddev_cv\" ), 4 )) . withColumn ( \"kurtosis_cv\" , F . round ( F . col ( \"kurtosis_cv\" ), 4 )) . drop ( \"si_array\" ) ) if print_impact : logger . info ( \"All Attributes:\" ) odf . show ( len ( list_of_cols )) logger . info ( \"Potential Unstable Attributes:\" ) unstable = odf . where ( F . col ( \"flagged\" ) == 1 ) unstable . show ( unstable . count ()) if persist : for i in range ( len ( idfs )): idfs [ i ] . unpersist () return odf","title":"<code>stability</code>"},{"location":"api/drift_stability/stability.html#stability","text":"Expand source code import numpy as np import pyspark import sympy as sp from loguru import logger from pyspark.sql import DataFrame from pyspark.sql import functions as F from pyspark.sql import types as T from scipy.stats import variation from anovos.data_transformer.transformers import attribute_binning from anovos.shared.utils import attributeType_segregation from .validations import compute_si , check_metric_weightages , check_threshold def stability_index_computation ( spark , idfs , list_of_cols = \"all\" , drop_cols = [], metric_weightages = { \"mean\" : 0.5 , \"stddev\" : 0.3 , \"kurtosis\" : 0.2 }, binary_cols = [], existing_metric_path = \"\" , appended_metric_path = \"\" , persist : bool = True , persist_option = pyspark . StorageLevel . MEMORY_AND_DISK , threshold = 1 , print_impact = False , ): \"\"\" The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 6 datasets collected in 6 consecutive time periods (D1, D2, \u2026, D6), data stability index of an attribute measures how stable the attribute is from D1 to D6. The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. In summary, given N datasets representing different time periods, we would like to measure the stability of some numerical attributes from the first to the N-th dataset. The whole process can be broken down into 2 steps: (1) Choose a few statistical metrics to describe the distribution of each attribute at each time period. (2) Compute attribute level stability by combining the stability of each statistical metric over time periods. In the first step, we choose mean, standard deviation and kurtosis as the statistical metrics in our implementation. Intuitively, they represent different aspects of a distribution: mean measures central tendency, standard deviation measures dispersion and kurtosis measures shape of a distribution. Reasons of selecting those 3 metrics will be explained in a later section. With mean, standard deviation and kurtosis computed for each attribute at each time interval, we can form 3 arrays of size N for each attribute. In the second step, Coefficient of Variation (CV) is used to measure the stability of each metric. CV represents the ratio of the standard deviation to the mean, which is a unitless statistic to compare the relative variation from one array to another. Considering the wide potential range of CV, the absolute value of CV is then mapped to an integer between 0 and 4 according to the table below, where 0 indicates highly unstable and 4 indicates highly stable. We call this integer a metric stability index. | abs(CV) Interval | Metric Stability Index | |------------------|------------------------| | [0, 0.03) | 4 | | [0.03, 0.1) | 3 | | [0.1, 0.2) | 2 | | [0.2, 0.5) | 1 | | [0.5, +inf) | 0 | Finally, the attribute stability index (SI) is a weighted sum of 3 metric stability indexes, where we assign 50% for mean, 30% for standard deviation and 20% for kurtosis by default. The final output is a float between 0 and 4 and an attribute can be classified as one of the following categories: very unstable (0\u2264SI<1), unstable (1\u2264SI<2), marginally stable (2\u2264SI<3), stable (3\u2264SI<3.5) and very stable (3.5\u2264SI\u22644). For example, there are 6 samples of attribute X from T1 to T6. For each sample, we have computed the statistical metrics of X from T1 to T6: | idx | Mean | Standard deviation | Kurtosis | |-----|------|--------------------|----------| | 1 | 11 | 2 | 3.9 | | 2 | 12 | 1 | 4.2 | | 3 | 15 | 3 | 4.0 | | 4 | 10 | 2 | 4.1 | | 5 | 11 | 1 | 4.2 | | 6 | 13 | 0.5 | 4.0 | Then we calculate the Coefficient of Variation for each array: - CV of mean = CV([11, 12, 15, 10, 11, 13]) = 0.136 - CV of standard deviation = CV([2, 1, 3, 2, 1, 0.5]) = 0.529 - CV of kurtosis = CV([3.9, 4.2, 4.0, 4.1, 4.2, 4.0]) = 0.027 Metric stability indexes are then computed by mapping each CV value to an integer accordingly. As a result, metric stability index is 2 for mean, 0 for standard deviation and 4 for kurtosis. Why mean is chosen over median? - Dummy variables which take only the value 0 or 1 are frequently seen in machine learning features. Mean of a dummy variable represents the proportion of value 1 and median of a dummy variable is either 0 or 1 whichever is more frequent. However, CV may not work well when 0 appears in the array or the array contains both positive and negative values. For example, intuitively [0,0,0,0,0,1,0,0,0] is a stable array but its CV is 2.83 which is extremely high, but cv of [0.45,0.44,0.48,0.49,0.42,0.52,0.49,0.47,0.48] is 0.06 which is much more reasonable. Thus we decided to use mean instead of median. Although median is considered as a more robust choice, outlier treatment can be applied prior to data stability analysis to handle this issue. Why kurtosis is chosen over skewness? - Kurtosis is a positive value (note that we use kurtosis instead of excess kurtosis which) but skewness can range from \u2013inf to +inf. Usually, if skewness is between -0.5 and 0.5, the distribution is approximately symmetric. Thus, if the skewness fluctuates around 0, the CV is highly likely to be high or invalid because the mean will be close to 0. Stability index is preferred in the following scenario: - Pairwise drift analysis can be performed between the source dataset and each of the target dataset to quantify the drift over time. However this can be time-consuming especially when the number of target dataset is large. In this case, measuring data stability instead of data drift would be a much faster alternative and the source/baseline dataset is not required as well Troubleshooting - If the attribute stability index appears to be nan, it may due to one of the following reasons: - One metric ( likely to be kurtosis) is nan. For example, the kurtosis of a sample is nan If its standard deviation is 0. - The mean of a metric from the first to the N-th dataset is zero, causing the denominator of CV to be 0. For example, when mean of attribute X is always zero for all datasets, its stability index would be nan. Limitation - Limitation of CV: CV may not work well when 0 appears in the array or the array contains both positive and negative values. Parameters ---------- spark Spark Session idfs Variable number of input dataframes list_of_cols List of numerical columns to check stability e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". \"all\" can be passed to include all numerical columns for analysis. Please note that this argument is used in conjunction with drop_cols i.e. a column mentioned in drop_cols argument is not considered for analysis even if it is mentioned in list_of_cols. (Default value = \"all\") drop_cols List of columns to be dropped e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d e.g., \"col1|col2\". (Default value = []) metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) binary_cols List of numerical columns to be treated as binary columns e.g., [\"col1\",\"col2\"]. Alternatively, columns can be specified in a string format, where different column names are separated by pipe delimiter \u201c|\u201d. For the specified binary columns, only the mean value will be used as the statistical metric (standard deviation and kurtosis will be skipped). In addition, standard deviation will be used to measure the stability of mean instead of CV. (Default value = []) existing_metric_path This argument is path for referring pre-existing metrics of historical datasets and is of schema [idx, attribute, mean, stdev, kurtosis]. idx is index number of historical datasets assigned in chronological order. (Default value = \"\") appended_metric_path This argument is path for saving input dataframes metrics after appending to the historical datasets' metrics. (Default value = \"\") persist Boolean argument - True or False. This argument is used to determine whether to persist on binning result of source and target dataset, True will enable the use of persist, otherwise False. It is recommended to set this as True for large datasets. (Default value = True) persist_option If persist is True, this argument is used to determine the type of persist. (Default value = pyspark.StorageLevel.MEMORY_AND_DISK) threshold A column is flagged if the stability index is below the threshold, which varies between 0 to 4. The following criteria can be used to classifiy stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all attributes and potential unstable attributes. Returns ------- DataFrame [attribute, mean_stddev, mean_si, stddev_si, kurtosis_si, mean_cv, stddev_cv, kurtosis_cv, stability_index]. *_cv is coefficient of variation for each metric. *_si is stability index for each metric. stability_index is net weighted stability index based on the individual metrics' stability index. \"\"\" num_cols = attributeType_segregation ( idfs [ 0 ])[ 0 ] if list_of_cols == \"all\" : list_of_cols = num_cols if isinstance ( list_of_cols , str ): list_of_cols = [ x . strip () for x in list_of_cols . split ( \"|\" )] if isinstance ( drop_cols , str ): drop_cols = [ x . strip () for x in drop_cols . split ( \"|\" )] if isinstance ( binary_cols , str ): binary_cols = [ x . strip () for x in binary_cols . split ( \"|\" )] list_of_cols = list ( set ([ e for e in list_of_cols if e not in drop_cols ])) if any ( x not in num_cols for x in list_of_cols ) | ( len ( list_of_cols ) == 0 ): raise TypeError ( \"Invalid input for Column(s)\" ) if any ( x not in list_of_cols for x in binary_cols ): raise TypeError ( \"Invalid input for Binary Column(s)\" ) check_metric_weightages ( metric_weightages ) check_threshold ( threshold ) if existing_metric_path : existing_metric_df = spark . read . csv ( existing_metric_path , header = True , inferSchema = True ) dfs_count = int ( existing_metric_df . select ( F . max ( F . col ( \"idx\" ))) . first ()[ 0 ]) + 1 else : existing_metric_df = None dfs_count = 1 def unionAll ( dfs ): first , * _ = dfs return first . sql_ctx . createDataFrame ( first . sql_ctx . _sc . union ([ df . rdd for df in dfs ]), first . schema ) if persist : for i in range ( len ( idfs )): idfs [ i ] = idfs [ i ] . select ( list_of_cols ) idfs [ i ] . persist ( persist_option ) list_temp_all_col = [] if appended_metric_path : list_append_all = [] for i in list_of_cols : if i in binary_cols : col_type = \"Binary\" else : col_type = \"Numerical\" count_idf = dfs_count list_temp_col_in_idf = [] for idf in idfs : df_stat_each = idf . select ( F . mean ( i ) . alias ( \"mean\" ), F . stddev ( i ) . alias ( \"stddev\" ), ( F . kurtosis ( i ) + F . lit ( 3 )) . alias ( \"kurtosis\" ), ) list_temp_col_in_idf . append ( df_stat_each ) if appended_metric_path : df_append_single = df_stat_each . select ( F . lit ( str ( count_idf )) . alias ( \"idx\" ), F . lit ( str ( i )) . alias ( \"attribute\" ), F . lit ( col_type ) . alias ( \"type\" ), \"mean\" , \"stddev\" , \"kurtosis\" , ) list_append_all . append ( df_append_single ) count_idf += 1 if existing_metric_df : existing_df_for_single_col = existing_metric_df . where ( F . col ( \"attribute\" ) == str ( i ) ) . select ( \"mean\" , \"stddev\" , \"kurtosis\" ) if existing_df_for_single_col . count () > 0 : list_temp_col_in_idf . append ( existing_df_for_single_col ) df_stat_col = ( unionAll ( list_temp_col_in_idf ) . select ( F . stddev ( \"mean\" ) . alias ( \"std_of_mean\" ), F . mean ( \"mean\" ) . alias ( \"mean_of_mean\" ), F . stddev ( \"stddev\" ) . alias ( \"std_of_stddev\" ), F . mean ( \"stddev\" ) . alias ( \"mean_of_stddev\" ), F . stddev ( \"kurtosis\" ) . alias ( \"std_of_kurtosis\" ), F . mean ( \"kurtosis\" ) . alias ( \"mean_of_kurtosis\" ), ) . select ( F . lit ( str ( i )) . alias ( \"attribute\" ), F . lit ( col_type ) . alias ( \"type\" ), F . col ( \"std_of_mean\" ) . alias ( \"mean_stddev\" ), ( F . col ( \"std_of_mean\" ) / F . col ( \"mean_of_mean\" )) . alias ( \"mean_cv\" ), ( F . col ( \"std_of_stddev\" ) / F . col ( \"mean_of_stddev\" )) . alias ( \"stddev_cv\" ), ( F . col ( \"std_of_kurtosis\" ) / F . col ( \"mean_of_kurtosis\" )) . alias ( \"kurtosis_cv\" ), ) ) list_temp_all_col . append ( df_stat_col ) odf = unionAll ( list_temp_all_col ) if appended_metric_path : if existing_metric_df : list_append_all . append ( existing_metric_df ) df_append = unionAll ( list_append_all ) . orderBy ( F . col ( \"idx\" )) df_append . coalesce ( 1 ) . write . csv ( appended_metric_path , header = True , mode = \"overwrite\" ) f_compute_si = F . udf ( compute_si ( metric_weightages ), T . ArrayType ( T . FloatType ())) odf = ( odf . replace ( np . nan , None ) . withColumn ( \"si_array\" , f_compute_si ( \"type\" , \"mean_stddev\" , \"mean_cv\" , \"stddev_cv\" , \"kurtosis_cv\" ), ) . withColumn ( \"mean_si\" , F . col ( \"si_array\" ) . getItem ( 0 )) . withColumn ( \"stddev_si\" , F . col ( \"si_array\" ) . getItem ( 1 )) . withColumn ( \"kurtosis_si\" , F . col ( \"si_array\" ) . getItem ( 2 )) . withColumn ( \"stability_index\" , F . col ( \"si_array\" ) . getItem ( 3 )) . withColumn ( \"flagged\" , F . when ( ( F . col ( \"stability_index\" ) < threshold ) | ( F . col ( \"stability_index\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) . withColumn ( \"mean_stddev\" , F . round ( F . col ( \"mean_stddev\" ), 4 )) . withColumn ( \"mean_cv\" , F . round ( F . col ( \"mean_cv\" ), 4 )) . withColumn ( \"stddev_cv\" , F . round ( F . col ( \"stddev_cv\" ), 4 )) . withColumn ( \"kurtosis_cv\" , F . round ( F . col ( \"kurtosis_cv\" ), 4 )) . drop ( \"si_array\" ) ) if print_impact : logger . info ( \"All Attributes:\" ) odf . show ( len ( list_of_cols )) logger . info ( \"Potential Unstable Attributes:\" ) unstable = odf . where ( F . col ( \"flagged\" ) == 1 ) unstable . show ( unstable . count ()) if persist : for i in range ( len ( idfs )): idfs [ i ] . unpersist () return odf def feature_stability_estimation ( spark , attribute_stats , attribute_transformation , metric_weightages = { \"mean\" : 0.5 , \"stddev\" : 0.3 , \"kurtosis\" : 0.2 }, threshold = 1 , print_impact = False , ): \"\"\" This function is able to estimate the stability index of a new feature composed of certain attributes whose stability metrics are known. For example, the new feature F can be expressed as F = g(X1, X2, \u2026, Xn), where X1, X2, \u2026, Xn represent different attributes and g represents the transformation function. The most straightforward way is to generate the new feature for all periods and calculate its stability index. However, it requires reading all historical data again which can be unrealistic for large datasets. Thus, the objective of this function is to estimate feature stability index without reading historical data. One example can be the following scenario: we have attributes A and B, we have their respective stability statistics from T1 to T7. At T7 we realise we need to generate a new feature: A/B, but we don\u2019t have statistics metrics of A/B from T1 to T6 and this is where this function can be applied to generate an estimation without reading datasets from T1 to T6. The estimation can be broken down into 3 steps. 1. Estimate mean and stddev for the new feature based on attribute metrics (no existing resource found to estimate Feature kurtosis). Estimated mean and stddev are generated for each time period using the formula below according to [1]: ![https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/feature_stability_formulae.png](https://raw.githubusercontent.com/anovos/anovos-docs/main/docs/assets/feature_stability_formulae.png) 2. Calculate Coefficient of variation (CV) for estimated feature mean and stddev. Each CV can be then mapped to an integer between 0 and 4 to generate the metric stability index. 3. Similar to the attribute stability index, each metric is assigned a weightage between 0 and 1, where the default values are 50 for mean, 30% for standard deviation and 20% for kurtosis. Because we are unable to generate kurtosis stability index, its minimum and maximum possible values (0 and 4) are used to output a range for global stability index (GSI): * Lower bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfce * Upper bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfd2 [1] Benaroya, H., Han, S. M., & Nagurka, M. (2005). Probability models in engineering and science (Vol. 192, pp. 168-169). CRC press. Parameters ---------- spark Spark Session attribute_stats Spark dataframe. The intermediate dataframe saved by running function stabilityIndex_computation with schema [idx, attribute, mean, stddev, kurtosis]. It should contain all the attributes used in argument attribute_transformation. attribute_transformation Takes input in dictionary format: each key-value combination represents one new feature. Each key is a string containing all the attributes involved in the new feature seperated by '|'. Each value is the transformation of the attributes in string. For example, {'X|Y|Z': 'X**2+Y/Z', 'A': 'log(A)'} metric_weightages Takes input in dictionary format with keys being the metric name - \"mean\",\"stdev\",\"kurtosis\" and value being the weightage of the metric (between 0 and 1). Sum of all weightages must be 1. (Default value = {\"mean\": 0.5, \"stddev\": 0.3, \"kurtosis\": 0.2}) threshold A column is flagged if the stability index is below the threshold, which varies between 0 and 4. The following criteria can be used to classify stability_index (SI): very unstable: 0\u2264SI<1, unstable: 1\u2264SI<2, marginally stable: 2\u2264SI<3, stable: 3\u2264SI<3.5 and very stable: 3.5\u2264SI\u22644. (Default value = 1) print_impact True, False (Default value = False) This argument is to print out the stability metrics of all newly generated features and potential unstable features. Returns ------- DataFrame [feature_formula, mean_cv, stddev_cv, mean_si, stddev_si, stability_index_lower_bound, stability_index_upper_bound, flagged_lower, flagged_upper]. *_cv is coefficient of variation for each metric. *_si is stability index for each metric. stability_index_lower_bound and stability_index_upper_bound form a range for estimated stability index. flagged_lower and flagged_upper indicate whether the feature is potentially unstable based on the lower and upper bounds for stability index. \"\"\" if ( round ( metric_weightages . get ( \"mean\" , 0 ) + metric_weightages . get ( \"stddev\" , 0 ) + metric_weightages . get ( \"kurtosis\" , 0 ), 3 , ) != 1 ): raise ValueError ( \"Invalid input for metric weightages. Either metric name is incorrect or sum of metric weightages is not 1.0.\" ) def stats_estimation ( attributes , transformation , mean , stddev ): attribute_means = list ( zip ( attributes , mean )) first_dev = [] second_dev = [] est_mean = 0 est_var = 0 for attr , s in zip ( attributes , stddev ): first_dev = sp . diff ( transformation , attr ) second_dev = sp . diff ( transformation , attr , 2 ) est_mean += s ** 2 * second_dev . subs ( attribute_means ) / 2 est_var += s ** 2 * ( first_dev . subs ( attribute_means )) ** 2 transformation = sp . parse_expr ( transformation ) est_mean += transformation . subs ( attribute_means ) return [ float ( est_mean ), float ( est_var )] f_stats_estimation = F . udf ( stats_estimation , T . ArrayType ( T . FloatType ())) index = ( attribute_stats . select ( \"idx\" ) . distinct () . orderBy ( \"idx\" ) . rdd . flatMap ( list ) . collect () ) attribute_names = list ( attribute_transformation . keys ()) transformations = list ( attribute_transformation . values ()) feature_metric = [] for attributes , transformation in zip ( attribute_names , transformations ): attributes = [ x . strip () for x in attributes . split ( \"|\" )] for idx in index : attr_mean_list , attr_stddev_list = [], [] for attr in attributes : df_temp = attribute_stats . where ( ( F . col ( \"idx\" ) == idx ) & ( F . col ( \"attribute\" ) == attr ) ) if df_temp . count () == 0 : raise TypeError ( \"Invalid input for attribute_stats: all involved attributes must have available statistics across all time periods (idx)\" ) attr_mean_list . append ( df_temp . select ( \"mean\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) attr_stddev_list . append ( df_temp . select ( \"stddev\" ) . rdd . flatMap ( lambda x : x ) . collect ()[ 0 ] ) feature_metric . append ( [ idx , transformation , attributes , attr_mean_list , attr_stddev_list ] ) schema = T . StructType ( [ T . StructField ( \"idx\" , T . IntegerType (), True ), T . StructField ( \"transformation\" , T . StringType (), True ), T . StructField ( \"attributes\" , T . ArrayType ( T . StringType ()), True ), T . StructField ( \"attr_mean_list\" , T . ArrayType ( T . FloatType ()), True ), T . StructField ( \"attr_stddev_list\" , T . ArrayType ( T . FloatType ()), True ), ] ) df_feature_metric = ( spark . createDataFrame ( feature_metric , schema = schema ) . withColumn ( \"est_feature_stats\" , f_stats_estimation ( \"attributes\" , \"transformation\" , \"attr_mean_list\" , \"attr_stddev_list\" ), ) . withColumn ( \"est_feature_mean\" , F . col ( \"est_feature_stats\" )[ 0 ]) . withColumn ( \"est_feature_stddev\" , F . sqrt ( F . col ( \"est_feature_stats\" )[ 1 ])) . select ( \"idx\" , \"attributes\" , \"transformation\" , \"est_feature_mean\" , \"est_feature_stddev\" , ) ) output = [] for idx , i in enumerate ( transformations ): i_output = [ i ] for metric in [ \"est_feature_mean\" , \"est_feature_stddev\" ]: metric_stats = ( df_feature_metric . where ( F . col ( \"transformation\" ) == i ) . orderBy ( \"idx\" ) . select ( metric ) . fillna ( np . nan ) . rdd . flatMap ( list ) . collect () ) metric_cv = round ( float ( variation ([ a for a in metric_stats ])), 4 ) i_output . append ( metric_cv ) output . append ( i_output ) schema = T . StructType ( [ T . StructField ( \"feature_formula\" , T . StringType (), True ), T . StructField ( \"mean_cv\" , T . FloatType (), True ), T . StructField ( \"stddev_cv\" , T . FloatType (), True ), ] ) odf = spark . createDataFrame ( output , schema = schema ) def score_cv ( cv , thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): if cv is None : return None else : cv = abs ( cv ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( thresholds ): if cv < thresh : return stability_index [ i ] return stability_index [ - 1 ] f_score_cv = F . udf ( score_cv , T . IntegerType ()) odf = ( odf . replace ( np . nan , None ) . withColumn ( \"mean_si\" , f_score_cv ( F . col ( \"mean_cv\" ))) . withColumn ( \"stddev_si\" , f_score_cv ( F . col ( \"stddev_cv\" ))) . withColumn ( \"stability_index_lower_bound\" , F . round ( F . col ( \"mean_si\" ) * metric_weightages . get ( \"mean\" , 0 ) + F . col ( \"stddev_si\" ) * metric_weightages . get ( \"stddev\" , 0 ), 4 , ), ) . withColumn ( \"stability_index_upper_bound\" , F . round ( F . col ( \"stability_index_lower_bound\" ) + 4 * metric_weightages . get ( \"kurtosis\" , 0 ), 4 , ), ) . withColumn ( \"flagged_lower\" , F . when ( ( F . col ( \"stability_index_lower_bound\" ) < threshold ) | ( F . col ( \"stability_index_lower_bound\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) . withColumn ( \"flagged_upper\" , F . when ( ( F . col ( \"stability_index_upper_bound\" ) < threshold ) | ( F . col ( \"stability_index_upper_bound\" ) . isNull ()), 1 , ) . otherwise ( 0 ), ) ) if print_impact : logger . info ( \"All Features:\" ) odf . show ( len ( attribute_names ), False ) logger . info ( \"Potential Unstable Features Identified by Both Lower and Upper Bounds:\" ) unstable = odf . where ( F . col ( \"flagged_upper\" ) == 1 ) unstable . show ( unstable . count ()) return odf","title":"stability"},{"location":"api/drift_stability/stability.html#functions","text":"def feature_stability_estimation ( spark, attribute_stats, attribute_transformation, metric_weightages={'mean': 0.5, 'stddev': 0.3, 'kurtosis': 0.2}, threshold=1, print_impact=False) This function is able to estimate the stability index of a new feature composed of certain attributes whose stability metrics are known. For example, the new feature F can be expressed as F = g(X1, X2, \u2026, Xn), where X1, X2, \u2026, Xn represent different attributes and g represents the transformation function. The most straightforward way is to generate the new feature for all periods and calculate its stability index. However, it requires reading all historical data again which can be unrealistic for large datasets. Thus, the objective of this function is to estimate feature stability index without reading historical data. One example can be the following scenario: we have attributes A and B, we have their respective stability statistics from T1 to T7. At T7 we realise we need to generate a new feature: A/B, but we don\u2019t have statistics metrics of A/B from T1 to T6 and this is where this function can be applied to generate an estimation without reading datasets from T1 to T6. The estimation can be broken down into 3 steps. 1. Estimate mean and stddev for the new feature based on attribute metrics (no existing resource found to estimate Feature kurtosis). Estimated mean and stddev are generated for each time period using the formula below according to [1]: 2. Calculate Coefficient of variation (CV) for estimated feature mean and stddev. Each CV can be then mapped to an integer between 0 and 4 to generate the metric stability index. 3. Similar to the attribute stability index, each metric is assigned a weightage between 0 and 1, where the default values are 50 for mean, 30% for standard deviation and 20% for kurtosis. Because we are unable to generate kurtosis stability index, its minimum and maximum possible values (0 and 4) are used to output a range for global stability index (GSI): * Lower bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfce * Upper bound of GSI = 0.5\u2217mean stability index + 0.3\u2217stddev stability index + 0.2 \u2217 \ud835\udfd2 [1] Benaroya, H., Han, S. M., & Nagurka, M. (2005). Probability models in engineering and science (Vol. 192, pp. 168-169). CRC press.","title":"Functions"},{"location":"api/drift_stability/validations.html","text":"validations Expand source code from functools import partial , wraps from loguru import logger from anovos.shared.utils import attributeType_segregation def check_list_of_columns ( func = None , columns = \"list_of_cols\" , target_idx : int = 1 , target : str = \"idf_target\" , drop = \"drop_cols\" , ): if func is None : return partial ( check_list_of_columns , columns = columns , target = target , drop = drop ) @wraps ( func ) def validate ( * args , ** kwargs ): logger . debug ( \"check the list of columns\" ) idf_target = kwargs . get ( target , \"\" ) or args [ target_idx ] cols_raw = kwargs . get ( columns , \"all\" ) if isinstance ( cols_raw , str ): if cols_raw == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf_target ) cols = num_cols + cat_cols else : cols = [ x . strip () for x in cols_raw . split ( \"|\" )] elif isinstance ( cols_raw , list ): cols = cols_raw else : raise TypeError ( f \"' { columns } ' must be either a string or a list of strings.\" f \" Received { type ( cols_raw ) } .\" ) drops_raw = kwargs . get ( drop , []) if isinstance ( drops_raw , str ): drops = [ x . strip () for x in drops_raw . split ( \"|\" )] elif isinstance ( drops_raw , list ): drops = drops_raw else : raise TypeError ( f \"' { drop } ' must be either a string or a list of strings. \" f \"Received { type ( drops_raw ) } .\" ) final_cols = list ( set ( e for e in cols if e not in drops )) if not final_cols : raise ValueError ( f \"Empty set of columns is given. Columns to select: { cols } , columns to drop: { drops } .\" ) if any ( x not in idf_target . columns for x in final_cols ): raise ValueError ( f \"Not all columns are in the input dataframe. \" f \"Missing columns: { set ( final_cols ) - set ( idf_target . columns ) } \" ) kwargs [ columns ] = final_cols kwargs [ drop ] = [] return func ( * args , ** kwargs ) return validate def check_distance_method ( func = None , param = \"method_type\" ): if func is None : return partial ( check_distance_method , param = param ) @wraps ( func ) def validate ( * args , ** kwargs ): dist_distance_methods = kwargs . get ( param , \"PSI\" ) if isinstance ( dist_distance_methods , str ): if dist_distance_methods == \"all\" : dist_distance_methods = [ \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ] else : dist_distance_methods = [ x . strip () for x in dist_distance_methods . split ( \"|\" ) ] if any ( x not in ( \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ) for x in dist_distance_methods ): raise TypeError ( f \"Invalid input for { param } \" ) kwargs [ param ] = dist_distance_methods return func ( * args , ** kwargs ) return validate def compute_score ( value , method_type , cv_thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): \"\"\" This function maps CV or SD to a score between 0 and 4. \"\"\" if value is None : return None if method_type == \"cv\" : cv = abs ( value ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( cv_thresholds ): if cv < thresh : return float ( stability_index [ i ]) return float ( stability_index [ - 1 ]) elif method_type == \"sd\" : sd = value if sd <= 0.005 : return 4.0 elif sd <= 0.01 : return round ( - 100 * sd + 4.5 , 1 ) elif sd <= 0.05 : return round ( - 50 * sd + 4 , 1 ) elif sd <= 0.1 : return round ( - 30 * sd + 3 , 1 ) else : return 0.0 else : raise TypeError ( \"method_type must be either 'cv' or 'sd'.\" ) def compute_si ( metric_weightages ): def compute_si_ ( attr_type , mean_stddev , mean_cv , stddev_cv , kurtosis_cv ): if attr_type == \"Binary\" : mean_si = compute_score ( mean_stddev , \"sd\" ) stability_index = mean_si stddev_si , kurtosis_si = None , None else : mean_si = compute_score ( mean_cv , \"cv\" ) stddev_si = compute_score ( stddev_cv , \"cv\" ) kurtosis_si = compute_score ( kurtosis_cv , \"cv\" ) if mean_si is None or stddev_si is None or kurtosis_si is None : stability_index = None else : stability_index = round ( mean_si * metric_weightages . get ( \"mean\" , 0 ) + stddev_si * metric_weightages . get ( \"stddev\" , 0 ) + kurtosis_si * metric_weightages . get ( \"kurtosis\" , 0 ), 4 , ) return [ mean_si , stddev_si , kurtosis_si , stability_index ] return compute_si_ def check_metric_weightages ( metric_weightages ): if ( round ( metric_weightages . get ( \"mean\" , 0 ) + metric_weightages . get ( \"stddev\" , 0 ) + metric_weightages . get ( \"kurtosis\" , 0 ), 3 , ) != 1 ): raise ValueError ( \"Invalid input for metric weightages. Either metric name is incorrect or sum of metric weightages is not 1.0.\" ) def check_threshold ( threshold ): if ( threshold < 0 ) or ( threshold > 4 ): raise ValueError ( \"Invalid input for metric threshold. It must be a number between 0 and 4.\" ) Functions def check_distance_method ( func=None, param='method_type') Expand source code def check_distance_method ( func = None , param = \"method_type\" ): if func is None : return partial ( check_distance_method , param = param ) @wraps ( func ) def validate ( * args , ** kwargs ): dist_distance_methods = kwargs . get ( param , \"PSI\" ) if isinstance ( dist_distance_methods , str ): if dist_distance_methods == \"all\" : dist_distance_methods = [ \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ] else : dist_distance_methods = [ x . strip () for x in dist_distance_methods . split ( \"|\" ) ] if any ( x not in ( \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ) for x in dist_distance_methods ): raise TypeError ( f \"Invalid input for { param } \" ) kwargs [ param ] = dist_distance_methods return func ( * args , ** kwargs ) return validate def check_list_of_columns ( func=None, columns='list_of_cols', target_idx: int = 1, target: str = 'idf_target', drop='drop_cols') Expand source code def check_list_of_columns ( func = None , columns = \"list_of_cols\" , target_idx : int = 1 , target : str = \"idf_target\" , drop = \"drop_cols\" , ): if func is None : return partial ( check_list_of_columns , columns = columns , target = target , drop = drop ) @wraps ( func ) def validate ( * args , ** kwargs ): logger . debug ( \"check the list of columns\" ) idf_target = kwargs . get ( target , \"\" ) or args [ target_idx ] cols_raw = kwargs . get ( columns , \"all\" ) if isinstance ( cols_raw , str ): if cols_raw == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf_target ) cols = num_cols + cat_cols else : cols = [ x . strip () for x in cols_raw . split ( \"|\" )] elif isinstance ( cols_raw , list ): cols = cols_raw else : raise TypeError ( f \"' { columns } ' must be either a string or a list of strings.\" f \" Received { type ( cols_raw ) } .\" ) drops_raw = kwargs . get ( drop , []) if isinstance ( drops_raw , str ): drops = [ x . strip () for x in drops_raw . split ( \"|\" )] elif isinstance ( drops_raw , list ): drops = drops_raw else : raise TypeError ( f \"' { drop } ' must be either a string or a list of strings. \" f \"Received { type ( drops_raw ) } .\" ) final_cols = list ( set ( e for e in cols if e not in drops )) if not final_cols : raise ValueError ( f \"Empty set of columns is given. Columns to select: { cols } , columns to drop: { drops } .\" ) if any ( x not in idf_target . columns for x in final_cols ): raise ValueError ( f \"Not all columns are in the input dataframe. \" f \"Missing columns: { set ( final_cols ) - set ( idf_target . columns ) } \" ) kwargs [ columns ] = final_cols kwargs [ drop ] = [] return func ( * args , ** kwargs ) return validate def check_metric_weightages ( metric_weightages) Expand source code def check_metric_weightages ( metric_weightages ): if ( round ( metric_weightages . get ( \"mean\" , 0 ) + metric_weightages . get ( \"stddev\" , 0 ) + metric_weightages . get ( \"kurtosis\" , 0 ), 3 , ) != 1 ): raise ValueError ( \"Invalid input for metric weightages. Either metric name is incorrect or sum of metric weightages is not 1.0.\" ) def check_threshold ( threshold) Expand source code def check_threshold ( threshold ): if ( threshold < 0 ) or ( threshold > 4 ): raise ValueError ( \"Invalid input for metric threshold. It must be a number between 0 and 4.\" ) def compute_score ( value, method_type, cv_thresholds=[0.03, 0.1, 0.2, 0.5]) This function maps CV or SD to a score between 0 and 4. Expand source code def compute_score ( value , method_type , cv_thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): \"\"\" This function maps CV or SD to a score between 0 and 4. \"\"\" if value is None : return None if method_type == \"cv\" : cv = abs ( value ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( cv_thresholds ): if cv < thresh : return float ( stability_index [ i ]) return float ( stability_index [ - 1 ]) elif method_type == \"sd\" : sd = value if sd <= 0.005 : return 4.0 elif sd <= 0.01 : return round ( - 100 * sd + 4.5 , 1 ) elif sd <= 0.05 : return round ( - 50 * sd + 4 , 1 ) elif sd <= 0.1 : return round ( - 30 * sd + 3 , 1 ) else : return 0.0 else : raise TypeError ( \"method_type must be either 'cv' or 'sd'.\" ) def compute_si ( metric_weightages) Expand source code def compute_si ( metric_weightages ): def compute_si_ ( attr_type , mean_stddev , mean_cv , stddev_cv , kurtosis_cv ): if attr_type == \"Binary\" : mean_si = compute_score ( mean_stddev , \"sd\" ) stability_index = mean_si stddev_si , kurtosis_si = None , None else : mean_si = compute_score ( mean_cv , \"cv\" ) stddev_si = compute_score ( stddev_cv , \"cv\" ) kurtosis_si = compute_score ( kurtosis_cv , \"cv\" ) if mean_si is None or stddev_si is None or kurtosis_si is None : stability_index = None else : stability_index = round ( mean_si * metric_weightages . get ( \"mean\" , 0 ) + stddev_si * metric_weightages . get ( \"stddev\" , 0 ) + kurtosis_si * metric_weightages . get ( \"kurtosis\" , 0 ), 4 , ) return [ mean_si , stddev_si , kurtosis_si , stability_index ] return compute_si_","title":"<code>validations</code>"},{"location":"api/drift_stability/validations.html#validations","text":"Expand source code from functools import partial , wraps from loguru import logger from anovos.shared.utils import attributeType_segregation def check_list_of_columns ( func = None , columns = \"list_of_cols\" , target_idx : int = 1 , target : str = \"idf_target\" , drop = \"drop_cols\" , ): if func is None : return partial ( check_list_of_columns , columns = columns , target = target , drop = drop ) @wraps ( func ) def validate ( * args , ** kwargs ): logger . debug ( \"check the list of columns\" ) idf_target = kwargs . get ( target , \"\" ) or args [ target_idx ] cols_raw = kwargs . get ( columns , \"all\" ) if isinstance ( cols_raw , str ): if cols_raw == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf_target ) cols = num_cols + cat_cols else : cols = [ x . strip () for x in cols_raw . split ( \"|\" )] elif isinstance ( cols_raw , list ): cols = cols_raw else : raise TypeError ( f \"' { columns } ' must be either a string or a list of strings.\" f \" Received { type ( cols_raw ) } .\" ) drops_raw = kwargs . get ( drop , []) if isinstance ( drops_raw , str ): drops = [ x . strip () for x in drops_raw . split ( \"|\" )] elif isinstance ( drops_raw , list ): drops = drops_raw else : raise TypeError ( f \"' { drop } ' must be either a string or a list of strings. \" f \"Received { type ( drops_raw ) } .\" ) final_cols = list ( set ( e for e in cols if e not in drops )) if not final_cols : raise ValueError ( f \"Empty set of columns is given. Columns to select: { cols } , columns to drop: { drops } .\" ) if any ( x not in idf_target . columns for x in final_cols ): raise ValueError ( f \"Not all columns are in the input dataframe. \" f \"Missing columns: { set ( final_cols ) - set ( idf_target . columns ) } \" ) kwargs [ columns ] = final_cols kwargs [ drop ] = [] return func ( * args , ** kwargs ) return validate def check_distance_method ( func = None , param = \"method_type\" ): if func is None : return partial ( check_distance_method , param = param ) @wraps ( func ) def validate ( * args , ** kwargs ): dist_distance_methods = kwargs . get ( param , \"PSI\" ) if isinstance ( dist_distance_methods , str ): if dist_distance_methods == \"all\" : dist_distance_methods = [ \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ] else : dist_distance_methods = [ x . strip () for x in dist_distance_methods . split ( \"|\" ) ] if any ( x not in ( \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ) for x in dist_distance_methods ): raise TypeError ( f \"Invalid input for { param } \" ) kwargs [ param ] = dist_distance_methods return func ( * args , ** kwargs ) return validate def compute_score ( value , method_type , cv_thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): \"\"\" This function maps CV or SD to a score between 0 and 4. \"\"\" if value is None : return None if method_type == \"cv\" : cv = abs ( value ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( cv_thresholds ): if cv < thresh : return float ( stability_index [ i ]) return float ( stability_index [ - 1 ]) elif method_type == \"sd\" : sd = value if sd <= 0.005 : return 4.0 elif sd <= 0.01 : return round ( - 100 * sd + 4.5 , 1 ) elif sd <= 0.05 : return round ( - 50 * sd + 4 , 1 ) elif sd <= 0.1 : return round ( - 30 * sd + 3 , 1 ) else : return 0.0 else : raise TypeError ( \"method_type must be either 'cv' or 'sd'.\" ) def compute_si ( metric_weightages ): def compute_si_ ( attr_type , mean_stddev , mean_cv , stddev_cv , kurtosis_cv ): if attr_type == \"Binary\" : mean_si = compute_score ( mean_stddev , \"sd\" ) stability_index = mean_si stddev_si , kurtosis_si = None , None else : mean_si = compute_score ( mean_cv , \"cv\" ) stddev_si = compute_score ( stddev_cv , \"cv\" ) kurtosis_si = compute_score ( kurtosis_cv , \"cv\" ) if mean_si is None or stddev_si is None or kurtosis_si is None : stability_index = None else : stability_index = round ( mean_si * metric_weightages . get ( \"mean\" , 0 ) + stddev_si * metric_weightages . get ( \"stddev\" , 0 ) + kurtosis_si * metric_weightages . get ( \"kurtosis\" , 0 ), 4 , ) return [ mean_si , stddev_si , kurtosis_si , stability_index ] return compute_si_ def check_metric_weightages ( metric_weightages ): if ( round ( metric_weightages . get ( \"mean\" , 0 ) + metric_weightages . get ( \"stddev\" , 0 ) + metric_weightages . get ( \"kurtosis\" , 0 ), 3 , ) != 1 ): raise ValueError ( \"Invalid input for metric weightages. Either metric name is incorrect or sum of metric weightages is not 1.0.\" ) def check_threshold ( threshold ): if ( threshold < 0 ) or ( threshold > 4 ): raise ValueError ( \"Invalid input for metric threshold. It must be a number between 0 and 4.\" )","title":"validations"},{"location":"api/drift_stability/validations.html#functions","text":"def check_distance_method ( func=None, param='method_type') Expand source code def check_distance_method ( func = None , param = \"method_type\" ): if func is None : return partial ( check_distance_method , param = param ) @wraps ( func ) def validate ( * args , ** kwargs ): dist_distance_methods = kwargs . get ( param , \"PSI\" ) if isinstance ( dist_distance_methods , str ): if dist_distance_methods == \"all\" : dist_distance_methods = [ \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ] else : dist_distance_methods = [ x . strip () for x in dist_distance_methods . split ( \"|\" ) ] if any ( x not in ( \"PSI\" , \"JSD\" , \"HD\" , \"KS\" ) for x in dist_distance_methods ): raise TypeError ( f \"Invalid input for { param } \" ) kwargs [ param ] = dist_distance_methods return func ( * args , ** kwargs ) return validate def check_list_of_columns ( func=None, columns='list_of_cols', target_idx: int = 1, target: str = 'idf_target', drop='drop_cols') Expand source code def check_list_of_columns ( func = None , columns = \"list_of_cols\" , target_idx : int = 1 , target : str = \"idf_target\" , drop = \"drop_cols\" , ): if func is None : return partial ( check_list_of_columns , columns = columns , target = target , drop = drop ) @wraps ( func ) def validate ( * args , ** kwargs ): logger . debug ( \"check the list of columns\" ) idf_target = kwargs . get ( target , \"\" ) or args [ target_idx ] cols_raw = kwargs . get ( columns , \"all\" ) if isinstance ( cols_raw , str ): if cols_raw == \"all\" : num_cols , cat_cols , other_cols = attributeType_segregation ( idf_target ) cols = num_cols + cat_cols else : cols = [ x . strip () for x in cols_raw . split ( \"|\" )] elif isinstance ( cols_raw , list ): cols = cols_raw else : raise TypeError ( f \"' { columns } ' must be either a string or a list of strings.\" f \" Received { type ( cols_raw ) } .\" ) drops_raw = kwargs . get ( drop , []) if isinstance ( drops_raw , str ): drops = [ x . strip () for x in drops_raw . split ( \"|\" )] elif isinstance ( drops_raw , list ): drops = drops_raw else : raise TypeError ( f \"' { drop } ' must be either a string or a list of strings. \" f \"Received { type ( drops_raw ) } .\" ) final_cols = list ( set ( e for e in cols if e not in drops )) if not final_cols : raise ValueError ( f \"Empty set of columns is given. Columns to select: { cols } , columns to drop: { drops } .\" ) if any ( x not in idf_target . columns for x in final_cols ): raise ValueError ( f \"Not all columns are in the input dataframe. \" f \"Missing columns: { set ( final_cols ) - set ( idf_target . columns ) } \" ) kwargs [ columns ] = final_cols kwargs [ drop ] = [] return func ( * args , ** kwargs ) return validate def check_metric_weightages ( metric_weightages) Expand source code def check_metric_weightages ( metric_weightages ): if ( round ( metric_weightages . get ( \"mean\" , 0 ) + metric_weightages . get ( \"stddev\" , 0 ) + metric_weightages . get ( \"kurtosis\" , 0 ), 3 , ) != 1 ): raise ValueError ( \"Invalid input for metric weightages. Either metric name is incorrect or sum of metric weightages is not 1.0.\" ) def check_threshold ( threshold) Expand source code def check_threshold ( threshold ): if ( threshold < 0 ) or ( threshold > 4 ): raise ValueError ( \"Invalid input for metric threshold. It must be a number between 0 and 4.\" ) def compute_score ( value, method_type, cv_thresholds=[0.03, 0.1, 0.2, 0.5]) This function maps CV or SD to a score between 0 and 4. Expand source code def compute_score ( value , method_type , cv_thresholds = [ 0.03 , 0.1 , 0.2 , 0.5 ]): \"\"\" This function maps CV or SD to a score between 0 and 4. \"\"\" if value is None : return None if method_type == \"cv\" : cv = abs ( value ) stability_index = [ 4 , 3 , 2 , 1 , 0 ] for i , thresh in enumerate ( cv_thresholds ): if cv < thresh : return float ( stability_index [ i ]) return float ( stability_index [ - 1 ]) elif method_type == \"sd\" : sd = value if sd <= 0.005 : return 4.0 elif sd <= 0.01 : return round ( - 100 * sd + 4.5 , 1 ) elif sd <= 0.05 : return round ( - 50 * sd + 4 , 1 ) elif sd <= 0.1 : return round ( - 30 * sd + 3 , 1 ) else : return 0.0 else : raise TypeError ( \"method_type must be either 'cv' or 'sd'.\" ) def compute_si ( metric_weightages) Expand source code def compute_si ( metric_weightages ): def compute_si_ ( attr_type , mean_stddev , mean_cv , stddev_cv , kurtosis_cv ): if attr_type == \"Binary\" : mean_si = compute_score ( mean_stddev , \"sd\" ) stability_index = mean_si stddev_si , kurtosis_si = None , None else : mean_si = compute_score ( mean_cv , \"cv\" ) stddev_si = compute_score ( stddev_cv , \"cv\" ) kurtosis_si = compute_score ( kurtosis_cv , \"cv\" ) if mean_si is None or stddev_si is None or kurtosis_si is None : stability_index = None else : stability_index = round ( mean_si * metric_weightages . get ( \"mean\" , 0 ) + stddev_si * metric_weightages . get ( \"stddev\" , 0 ) + kurtosis_si * metric_weightages . get ( \"kurtosis\" , 0 ), 4 , ) return [ mean_si , stddev_si , kurtosis_si , stability_index ] return compute_si_","title":"Functions"},{"location":"api/feature_recommender/_index.html","text":"Overview Sub-modules anovos.feature_recommender.featrec_init anovos.feature_recommender.feature_explorer Feature explorer helps list down the potential features from our corpus based on user defined industry or/and use case. anovos.feature_recommender.feature_mapper Feature mapper maps attributes to features based on ingested data dictionary by the user.","title":"Overview"},{"location":"api/feature_recommender/_index.html#overview","text":"","title":"Overview"},{"location":"api/feature_recommender/_index.html#sub-modules","text":"anovos.feature_recommender.featrec_init anovos.feature_recommender.feature_explorer Feature explorer helps list down the potential features from our corpus based on user defined industry or/and use case. anovos.feature_recommender.feature_mapper Feature mapper maps attributes to features based on ingested data dictionary by the user.","title":"Sub-modules"},{"location":"api/feature_recommender/featrec_init.html","text":"featrec_init Expand source code import copy import os import site from re import finditer import pandas as pd from sentence_transformers import SentenceTransformer from torch.hub import _get_torch_home def detect_model_path (): \"\"\" Returns ------- Local Feature Explorer and Recommender semantic model path (if the model is pre-downloaded) \"\"\" transformers_path = os . getenv ( \"SENTENCE_TRANSFORMERS_HOME\" ) if transformers_path is None : try : torch_home = _get_torch_home () except ImportError : torch_home = os . path . expanduser ( os . getenv ( \"TORCH_HOME\" , os . path . join ( os . getenv ( \"XDG_CACHE_HOME\" , \"~/.cache\" ), \"torch\" ), ) ) transformers_path = os . path . join ( torch_home , \"sentence_transformers\" ) model_path = os . path . join ( transformers_path , \"sentence-transformers_all-mpnet-base-v2\" ) return model_path def model_download (): print ( \"Starting the Semantic Model download\" ) SentenceTransformer ( \"all-mpnet-base-v2\" ) print ( \"Model downloading finished\" ) class _TransformerModel : def __init__ ( self ): self . _model = None @property def model ( self ) -> SentenceTransformer : if self . _model is None : model_path = detect_model_path () if os . path . exists ( model_path ): self . _model = SentenceTransformer ( model_path ) else : raise FileNotFoundError ( \"Model has not been downloaded. Please use model_download() function to download the model first\" ) return self . _model model_fer = _TransformerModel () def init_input_fer (): \"\"\" Returns ------- Loading the Feature Explorer and Recommender (FER) Input DataFrame (FER corpus) \"\"\" local_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"data\" , \"flatten_fr_db.csv\" ) if os . path . exists ( local_path ): input_path_fer = local_path else : site_path = site . getsitepackages ()[ 0 ] input_path_fer = os . path . join ( site_path , \"anovos/feature_recommender/data/flatten_fr_db.csv\" ) df_input_fer = pd . read_csv ( input_path_fer ) return df_input_fer def get_column_name ( df ): \"\"\" Parameters ---------- df Input DataFrame Returns ------- feature_name_column Column name of Feature Name in the input DataFrame (string) feature_desc_column Column name of Feature Description in the input DataFrame (string) industry_column Column name of Industry in the input DataFrame (string) usecase_column Column name of Usecase in the input DataFrame (string) \"\"\" feature_name_column = str ( df . columns . tolist ()[ 0 ]) feature_desc_column = str ( df . columns . tolist ()[ 1 ]) industry_column = str ( df . columns . tolist ()[ 2 ]) usecase_column = str ( df . columns . tolist ()[ 3 ]) return ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) def camel_case_split ( input ): \"\"\" Parameters ---------- input Input (string) which requires cleaning Returns ------- \"\"\" processed_input = \"\" matches = finditer ( \".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)\" , input ) for m in matches : processed_input += str ( m . group ( 0 )) + str ( \" \" ) return processed_input def recommendation_data_prep ( df , name_column , desc_column ): \"\"\" Parameters ---------- df Input DataFrame name_column Column name of Input DataFrame attribute/ feature name (string) desc_column Column name of Input DataFrame attribute/ feature description (string) Returns ------- list_corpus List of prepared data for Feature Recommender functions return df_prep Processed DataFrame for Feature Recommender functions \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if name_column not in df . columns and name_column is not None : raise TypeError ( \"Invalid input for name_column\" ) if desc_column not in df . columns and desc_column is not None : raise TypeError ( \"Invalid input for desc_column\" ) if name_column is None and desc_column is None : raise TypeError ( \"Need at least one input for either name_column or desc_column\" ) df_prep = copy . deepcopy ( df ) if name_column is None : df_prep [ desc_column ] = df_prep [ desc_column ] . astype ( str ) df_prep_com = df_prep [ desc_column ] elif desc_column is None : df_prep [ name_column ] = df_prep [ name_column ] . astype ( str ) df_prep_com = df_prep [ name_column ] else : df_prep [ name_column ] = df_prep [ name_column ] . str . replace ( \"_\" , \" \" ) df_prep [ name_column ] = df_prep [ name_column ] . astype ( str ) df_prep [ desc_column ] = df_prep [ desc_column ] . astype ( str ) df_prep_com = df_prep [[ name_column , desc_column ]] . agg ( \" \" . join , axis = 1 ) df_prep_com = df_prep_com . replace ({ \"[^A-Za-z0-9 ]+\" : \" \" }, regex = True ) for i in range ( len ( df_prep_com )): df_prep_com [ i ] = df_prep_com [ i ] . strip () df_prep_com [ i ] = camel_case_split ( df_prep_com [ i ]) list_corpus = df_prep_com . to_list () return list_corpus , df_prep def feature_exploration_prep (): \"\"\" Returns ------- df_input_fer DataFrame used in Feature Exploration functions \"\"\" df_input_fer = init_input_fer () df_input_fer = df_input_fer . rename ( columns = lambda x : x . strip () . replace ( \" \" , \"_\" )) return df_input_fer def feature_recommendation_prep (): \"\"\" Returns ------- list_train_fer List of prepared data for Feature Recommendation functions df_red_fer DataFrame used in Feature Recommendation functions list_embedding_train_fer List of embedding tensor for Feature Recommendation functions \"\"\" df_input_fer = init_input_fer () ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_input_fer ) df_groupby_fer = ( df_input_fer . groupby ([ feature_name_column , feature_desc_column ]) . agg ( { industry_column : lambda x : \", \" . join ( set ( x . dropna ())), usecase_column : lambda x : \", \" . join ( set ( x . dropna ())), } ) . reset_index () ) list_train_fer , df_rec_fer = recommendation_data_prep ( df_groupby_fer , feature_name_column , feature_name_column ) return list_train_fer , df_rec_fer class EmbeddingsTrainFer : def __init__ ( self , list_train_fer ): self . list_train_fer = list_train_fer self . _embeddings = None @property def get ( self ): if self . _embeddings is None : self . _embeddings = model_fer . model . encode ( self . list_train_fer , convert_to_tensor = True ) return self . _embeddings Functions def camel_case_split ( input) Parameters input Input (string) which requires cleaning Returns Expand source code def camel_case_split ( input ): \"\"\" Parameters ---------- input Input (string) which requires cleaning Returns ------- \"\"\" processed_input = \"\" matches = finditer ( \".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)\" , input ) for m in matches : processed_input += str ( m . group ( 0 )) + str ( \" \" ) return processed_input def detect_model_path ( ) Returns Local Feature Explorer and Recommender semantic model path (if the model is pre-downloaded) Expand source code def detect_model_path (): \"\"\" Returns ------- Local Feature Explorer and Recommender semantic model path (if the model is pre-downloaded) \"\"\" transformers_path = os . getenv ( \"SENTENCE_TRANSFORMERS_HOME\" ) if transformers_path is None : try : torch_home = _get_torch_home () except ImportError : torch_home = os . path . expanduser ( os . getenv ( \"TORCH_HOME\" , os . path . join ( os . getenv ( \"XDG_CACHE_HOME\" , \"~/.cache\" ), \"torch\" ), ) ) transformers_path = os . path . join ( torch_home , \"sentence_transformers\" ) model_path = os . path . join ( transformers_path , \"sentence-transformers_all-mpnet-base-v2\" ) return model_path def feature_exploration_prep ( ) Returns df_input_fer DataFrame used in Feature Exploration functions Expand source code def feature_exploration_prep (): \"\"\" Returns ------- df_input_fer DataFrame used in Feature Exploration functions \"\"\" df_input_fer = init_input_fer () df_input_fer = df_input_fer . rename ( columns = lambda x : x . strip () . replace ( \" \" , \"_\" )) return df_input_fer def feature_recommendation_prep ( ) Returns list_train_fer List of prepared data for Feature Recommendation functions df_red_fer DataFrame used in Feature Recommendation functions list_embedding_train_fer List of embedding tensor for Feature Recommendation functions Expand source code def feature_recommendation_prep (): \"\"\" Returns ------- list_train_fer List of prepared data for Feature Recommendation functions df_red_fer DataFrame used in Feature Recommendation functions list_embedding_train_fer List of embedding tensor for Feature Recommendation functions \"\"\" df_input_fer = init_input_fer () ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_input_fer ) df_groupby_fer = ( df_input_fer . groupby ([ feature_name_column , feature_desc_column ]) . agg ( { industry_column : lambda x : \", \" . join ( set ( x . dropna ())), usecase_column : lambda x : \", \" . join ( set ( x . dropna ())), } ) . reset_index () ) list_train_fer , df_rec_fer = recommendation_data_prep ( df_groupby_fer , feature_name_column , feature_name_column ) return list_train_fer , df_rec_fer def get_column_name ( df) Parameters df Input DataFrame Returns feature_name_column Column name of Feature Name in the input DataFrame (string) feature_desc_column Column name of Feature Description in the input DataFrame (string) industry_column Column name of Industry in the input DataFrame (string) usecase_column Column name of Usecase in the input DataFrame (string) Expand source code def get_column_name ( df ): \"\"\" Parameters ---------- df Input DataFrame Returns ------- feature_name_column Column name of Feature Name in the input DataFrame (string) feature_desc_column Column name of Feature Description in the input DataFrame (string) industry_column Column name of Industry in the input DataFrame (string) usecase_column Column name of Usecase in the input DataFrame (string) \"\"\" feature_name_column = str ( df . columns . tolist ()[ 0 ]) feature_desc_column = str ( df . columns . tolist ()[ 1 ]) industry_column = str ( df . columns . tolist ()[ 2 ]) usecase_column = str ( df . columns . tolist ()[ 3 ]) return ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) def init_input_fer ( ) Returns Loading the Feature Explorer and Recommender (FER) Input DataFrame (FER corpus) Expand source code def init_input_fer (): \"\"\" Returns ------- Loading the Feature Explorer and Recommender (FER) Input DataFrame (FER corpus) \"\"\" local_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"data\" , \"flatten_fr_db.csv\" ) if os . path . exists ( local_path ): input_path_fer = local_path else : site_path = site . getsitepackages ()[ 0 ] input_path_fer = os . path . join ( site_path , \"anovos/feature_recommender/data/flatten_fr_db.csv\" ) df_input_fer = pd . read_csv ( input_path_fer ) return df_input_fer def model_download ( ) Expand source code def model_download (): print ( \"Starting the Semantic Model download\" ) SentenceTransformer ( \"all-mpnet-base-v2\" ) print ( \"Model downloading finished\" ) def recommendation_data_prep ( df, name_column, desc_column) Parameters df Input DataFrame name_column Column name of Input DataFrame attribute/ feature name (string) desc_column Column name of Input DataFrame attribute/ feature description (string) Returns list_corpus List of prepared data for Feature Recommender functions return df_prep Processed DataFrame for Feature Recommender functions Expand source code def recommendation_data_prep ( df , name_column , desc_column ): \"\"\" Parameters ---------- df Input DataFrame name_column Column name of Input DataFrame attribute/ feature name (string) desc_column Column name of Input DataFrame attribute/ feature description (string) Returns ------- list_corpus List of prepared data for Feature Recommender functions return df_prep Processed DataFrame for Feature Recommender functions \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if name_column not in df . columns and name_column is not None : raise TypeError ( \"Invalid input for name_column\" ) if desc_column not in df . columns and desc_column is not None : raise TypeError ( \"Invalid input for desc_column\" ) if name_column is None and desc_column is None : raise TypeError ( \"Need at least one input for either name_column or desc_column\" ) df_prep = copy . deepcopy ( df ) if name_column is None : df_prep [ desc_column ] = df_prep [ desc_column ] . astype ( str ) df_prep_com = df_prep [ desc_column ] elif desc_column is None : df_prep [ name_column ] = df_prep [ name_column ] . astype ( str ) df_prep_com = df_prep [ name_column ] else : df_prep [ name_column ] = df_prep [ name_column ] . str . replace ( \"_\" , \" \" ) df_prep [ name_column ] = df_prep [ name_column ] . astype ( str ) df_prep [ desc_column ] = df_prep [ desc_column ] . astype ( str ) df_prep_com = df_prep [[ name_column , desc_column ]] . agg ( \" \" . join , axis = 1 ) df_prep_com = df_prep_com . replace ({ \"[^A-Za-z0-9 ]+\" : \" \" }, regex = True ) for i in range ( len ( df_prep_com )): df_prep_com [ i ] = df_prep_com [ i ] . strip () df_prep_com [ i ] = camel_case_split ( df_prep_com [ i ]) list_corpus = df_prep_com . to_list () return list_corpus , df_prep Classes class EmbeddingsTrainFer ( list_train_fer) Expand source code class EmbeddingsTrainFer : def __init__ ( self , list_train_fer ): self . list_train_fer = list_train_fer self . _embeddings = None @property def get ( self ): if self . _embeddings is None : self . _embeddings = model_fer . model . encode ( self . list_train_fer , convert_to_tensor = True ) return self . _embeddings Instance variables var get Expand source code @property def get ( self ): if self . _embeddings is None : self . _embeddings = model_fer . model . encode ( self . list_train_fer , convert_to_tensor = True ) return self . _embeddings","title":"<code>featrec_init</code>"},{"location":"api/feature_recommender/featrec_init.html#featrec_init","text":"Expand source code import copy import os import site from re import finditer import pandas as pd from sentence_transformers import SentenceTransformer from torch.hub import _get_torch_home def detect_model_path (): \"\"\" Returns ------- Local Feature Explorer and Recommender semantic model path (if the model is pre-downloaded) \"\"\" transformers_path = os . getenv ( \"SENTENCE_TRANSFORMERS_HOME\" ) if transformers_path is None : try : torch_home = _get_torch_home () except ImportError : torch_home = os . path . expanduser ( os . getenv ( \"TORCH_HOME\" , os . path . join ( os . getenv ( \"XDG_CACHE_HOME\" , \"~/.cache\" ), \"torch\" ), ) ) transformers_path = os . path . join ( torch_home , \"sentence_transformers\" ) model_path = os . path . join ( transformers_path , \"sentence-transformers_all-mpnet-base-v2\" ) return model_path def model_download (): print ( \"Starting the Semantic Model download\" ) SentenceTransformer ( \"all-mpnet-base-v2\" ) print ( \"Model downloading finished\" ) class _TransformerModel : def __init__ ( self ): self . _model = None @property def model ( self ) -> SentenceTransformer : if self . _model is None : model_path = detect_model_path () if os . path . exists ( model_path ): self . _model = SentenceTransformer ( model_path ) else : raise FileNotFoundError ( \"Model has not been downloaded. Please use model_download() function to download the model first\" ) return self . _model model_fer = _TransformerModel () def init_input_fer (): \"\"\" Returns ------- Loading the Feature Explorer and Recommender (FER) Input DataFrame (FER corpus) \"\"\" local_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"data\" , \"flatten_fr_db.csv\" ) if os . path . exists ( local_path ): input_path_fer = local_path else : site_path = site . getsitepackages ()[ 0 ] input_path_fer = os . path . join ( site_path , \"anovos/feature_recommender/data/flatten_fr_db.csv\" ) df_input_fer = pd . read_csv ( input_path_fer ) return df_input_fer def get_column_name ( df ): \"\"\" Parameters ---------- df Input DataFrame Returns ------- feature_name_column Column name of Feature Name in the input DataFrame (string) feature_desc_column Column name of Feature Description in the input DataFrame (string) industry_column Column name of Industry in the input DataFrame (string) usecase_column Column name of Usecase in the input DataFrame (string) \"\"\" feature_name_column = str ( df . columns . tolist ()[ 0 ]) feature_desc_column = str ( df . columns . tolist ()[ 1 ]) industry_column = str ( df . columns . tolist ()[ 2 ]) usecase_column = str ( df . columns . tolist ()[ 3 ]) return ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) def camel_case_split ( input ): \"\"\" Parameters ---------- input Input (string) which requires cleaning Returns ------- \"\"\" processed_input = \"\" matches = finditer ( \".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)\" , input ) for m in matches : processed_input += str ( m . group ( 0 )) + str ( \" \" ) return processed_input def recommendation_data_prep ( df , name_column , desc_column ): \"\"\" Parameters ---------- df Input DataFrame name_column Column name of Input DataFrame attribute/ feature name (string) desc_column Column name of Input DataFrame attribute/ feature description (string) Returns ------- list_corpus List of prepared data for Feature Recommender functions return df_prep Processed DataFrame for Feature Recommender functions \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if name_column not in df . columns and name_column is not None : raise TypeError ( \"Invalid input for name_column\" ) if desc_column not in df . columns and desc_column is not None : raise TypeError ( \"Invalid input for desc_column\" ) if name_column is None and desc_column is None : raise TypeError ( \"Need at least one input for either name_column or desc_column\" ) df_prep = copy . deepcopy ( df ) if name_column is None : df_prep [ desc_column ] = df_prep [ desc_column ] . astype ( str ) df_prep_com = df_prep [ desc_column ] elif desc_column is None : df_prep [ name_column ] = df_prep [ name_column ] . astype ( str ) df_prep_com = df_prep [ name_column ] else : df_prep [ name_column ] = df_prep [ name_column ] . str . replace ( \"_\" , \" \" ) df_prep [ name_column ] = df_prep [ name_column ] . astype ( str ) df_prep [ desc_column ] = df_prep [ desc_column ] . astype ( str ) df_prep_com = df_prep [[ name_column , desc_column ]] . agg ( \" \" . join , axis = 1 ) df_prep_com = df_prep_com . replace ({ \"[^A-Za-z0-9 ]+\" : \" \" }, regex = True ) for i in range ( len ( df_prep_com )): df_prep_com [ i ] = df_prep_com [ i ] . strip () df_prep_com [ i ] = camel_case_split ( df_prep_com [ i ]) list_corpus = df_prep_com . to_list () return list_corpus , df_prep def feature_exploration_prep (): \"\"\" Returns ------- df_input_fer DataFrame used in Feature Exploration functions \"\"\" df_input_fer = init_input_fer () df_input_fer = df_input_fer . rename ( columns = lambda x : x . strip () . replace ( \" \" , \"_\" )) return df_input_fer def feature_recommendation_prep (): \"\"\" Returns ------- list_train_fer List of prepared data for Feature Recommendation functions df_red_fer DataFrame used in Feature Recommendation functions list_embedding_train_fer List of embedding tensor for Feature Recommendation functions \"\"\" df_input_fer = init_input_fer () ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_input_fer ) df_groupby_fer = ( df_input_fer . groupby ([ feature_name_column , feature_desc_column ]) . agg ( { industry_column : lambda x : \", \" . join ( set ( x . dropna ())), usecase_column : lambda x : \", \" . join ( set ( x . dropna ())), } ) . reset_index () ) list_train_fer , df_rec_fer = recommendation_data_prep ( df_groupby_fer , feature_name_column , feature_name_column ) return list_train_fer , df_rec_fer class EmbeddingsTrainFer : def __init__ ( self , list_train_fer ): self . list_train_fer = list_train_fer self . _embeddings = None @property def get ( self ): if self . _embeddings is None : self . _embeddings = model_fer . model . encode ( self . list_train_fer , convert_to_tensor = True ) return self . _embeddings","title":"featrec_init"},{"location":"api/feature_recommender/featrec_init.html#functions","text":"def camel_case_split ( input)","title":"Functions"},{"location":"api/feature_recommender/featrec_init.html#_1","text":"Classes class EmbeddingsTrainFer ( list_train_fer) Expand source code class EmbeddingsTrainFer : def __init__ ( self , list_train_fer ): self . list_train_fer = list_train_fer self . _embeddings = None @property def get ( self ): if self . _embeddings is None : self . _embeddings = model_fer . model . encode ( self . list_train_fer , convert_to_tensor = True ) return self . _embeddings","title":""},{"location":"api/feature_recommender/feature_explorer.html","text":"feature_explorer Feature explorer helps list down the potential features from our corpus based on user defined industry or/and use case. Expand source code \"\"\"Feature explorer helps list down the potential features from our corpus based on user defined industry or/and use case. \"\"\" import numpy as np import pandas as pd from sentence_transformers import util from anovos.feature_recommender.featrec_init import ( feature_exploration_prep , get_column_name , model_fer , ) df_input_fer = feature_exploration_prep () ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_input_fer ) def list_all_industry (): \"\"\" Lists down all the Industries that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported industries as part of feature exploration/recommendation \"\"\" odf_uni = df_input_fer . iloc [:, 2 ] . unique () odf = pd . DataFrame ( odf_uni , columns = [ \"Industry\" ]) return odf def list_all_usecase (): \"\"\" Lists down all the Use cases that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported usecases as part of feature exploration/recommendation \"\"\" odf_uni = df_input_fer . iloc [:, 3 ] . unique () odf = pd . DataFrame ( odf_uni , columns = [ \"Usecase\" ]) return odf def list_all_pair (): \"\"\" Lists down all the Industry/Use case pairs that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported Industry/Usecase pairs as part of feature exploration/recommendation \"\"\" odf = df_input_fer . iloc [:, [ 2 , 3 ]] . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def process_usecase ( usecase : str , semantic : bool ): \"\"\" Parameters ---------- usecase : str Input usecase semantic : bool Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" if type ( semantic ) != bool : raise TypeError ( \"Invalid input for semantic\" ) if type ( usecase ) != str : raise TypeError ( \"Invalid input for usecase\" ) usecase = usecase . lower () . strip () usecase = usecase . replace ( \"[^A-Za-z0-9 ]+\" , \" \" ) all_usecase = list_all_usecase ()[ \"Usecase\" ] . to_list () if semantic and usecase not in all_usecase : all_usecase_embeddings = model_fer . model . encode ( all_usecase , convert_to_tensor = True ) usecase_embeddings = model_fer . model . encode ( usecase , convert_to_tensor = True ) cos_scores = util . pytorch_cos_sim ( usecase_embeddings , all_usecase_embeddings )[ 0 ] first_match_index = int ( np . argpartition ( - cos_scores , 0 )[ 0 ]) processed_usecase = all_usecase [ first_match_index ] print ( \"Given input Usecase is not available. Showing the most semantically relevant Usecase result: \" , processed_usecase , ) else : processed_usecase = usecase return processed_usecase def process_industry ( industry : str , semantic : bool ): \"\"\" Parameters ---------- industry : str Input industry semantic : bool Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" if type ( semantic ) != bool : raise TypeError ( \"Invalid input for semantic\" ) if type ( industry ) != str : raise TypeError ( \"Invalid input for industry\" ) industry = industry . lower () . strip () industry = industry . replace ( \"[^A-Za-z0-9 ]+\" , \" \" ) all_industry = list_all_industry ()[ \"Industry\" ] . to_list () if semantic and industry not in all_industry : all_industry_embeddings = model_fer . model . encode ( all_industry , convert_to_tensor = True ) industry_embeddings = model_fer . model . encode ( industry , convert_to_tensor = True ) cos_scores = util . pytorch_cos_sim ( industry_embeddings , all_industry_embeddings )[ 0 ] first_match_index = int ( np . argpartition ( - cos_scores , 0 )[ 0 ]) processed_industry = all_industry [ first_match_index ] print ( \"Given input Industry is not available. Showing the most semantically relevant Industry result: \" , processed_industry , ) else : processed_industry = industry return processed_industry def list_usecase_by_industry ( industry , semantic = True ): \"\"\" Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters ---------- industry : str Input industry semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" industry = process_industry ( industry , semantic ) odf = pd . DataFrame ( df_input_fer . loc [ df_input_fer . iloc [:, 2 ] == industry ] . iloc [:, 3 ]) odf = odf . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_industry_by_usecase ( usecase , semantic = True ): \"\"\" Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters ---------- usecase : str Input usecase semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" usecase = process_usecase ( usecase , semantic ) odf = pd . DataFrame ( df_input_fer . loc [ df_input_fer . iloc [:, 3 ] == usecase ] . iloc [:, 2 ]) odf = odf . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_feature_by_industry ( industry , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Industry. Parameters ---------- industry : str Input industry num_of_feat : int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Usecases' Feature Popularity to the Input Industry. \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) industry = process_industry ( industry , semantic ) odf = df_input_fer . loc [ df_input_fer . iloc [:, 2 ] == industry ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) if len ( odf ) > 0 : odf [ \"count\" ] = odf . groupby ( usecase_column )[ usecase_column ] . transform ( \"count\" ) odf . sort_values ( \"count\" , inplace = True , ascending = False ) odf = odf . drop ( \"count\" , axis = 1 ) if num_of_feat != \"all\" : odf = odf . head ( num_of_feat ) . reset_index ( drop = True ) else : odf = odf . reset_index ( drop = True ) return odf def list_feature_by_usecase ( usecase , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Usecase. Parameters ---------- usecase : str Input usecase num_of_feat : int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Industries' Feature Popularity to the Input Usecase. \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) usecase = process_usecase ( usecase , semantic ) odf = df_input_fer . loc [ df_input_fer . iloc [:, 3 ] == usecase ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) if len ( odf ) > 0 : odf [ \"count\" ] = odf . groupby ( industry_column )[ industry_column ] . transform ( \"count\" ) odf . sort_values ( \"count\" , inplace = True , ascending = False ) odf = odf . drop ( \"count\" , axis = 1 ) if num_of_feat != \"all\" : odf = odf . head ( num_of_feat ) . reset_index ( drop = True ) else : odf = odf . reset_index ( drop = True ) return odf def list_feature_by_pair ( industry , usecase , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Industry/Usecase pair Parameters ---------- industry Input industry (string) usecase Input usecase (string) num_of_feat Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic Input semantic (boolean) - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) industry = process_industry ( industry , semantic ) usecase = process_usecase ( usecase , semantic ) if num_of_feat != \"all\" : odf = ( df_input_fer . loc [ ( df_input_fer . iloc [:, 2 ] == industry ) & ( df_input_fer . iloc [:, 3 ] == usecase ) ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) . head ( num_of_feat ) ) else : odf = df_input_fer . loc [ ( df_input_fer . iloc [:, 2 ] == industry ) & ( df_input_fer . iloc [:, 3 ] == usecase ) ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf Functions def list_all_industry ( ) Lists down all the Industries that are supported in Feature Recommender module. Returns DataFrame of all the supported industries as part of feature exploration/recommendation Expand source code def list_all_industry (): \"\"\" Lists down all the Industries that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported industries as part of feature exploration/recommendation \"\"\" odf_uni = df_input_fer . iloc [:, 2 ] . unique () odf = pd . DataFrame ( odf_uni , columns = [ \"Industry\" ]) return odf def list_all_pair ( ) Lists down all the Industry/Use case pairs that are supported in Feature Recommender module. Returns DataFrame of all the supported Industry/Usecase pairs as part of feature exploration/recommendation Expand source code def list_all_pair (): \"\"\" Lists down all the Industry/Use case pairs that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported Industry/Usecase pairs as part of feature exploration/recommendation \"\"\" odf = df_input_fer . iloc [:, [ 2 , 3 ]] . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_all_usecase ( ) Lists down all the Use cases that are supported in Feature Recommender module. Returns DataFrame of all the supported usecases as part of feature exploration/recommendation Expand source code def list_all_usecase (): \"\"\" Lists down all the Use cases that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported usecases as part of feature exploration/recommendation \"\"\" odf_uni = df_input_fer . iloc [:, 3 ] . unique () odf = pd . DataFrame ( odf_uni , columns = [ \"Usecase\" ]) return odf def list_feature_by_industry ( industry, num_of_feat=100, semantic=True) Lists down all the Features that are available in Feature Recommender Package based on the Input Industry. Parameters industry :\u2002 str Input industry num_of_feat :\u2002 int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic :\u2002 bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Usecases' Feature Popularity to the Input Industry. Expand source code def list_feature_by_industry ( industry , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Industry. Parameters ---------- industry : str Input industry num_of_feat : int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Usecases' Feature Popularity to the Input Industry. \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) industry = process_industry ( industry , semantic ) odf = df_input_fer . loc [ df_input_fer . iloc [:, 2 ] == industry ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) if len ( odf ) > 0 : odf [ \"count\" ] = odf . groupby ( usecase_column )[ usecase_column ] . transform ( \"count\" ) odf . sort_values ( \"count\" , inplace = True , ascending = False ) odf = odf . drop ( \"count\" , axis = 1 ) if num_of_feat != \"all\" : odf = odf . head ( num_of_feat ) . reset_index ( drop = True ) else : odf = odf . reset_index ( drop = True ) return odf def list_feature_by_pair ( industry, usecase, num_of_feat=100, semantic=True) Lists down all the Features that are available in Feature Recommender Package based on the Input Industry/Usecase pair Parameters industry Input industry (string) usecase Input usecase (string) num_of_feat Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic Input semantic (boolean) - Whether the input needs to go through semantic similarity or not. Default is True. Returns DataFrame Columns are: Feature Name: Name of the suggested Feature Feature Description: Description of the suggested Feature Industry: Industry name of the suggested Feature Usecase: Usecase name of the suggested Feature Source: Source of the suggested Feature Expand source code def list_feature_by_pair ( industry , usecase , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Industry/Usecase pair Parameters ---------- industry Input industry (string) usecase Input usecase (string) num_of_feat Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic Input semantic (boolean) - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) industry = process_industry ( industry , semantic ) usecase = process_usecase ( usecase , semantic ) if num_of_feat != \"all\" : odf = ( df_input_fer . loc [ ( df_input_fer . iloc [:, 2 ] == industry ) & ( df_input_fer . iloc [:, 3 ] == usecase ) ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) . head ( num_of_feat ) ) else : odf = df_input_fer . loc [ ( df_input_fer . iloc [:, 2 ] == industry ) & ( df_input_fer . iloc [:, 3 ] == usecase ) ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_feature_by_usecase ( usecase, num_of_feat=100, semantic=True) Lists down all the Features that are available in Feature Recommender Package based on the Input Usecase. Parameters usecase :\u2002 str Input usecase num_of_feat :\u2002 int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic :\u2002 bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns DataFrame Columns are: Feature Name: Name of the suggested Feature Feature Description: Description of the suggested Feature Industry: Industry name of the suggested Feature Usecase: Usecase name of the suggested Feature Source: Source of the suggested Feature The list of features is sorted by the Industries' Feature Popularity to the Input Usecase. Expand source code def list_feature_by_usecase ( usecase , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Usecase. Parameters ---------- usecase : str Input usecase num_of_feat : int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Industries' Feature Popularity to the Input Usecase. \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) usecase = process_usecase ( usecase , semantic ) odf = df_input_fer . loc [ df_input_fer . iloc [:, 3 ] == usecase ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) if len ( odf ) > 0 : odf [ \"count\" ] = odf . groupby ( industry_column )[ industry_column ] . transform ( \"count\" ) odf . sort_values ( \"count\" , inplace = True , ascending = False ) odf = odf . drop ( \"count\" , axis = 1 ) if num_of_feat != \"all\" : odf = odf . head ( num_of_feat ) . reset_index ( drop = True ) else : odf = odf . reset_index ( drop = True ) return odf def list_industry_by_usecase ( usecase, semantic=True) Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters usecase :\u2002 str Input usecase semantic :\u2002 bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns Expand source code def list_industry_by_usecase ( usecase , semantic = True ): \"\"\" Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters ---------- usecase : str Input usecase semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" usecase = process_usecase ( usecase , semantic ) odf = pd . DataFrame ( df_input_fer . loc [ df_input_fer . iloc [:, 3 ] == usecase ] . iloc [:, 2 ]) odf = odf . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_usecase_by_industry ( industry, semantic=True) Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters industry :\u2002 str Input industry semantic :\u2002 bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns Expand source code def list_usecase_by_industry ( industry , semantic = True ): \"\"\" Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters ---------- industry : str Input industry semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" industry = process_industry ( industry , semantic ) odf = pd . DataFrame ( df_input_fer . loc [ df_input_fer . iloc [:, 2 ] == industry ] . iloc [:, 3 ]) odf = odf . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def process_industry ( industry: str, semantic: bool) Parameters industry :\u2002 str Input industry semantic :\u2002 bool Whether the input needs to go through semantic similarity or not. Default is True. Returns Expand source code def process_industry ( industry : str , semantic : bool ): \"\"\" Parameters ---------- industry : str Input industry semantic : bool Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" if type ( semantic ) != bool : raise TypeError ( \"Invalid input for semantic\" ) if type ( industry ) != str : raise TypeError ( \"Invalid input for industry\" ) industry = industry . lower () . strip () industry = industry . replace ( \"[^A-Za-z0-9 ]+\" , \" \" ) all_industry = list_all_industry ()[ \"Industry\" ] . to_list () if semantic and industry not in all_industry : all_industry_embeddings = model_fer . model . encode ( all_industry , convert_to_tensor = True ) industry_embeddings = model_fer . model . encode ( industry , convert_to_tensor = True ) cos_scores = util . pytorch_cos_sim ( industry_embeddings , all_industry_embeddings )[ 0 ] first_match_index = int ( np . argpartition ( - cos_scores , 0 )[ 0 ]) processed_industry = all_industry [ first_match_index ] print ( \"Given input Industry is not available. Showing the most semantically relevant Industry result: \" , processed_industry , ) else : processed_industry = industry return processed_industry def process_usecase ( usecase: str, semantic: bool) Parameters usecase :\u2002 str Input usecase semantic :\u2002 bool Whether the input needs to go through semantic similarity or not. Default is True. Returns Expand source code def process_usecase ( usecase : str , semantic : bool ): \"\"\" Parameters ---------- usecase : str Input usecase semantic : bool Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" if type ( semantic ) != bool : raise TypeError ( \"Invalid input for semantic\" ) if type ( usecase ) != str : raise TypeError ( \"Invalid input for usecase\" ) usecase = usecase . lower () . strip () usecase = usecase . replace ( \"[^A-Za-z0-9 ]+\" , \" \" ) all_usecase = list_all_usecase ()[ \"Usecase\" ] . to_list () if semantic and usecase not in all_usecase : all_usecase_embeddings = model_fer . model . encode ( all_usecase , convert_to_tensor = True ) usecase_embeddings = model_fer . model . encode ( usecase , convert_to_tensor = True ) cos_scores = util . pytorch_cos_sim ( usecase_embeddings , all_usecase_embeddings )[ 0 ] first_match_index = int ( np . argpartition ( - cos_scores , 0 )[ 0 ]) processed_usecase = all_usecase [ first_match_index ] print ( \"Given input Usecase is not available. Showing the most semantically relevant Usecase result: \" , processed_usecase , ) else : processed_usecase = usecase return processed_usecase","title":"<code>feature_explorer</code>"},{"location":"api/feature_recommender/feature_explorer.html#feature_explorer","text":"Feature explorer helps list down the potential features from our corpus based on user defined industry or/and use case. Expand source code \"\"\"Feature explorer helps list down the potential features from our corpus based on user defined industry or/and use case. \"\"\" import numpy as np import pandas as pd from sentence_transformers import util from anovos.feature_recommender.featrec_init import ( feature_exploration_prep , get_column_name , model_fer , ) df_input_fer = feature_exploration_prep () ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_input_fer ) def list_all_industry (): \"\"\" Lists down all the Industries that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported industries as part of feature exploration/recommendation \"\"\" odf_uni = df_input_fer . iloc [:, 2 ] . unique () odf = pd . DataFrame ( odf_uni , columns = [ \"Industry\" ]) return odf def list_all_usecase (): \"\"\" Lists down all the Use cases that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported usecases as part of feature exploration/recommendation \"\"\" odf_uni = df_input_fer . iloc [:, 3 ] . unique () odf = pd . DataFrame ( odf_uni , columns = [ \"Usecase\" ]) return odf def list_all_pair (): \"\"\" Lists down all the Industry/Use case pairs that are supported in Feature Recommender module. Returns ------- DataFrame of all the supported Industry/Usecase pairs as part of feature exploration/recommendation \"\"\" odf = df_input_fer . iloc [:, [ 2 , 3 ]] . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def process_usecase ( usecase : str , semantic : bool ): \"\"\" Parameters ---------- usecase : str Input usecase semantic : bool Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" if type ( semantic ) != bool : raise TypeError ( \"Invalid input for semantic\" ) if type ( usecase ) != str : raise TypeError ( \"Invalid input for usecase\" ) usecase = usecase . lower () . strip () usecase = usecase . replace ( \"[^A-Za-z0-9 ]+\" , \" \" ) all_usecase = list_all_usecase ()[ \"Usecase\" ] . to_list () if semantic and usecase not in all_usecase : all_usecase_embeddings = model_fer . model . encode ( all_usecase , convert_to_tensor = True ) usecase_embeddings = model_fer . model . encode ( usecase , convert_to_tensor = True ) cos_scores = util . pytorch_cos_sim ( usecase_embeddings , all_usecase_embeddings )[ 0 ] first_match_index = int ( np . argpartition ( - cos_scores , 0 )[ 0 ]) processed_usecase = all_usecase [ first_match_index ] print ( \"Given input Usecase is not available. Showing the most semantically relevant Usecase result: \" , processed_usecase , ) else : processed_usecase = usecase return processed_usecase def process_industry ( industry : str , semantic : bool ): \"\"\" Parameters ---------- industry : str Input industry semantic : bool Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" if type ( semantic ) != bool : raise TypeError ( \"Invalid input for semantic\" ) if type ( industry ) != str : raise TypeError ( \"Invalid input for industry\" ) industry = industry . lower () . strip () industry = industry . replace ( \"[^A-Za-z0-9 ]+\" , \" \" ) all_industry = list_all_industry ()[ \"Industry\" ] . to_list () if semantic and industry not in all_industry : all_industry_embeddings = model_fer . model . encode ( all_industry , convert_to_tensor = True ) industry_embeddings = model_fer . model . encode ( industry , convert_to_tensor = True ) cos_scores = util . pytorch_cos_sim ( industry_embeddings , all_industry_embeddings )[ 0 ] first_match_index = int ( np . argpartition ( - cos_scores , 0 )[ 0 ]) processed_industry = all_industry [ first_match_index ] print ( \"Given input Industry is not available. Showing the most semantically relevant Industry result: \" , processed_industry , ) else : processed_industry = industry return processed_industry def list_usecase_by_industry ( industry , semantic = True ): \"\"\" Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters ---------- industry : str Input industry semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" industry = process_industry ( industry , semantic ) odf = pd . DataFrame ( df_input_fer . loc [ df_input_fer . iloc [:, 2 ] == industry ] . iloc [:, 3 ]) odf = odf . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_industry_by_usecase ( usecase , semantic = True ): \"\"\" Lists down all the Use cases that are supported in Feature Recommender Package based on the Input Industry. Parameters ---------- usecase : str Input usecase semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- \"\"\" usecase = process_usecase ( usecase , semantic ) odf = pd . DataFrame ( df_input_fer . loc [ df_input_fer . iloc [:, 3 ] == usecase ] . iloc [:, 2 ]) odf = odf . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf def list_feature_by_industry ( industry , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Industry. Parameters ---------- industry : str Input industry num_of_feat : int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Usecases' Feature Popularity to the Input Industry. \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) industry = process_industry ( industry , semantic ) odf = df_input_fer . loc [ df_input_fer . iloc [:, 2 ] == industry ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) if len ( odf ) > 0 : odf [ \"count\" ] = odf . groupby ( usecase_column )[ usecase_column ] . transform ( \"count\" ) odf . sort_values ( \"count\" , inplace = True , ascending = False ) odf = odf . drop ( \"count\" , axis = 1 ) if num_of_feat != \"all\" : odf = odf . head ( num_of_feat ) . reset_index ( drop = True ) else : odf = odf . reset_index ( drop = True ) return odf def list_feature_by_usecase ( usecase , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Usecase. Parameters ---------- usecase : str Input usecase num_of_feat : int Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature The list of features is sorted by the Industries' Feature Popularity to the Input Usecase. \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) usecase = process_usecase ( usecase , semantic ) odf = df_input_fer . loc [ df_input_fer . iloc [:, 3 ] == usecase ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) if len ( odf ) > 0 : odf [ \"count\" ] = odf . groupby ( industry_column )[ industry_column ] . transform ( \"count\" ) odf . sort_values ( \"count\" , inplace = True , ascending = False ) odf = odf . drop ( \"count\" , axis = 1 ) if num_of_feat != \"all\" : odf = odf . head ( num_of_feat ) . reset_index ( drop = True ) else : odf = odf . reset_index ( drop = True ) return odf def list_feature_by_pair ( industry , usecase , num_of_feat = 100 , semantic = True ): \"\"\" Lists down all the Features that are available in Feature Recommender Package based on the Input Industry/Usecase pair Parameters ---------- industry Input industry (string) usecase Input usecase (string) num_of_feat Number of features to be displayed in the output. Value can be either integer, or 'all' - display all features matched with the input. Default is 100. semantic Input semantic (boolean) - Whether the input needs to go through semantic similarity or not. Default is True. Returns ------- DataFrame Columns are: - Feature Name: Name of the suggested Feature - Feature Description: Description of the suggested Feature - Industry: Industry name of the suggested Feature - Usecase: Usecase name of the suggested Feature - Source: Source of the suggested Feature \"\"\" if type ( num_of_feat ) != int or num_of_feat < 0 : if num_of_feat != \"all\" : raise TypeError ( \"Invalid input for num_of_feat\" ) industry = process_industry ( industry , semantic ) usecase = process_usecase ( usecase , semantic ) if num_of_feat != \"all\" : odf = ( df_input_fer . loc [ ( df_input_fer . iloc [:, 2 ] == industry ) & ( df_input_fer . iloc [:, 3 ] == usecase ) ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) . head ( num_of_feat ) ) else : odf = df_input_fer . loc [ ( df_input_fer . iloc [:, 2 ] == industry ) & ( df_input_fer . iloc [:, 3 ] == usecase ) ] . drop_duplicates ( keep = \"last\" , ignore_index = True ) return odf","title":"feature_explorer"},{"location":"api/feature_recommender/feature_explorer.html#functions","text":"def list_all_industry ( ) Lists down all the Industries that are supported in Feature Recommender module.","title":"Functions"},{"location":"api/feature_recommender/feature_mapper.html","text":"feature_mapper Feature mapper maps attributes to features based on ingested data dictionary by the user. Expand source code \"\"\"Feature mapper maps attributes to features based on ingested data dictionary by the user.\"\"\" import copy import random import re import numpy as np import pandas as pd import plotly.graph_objects as go from sentence_transformers import util from anovos.feature_recommender.featrec_init import ( EmbeddingsTrainFer , camel_case_split , feature_recommendation_prep , get_column_name , model_fer , recommendation_data_prep , ) from anovos.feature_recommender.feature_explorer import ( list_usecase_by_industry , process_industry , process_usecase , ) list_train_fer , df_rec_fer = feature_recommendation_prep () list_embedding_train_fer = EmbeddingsTrainFer ( list_train_fer ) ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_rec_fer ) def feature_mapper ( df , name_column = None , desc_column = None , suggested_industry = \"all\" , suggested_usecase = \"all\" , semantic = True , top_n = 2 , threshold = 0.3 , ): \"\"\"Matches features for users based on their input attributes, and their goal industry and/or use case Parameters ---------- df : DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description name_column : str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column : str Input, column name of Attribute Description in Input DataFrame. Default is None. suggested_industry : str Input, Industry of interest to the user (if any) to be filtered out. Default is 'all', meaning all Industries available. suggested_usecase : str Input, Usecase of interest to the user (if any) to be filtered out. Default is 'all', meaning all Usecases available. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. top_n : int Number of features displayed. Default is 2 threshold : float Input threshold value. Default is 0.3 Returns ------- DataFrame Columns are: - Input Attribute Name: Name of the input Attribute - Input Attribute Description: Description of the input Attribute - Matched Feature Name: Name of the matched Feature - Matched Feature Description: Description of the matched Feature - Feature Similarity Score: Semantic similarity score between input Attribute and matched Feature - Industry: Industry name of the matched Feature - Usecase: Usecase name of the matched Feature \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if type ( top_n ) != int or top_n < 0 : raise TypeError ( \"Invalid input for top_n\" ) if top_n > len ( list_train_fer ): raise TypeError ( \"top_n value is too large\" ) if type ( threshold ) != float : raise TypeError ( \"Invalid input for threshold\" ) if threshold < 0 or threshold > 1 : raise TypeError ( \"Invalid input for threshold. Threshold value is between 0 and 1\" ) list_user , df_user = recommendation_data_prep ( df , name_column , desc_column ) if suggested_industry != \"all\" and suggested_industry == \"all\" : suggested_industry = process_industry ( suggested_industry , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 2 ] . str . contains ( suggested_industry )] list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) elif suggested_usecase != \"all\" and suggested_industry == \"all\" : suggested_usecase = process_usecase ( suggested_usecase , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 3 ] . str . contains ( suggested_usecase )] list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) elif suggested_usecase != \"all\" and suggested_industry != \"all\" : suggested_industry = process_industry ( suggested_industry , semantic ) suggested_usecase = process_usecase ( suggested_usecase , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 2 ] . str . contains ( suggested_industry ) & df_rec_fer . iloc [:, 3 ] . str . contains ( suggested_usecase ) ] if len ( df_rec_fr ) > 0 : list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) else : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) print ( \"Industry/Usecase pair does not exist.\" ) return df_out else : df_rec_fr = df_rec_fer list_embedding_train_fr = list_embedding_train_fer . get if name_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) elif desc_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) else : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) list_embedding_user = model_fer . model . encode ( list_user , convert_to_tensor = True ) for i , feature in enumerate ( list_user ): cos_scores = util . pytorch_cos_sim ( list_embedding_user , list_embedding_train_fr )[ i ] top_results = np . argpartition ( - cos_scores , range ( top_n ))[ 0 : top_n ] for idx in top_results [ 0 : top_n ]: single_score = \" %.4f \" % ( cos_scores [ idx ]) if name_column is None : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ desc_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ desc_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) elif desc_column is None : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Name\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Name\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_user [ desc_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_user [ desc_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) df_out = pd . concat ( [ df_out , df_append ], ignore_index = True , axis = 0 , join = \"outer\" ) return df_out def find_attr_by_relevance ( df , building_corpus , name_column = None , desc_column = None , threshold = 0.3 ): \"\"\"Provide a comprehensive mapping method from users&#39; input attributes to their own feature corpus, and therefore, help with the process of creating features in cold-start problems Parameters ---------- df : DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description building_corpus : list Input Feature Description name_column : str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column : str Input, column name of Attribute Description in Input DataFrame. Default is None. threshold : float Input threshold value Default is 0.3 Returns ------- DataFrame Columns are: - Input Feature Desc: Description of the input Feature - Recommended Input Attribute Name: Name of the recommended Feature - Recommended Input Attribute Description: Description of the recommended Feature - Input Attribute Similarity Score: Semantic similarity score between input Attribute and recommended Feature \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if type ( building_corpus ) != list : raise TypeError ( \"Invalid input for building_corpus\" ) if type ( threshold ) != float : raise TypeError ( \"Invalid input for building_corpus\" ) if threshold < 0 or threshold > 1 : raise TypeError ( \"Invalid input for threshold. Threshold value is between 0 and 1\" ) for i in range ( len ( building_corpus )): if type ( building_corpus [ i ]) != str : raise TypeError ( \"Invalid input inside building_corpus:\" , building_corpus [ i ]) building_corpus [ i ] = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , building_corpus [ i ]) building_corpus [ i ] = camel_case_split ( building_corpus [ i ]) building_corpus [ i ] = building_corpus [ i ] . lower () . strip () if name_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) elif desc_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Input_Attribute_Similarity_Score\" , ] ) else : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) list_user , df_user = recommendation_data_prep ( df , name_column , desc_column ) list_embedding_user = model_fer . model . encode ( list_user , convert_to_tensor = True ) list_embedding_building = model_fer . model . encode ( building_corpus , convert_to_tensor = True ) for i , feature in enumerate ( building_corpus ): if name_column is None : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) elif desc_column is None : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Input_Attribute_Similarity_Score\" , ] ) else : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) cos_scores = util . pytorch_cos_sim ( list_embedding_building , list_embedding_user )[ i ] top_results = np . argpartition ( - cos_scores , range ( len ( list_user )))[ 0 : len ( list_user ) ] for idx in top_results [ 0 : len ( list_user )]: single_score = \" %.4f \" % ( cos_scores [ idx ]) if float ( single_score ) >= threshold : if name_column is None : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ desc_column ] . iloc [ int ( idx )], single_score , ] elif desc_column is None : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ name_column ] . iloc [ int ( idx )], single_score , ] else : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ name_column ] . iloc [ int ( idx )], df_user [ desc_column ] . iloc [ int ( idx )], single_score , ] if len ( df_append ) == 0 : if name_column is None : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" ] elif desc_column is None : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" ] else : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" , \"N/A\" ] df_out = pd . concat ([ df_out , df_append ], ignore_index = True , axis = 0 , join = \"outer\" ) return df_out def sankey_visualization ( df , industry_included = False , usecase_included = False ): \"\"\"Visualize Feature Mapper functions through Sankey plots Parameters ---------- df : DataFrame Input DataFrame. This DataFrame needs to be output of feature_mapper or find_attr_by_relevance, or in the same format. industry_included : bool Whether the plot needs to include industry mapping or not. Default is False usecase_included : bool Whether the plot needs to include usecase mapping or not. Default is False Returns ------- A `plotly` graph object. \"\"\" fr_proper_col_list = [ \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] attr_proper_col_list = [ \"Input_Feature_Description\" , \"Input_Attribute_Similarity_Score\" , ] if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if not all ( x in list ( df . columns ) for x in fr_proper_col_list ) and not all ( x in list ( df . columns ) for x in attr_proper_col_list ): raise TypeError ( \"df is not output DataFrame of Feature Recommendation functions\" ) if type ( industry_included ) != bool : raise TypeError ( \"Invalid input for industry_included\" ) if type ( usecase_included ) != bool : raise TypeError ( \"Invalid input for usecase_included\" ) if \"Feature_Similarity_Score\" in df . columns : if \"Input_Attribute_Name\" in df . columns : name_source = \"Input_Attribute_Name\" else : name_source = \"Input_Attribute_Description\" name_target = \"Matched_Feature_Name\" name_score = \"Feature_Similarity_Score\" else : name_source = \"Input_Feature_Description\" if \"Recommended_Input_Attribute_Name\" in df . columns : name_target = \"Recommended_Input_Attribute_Name\" else : name_target = \"Recommended_Input_Attribute_Description\" name_score = \"Input_Attribute_Similarity_Score\" if industry_included or usecase_included : print ( \"Input is find_attr_by_relevance output DataFrame. There is no suggested Industry and/or Usecase.\" ) industry_included = False usecase_included = False industry_target = \"Industry\" usecase_target = \"Usecase\" df_iter = copy . deepcopy ( df ) for i in range ( len ( df_iter )): if str ( df_iter [ name_score ][ i ]) == \"N/A\" : df = df . drop ([ i ]) df = df . reset_index ( drop = True ) source = [] target = [] value = [] if not industry_included and not usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () label = source_list + target_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) elif not industry_included and usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_usecase_list = df [ usecase_target ] . unique () . tolist () usecase_list = [] for i , item in enumerate ( raw_usecase_list ): if \", \" in raw_usecase_list [ i ]: raw_usecase_list [ i ] = raw_usecase_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_usecase_list [ i ]): usecase_list . append ( sub_item ) else : usecase_list . append ( item ) label = source_list + target_list + usecase_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list = df [ usecase_target ][ i ] . split ( \", \" ) for k , item in enumerate ( temp_list ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item ))) value . append ( float ( 1 )) elif industry_included and not usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_industry_list = df [ industry_target ] . unique () . tolist () industry_list = [] for i , item in enumerate ( raw_industry_list ): if \", \" in raw_industry_list [ i ]: raw_industry_list [ i ] = raw_industry_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_industry_list [ i ]): industry_list . append ( sub_item ) else : industry_list . append ( item ) label = source_list + target_list + industry_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list = df [ industry_target ][ i ] . split ( \", \" ) for k , item in enumerate ( temp_list ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item ))) value . append ( float ( 1 )) else : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_industry_list = df [ industry_target ] . unique () . tolist () raw_usecase_list = df [ usecase_target ] . unique () . tolist () industry_list = [] for i , item in enumerate ( raw_industry_list ): if \", \" in raw_industry_list [ i ]: raw_industry_list [ i ] = raw_industry_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_industry_list [ i ]): industry_list . append ( sub_item ) else : industry_list . append ( item ) usecase_list = [] for i , item in enumerate ( raw_usecase_list ): if \", \" in raw_usecase_list [ i ]: raw_usecase_list [ i ] = raw_usecase_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_usecase_list [ i ]): usecase_list . append ( sub_item ) else : usecase_list . append ( item ) label = source_list + target_list + industry_list + usecase_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list_industry = df [ industry_target ][ i ] . split ( \", \" ) temp_list_usecase = df [ usecase_target ][ i ] . split ( \", \" ) for k , item_industry in enumerate ( temp_list_industry ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item_industry ))) value . append ( float ( 1 )) for j , item_usecase in enumerate ( temp_list_usecase ): if ( item_usecase in list_usecase_by_industry ( item_industry )[ usecase_column ] . tolist () ): source . append ( label . index ( str ( item_industry ))) target . append ( label . index ( str ( item_usecase ))) value . append ( float ( 1 )) line_color = [ \"#\" + \"\" . join ([ random . choice ( \"0123456789ABCDEF\" ) for j in range ( 6 )]) for k in range ( len ( value )) ] label_color = [ \"#\" + \"\" . join ([ random . choice ( \"0123456789ABCDEF\" ) for e in range ( 6 )]) for f in range ( len ( label )) ] fig = go . Figure ( data = [ go . Sankey ( node = dict ( pad = 15 , thickness = 20 , line = dict ( color = line_color , width = 0.5 ), label = label , color = label_color , ), link = dict ( source = source , target = target , value = value ), ) ] ) fig . update_layout ( title_text = \"Feature Mapper Sankey Visualization\" , font_size = 10 ) return fig Functions def feature_mapper ( df, name_column=None, desc_column=None, suggested_industry='all', suggested_usecase='all', semantic=True, top_n=2, threshold=0.3) Matches features for users based on their input attributes, and their goal industry and/or use case Parameters df :\u2002 DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description name_column :\u2002 str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column :\u2002 str Input, column name of Attribute Description in Input DataFrame. Default is None. suggested_industry :\u2002 str Input, Industry of interest to the user (if any) to be filtered out. Default is 'all', meaning all Industries available. suggested_usecase :\u2002 str Input, Usecase of interest to the user (if any) to be filtered out. Default is 'all', meaning all Usecases available. semantic :\u2002 bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. top_n :\u2002 int Number of features displayed. Default is 2 threshold :\u2002 float Input threshold value. Default is 0.3 Returns DataFrame Columns are: Input Attribute Name: Name of the input Attribute Input Attribute Description: Description of the input Attribute Matched Feature Name: Name of the matched Feature Matched Feature Description: Description of the matched Feature Feature Similarity Score: Semantic similarity score between input Attribute and matched Feature Industry: Industry name of the matched Feature Usecase: Usecase name of the matched Feature Expand source code def feature_mapper ( df , name_column = None , desc_column = None , suggested_industry = \"all\" , suggested_usecase = \"all\" , semantic = True , top_n = 2 , threshold = 0.3 , ): \"\"\"Matches features for users based on their input attributes, and their goal industry and/or use case Parameters ---------- df : DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description name_column : str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column : str Input, column name of Attribute Description in Input DataFrame. Default is None. suggested_industry : str Input, Industry of interest to the user (if any) to be filtered out. Default is 'all', meaning all Industries available. suggested_usecase : str Input, Usecase of interest to the user (if any) to be filtered out. Default is 'all', meaning all Usecases available. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. top_n : int Number of features displayed. Default is 2 threshold : float Input threshold value. Default is 0.3 Returns ------- DataFrame Columns are: - Input Attribute Name: Name of the input Attribute - Input Attribute Description: Description of the input Attribute - Matched Feature Name: Name of the matched Feature - Matched Feature Description: Description of the matched Feature - Feature Similarity Score: Semantic similarity score between input Attribute and matched Feature - Industry: Industry name of the matched Feature - Usecase: Usecase name of the matched Feature \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if type ( top_n ) != int or top_n < 0 : raise TypeError ( \"Invalid input for top_n\" ) if top_n > len ( list_train_fer ): raise TypeError ( \"top_n value is too large\" ) if type ( threshold ) != float : raise TypeError ( \"Invalid input for threshold\" ) if threshold < 0 or threshold > 1 : raise TypeError ( \"Invalid input for threshold. Threshold value is between 0 and 1\" ) list_user , df_user = recommendation_data_prep ( df , name_column , desc_column ) if suggested_industry != \"all\" and suggested_industry == \"all\" : suggested_industry = process_industry ( suggested_industry , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 2 ] . str . contains ( suggested_industry )] list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) elif suggested_usecase != \"all\" and suggested_industry == \"all\" : suggested_usecase = process_usecase ( suggested_usecase , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 3 ] . str . contains ( suggested_usecase )] list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) elif suggested_usecase != \"all\" and suggested_industry != \"all\" : suggested_industry = process_industry ( suggested_industry , semantic ) suggested_usecase = process_usecase ( suggested_usecase , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 2 ] . str . contains ( suggested_industry ) & df_rec_fer . iloc [:, 3 ] . str . contains ( suggested_usecase ) ] if len ( df_rec_fr ) > 0 : list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) else : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) print ( \"Industry/Usecase pair does not exist.\" ) return df_out else : df_rec_fr = df_rec_fer list_embedding_train_fr = list_embedding_train_fer . get if name_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) elif desc_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) else : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) list_embedding_user = model_fer . model . encode ( list_user , convert_to_tensor = True ) for i , feature in enumerate ( list_user ): cos_scores = util . pytorch_cos_sim ( list_embedding_user , list_embedding_train_fr )[ i ] top_results = np . argpartition ( - cos_scores , range ( top_n ))[ 0 : top_n ] for idx in top_results [ 0 : top_n ]: single_score = \" %.4f \" % ( cos_scores [ idx ]) if name_column is None : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ desc_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ desc_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) elif desc_column is None : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Name\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Name\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_user [ desc_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_user [ desc_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) df_out = pd . concat ( [ df_out , df_append ], ignore_index = True , axis = 0 , join = \"outer\" ) return df_out def find_attr_by_relevance ( df, building_corpus, name_column=None, desc_column=None, threshold=0.3) Provide a comprehensive mapping method from users' input attributes to their own feature corpus, and therefore, help with the process of creating features in cold-start problems Parameters df :\u2002 DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description building_corpus :\u2002 list Input Feature Description name_column :\u2002 str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column :\u2002 str Input, column name of Attribute Description in Input DataFrame. Default is None. threshold :\u2002 float Input threshold value Default is 0.3 Returns DataFrame Columns are: Input Feature Desc: Description of the input Feature Recommended Input Attribute Name: Name of the recommended Feature Recommended Input Attribute Description: Description of the recommended Feature Input Attribute Similarity Score: Semantic similarity score between input Attribute and recommended Feature Expand source code def find_attr_by_relevance ( df , building_corpus , name_column = None , desc_column = None , threshold = 0.3 ): \"\"\"Provide a comprehensive mapping method from users&#39; input attributes to their own feature corpus, and therefore, help with the process of creating features in cold-start problems Parameters ---------- df : DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description building_corpus : list Input Feature Description name_column : str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column : str Input, column name of Attribute Description in Input DataFrame. Default is None. threshold : float Input threshold value Default is 0.3 Returns ------- DataFrame Columns are: - Input Feature Desc: Description of the input Feature - Recommended Input Attribute Name: Name of the recommended Feature - Recommended Input Attribute Description: Description of the recommended Feature - Input Attribute Similarity Score: Semantic similarity score between input Attribute and recommended Feature \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if type ( building_corpus ) != list : raise TypeError ( \"Invalid input for building_corpus\" ) if type ( threshold ) != float : raise TypeError ( \"Invalid input for building_corpus\" ) if threshold < 0 or threshold > 1 : raise TypeError ( \"Invalid input for threshold. Threshold value is between 0 and 1\" ) for i in range ( len ( building_corpus )): if type ( building_corpus [ i ]) != str : raise TypeError ( \"Invalid input inside building_corpus:\" , building_corpus [ i ]) building_corpus [ i ] = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , building_corpus [ i ]) building_corpus [ i ] = camel_case_split ( building_corpus [ i ]) building_corpus [ i ] = building_corpus [ i ] . lower () . strip () if name_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) elif desc_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Input_Attribute_Similarity_Score\" , ] ) else : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) list_user , df_user = recommendation_data_prep ( df , name_column , desc_column ) list_embedding_user = model_fer . model . encode ( list_user , convert_to_tensor = True ) list_embedding_building = model_fer . model . encode ( building_corpus , convert_to_tensor = True ) for i , feature in enumerate ( building_corpus ): if name_column is None : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) elif desc_column is None : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Input_Attribute_Similarity_Score\" , ] ) else : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) cos_scores = util . pytorch_cos_sim ( list_embedding_building , list_embedding_user )[ i ] top_results = np . argpartition ( - cos_scores , range ( len ( list_user )))[ 0 : len ( list_user ) ] for idx in top_results [ 0 : len ( list_user )]: single_score = \" %.4f \" % ( cos_scores [ idx ]) if float ( single_score ) >= threshold : if name_column is None : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ desc_column ] . iloc [ int ( idx )], single_score , ] elif desc_column is None : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ name_column ] . iloc [ int ( idx )], single_score , ] else : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ name_column ] . iloc [ int ( idx )], df_user [ desc_column ] . iloc [ int ( idx )], single_score , ] if len ( df_append ) == 0 : if name_column is None : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" ] elif desc_column is None : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" ] else : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" , \"N/A\" ] df_out = pd . concat ([ df_out , df_append ], ignore_index = True , axis = 0 , join = \"outer\" ) return df_out def sankey_visualization ( df, industry_included=False, usecase_included=False) Visualize Feature Mapper functions through Sankey plots Parameters df :\u2002 DataFrame Input DataFrame. This DataFrame needs to be output of feature_mapper or find_attr_by_relevance, or in the same format. industry_included :\u2002 bool Whether the plot needs to include industry mapping or not. Default is False usecase_included :\u2002 bool Whether the plot needs to include usecase mapping or not. Default is False Returns A plotly graph object. Expand source code def sankey_visualization ( df , industry_included = False , usecase_included = False ): \"\"\"Visualize Feature Mapper functions through Sankey plots Parameters ---------- df : DataFrame Input DataFrame. This DataFrame needs to be output of feature_mapper or find_attr_by_relevance, or in the same format. industry_included : bool Whether the plot needs to include industry mapping or not. Default is False usecase_included : bool Whether the plot needs to include usecase mapping or not. Default is False Returns ------- A `plotly` graph object. \"\"\" fr_proper_col_list = [ \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] attr_proper_col_list = [ \"Input_Feature_Description\" , \"Input_Attribute_Similarity_Score\" , ] if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if not all ( x in list ( df . columns ) for x in fr_proper_col_list ) and not all ( x in list ( df . columns ) for x in attr_proper_col_list ): raise TypeError ( \"df is not output DataFrame of Feature Recommendation functions\" ) if type ( industry_included ) != bool : raise TypeError ( \"Invalid input for industry_included\" ) if type ( usecase_included ) != bool : raise TypeError ( \"Invalid input for usecase_included\" ) if \"Feature_Similarity_Score\" in df . columns : if \"Input_Attribute_Name\" in df . columns : name_source = \"Input_Attribute_Name\" else : name_source = \"Input_Attribute_Description\" name_target = \"Matched_Feature_Name\" name_score = \"Feature_Similarity_Score\" else : name_source = \"Input_Feature_Description\" if \"Recommended_Input_Attribute_Name\" in df . columns : name_target = \"Recommended_Input_Attribute_Name\" else : name_target = \"Recommended_Input_Attribute_Description\" name_score = \"Input_Attribute_Similarity_Score\" if industry_included or usecase_included : print ( \"Input is find_attr_by_relevance output DataFrame. There is no suggested Industry and/or Usecase.\" ) industry_included = False usecase_included = False industry_target = \"Industry\" usecase_target = \"Usecase\" df_iter = copy . deepcopy ( df ) for i in range ( len ( df_iter )): if str ( df_iter [ name_score ][ i ]) == \"N/A\" : df = df . drop ([ i ]) df = df . reset_index ( drop = True ) source = [] target = [] value = [] if not industry_included and not usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () label = source_list + target_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) elif not industry_included and usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_usecase_list = df [ usecase_target ] . unique () . tolist () usecase_list = [] for i , item in enumerate ( raw_usecase_list ): if \", \" in raw_usecase_list [ i ]: raw_usecase_list [ i ] = raw_usecase_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_usecase_list [ i ]): usecase_list . append ( sub_item ) else : usecase_list . append ( item ) label = source_list + target_list + usecase_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list = df [ usecase_target ][ i ] . split ( \", \" ) for k , item in enumerate ( temp_list ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item ))) value . append ( float ( 1 )) elif industry_included and not usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_industry_list = df [ industry_target ] . unique () . tolist () industry_list = [] for i , item in enumerate ( raw_industry_list ): if \", \" in raw_industry_list [ i ]: raw_industry_list [ i ] = raw_industry_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_industry_list [ i ]): industry_list . append ( sub_item ) else : industry_list . append ( item ) label = source_list + target_list + industry_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list = df [ industry_target ][ i ] . split ( \", \" ) for k , item in enumerate ( temp_list ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item ))) value . append ( float ( 1 )) else : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_industry_list = df [ industry_target ] . unique () . tolist () raw_usecase_list = df [ usecase_target ] . unique () . tolist () industry_list = [] for i , item in enumerate ( raw_industry_list ): if \", \" in raw_industry_list [ i ]: raw_industry_list [ i ] = raw_industry_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_industry_list [ i ]): industry_list . append ( sub_item ) else : industry_list . append ( item ) usecase_list = [] for i , item in enumerate ( raw_usecase_list ): if \", \" in raw_usecase_list [ i ]: raw_usecase_list [ i ] = raw_usecase_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_usecase_list [ i ]): usecase_list . append ( sub_item ) else : usecase_list . append ( item ) label = source_list + target_list + industry_list + usecase_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list_industry = df [ industry_target ][ i ] . split ( \", \" ) temp_list_usecase = df [ usecase_target ][ i ] . split ( \", \" ) for k , item_industry in enumerate ( temp_list_industry ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item_industry ))) value . append ( float ( 1 )) for j , item_usecase in enumerate ( temp_list_usecase ): if ( item_usecase in list_usecase_by_industry ( item_industry )[ usecase_column ] . tolist () ): source . append ( label . index ( str ( item_industry ))) target . append ( label . index ( str ( item_usecase ))) value . append ( float ( 1 )) line_color = [ \"#\" + \"\" . join ([ random . choice ( \"0123456789ABCDEF\" ) for j in range ( 6 )]) for k in range ( len ( value )) ] label_color = [ \"#\" + \"\" . join ([ random . choice ( \"0123456789ABCDEF\" ) for e in range ( 6 )]) for f in range ( len ( label )) ] fig = go . Figure ( data = [ go . Sankey ( node = dict ( pad = 15 , thickness = 20 , line = dict ( color = line_color , width = 0.5 ), label = label , color = label_color , ), link = dict ( source = source , target = target , value = value ), ) ] ) fig . update_layout ( title_text = \"Feature Mapper Sankey Visualization\" , font_size = 10 ) return fig","title":"<code>feature_mapper</code>"},{"location":"api/feature_recommender/feature_mapper.html#feature_mapper","text":"Feature mapper maps attributes to features based on ingested data dictionary by the user. Expand source code \"\"\"Feature mapper maps attributes to features based on ingested data dictionary by the user.\"\"\" import copy import random import re import numpy as np import pandas as pd import plotly.graph_objects as go from sentence_transformers import util from anovos.feature_recommender.featrec_init import ( EmbeddingsTrainFer , camel_case_split , feature_recommendation_prep , get_column_name , model_fer , recommendation_data_prep , ) from anovos.feature_recommender.feature_explorer import ( list_usecase_by_industry , process_industry , process_usecase , ) list_train_fer , df_rec_fer = feature_recommendation_prep () list_embedding_train_fer = EmbeddingsTrainFer ( list_train_fer ) ( feature_name_column , feature_desc_column , industry_column , usecase_column , ) = get_column_name ( df_rec_fer ) def feature_mapper ( df , name_column = None , desc_column = None , suggested_industry = \"all\" , suggested_usecase = \"all\" , semantic = True , top_n = 2 , threshold = 0.3 , ): \"\"\"Matches features for users based on their input attributes, and their goal industry and/or use case Parameters ---------- df : DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description name_column : str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column : str Input, column name of Attribute Description in Input DataFrame. Default is None. suggested_industry : str Input, Industry of interest to the user (if any) to be filtered out. Default is 'all', meaning all Industries available. suggested_usecase : str Input, Usecase of interest to the user (if any) to be filtered out. Default is 'all', meaning all Usecases available. semantic : bool Input semantic - Whether the input needs to go through semantic similarity or not. Default is True. top_n : int Number of features displayed. Default is 2 threshold : float Input threshold value. Default is 0.3 Returns ------- DataFrame Columns are: - Input Attribute Name: Name of the input Attribute - Input Attribute Description: Description of the input Attribute - Matched Feature Name: Name of the matched Feature - Matched Feature Description: Description of the matched Feature - Feature Similarity Score: Semantic similarity score between input Attribute and matched Feature - Industry: Industry name of the matched Feature - Usecase: Usecase name of the matched Feature \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if type ( top_n ) != int or top_n < 0 : raise TypeError ( \"Invalid input for top_n\" ) if top_n > len ( list_train_fer ): raise TypeError ( \"top_n value is too large\" ) if type ( threshold ) != float : raise TypeError ( \"Invalid input for threshold\" ) if threshold < 0 or threshold > 1 : raise TypeError ( \"Invalid input for threshold. Threshold value is between 0 and 1\" ) list_user , df_user = recommendation_data_prep ( df , name_column , desc_column ) if suggested_industry != \"all\" and suggested_industry == \"all\" : suggested_industry = process_industry ( suggested_industry , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 2 ] . str . contains ( suggested_industry )] list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) elif suggested_usecase != \"all\" and suggested_industry == \"all\" : suggested_usecase = process_usecase ( suggested_usecase , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 3 ] . str . contains ( suggested_usecase )] list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) elif suggested_usecase != \"all\" and suggested_industry != \"all\" : suggested_industry = process_industry ( suggested_industry , semantic ) suggested_usecase = process_usecase ( suggested_usecase , semantic ) df_rec_fr = df_rec_fer [ df_rec_fer . iloc [:, 2 ] . str . contains ( suggested_industry ) & df_rec_fer . iloc [:, 3 ] . str . contains ( suggested_usecase ) ] if len ( df_rec_fr ) > 0 : list_keep = list ( df_rec_fr . index ) list_embedding_train_fr = [ list_embedding_train_fer . get . tolist ()[ x ] for x in list_keep ] df_rec_fr = df_rec_fr . reset_index ( drop = True ) else : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) print ( \"Industry/Usecase pair does not exist.\" ) return df_out else : df_rec_fr = df_rec_fer list_embedding_train_fr = list_embedding_train_fer . get if name_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) elif desc_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) else : df_out = pd . DataFrame ( columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] ) list_embedding_user = model_fer . model . encode ( list_user , convert_to_tensor = True ) for i , feature in enumerate ( list_user ): cos_scores = util . pytorch_cos_sim ( list_embedding_user , list_embedding_train_fr )[ i ] top_results = np . argpartition ( - cos_scores , range ( top_n ))[ 0 : top_n ] for idx in top_results [ 0 : top_n ]: single_score = \" %.4f \" % ( cos_scores [ idx ]) if name_column is None : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ desc_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ desc_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) elif desc_column is None : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Name\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Name\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : if float ( single_score ) >= threshold : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_user [ desc_column ] . iloc [ i ], df_rec_fr [ feature_name_column ] . iloc [ int ( idx )], df_rec_fr [ feature_desc_column ] . iloc [ int ( idx )], \" %.4f \" % ( cos_scores [ idx ]), df_rec_fr [ industry_column ] . iloc [ int ( idx )], df_rec_fr [ usecase_column ] . iloc [ int ( idx )], ] ], columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) else : df_append = pd . DataFrame ( [ [ df_user [ name_column ] . iloc [ i ], df_user [ desc_column ] . iloc [ i ], \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , \"N/A\" , ] ], columns = [ \"Input_Attribute_Name\" , \"Input_Attribute_Description\" , \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ], ) df_out = pd . concat ( [ df_out , df_append ], ignore_index = True , axis = 0 , join = \"outer\" ) return df_out def find_attr_by_relevance ( df , building_corpus , name_column = None , desc_column = None , threshold = 0.3 ): \"\"\"Provide a comprehensive mapping method from users&#39; input attributes to their own feature corpus, and therefore, help with the process of creating features in cold-start problems Parameters ---------- df : DataFrame Input DataFrame - Users' Data dictionary. It is expected to consist of attribute name and/or attribute description building_corpus : list Input Feature Description name_column : str Input, column name of Attribute Name in Input DataFrame. Default is None. desc_column : str Input, column name of Attribute Description in Input DataFrame. Default is None. threshold : float Input threshold value Default is 0.3 Returns ------- DataFrame Columns are: - Input Feature Desc: Description of the input Feature - Recommended Input Attribute Name: Name of the recommended Feature - Recommended Input Attribute Description: Description of the recommended Feature - Input Attribute Similarity Score: Semantic similarity score between input Attribute and recommended Feature \"\"\" if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if type ( building_corpus ) != list : raise TypeError ( \"Invalid input for building_corpus\" ) if type ( threshold ) != float : raise TypeError ( \"Invalid input for building_corpus\" ) if threshold < 0 or threshold > 1 : raise TypeError ( \"Invalid input for threshold. Threshold value is between 0 and 1\" ) for i in range ( len ( building_corpus )): if type ( building_corpus [ i ]) != str : raise TypeError ( \"Invalid input inside building_corpus:\" , building_corpus [ i ]) building_corpus [ i ] = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , building_corpus [ i ]) building_corpus [ i ] = camel_case_split ( building_corpus [ i ]) building_corpus [ i ] = building_corpus [ i ] . lower () . strip () if name_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) elif desc_column is None : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Input_Attribute_Similarity_Score\" , ] ) else : df_out = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) list_user , df_user = recommendation_data_prep ( df , name_column , desc_column ) list_embedding_user = model_fer . model . encode ( list_user , convert_to_tensor = True ) list_embedding_building = model_fer . model . encode ( building_corpus , convert_to_tensor = True ) for i , feature in enumerate ( building_corpus ): if name_column is None : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) elif desc_column is None : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Input_Attribute_Similarity_Score\" , ] ) else : df_append = pd . DataFrame ( columns = [ \"Input_Feature_Description\" , \"Recommended_Input_Attribute_Name\" , \"Recommended_Input_Attribute_Description\" , \"Input_Attribute_Similarity_Score\" , ] ) cos_scores = util . pytorch_cos_sim ( list_embedding_building , list_embedding_user )[ i ] top_results = np . argpartition ( - cos_scores , range ( len ( list_user )))[ 0 : len ( list_user ) ] for idx in top_results [ 0 : len ( list_user )]: single_score = \" %.4f \" % ( cos_scores [ idx ]) if float ( single_score ) >= threshold : if name_column is None : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ desc_column ] . iloc [ int ( idx )], single_score , ] elif desc_column is None : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ name_column ] . iloc [ int ( idx )], single_score , ] else : df_append . loc [ len ( df_append . index )] = [ feature , df_user [ name_column ] . iloc [ int ( idx )], df_user [ desc_column ] . iloc [ int ( idx )], single_score , ] if len ( df_append ) == 0 : if name_column is None : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" ] elif desc_column is None : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" ] else : df_append . loc [ len ( df_append . index )] = [ feature , \"N/A\" , \"N/A\" , \"N/A\" ] df_out = pd . concat ([ df_out , df_append ], ignore_index = True , axis = 0 , join = \"outer\" ) return df_out def sankey_visualization ( df , industry_included = False , usecase_included = False ): \"\"\"Visualize Feature Mapper functions through Sankey plots Parameters ---------- df : DataFrame Input DataFrame. This DataFrame needs to be output of feature_mapper or find_attr_by_relevance, or in the same format. industry_included : bool Whether the plot needs to include industry mapping or not. Default is False usecase_included : bool Whether the plot needs to include usecase mapping or not. Default is False Returns ------- A `plotly` graph object. \"\"\" fr_proper_col_list = [ \"Matched_Feature_Name\" , \"Matched_Feature_Description\" , \"Feature_Similarity_Score\" , \"Industry\" , \"Usecase\" , ] attr_proper_col_list = [ \"Input_Feature_Description\" , \"Input_Attribute_Similarity_Score\" , ] if not isinstance ( df , pd . DataFrame ): raise TypeError ( \"Invalid input for df\" ) if not all ( x in list ( df . columns ) for x in fr_proper_col_list ) and not all ( x in list ( df . columns ) for x in attr_proper_col_list ): raise TypeError ( \"df is not output DataFrame of Feature Recommendation functions\" ) if type ( industry_included ) != bool : raise TypeError ( \"Invalid input for industry_included\" ) if type ( usecase_included ) != bool : raise TypeError ( \"Invalid input for usecase_included\" ) if \"Feature_Similarity_Score\" in df . columns : if \"Input_Attribute_Name\" in df . columns : name_source = \"Input_Attribute_Name\" else : name_source = \"Input_Attribute_Description\" name_target = \"Matched_Feature_Name\" name_score = \"Feature_Similarity_Score\" else : name_source = \"Input_Feature_Description\" if \"Recommended_Input_Attribute_Name\" in df . columns : name_target = \"Recommended_Input_Attribute_Name\" else : name_target = \"Recommended_Input_Attribute_Description\" name_score = \"Input_Attribute_Similarity_Score\" if industry_included or usecase_included : print ( \"Input is find_attr_by_relevance output DataFrame. There is no suggested Industry and/or Usecase.\" ) industry_included = False usecase_included = False industry_target = \"Industry\" usecase_target = \"Usecase\" df_iter = copy . deepcopy ( df ) for i in range ( len ( df_iter )): if str ( df_iter [ name_score ][ i ]) == \"N/A\" : df = df . drop ([ i ]) df = df . reset_index ( drop = True ) source = [] target = [] value = [] if not industry_included and not usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () label = source_list + target_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) elif not industry_included and usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_usecase_list = df [ usecase_target ] . unique () . tolist () usecase_list = [] for i , item in enumerate ( raw_usecase_list ): if \", \" in raw_usecase_list [ i ]: raw_usecase_list [ i ] = raw_usecase_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_usecase_list [ i ]): usecase_list . append ( sub_item ) else : usecase_list . append ( item ) label = source_list + target_list + usecase_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list = df [ usecase_target ][ i ] . split ( \", \" ) for k , item in enumerate ( temp_list ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item ))) value . append ( float ( 1 )) elif industry_included and not usecase_included : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_industry_list = df [ industry_target ] . unique () . tolist () industry_list = [] for i , item in enumerate ( raw_industry_list ): if \", \" in raw_industry_list [ i ]: raw_industry_list [ i ] = raw_industry_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_industry_list [ i ]): industry_list . append ( sub_item ) else : industry_list . append ( item ) label = source_list + target_list + industry_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list = df [ industry_target ][ i ] . split ( \", \" ) for k , item in enumerate ( temp_list ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item ))) value . append ( float ( 1 )) else : source_list = df [ name_source ] . unique () . tolist () target_list = df [ name_target ] . unique () . tolist () raw_industry_list = df [ industry_target ] . unique () . tolist () raw_usecase_list = df [ usecase_target ] . unique () . tolist () industry_list = [] for i , item in enumerate ( raw_industry_list ): if \", \" in raw_industry_list [ i ]: raw_industry_list [ i ] = raw_industry_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_industry_list [ i ]): industry_list . append ( sub_item ) else : industry_list . append ( item ) usecase_list = [] for i , item in enumerate ( raw_usecase_list ): if \", \" in raw_usecase_list [ i ]: raw_usecase_list [ i ] = raw_usecase_list [ i ] . split ( \", \" ) for j , sub_item in enumerate ( raw_usecase_list [ i ]): usecase_list . append ( sub_item ) else : usecase_list . append ( item ) label = source_list + target_list + industry_list + usecase_list for i in range ( len ( df )): source . append ( label . index ( str ( df [ name_source ][ i ]))) target . append ( label . index ( str ( df [ name_target ][ i ]))) value . append ( float ( df [ name_score ][ i ])) temp_list_industry = df [ industry_target ][ i ] . split ( \", \" ) temp_list_usecase = df [ usecase_target ][ i ] . split ( \", \" ) for k , item_industry in enumerate ( temp_list_industry ): source . append ( label . index ( str ( df [ name_target ][ i ]))) target . append ( label . index ( str ( item_industry ))) value . append ( float ( 1 )) for j , item_usecase in enumerate ( temp_list_usecase ): if ( item_usecase in list_usecase_by_industry ( item_industry )[ usecase_column ] . tolist () ): source . append ( label . index ( str ( item_industry ))) target . append ( label . index ( str ( item_usecase ))) value . append ( float ( 1 )) line_color = [ \"#\" + \"\" . join ([ random . choice ( \"0123456789ABCDEF\" ) for j in range ( 6 )]) for k in range ( len ( value )) ] label_color = [ \"#\" + \"\" . join ([ random . choice ( \"0123456789ABCDEF\" ) for e in range ( 6 )]) for f in range ( len ( label )) ] fig = go . Figure ( data = [ go . Sankey ( node = dict ( pad = 15 , thickness = 20 , line = dict ( color = line_color , width = 0.5 ), label = label , color = label_color , ), link = dict ( source = source , target = target , value = value ), ) ] ) fig . update_layout ( title_text = \"Feature Mapper Sankey Visualization\" , font_size = 10 ) return fig","title":"feature_mapper"},{"location":"api/feature_recommender/feature_mapper.html#functions","text":"def feature_mapper ( df, name_column=None, desc_column=None, suggested_industry='all', suggested_usecase='all', semantic=True, top_n=2, threshold=0.3) Matches features for users based on their input attributes, and their goal industry and/or use case","title":"Functions"},{"location":"api/feature_store/_index.html","text":"Overview Sub-modules anovos.feature_store.feast_exporter anovos.feature_store.feature_retrieval","title":"Overview"},{"location":"api/feature_store/_index.html#overview","text":"","title":"Overview"},{"location":"api/feature_store/_index.html#sub-modules","text":"anovos.feature_store.feast_exporter anovos.feature_store.feature_retrieval","title":"Sub-modules"},{"location":"api/feature_store/feast_exporter.html","text":"feast_exporter Expand source code import os from datetime import datetime import isort from black import FileMode , format_str from jinja2 import Template from pyspark.sql import DataFrame from pyspark.sql.functions import lit ANOVOS_SOURCE = \"anovos_source\" dataframe_to_feast_type_mapping = { \"string\" : \"String\" , \"int\" : \"Int64\" , \"float\" : \"Float32\" , \"timestamp\" : \"String\" # TODO: default type } def check_feast_configuration ( feast_config : dict , repartition_count : int ): if repartition_count != 1 : raise ValueError ( \"Please, set repartition parameter to 1 in write_main block in your config yml!\" ) if \"file_path\" not in feast_config : raise ValueError ( \"Please, provide a path to the anovos feature_store repository!\" ) if \"entity\" not in feast_config : raise ValueError ( \"Please, provide an entity definition in your config yml!\" ) if \"file_source\" not in feast_config : raise ValueError ( \"Please, provide a file source definition in your config yml!\" ) if \"feature_view\" not in feast_config : raise ValueError ( \"Please, provide a feature view definition in your config yml!\" ) def generate_entity_definition ( config : dict ) -> str : source_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"entity.txt\" ) with open ( source_template_path , \"r\" ) as f : template_string = f . read () entity_template = Template ( template_string ) data = { \"entity_name\" : config [ \"name\" ], \"join_keys\" : config [ \"id_col\" ], \"value_type\" : \"STRING\" , \"description\" : config [ \"description\" ], } return entity_template . render ( data ) def generate_feature_view ( types : list , exclude_list : list , config : dict , entity_name : str ) -> str : source_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"feature_view.txt\" ) with open ( source_template_path , \"r\" ) as f : template_string = f . read () fields = generate_fields ( types , exclude_list ) feature_view_template = Template ( template_string ) data = { \"feature_view_name\" : config [ \"name\" ], \"source\" : ANOVOS_SOURCE , \"view_name\" : config [ \"name\" ], \"entity\" : entity_name , \"fields\" : fields , \"ttl_in_seconds\" : config [ \"ttl_in_seconds\" ], \"owner\" : config [ \"owner\" ], } return feature_view_template . render ( data ) def generate_fields ( types : list , exclude_list : list ) -> str : fields = \"\" for ( field_name , field_type ) in types : if field_name not in exclude_list : fields += generate_field ( field_name , dataframe_to_feast_type_mapping [ field_type ] ) return fields def generate_field ( field_name : str , field_type : str ) -> str : template_string = \"\"\" Field(name=\"{{name}}\", dtype={{type}}), \\n \"\"\" field_template = Template ( template_string ) return field_template . render ({ \"name\" : field_name , \"type\" : field_type }) def generate_file_source ( config : dict , file_name = \"Test\" ) -> str : source_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"file_source.txt\" ) with open ( source_template_path , \"r\" ) as f : template_string = f . read () file_source_template = Template ( template_string ) data = { \"source_name\" : ANOVOS_SOURCE , \"filename\" : file_name , \"ts_column\" : config [ \"timestamp_col\" ], \"create_ts_column\" : config [ \"create_timestamp_col\" ], \"source_description\" : config [ \"description\" ], \"owner\" : config [ \"owner\" ], } return file_source_template . render ( data ) def generate_prefix (): prefix_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"prefix.txt\" ) with open ( prefix_path , \"r\" ) as f : prefix = f . read () return prefix def generate_feature_service ( service_name : str , view_name : str ): service_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"feature_service.txt\" ) with open ( service_template_path , \"r\" ) as f : template_string = f . read () service_template = Template ( template_string ) data = { \"feature_service_name\" : service_name , \"view_name\" : view_name , } return service_template . render ( data ) def generate_feature_description ( types : list , feast_config : dict , file_name : str ): print ( \"Building feature definitions for feature_store\" ) prefix = generate_prefix () file_source_config = feast_config [ \"file_source\" ] file_source_definition = generate_file_source ( file_source_config , file_name ) entity_config = feast_config [ \"entity\" ] entity_definition = generate_entity_definition ( entity_config ) feature_view_config = feast_config [ \"feature_view\" ] columns_to_exclude = [ feast_config [ \"entity\" ][ \"id_col\" ], feast_config [ \"file_source\" ][ \"timestamp_col\" ], feast_config [ \"file_source\" ][ \"create_timestamp_col\" ], ] feature_view = generate_feature_view ( types , columns_to_exclude , feature_view_config , entity_config [ \"name\" ] ) feature_service = ( generate_feature_service ( feast_config [ \"service_name\" ], feature_view_config [ \"name\" ] ) if \"service_name\" in feast_config else \"\" ) complete_file_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"complete_file.txt\" ) with open ( complete_file_path , \"r\" ) as f : template_string = f . read () complete_file_template = Template ( template_string ) data = { \"prefix\" : prefix , \"file_source\" : file_source_definition , \"entity\" : entity_definition , \"feature_view\" : feature_view , \"feature_service\" : feature_service , } file_content = complete_file_template . render ( data ) file_content = format_str ( file_content , mode = FileMode ()) file_content = isort . code ( file_content ) feature_file = os . path . join ( feast_config [ \"file_path\" ], \"anovos.py\" ) with open ( feature_file , \"w\" ) as of : of . write ( file_content ) def add_timestamp_columns ( idf : DataFrame , feast_file_source__config : dict ): print ( \"Adding timestamp columns\" ) return idf . withColumn ( feast_file_source__config [ \"timestamp_col\" ], lit ( datetime . now ()) ) . withColumn ( feast_file_source__config [ \"create_timestamp_col\" ], lit ( datetime . now ())) Functions def add_timestamp_columns ( idf: pyspark.sql.dataframe.DataFrame, feast_file_source__config: dict) Expand source code def add_timestamp_columns ( idf : DataFrame , feast_file_source__config : dict ): print ( \"Adding timestamp columns\" ) return idf . withColumn ( feast_file_source__config [ \"timestamp_col\" ], lit ( datetime . now ()) ) . withColumn ( feast_file_source__config [ \"create_timestamp_col\" ], lit ( datetime . now ())) def check_feast_configuration ( feast_config: dict, repartition_count: int) Expand source code def check_feast_configuration ( feast_config : dict , repartition_count : int ): if repartition_count != 1 : raise ValueError ( \"Please, set repartition parameter to 1 in write_main block in your config yml!\" ) if \"file_path\" not in feast_config : raise ValueError ( \"Please, provide a path to the anovos feature_store repository!\" ) if \"entity\" not in feast_config : raise ValueError ( \"Please, provide an entity definition in your config yml!\" ) if \"file_source\" not in feast_config : raise ValueError ( \"Please, provide a file source definition in your config yml!\" ) if \"feature_view\" not in feast_config : raise ValueError ( \"Please, provide a feature view definition in your config yml!\" ) def generate_entity_definition ( config: dict) \u2011> str Expand source code def generate_entity_definition ( config : dict ) -> str : source_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"entity.txt\" ) with open ( source_template_path , \"r\" ) as f : template_string = f . read () entity_template = Template ( template_string ) data = { \"entity_name\" : config [ \"name\" ], \"join_keys\" : config [ \"id_col\" ], \"value_type\" : \"STRING\" , \"description\" : config [ \"description\" ], } return entity_template . render ( data ) def generate_feature_description ( types: list, feast_config: dict, file_name: str) Expand source code def generate_feature_description ( types : list , feast_config : dict , file_name : str ): print ( \"Building feature definitions for feature_store\" ) prefix = generate_prefix () file_source_config = feast_config [ \"file_source\" ] file_source_definition = generate_file_source ( file_source_config , file_name ) entity_config = feast_config [ \"entity\" ] entity_definition = generate_entity_definition ( entity_config ) feature_view_config = feast_config [ \"feature_view\" ] columns_to_exclude = [ feast_config [ \"entity\" ][ \"id_col\" ], feast_config [ \"file_source\" ][ \"timestamp_col\" ], feast_config [ \"file_source\" ][ \"create_timestamp_col\" ], ] feature_view = generate_feature_view ( types , columns_to_exclude , feature_view_config , entity_config [ \"name\" ] ) feature_service = ( generate_feature_service ( feast_config [ \"service_name\" ], feature_view_config [ \"name\" ] ) if \"service_name\" in feast_config else \"\" ) complete_file_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"complete_file.txt\" ) with open ( complete_file_path , \"r\" ) as f : template_string = f . read () complete_file_template = Template ( template_string ) data = { \"prefix\" : prefix , \"file_source\" : file_source_definition , \"entity\" : entity_definition , \"feature_view\" : feature_view , \"feature_service\" : feature_service , } file_content = complete_file_template . render ( data ) file_content = format_str ( file_content , mode = FileMode ()) file_content = isort . code ( file_content ) feature_file = os . path . join ( feast_config [ \"file_path\" ], \"anovos.py\" ) with open ( feature_file , \"w\" ) as of : of . write ( file_content ) def generate_feature_service ( service_name: str, view_name: str) Expand source code def generate_feature_service ( service_name : str , view_name : str ): service_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"feature_service.txt\" ) with open ( service_template_path , \"r\" ) as f : template_string = f . read () service_template = Template ( template_string ) data = { \"feature_service_name\" : service_name , \"view_name\" : view_name , } return service_template . render ( data ) def generate_feature_view ( types: list, exclude_list: list, config: dict, entity_name: str) \u2011> str Expand source code def generate_feature_view ( types : list , exclude_list : list , config : dict , entity_name : str ) -> str : source_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"feature_view.txt\" ) with open ( source_template_path , \"r\" ) as f : template_string = f . read () fields = generate_fields ( types , exclude_list ) feature_view_template = Template ( template_string ) data = { \"feature_view_name\" : config [ \"name\" ], \"source\" : ANOVOS_SOURCE , \"view_name\" : config [ \"name\" ], \"entity\" : entity_name , \"fields\" : fields , \"ttl_in_seconds\" : config [ \"ttl_in_seconds\" ], \"owner\" : config [ \"owner\" ], } return feature_view_template . render ( data ) def generate_field ( field_name: str, field_type: str) \u2011> str Expand source code def generate_field ( field_name : str , field_type : str ) -> str : template_string = \"\"\" Field(name=\"{{name}}\", dtype={{type}}), \\n \"\"\" field_template = Template ( template_string ) return field_template . render ({ \"name\" : field_name , \"type\" : field_type }) def generate_fields ( types: list, exclude_list: list) \u2011> str Expand source code def generate_fields ( types : list , exclude_list : list ) -> str : fields = \"\" for ( field_name , field_type ) in types : if field_name not in exclude_list : fields += generate_field ( field_name , dataframe_to_feast_type_mapping [ field_type ] ) return fields def generate_file_source ( config: dict, file_name='Test') \u2011> str Expand source code def generate_file_source ( config : dict , file_name = \"Test\" ) -> str : source_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"file_source.txt\" ) with open ( source_template_path , \"r\" ) as f : template_string = f . read () file_source_template = Template ( template_string ) data = { \"source_name\" : ANOVOS_SOURCE , \"filename\" : file_name , \"ts_column\" : config [ \"timestamp_col\" ], \"create_ts_column\" : config [ \"create_timestamp_col\" ], \"source_description\" : config [ \"description\" ], \"owner\" : config [ \"owner\" ], } return file_source_template . render ( data ) def generate_prefix ( ) Expand source code def generate_prefix (): prefix_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"prefix.txt\" ) with open ( prefix_path , \"r\" ) as f : prefix = f . read () return prefix","title":"<code>feast_exporter</code>"},{"location":"api/feature_store/feast_exporter.html#feast_exporter","text":"Expand source code import os from datetime import datetime import isort from black import FileMode , format_str from jinja2 import Template from pyspark.sql import DataFrame from pyspark.sql.functions import lit ANOVOS_SOURCE = \"anovos_source\" dataframe_to_feast_type_mapping = { \"string\" : \"String\" , \"int\" : \"Int64\" , \"float\" : \"Float32\" , \"timestamp\" : \"String\" # TODO: default type } def check_feast_configuration ( feast_config : dict , repartition_count : int ): if repartition_count != 1 : raise ValueError ( \"Please, set repartition parameter to 1 in write_main block in your config yml!\" ) if \"file_path\" not in feast_config : raise ValueError ( \"Please, provide a path to the anovos feature_store repository!\" ) if \"entity\" not in feast_config : raise ValueError ( \"Please, provide an entity definition in your config yml!\" ) if \"file_source\" not in feast_config : raise ValueError ( \"Please, provide a file source definition in your config yml!\" ) if \"feature_view\" not in feast_config : raise ValueError ( \"Please, provide a feature view definition in your config yml!\" ) def generate_entity_definition ( config : dict ) -> str : source_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"entity.txt\" ) with open ( source_template_path , \"r\" ) as f : template_string = f . read () entity_template = Template ( template_string ) data = { \"entity_name\" : config [ \"name\" ], \"join_keys\" : config [ \"id_col\" ], \"value_type\" : \"STRING\" , \"description\" : config [ \"description\" ], } return entity_template . render ( data ) def generate_feature_view ( types : list , exclude_list : list , config : dict , entity_name : str ) -> str : source_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"feature_view.txt\" ) with open ( source_template_path , \"r\" ) as f : template_string = f . read () fields = generate_fields ( types , exclude_list ) feature_view_template = Template ( template_string ) data = { \"feature_view_name\" : config [ \"name\" ], \"source\" : ANOVOS_SOURCE , \"view_name\" : config [ \"name\" ], \"entity\" : entity_name , \"fields\" : fields , \"ttl_in_seconds\" : config [ \"ttl_in_seconds\" ], \"owner\" : config [ \"owner\" ], } return feature_view_template . render ( data ) def generate_fields ( types : list , exclude_list : list ) -> str : fields = \"\" for ( field_name , field_type ) in types : if field_name not in exclude_list : fields += generate_field ( field_name , dataframe_to_feast_type_mapping [ field_type ] ) return fields def generate_field ( field_name : str , field_type : str ) -> str : template_string = \"\"\" Field(name=\"{{name}}\", dtype={{type}}), \\n \"\"\" field_template = Template ( template_string ) return field_template . render ({ \"name\" : field_name , \"type\" : field_type }) def generate_file_source ( config : dict , file_name = \"Test\" ) -> str : source_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"file_source.txt\" ) with open ( source_template_path , \"r\" ) as f : template_string = f . read () file_source_template = Template ( template_string ) data = { \"source_name\" : ANOVOS_SOURCE , \"filename\" : file_name , \"ts_column\" : config [ \"timestamp_col\" ], \"create_ts_column\" : config [ \"create_timestamp_col\" ], \"source_description\" : config [ \"description\" ], \"owner\" : config [ \"owner\" ], } return file_source_template . render ( data ) def generate_prefix (): prefix_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"prefix.txt\" ) with open ( prefix_path , \"r\" ) as f : prefix = f . read () return prefix def generate_feature_service ( service_name : str , view_name : str ): service_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"feature_service.txt\" ) with open ( service_template_path , \"r\" ) as f : template_string = f . read () service_template = Template ( template_string ) data = { \"feature_service_name\" : service_name , \"view_name\" : view_name , } return service_template . render ( data ) def generate_feature_description ( types : list , feast_config : dict , file_name : str ): print ( \"Building feature definitions for feature_store\" ) prefix = generate_prefix () file_source_config = feast_config [ \"file_source\" ] file_source_definition = generate_file_source ( file_source_config , file_name ) entity_config = feast_config [ \"entity\" ] entity_definition = generate_entity_definition ( entity_config ) feature_view_config = feast_config [ \"feature_view\" ] columns_to_exclude = [ feast_config [ \"entity\" ][ \"id_col\" ], feast_config [ \"file_source\" ][ \"timestamp_col\" ], feast_config [ \"file_source\" ][ \"create_timestamp_col\" ], ] feature_view = generate_feature_view ( types , columns_to_exclude , feature_view_config , entity_config [ \"name\" ] ) feature_service = ( generate_feature_service ( feast_config [ \"service_name\" ], feature_view_config [ \"name\" ] ) if \"service_name\" in feast_config else \"\" ) complete_file_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"complete_file.txt\" ) with open ( complete_file_path , \"r\" ) as f : template_string = f . read () complete_file_template = Template ( template_string ) data = { \"prefix\" : prefix , \"file_source\" : file_source_definition , \"entity\" : entity_definition , \"feature_view\" : feature_view , \"feature_service\" : feature_service , } file_content = complete_file_template . render ( data ) file_content = format_str ( file_content , mode = FileMode ()) file_content = isort . code ( file_content ) feature_file = os . path . join ( feast_config [ \"file_path\" ], \"anovos.py\" ) with open ( feature_file , \"w\" ) as of : of . write ( file_content ) def add_timestamp_columns ( idf : DataFrame , feast_file_source__config : dict ): print ( \"Adding timestamp columns\" ) return idf . withColumn ( feast_file_source__config [ \"timestamp_col\" ], lit ( datetime . now ()) ) . withColumn ( feast_file_source__config [ \"create_timestamp_col\" ], lit ( datetime . now ()))","title":"feast_exporter"},{"location":"api/feature_store/feast_exporter.html#functions","text":"def add_timestamp_columns ( idf: pyspark.sql.dataframe.DataFrame, feast_file_source__config: dict) Expand source code def add_timestamp_columns ( idf : DataFrame , feast_file_source__config : dict ): print ( \"Adding timestamp columns\" ) return idf . withColumn ( feast_file_source__config [ \"timestamp_col\" ], lit ( datetime . now ()) ) . withColumn ( feast_file_source__config [ \"create_timestamp_col\" ], lit ( datetime . now ())) def check_feast_configuration ( feast_config: dict, repartition_count: int) Expand source code def check_feast_configuration ( feast_config : dict , repartition_count : int ): if repartition_count != 1 : raise ValueError ( \"Please, set repartition parameter to 1 in write_main block in your config yml!\" ) if \"file_path\" not in feast_config : raise ValueError ( \"Please, provide a path to the anovos feature_store repository!\" ) if \"entity\" not in feast_config : raise ValueError ( \"Please, provide an entity definition in your config yml!\" ) if \"file_source\" not in feast_config : raise ValueError ( \"Please, provide a file source definition in your config yml!\" ) if \"feature_view\" not in feast_config : raise ValueError ( \"Please, provide a feature view definition in your config yml!\" ) def generate_entity_definition ( config: dict) \u2011> str Expand source code def generate_entity_definition ( config : dict ) -> str : source_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"entity.txt\" ) with open ( source_template_path , \"r\" ) as f : template_string = f . read () entity_template = Template ( template_string ) data = { \"entity_name\" : config [ \"name\" ], \"join_keys\" : config [ \"id_col\" ], \"value_type\" : \"STRING\" , \"description\" : config [ \"description\" ], } return entity_template . render ( data ) def generate_feature_description ( types: list, feast_config: dict, file_name: str) Expand source code def generate_feature_description ( types : list , feast_config : dict , file_name : str ): print ( \"Building feature definitions for feature_store\" ) prefix = generate_prefix () file_source_config = feast_config [ \"file_source\" ] file_source_definition = generate_file_source ( file_source_config , file_name ) entity_config = feast_config [ \"entity\" ] entity_definition = generate_entity_definition ( entity_config ) feature_view_config = feast_config [ \"feature_view\" ] columns_to_exclude = [ feast_config [ \"entity\" ][ \"id_col\" ], feast_config [ \"file_source\" ][ \"timestamp_col\" ], feast_config [ \"file_source\" ][ \"create_timestamp_col\" ], ] feature_view = generate_feature_view ( types , columns_to_exclude , feature_view_config , entity_config [ \"name\" ] ) feature_service = ( generate_feature_service ( feast_config [ \"service_name\" ], feature_view_config [ \"name\" ] ) if \"service_name\" in feast_config else \"\" ) complete_file_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"complete_file.txt\" ) with open ( complete_file_path , \"r\" ) as f : template_string = f . read () complete_file_template = Template ( template_string ) data = { \"prefix\" : prefix , \"file_source\" : file_source_definition , \"entity\" : entity_definition , \"feature_view\" : feature_view , \"feature_service\" : feature_service , } file_content = complete_file_template . render ( data ) file_content = format_str ( file_content , mode = FileMode ()) file_content = isort . code ( file_content ) feature_file = os . path . join ( feast_config [ \"file_path\" ], \"anovos.py\" ) with open ( feature_file , \"w\" ) as of : of . write ( file_content ) def generate_feature_service ( service_name: str, view_name: str) Expand source code def generate_feature_service ( service_name : str , view_name : str ): service_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"feature_service.txt\" ) with open ( service_template_path , \"r\" ) as f : template_string = f . read () service_template = Template ( template_string ) data = { \"feature_service_name\" : service_name , \"view_name\" : view_name , } return service_template . render ( data ) def generate_feature_view ( types: list, exclude_list: list, config: dict, entity_name: str) \u2011> str Expand source code def generate_feature_view ( types : list , exclude_list : list , config : dict , entity_name : str ) -> str : source_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"feature_view.txt\" ) with open ( source_template_path , \"r\" ) as f : template_string = f . read () fields = generate_fields ( types , exclude_list ) feature_view_template = Template ( template_string ) data = { \"feature_view_name\" : config [ \"name\" ], \"source\" : ANOVOS_SOURCE , \"view_name\" : config [ \"name\" ], \"entity\" : entity_name , \"fields\" : fields , \"ttl_in_seconds\" : config [ \"ttl_in_seconds\" ], \"owner\" : config [ \"owner\" ], } return feature_view_template . render ( data ) def generate_field ( field_name: str, field_type: str) \u2011> str Expand source code def generate_field ( field_name : str , field_type : str ) -> str : template_string = \"\"\" Field(name=\"{{name}}\", dtype={{type}}), \\n \"\"\" field_template = Template ( template_string ) return field_template . render ({ \"name\" : field_name , \"type\" : field_type }) def generate_fields ( types: list, exclude_list: list) \u2011> str Expand source code def generate_fields ( types : list , exclude_list : list ) -> str : fields = \"\" for ( field_name , field_type ) in types : if field_name not in exclude_list : fields += generate_field ( field_name , dataframe_to_feast_type_mapping [ field_type ] ) return fields def generate_file_source ( config: dict, file_name='Test') \u2011> str Expand source code def generate_file_source ( config : dict , file_name = \"Test\" ) -> str : source_template_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"file_source.txt\" ) with open ( source_template_path , \"r\" ) as f : template_string = f . read () file_source_template = Template ( template_string ) data = { \"source_name\" : ANOVOS_SOURCE , \"filename\" : file_name , \"ts_column\" : config [ \"timestamp_col\" ], \"create_ts_column\" : config [ \"create_timestamp_col\" ], \"source_description\" : config [ \"description\" ], \"owner\" : config [ \"owner\" ], } return file_source_template . render ( data ) def generate_prefix ( ) Expand source code def generate_prefix (): prefix_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ )), \"templates\" , \"prefix.txt\" ) with open ( prefix_path , \"r\" ) as f : prefix = f . read () return prefix","title":"Functions"},{"location":"api/feature_store/feature_retrieval.html","text":"feature_retrieval Expand source code import sys from datetime import datetime import feast import pandas as pd def retrieve_historical_feature_demo ( repo_path : str ): income_entities = pd . DataFrame . from_dict ( { \"ifa\" : [ \"27a\" , \"30a\" , \"475a\" , \"965a\" , \"1678a\" , \"1698a\" , \"1807a\" , \"1951a\" , \"2041a\" , \"2215a\" , ], \"event_time\" : [ datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), ], } ) fs = feast . FeatureStore ( repo_path = repo_path ) income_features_df = fs . get_historical_features ( entity_df = income_entities , features = [ \"income_view:income\" , \"income_view:latent_0\" , \"income_view:latent_1\" , \"income_view:latent_2\" , \"income_view:latent_3\" , ], ) . to_df () print ( income_features_df . head ()) # train model from here ... feature_service = fs . get_feature_service ( \"income_feature_service\" ) income_features_by_service_df = fs . get_historical_features ( features = feature_service , entity_df = income_entities ) . to_df () print ( income_features_by_service_df . head ()) if __name__ == \"__main__\" : if len ( sys . argv ) < 2 : print ( \"Please, provide a path to anovos feature repo!\" ) exit ( 1 ) path = sys . argv [ 1 ] retrieve_historical_feature_demo ( repo_path = path ) Functions def retrieve_historical_feature_demo ( repo_path: str) Expand source code def retrieve_historical_feature_demo ( repo_path : str ): income_entities = pd . DataFrame . from_dict ( { \"ifa\" : [ \"27a\" , \"30a\" , \"475a\" , \"965a\" , \"1678a\" , \"1698a\" , \"1807a\" , \"1951a\" , \"2041a\" , \"2215a\" , ], \"event_time\" : [ datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), ], } ) fs = feast . FeatureStore ( repo_path = repo_path ) income_features_df = fs . get_historical_features ( entity_df = income_entities , features = [ \"income_view:income\" , \"income_view:latent_0\" , \"income_view:latent_1\" , \"income_view:latent_2\" , \"income_view:latent_3\" , ], ) . to_df () print ( income_features_df . head ()) # train model from here ... feature_service = fs . get_feature_service ( \"income_feature_service\" ) income_features_by_service_df = fs . get_historical_features ( features = feature_service , entity_df = income_entities ) . to_df () print ( income_features_by_service_df . head ())","title":"<code>feature_retrieval</code>"},{"location":"api/feature_store/feature_retrieval.html#feature_retrieval","text":"Expand source code import sys from datetime import datetime import feast import pandas as pd def retrieve_historical_feature_demo ( repo_path : str ): income_entities = pd . DataFrame . from_dict ( { \"ifa\" : [ \"27a\" , \"30a\" , \"475a\" , \"965a\" , \"1678a\" , \"1698a\" , \"1807a\" , \"1951a\" , \"2041a\" , \"2215a\" , ], \"event_time\" : [ datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), ], } ) fs = feast . FeatureStore ( repo_path = repo_path ) income_features_df = fs . get_historical_features ( entity_df = income_entities , features = [ \"income_view:income\" , \"income_view:latent_0\" , \"income_view:latent_1\" , \"income_view:latent_2\" , \"income_view:latent_3\" , ], ) . to_df () print ( income_features_df . head ()) # train model from here ... feature_service = fs . get_feature_service ( \"income_feature_service\" ) income_features_by_service_df = fs . get_historical_features ( features = feature_service , entity_df = income_entities ) . to_df () print ( income_features_by_service_df . head ()) if __name__ == \"__main__\" : if len ( sys . argv ) < 2 : print ( \"Please, provide a path to anovos feature repo!\" ) exit ( 1 ) path = sys . argv [ 1 ] retrieve_historical_feature_demo ( repo_path = path )","title":"feature_retrieval"},{"location":"api/feature_store/feature_retrieval.html#functions","text":"def retrieve_historical_feature_demo ( repo_path: str) Expand source code def retrieve_historical_feature_demo ( repo_path : str ): income_entities = pd . DataFrame . from_dict ( { \"ifa\" : [ \"27a\" , \"30a\" , \"475a\" , \"965a\" , \"1678a\" , \"1698a\" , \"1807a\" , \"1951a\" , \"2041a\" , \"2215a\" , ], \"event_time\" : [ datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), ], } ) fs = feast . FeatureStore ( repo_path = repo_path ) income_features_df = fs . get_historical_features ( entity_df = income_entities , features = [ \"income_view:income\" , \"income_view:latent_0\" , \"income_view:latent_1\" , \"income_view:latent_2\" , \"income_view:latent_3\" , ], ) . to_df () print ( income_features_df . head ()) # train model from here ... feature_service = fs . get_feature_service ( \"income_feature_service\" ) income_features_by_service_df = fs . get_historical_features ( features = feature_service , entity_df = income_entities ) . to_df () print ( income_features_by_service_df . head ())","title":"Functions"},{"location":"api/shared/_index.html","text":"Overview Sub-modules anovos.shared.spark anovos.shared.utils","title":"Overview"},{"location":"api/shared/_index.html#overview","text":"","title":"Overview"},{"location":"api/shared/_index.html#sub-modules","text":"anovos.shared.spark anovos.shared.utils","title":"Sub-modules"},{"location":"api/shared/spark.html","text":"spark Expand source code from os import environ import __main__ import findspark from loguru import logger from packaging import version findspark . init () import pyspark from pyspark.sql import SparkSession , SQLContext if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): SPARK_JARS_PACKAGES = [ \"io.github.histogrammar:histogrammar_2.11:1.0.20\" , \"io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20\" , \"org.apache.spark:spark-avro_2.11:\" + str ( pyspark . __version__ ), ] else : SPARK_JARS_PACKAGES = [ \"io.github.histogrammar:histogrammar_2.12:1.0.20\" , \"io.github.histogrammar:histogrammar-sparksql_2.12:1.0.20\" , \"org.apache.spark:spark-avro_2.12:\" + str ( pyspark . __version__ ), ] def init_spark ( app_name = \"anovos\" , master = \"local[*]\" , jars_packages = None , py_files = None , spark_config = None , ): \"\"\" Parameters ---------- app_name Name of Spark app. (Default value = \"anovos\") master Cluster connection details Defaults to local[*] which means to run Spark locally with as many worker threads as logical cores on the machine. jars_packages List of Spark JAR package names. (Default value = None) py_files List of files to send to Spark cluster (master and workers). (Default value = None) spark_config Dictionary of config key-value pairs. (Default value = None) Returns ------- \"\"\" logger . info ( f \"Getting spark session, context and sql context app_name: { app_name } \" ) # detect execution environment flag_repl = not ( hasattr ( __main__ , \"__file__\" )) flag_debug = \"DEBUG\" in environ . keys () if not ( flag_repl or flag_debug ): spark_builder = SparkSession . builder . appName ( app_name ) else : spark_builder = SparkSession . builder . master ( master ) . appName ( app_name ) if jars_packages is not None and jars_packages : spark_jars_packages = \",\" . join ( list ( jars_packages )) spark_builder . config ( \"spark.jars.packages\" , spark_jars_packages ) if py_files is not None and py_files : spark_files = \",\" . join ( list ( py_files )) spark_builder . config ( \"spark.files\" , spark_files ) if spark_config is not None and spark_config : for key , val in spark_config . items (): spark_builder . config ( key , val ) _spark = spark_builder . getOrCreate () _spark_context = _spark . sparkContext _sql_context = SQLContext ( _spark_context ) return _spark , _spark_context , _sql_context configs = { \"app_name\" : \"Anovos_pipeline\" , \"jars_packages\" : SPARK_JARS_PACKAGES , \"py_files\" : [], \"spark_config\" : { \"spark.python.profile\" : \"false\" , \"spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT\" : \"1\" , \"spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\" : \"1\" , \"spark.sql.session.timeZone\" : \"GMT\" , \"spark.python.profile\" : \"false\" , }, } spark , sc , sqlContext = init_spark ( ** configs ) Functions def init_spark ( app_name='anovos', master='local[*]', jars_packages=None, py_files=None, spark_config=None) Parameters app_name Name of Spark app. (Default value = \"anovos\") master Cluster connection details Defaults to local[*] which means to run Spark locally with as many worker threads as logical cores on the machine. jars_packages List of Spark JAR package names. (Default value = None) py_files List of files to send to Spark cluster (master and workers). (Default value = None) spark_config Dictionary of config key-value pairs. (Default value = None) Returns Expand source code def init_spark ( app_name = \"anovos\" , master = \"local[*]\" , jars_packages = None , py_files = None , spark_config = None , ): \"\"\" Parameters ---------- app_name Name of Spark app. (Default value = \"anovos\") master Cluster connection details Defaults to local[*] which means to run Spark locally with as many worker threads as logical cores on the machine. jars_packages List of Spark JAR package names. (Default value = None) py_files List of files to send to Spark cluster (master and workers). (Default value = None) spark_config Dictionary of config key-value pairs. (Default value = None) Returns ------- \"\"\" logger . info ( f \"Getting spark session, context and sql context app_name: { app_name } \" ) # detect execution environment flag_repl = not ( hasattr ( __main__ , \"__file__\" )) flag_debug = \"DEBUG\" in environ . keys () if not ( flag_repl or flag_debug ): spark_builder = SparkSession . builder . appName ( app_name ) else : spark_builder = SparkSession . builder . master ( master ) . appName ( app_name ) if jars_packages is not None and jars_packages : spark_jars_packages = \",\" . join ( list ( jars_packages )) spark_builder . config ( \"spark.jars.packages\" , spark_jars_packages ) if py_files is not None and py_files : spark_files = \",\" . join ( list ( py_files )) spark_builder . config ( \"spark.files\" , spark_files ) if spark_config is not None and spark_config : for key , val in spark_config . items (): spark_builder . config ( key , val ) _spark = spark_builder . getOrCreate () _spark_context = _spark . sparkContext _sql_context = SQLContext ( _spark_context ) return _spark , _spark_context , _sql_context","title":"<code>spark</code>"},{"location":"api/shared/spark.html#spark","text":"Expand source code from os import environ import __main__ import findspark from loguru import logger from packaging import version findspark . init () import pyspark from pyspark.sql import SparkSession , SQLContext if version . parse ( pyspark . __version__ ) < version . parse ( \"3.0.0\" ): SPARK_JARS_PACKAGES = [ \"io.github.histogrammar:histogrammar_2.11:1.0.20\" , \"io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20\" , \"org.apache.spark:spark-avro_2.11:\" + str ( pyspark . __version__ ), ] else : SPARK_JARS_PACKAGES = [ \"io.github.histogrammar:histogrammar_2.12:1.0.20\" , \"io.github.histogrammar:histogrammar-sparksql_2.12:1.0.20\" , \"org.apache.spark:spark-avro_2.12:\" + str ( pyspark . __version__ ), ] def init_spark ( app_name = \"anovos\" , master = \"local[*]\" , jars_packages = None , py_files = None , spark_config = None , ): \"\"\" Parameters ---------- app_name Name of Spark app. (Default value = \"anovos\") master Cluster connection details Defaults to local[*] which means to run Spark locally with as many worker threads as logical cores on the machine. jars_packages List of Spark JAR package names. (Default value = None) py_files List of files to send to Spark cluster (master and workers). (Default value = None) spark_config Dictionary of config key-value pairs. (Default value = None) Returns ------- \"\"\" logger . info ( f \"Getting spark session, context and sql context app_name: { app_name } \" ) # detect execution environment flag_repl = not ( hasattr ( __main__ , \"__file__\" )) flag_debug = \"DEBUG\" in environ . keys () if not ( flag_repl or flag_debug ): spark_builder = SparkSession . builder . appName ( app_name ) else : spark_builder = SparkSession . builder . master ( master ) . appName ( app_name ) if jars_packages is not None and jars_packages : spark_jars_packages = \",\" . join ( list ( jars_packages )) spark_builder . config ( \"spark.jars.packages\" , spark_jars_packages ) if py_files is not None and py_files : spark_files = \",\" . join ( list ( py_files )) spark_builder . config ( \"spark.files\" , spark_files ) if spark_config is not None and spark_config : for key , val in spark_config . items (): spark_builder . config ( key , val ) _spark = spark_builder . getOrCreate () _spark_context = _spark . sparkContext _sql_context = SQLContext ( _spark_context ) return _spark , _spark_context , _sql_context configs = { \"app_name\" : \"Anovos_pipeline\" , \"jars_packages\" : SPARK_JARS_PACKAGES , \"py_files\" : [], \"spark_config\" : { \"spark.python.profile\" : \"false\" , \"spark.yarn.appMasterEnv.ARROW_PRE_0_15_IPC_FORMAT\" : \"1\" , \"spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\" : \"1\" , \"spark.sql.session.timeZone\" : \"GMT\" , \"spark.python.profile\" : \"false\" , }, } spark , sc , sqlContext = init_spark ( ** configs )","title":"spark"},{"location":"api/shared/spark.html#functions","text":"def init_spark ( app_name='anovos', master='local[*]', jars_packages=None, py_files=None, spark_config=None)","title":"Functions"},{"location":"api/shared/utils.html","text":"utils Expand source code from itertools import chain from pyspark.sql import functions as F def flatten_dataframe ( idf , fixed_cols ): \"\"\" Parameters ---------- idf Input Dataframe fixed_cols All columns except in this list will be melted/unpivoted Returns ------- \"\"\" valid_cols = [ e for e in idf . columns if e not in fixed_cols ] key_and_val = F . create_map ( list ( chain . from_iterable ([[ F . lit ( c ), F . col ( c )] for c in valid_cols ])) ) odf = idf . select ( * fixed_cols , F . explode ( key_and_val )) return odf def transpose_dataframe ( idf , fixed_col ): \"\"\" Parameters ---------- idf Input Dataframe fixed_col Values in this column will be converted into columns as header. Ideally all values should be unique Returns ------- \"\"\" idf_flatten = flatten_dataframe ( idf , fixed_cols = [ fixed_col ]) odf = idf_flatten . groupBy ( \"key\" ) . pivot ( fixed_col ) . agg ( F . first ( \"value\" )) return odf def attributeType_segregation ( idf ): \"\"\" Parameters ---------- idf Input Dataframe Returns ------- \"\"\" cat_cols = [] num_cols = [] other_cols = [] for i in idf . dtypes : if i [ 1 ] == \"string\" : cat_cols . append ( i [ 0 ]) elif ( i [ 1 ] in ( \"double\" , \"int\" , \"bigint\" , \"float\" , \"long\" )) | ( i [ 1 ] . startswith ( \"decimal\" ) ): num_cols . append ( i [ 0 ]) else : other_cols . append ( i [ 0 ]) return num_cols , cat_cols , other_cols def get_dtype ( idf , col ): \"\"\" Parameters ---------- idf Input Dataframe col Column Name for datatype detection Returns ------- \"\"\" return [ dtype for name , dtype in idf . dtypes if name == col ][ 0 ] def ends_with ( string , end_str = \"/\" ): \"\"\" Parameters ---------- string \"s3:mw-bucket\" end_str return: \"s3:mw-bucket/\" (Default value = \"/\") Returns ------- \"\"\" string = str ( string ) if string . endswith ( end_str ): return string return string + end_str def pairwise_reduce ( op , x ): \"\"\" Parameters ---------- op Operation x Input list Returns ------- \"\"\" while len ( x ) > 1 : v = [ op ( i , j ) for i , j in zip ( x [:: 2 ], x [ 1 :: 2 ])] if len ( x ) > 1 and len ( x ) % 2 == 1 : v [ - 1 ] = op ( v [ - 1 ], x [ - 1 ]) x = v return x [ 0 ] def output_to_local ( output_path ): \"\"\" Parameters ---------- output_path : input_path. e.g. dbfs:/sample_path Returns ------- type path after removing \":\" and appending \"/\" . e.g. /dbfs/sample_path \"\"\" punctuations = \":\" for x in output_path : if x in punctuations : local_path = output_path . replace ( x , \"\" ) local_path = \"/\" + local_path return local_path def path_ak8s_modify ( output_path ): \"\"\" Parameters ---------- output_path : input_path. e.g. \"wasbs://anovos@anovosasktest.blob.core.windows.net/datasrc/report_stats_ts1\" Returns ------- type path after converting . e.g. \"https://anovosasktest.blob.core.windows.net/anovos/datasrc/report_stats_ts1\" \"\"\" container_name = output_path . split ( \"//\" )[ 1 ] . split ( \"@\" )[ 0 ] url = ( \"https://\" + output_path . split ( \"//\" )[ 1 ] . split ( \"@\" )[ 1 ] . split ( \"windows.net/\" )[ 0 ] + \"windows.net\" ) file_path_name = output_path . split ( \"//\" )[ 1 ] . split ( \"@\" )[ 1 ] . split ( \"windows.net/\" )[ 1 ] final_path = url + \"/\" + container_name + \"/\" + file_path_name return str ( final_path ) Functions def attributeType_segregation ( idf) Parameters idf Input Dataframe Returns Expand source code def attributeType_segregation ( idf ): \"\"\" Parameters ---------- idf Input Dataframe Returns ------- \"\"\" cat_cols = [] num_cols = [] other_cols = [] for i in idf . dtypes : if i [ 1 ] == \"string\" : cat_cols . append ( i [ 0 ]) elif ( i [ 1 ] in ( \"double\" , \"int\" , \"bigint\" , \"float\" , \"long\" )) | ( i [ 1 ] . startswith ( \"decimal\" ) ): num_cols . append ( i [ 0 ]) else : other_cols . append ( i [ 0 ]) return num_cols , cat_cols , other_cols def ends_with ( string, end_str='/') Parameters string \"s3:mw-bucket\" end_str return: \"s3:mw-bucket/\" (Default value = \"/\") Returns Expand source code def ends_with ( string , end_str = \"/\" ): \"\"\" Parameters ---------- string \"s3:mw-bucket\" end_str return: \"s3:mw-bucket/\" (Default value = \"/\") Returns ------- \"\"\" string = str ( string ) if string . endswith ( end_str ): return string return string + end_str def flatten_dataframe ( idf, fixed_cols) Parameters idf Input Dataframe fixed_cols All columns except in this list will be melted/unpivoted Returns Expand source code def flatten_dataframe ( idf , fixed_cols ): \"\"\" Parameters ---------- idf Input Dataframe fixed_cols All columns except in this list will be melted/unpivoted Returns ------- \"\"\" valid_cols = [ e for e in idf . columns if e not in fixed_cols ] key_and_val = F . create_map ( list ( chain . from_iterable ([[ F . lit ( c ), F . col ( c )] for c in valid_cols ])) ) odf = idf . select ( * fixed_cols , F . explode ( key_and_val )) return odf def get_dtype ( idf, col) Parameters idf Input Dataframe col Column Name for datatype detection Returns Expand source code def get_dtype ( idf , col ): \"\"\" Parameters ---------- idf Input Dataframe col Column Name for datatype detection Returns ------- \"\"\" return [ dtype for name , dtype in idf . dtypes if name == col ][ 0 ] def output_to_local ( output_path) Parameters output_path : input_path. e.g. dbfs:/sample_path Returns type path after removing \":\" and appending \"/\" . e.g. /dbfs/sample_path Expand source code def output_to_local ( output_path ): \"\"\" Parameters ---------- output_path : input_path. e.g. dbfs:/sample_path Returns ------- type path after removing \":\" and appending \"/\" . e.g. /dbfs/sample_path \"\"\" punctuations = \":\" for x in output_path : if x in punctuations : local_path = output_path . replace ( x , \"\" ) local_path = \"/\" + local_path return local_path def pairwise_reduce ( op, x) Parameters op Operation x Input list Returns Expand source code def pairwise_reduce ( op , x ): \"\"\" Parameters ---------- op Operation x Input list Returns ------- \"\"\" while len ( x ) > 1 : v = [ op ( i , j ) for i , j in zip ( x [:: 2 ], x [ 1 :: 2 ])] if len ( x ) > 1 and len ( x ) % 2 == 1 : v [ - 1 ] = op ( v [ - 1 ], x [ - 1 ]) x = v return x [ 0 ] def path_ak8s_modify ( output_path) Parameters output_path : input_path. e.g. \"wasbs://anovos@anovosasktest.blob.core.windows.net/datasrc/report_stats_ts1\" Returns type path after converting . e.g. \"https://anovosasktest.blob.core.windows.net/anovos/datasrc/report_stats_ts1\" Expand source code def path_ak8s_modify ( output_path ): \"\"\" Parameters ---------- output_path : input_path. e.g. \"wasbs://anovos@anovosasktest.blob.core.windows.net/datasrc/report_stats_ts1\" Returns ------- type path after converting . e.g. \"https://anovosasktest.blob.core.windows.net/anovos/datasrc/report_stats_ts1\" \"\"\" container_name = output_path . split ( \"//\" )[ 1 ] . split ( \"@\" )[ 0 ] url = ( \"https://\" + output_path . split ( \"//\" )[ 1 ] . split ( \"@\" )[ 1 ] . split ( \"windows.net/\" )[ 0 ] + \"windows.net\" ) file_path_name = output_path . split ( \"//\" )[ 1 ] . split ( \"@\" )[ 1 ] . split ( \"windows.net/\" )[ 1 ] final_path = url + \"/\" + container_name + \"/\" + file_path_name return str ( final_path ) def transpose_dataframe ( idf, fixed_col) Parameters idf Input Dataframe fixed_col Values in this column will be converted into columns as header. Ideally all values should be unique Returns Expand source code def transpose_dataframe ( idf , fixed_col ): \"\"\" Parameters ---------- idf Input Dataframe fixed_col Values in this column will be converted into columns as header. Ideally all values should be unique Returns ------- \"\"\" idf_flatten = flatten_dataframe ( idf , fixed_cols = [ fixed_col ]) odf = idf_flatten . groupBy ( \"key\" ) . pivot ( fixed_col ) . agg ( F . first ( \"value\" )) return odf","title":"<code>utils</code>"},{"location":"api/shared/utils.html#utils","text":"Expand source code from itertools import chain from pyspark.sql import functions as F def flatten_dataframe ( idf , fixed_cols ): \"\"\" Parameters ---------- idf Input Dataframe fixed_cols All columns except in this list will be melted/unpivoted Returns ------- \"\"\" valid_cols = [ e for e in idf . columns if e not in fixed_cols ] key_and_val = F . create_map ( list ( chain . from_iterable ([[ F . lit ( c ), F . col ( c )] for c in valid_cols ])) ) odf = idf . select ( * fixed_cols , F . explode ( key_and_val )) return odf def transpose_dataframe ( idf , fixed_col ): \"\"\" Parameters ---------- idf Input Dataframe fixed_col Values in this column will be converted into columns as header. Ideally all values should be unique Returns ------- \"\"\" idf_flatten = flatten_dataframe ( idf , fixed_cols = [ fixed_col ]) odf = idf_flatten . groupBy ( \"key\" ) . pivot ( fixed_col ) . agg ( F . first ( \"value\" )) return odf def attributeType_segregation ( idf ): \"\"\" Parameters ---------- idf Input Dataframe Returns ------- \"\"\" cat_cols = [] num_cols = [] other_cols = [] for i in idf . dtypes : if i [ 1 ] == \"string\" : cat_cols . append ( i [ 0 ]) elif ( i [ 1 ] in ( \"double\" , \"int\" , \"bigint\" , \"float\" , \"long\" )) | ( i [ 1 ] . startswith ( \"decimal\" ) ): num_cols . append ( i [ 0 ]) else : other_cols . append ( i [ 0 ]) return num_cols , cat_cols , other_cols def get_dtype ( idf , col ): \"\"\" Parameters ---------- idf Input Dataframe col Column Name for datatype detection Returns ------- \"\"\" return [ dtype for name , dtype in idf . dtypes if name == col ][ 0 ] def ends_with ( string , end_str = \"/\" ): \"\"\" Parameters ---------- string \"s3:mw-bucket\" end_str return: \"s3:mw-bucket/\" (Default value = \"/\") Returns ------- \"\"\" string = str ( string ) if string . endswith ( end_str ): return string return string + end_str def pairwise_reduce ( op , x ): \"\"\" Parameters ---------- op Operation x Input list Returns ------- \"\"\" while len ( x ) > 1 : v = [ op ( i , j ) for i , j in zip ( x [:: 2 ], x [ 1 :: 2 ])] if len ( x ) > 1 and len ( x ) % 2 == 1 : v [ - 1 ] = op ( v [ - 1 ], x [ - 1 ]) x = v return x [ 0 ] def output_to_local ( output_path ): \"\"\" Parameters ---------- output_path : input_path. e.g. dbfs:/sample_path Returns ------- type path after removing \":\" and appending \"/\" . e.g. /dbfs/sample_path \"\"\" punctuations = \":\" for x in output_path : if x in punctuations : local_path = output_path . replace ( x , \"\" ) local_path = \"/\" + local_path return local_path def path_ak8s_modify ( output_path ): \"\"\" Parameters ---------- output_path : input_path. e.g. \"wasbs://anovos@anovosasktest.blob.core.windows.net/datasrc/report_stats_ts1\" Returns ------- type path after converting . e.g. \"https://anovosasktest.blob.core.windows.net/anovos/datasrc/report_stats_ts1\" \"\"\" container_name = output_path . split ( \"//\" )[ 1 ] . split ( \"@\" )[ 0 ] url = ( \"https://\" + output_path . split ( \"//\" )[ 1 ] . split ( \"@\" )[ 1 ] . split ( \"windows.net/\" )[ 0 ] + \"windows.net\" ) file_path_name = output_path . split ( \"//\" )[ 1 ] . split ( \"@\" )[ 1 ] . split ( \"windows.net/\" )[ 1 ] final_path = url + \"/\" + container_name + \"/\" + file_path_name return str ( final_path )","title":"utils"},{"location":"api/shared/utils.html#functions","text":"def attributeType_segregation ( idf)","title":"Functions"},{"location":"community/code-of-conduct.html","text":"Anovos Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at coc@anovos.ai. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of Conduct"},{"location":"community/code-of-conduct.html#anovos-code-of-conduct","text":"","title":"Anovos Code of Conduct"},{"location":"community/code-of-conduct.html#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"community/code-of-conduct.html#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"community/code-of-conduct.html#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"community/code-of-conduct.html#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"community/code-of-conduct.html#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at coc@anovos.ai. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"community/code-of-conduct.html#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"community/code-of-conduct.html#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"community/code-of-conduct.html#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"community/code-of-conduct.html#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"community/code-of-conduct.html#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"community/code-of-conduct.html#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"community/communication.html","text":"Anovos Communication Channels There are different ways and channels that you can use as a community member or contributor to ask questions, meet fellow Anovos users, and collaborate on improving the library. \ud83d\udcac Slack The Anovos community has a channel on the Feature Engineers Slack Workspace called #anovos . Feel free to join us there! It's the perfect place get in touch with the community as well as the maintainers of Anovos . \ud83d\udea7 GitHub Issues If you encounter a bug or have a feature request, please file an issue on GitHub . We'll make sure to get back to you in time.","title":"Communication"},{"location":"community/communication.html#anovos-communication-channels","text":"There are different ways and channels that you can use as a community member or contributor to ask questions, meet fellow Anovos users, and collaborate on improving the library.","title":"Anovos Communication Channels"},{"location":"community/communication.html#slack","text":"The Anovos community has a channel on the Feature Engineers Slack Workspace called #anovos . Feel free to join us there! It's the perfect place get in touch with the community as well as the maintainers of Anovos .","title":"\ud83d\udcac Slack"},{"location":"community/communication.html#github-issues","text":"If you encounter a bug or have a feature request, please file an issue on GitHub . We'll make sure to get back to you in time.","title":"\ud83d\udea7 GitHub Issues"},{"location":"community/contributing.html","text":"Contributing to Anovos We'd love to have you join us in making Anovos the number one choice for ML feature engineering! \ud83d\ude80 Getting Started with Anovos Development Anovos is an open source project that brings automation to the feature engineering process. To get Anovos up and running on your local machine, follow the Getting Started Guide . The Anovos GitHub Organization contains all the repositories, sample data, and notebooks you need to start working on improvements to the library. \ud83d\udee0 How to Get Involved First of all: We appreciate each and every contribution, and we'd love to get your input! Contributions to Anovos can take many different shapes and forms, for example: Reporting a bug Discussing the current state of the code Submitting a fix Proposing new features Testing out new features Contributing to the docs Giving talks about Anovos at meetups, conferences, and webinars If you're interested in contributing but don't quite know where to start, please don't hesitate to reach out to the maintainers . \u2328 Contributing Code to Anovos Pull requests are the best way to propose changes to the codebase. We follow the GitHub Flow pattern, and everything happens through pull requests. We welcome pull requests by community contributors at all times. To make it simple, please follow these steps to contribute: Fork the repo you're updating and create your branch from main. If you've added code, add the corresponding tests. If you've changed APIs, update the documentation. Ensure that the test suite passes. Issue that pull request! \u2139 Any contributions you make will be under the Apache Software License 2.0. See the License page for more information. \ud83d\udcdd Conventions for Commit Messages Help reviewers know what you're contributing by writing good commit messages. The first line of the commit message is the subject; this should be followed by a blank line and then a message describing the intent and purpose of the commit. We based these guidelines on a post by Chris Beams . When you commit, you are accepting our DCO: Developer Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 1 Letterman Drive Suite D4700 San Francisco, CA, 94129 Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. When you run git commit make sure you sign-off the commit by typing git commit --signoff or git commit -s The commit subject-line should start with an uppercase letter The commit subject-line should not exceed 72 characters in length The commit subject-line should not end with punctuation (., etc) Note: please do not use the GitHub suggestions feature, since it will not allow your commits to be signed-off. When giving a commit body, be sure to: Leave a blank line after the subject-line Make sure all lines are wrapped to 72 characters Here's an example that would be accepted: Add luke to the contributors' _index.md file We need to add luke to the contributors' _index.md file as a contributor. Signed-off-by: Hans <hans@anovos.ai> Some invalid examples: (feat) Add page about X to documentation This example does not follow the convention by adding a custom scheme of (feat) Update the documentation for page X so including fixing A, B, C and D and F. This example will be truncated in the GitHub UI and via git log --oneline If you would like to amend your commit, follow this guide: Git: Rewriting History","title":"Contributing"},{"location":"community/contributing.html#contributing-to-anovos","text":"We'd love to have you join us in making Anovos the number one choice for ML feature engineering!","title":"Contributing to Anovos"},{"location":"community/contributing.html#getting-started-with-anovos-development","text":"Anovos is an open source project that brings automation to the feature engineering process. To get Anovos up and running on your local machine, follow the Getting Started Guide . The Anovos GitHub Organization contains all the repositories, sample data, and notebooks you need to start working on improvements to the library.","title":"\ud83d\ude80 Getting Started with Anovos Development"},{"location":"community/contributing.html#how-to-get-involved","text":"First of all: We appreciate each and every contribution, and we'd love to get your input! Contributions to Anovos can take many different shapes and forms, for example: Reporting a bug Discussing the current state of the code Submitting a fix Proposing new features Testing out new features Contributing to the docs Giving talks about Anovos at meetups, conferences, and webinars If you're interested in contributing but don't quite know where to start, please don't hesitate to reach out to the maintainers .","title":"\ud83d\udee0 How to Get Involved"},{"location":"community/contributing.html#contributing-code-to-anovos","text":"Pull requests are the best way to propose changes to the codebase. We follow the GitHub Flow pattern, and everything happens through pull requests. We welcome pull requests by community contributors at all times. To make it simple, please follow these steps to contribute: Fork the repo you're updating and create your branch from main. If you've added code, add the corresponding tests. If you've changed APIs, update the documentation. Ensure that the test suite passes. Issue that pull request! \u2139 Any contributions you make will be under the Apache Software License 2.0. See the License page for more information.","title":"\u2328 Contributing Code to Anovos"},{"location":"community/contributing.html#conventions-for-commit-messages","text":"Help reviewers know what you're contributing by writing good commit messages. The first line of the commit message is the subject; this should be followed by a blank line and then a message describing the intent and purpose of the commit. We based these guidelines on a post by Chris Beams . When you commit, you are accepting our DCO: Developer Certificate of Origin Version 1.1 Copyright (C) 2004, 2006 The Linux Foundation and its contributors. 1 Letterman Drive Suite D4700 San Francisco, CA, 94129 Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Developer's Certificate of Origin 1.1 By making a contribution to this project, I certify that: (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it. (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved. When you run git commit make sure you sign-off the commit by typing git commit --signoff or git commit -s The commit subject-line should start with an uppercase letter The commit subject-line should not exceed 72 characters in length The commit subject-line should not end with punctuation (., etc) Note: please do not use the GitHub suggestions feature, since it will not allow your commits to be signed-off. When giving a commit body, be sure to: Leave a blank line after the subject-line Make sure all lines are wrapped to 72 characters Here's an example that would be accepted: Add luke to the contributors' _index.md file We need to add luke to the contributors' _index.md file as a contributor. Signed-off-by: Hans <hans@anovos.ai> Some invalid examples: (feat) Add page about X to documentation This example does not follow the convention by adding a custom scheme of (feat) Update the documentation for page X so including fixing A, B, C and D and F. This example will be truncated in the GitHub UI and via git log --oneline If you would like to amend your commit, follow this guide: Git: Rewriting History","title":"\ud83d\udcdd Conventions for Commit Messages"},{"location":"using-anovos/config_file.html","text":"Configuring Workloads Anovos workloads can be described by a YAML configuration file. Such a configuration file defines: the input dataset(s) the analyses and transformations to be performed on the data the output files and dataset(s) the reports to be generated Defining workloads this way allows users to make full use of Anovos capabilities while maintaining an easy-to-grasp overview. Since each configuration file fully describes one workload, these files can be shared, versioned, and run across different compute environments. In the following, we'll describe in detail each of the sections in an Anovos configuration file. If you'd rather see a full example right away, have a look at this example . Note that each section of the configuration file maps to a module of Anovos . You'll find links to the respective sections of the API Documentation that provide much more detailed information on each modules' capabilities than we can squeeze into this guide. \ud83d\udcd1 input_dataset This configuration block describes how the input dataset is loaded and prepared using the data_ingest.data_ingest module. Each Anovos configuration file must contain exactly one input_dataset block. Note that the subsequent operations are performed in the order given here: First, columns are deleted, then selected, then renamed, and then recast. read_dataset \ud83d\udd0e Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers. In the case of a CSV file, this might look like: file_configs : delimiter : \", \" header : True inferSchema : True For more information on available configuration options, see the following external documentation: Read CSV files Read Parquet files Read Avro files delete_column \ud83d\udd0e Corresponds to data_ingest.delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data. \ud83e\udd13 Example: delete_column : [ 'unnecessary' , 'obsolete' , 'outdated' ] select_column \ud83d\udd0e Corresponds to data_ingest.select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing. \ud83e\udd13 Example: select_column : [ 'feature1' , 'feature2' , 'feature3' , 'label' ] rename_column \ud83d\udd0e Corresponds to data_ingest.rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on. \ud83e\udd13 Example: rename_column : list_of_cols : [ 'very_long_column_name' , 'price' ] list_of_newcols : [ 'short_name' , 'label' ] This will rename the column very_long_column_name to short_name and the column price to label . recast_column \ud83d\udd0e Corresponds to data_ingest.recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See \ud83d\udcd6 the Spark documentation for a list of valid datatypes. Note that this field is case-insensitive. \ud83e\udd13 Example: recast_column : list_of_cols : [ 'price' , 'quantity' ] list_of_dtypes : [ 'double' , 'int' ] \ud83d\udcd1 concatenate_dataset \ud83d\udd0e Corresponds to data_ingest.concatenate_dataset This configuration block describes how to combine multiple loaded dataframes into a single one. method There are two different methods to concatenate dataframes: index : Concatenate by column index, i.e., the first column of the first dataframe is matched with the first column of the second dataframe and so forth. name : Concatenate by column name, i.e., columns of the same name are matched. Note that in both cases, the first dataframe will define both the names and the order of the columns in the final dataframe. If the subsequent dataframes have too few columns ( index ) or are missing named columns (`name\u00b4) for the concatenation to proceed, an error will be raised. \ud83e\udd13 Example: method : name dataset1 read_dataset \ud83d\udd0e Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the other concatenating input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the other concatenating input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers. delete_column \ud83d\udd0e Corresponds to data_ingest.delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data. select_column \ud83d\udd0e Corresponds to data_ingest.select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing. rename_column \ud83d\udd0e Corresponds to data_ingest.rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on. recast_column \ud83d\udd0e Corresponds to data_ingest.recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See \ud83d\udcd6 the Spark documentation for a list of valid datatypes. Note that this field is case-insensitive. dataset2 , dataset3 , \u2026 Additional datasets are configured in the same manner as dataset1 . \ud83d\udcd1 join_dataset \ud83d\udd0e Corresponds to data_ingest.join_dataset This configuration block describes how multiple dataframes are joined into a single one. join_cols The key of the column(s) to join on. In the case that the key consists of multiple columns, they can be passed as a list of strings or a single string where the column names are separated by | . \ud83e\udd13 Example: join_cols : id_column join_type The type of join to perform: inner , full , left , right , left_semi , or left_anti . For a general introduction to joins, see \ud83d\udcd6 this tutorial . \ud83e\udd13 Example: join_type : inner dataset1 read_dataset \ud83d\udd0e Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the other joining input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the other joining input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers. delete_column \ud83d\udd0e Corresponds to data_ingest.delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data. select_column \ud83d\udd0e Corresponds to data_ingest.select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing. rename_column \ud83d\udd0e Corresponds to data_ingest.rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on. recast_column \ud83d\udd0e Corresponds to data_ingest.recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See \ud83d\udcd6 the Spark documentation for a list of valid datatypes. Note that this field is case-insensitive. dataset2 , dataset3 , \u2026 Additional datasets are configured in the same manner as dataset1 . \ud83d\udcd1 timeseries_analyzer \ud83d\udd0e Corresponds to data_analyzer.ts_analyzer Configuration for the time series analyzer. auto_detection : Can be set to True or False . If True , it attempts to automatically infer the date/timestamp format in the input dataset. id_col : Name of the ID column in the input dataset. tz_offset : The timezone offset of the timestamps in the input dataset. Can be set to either local , gmt , or utc . The default setting is local . inspection : Can be set to True or False . If True , the time series elements undergo an inspection. analysis_level : Can be set to daily , weekly , or hourly . The default setting is daily . If set to daily , the daily view is populated. If set to hourly , the view is shown at a day part level. If set to weekly , the display it per individual weekdays (1-7) as captured. max_days : Maximum number of days up to which the data will be aggregated. If the dataset contains a timestamp/date field with very high number of unique dates (e.g., 20 years worth of daily data), this option can be used to reduce the timespan that is analyzed. \ud83e\udd13 Example: timeseries_analyzer : auto_detection : True id_col : 'id_column' tz_offset : 'local' inspection : True analysis_level : 'daily' max_days : 3600 \ud83d\udcd1 anovos_basic_report \ud83d\udd0e Corresponds to data_report.basic_report_generation The basic report consists of a summary of the outputs of the stats_generator , quality_checker , and association evaluator See the \ud83d\udcd6 documentation for data reports for more details. The basic report can be customized using the following options: basic_report If True , a basic report is generated after completion of the data_analyzer modules. If False , no report is generated. Nevertheless, all the computed statistics and metrics will be available in the final report. report_args id_col : The name of the ID column in the input dataset. label_col : The name of the label or target column in the input dataset. event_lable : The value of the event (label 1 / true ) in the label column. output_path : Path where the basic report is saved. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). \ud83e\udd13 Example: report_args : id_col : id_column label_col : label_col event_label : 'class1' output_path : report_stats \ud83d\udcd1 stats_generator \ud83d\udd0e Corresponds to data_analyzer.stats_generator This module generates descriptive statistics of the ingested data. Descriptive statistics are split into different metric types. Each function corresponds to one metric type. metric List of metrics to calculate for the input dataset. Available options are: \ud83d\udcd6 global_summary \ud83d\udcd6 measures_of_count \ud83d\udcd6 measures_of_centralTendency \ud83d\udcd6 measures_of_cardinality \ud83d\udcd6 measures_of_dispersion \ud83d\udcd6 measures_of_percentiles \ud83d\udcd6 measures_of_shape \ud83e\udd13 Example: metric : [ 'global_summary' , 'measures_of_counts' , 'measures_of_cardinality' , 'measures_of_dispersion' ] metric_args list_of_cols : List of column names (list of strings or string of column names separated by | ) to compute the metrics for. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from metrics computation. This option is especially useful if list_of_cols is set to \"all\" , as it allows computing metrics for all except a few columns without having to specify a potentially very long list of column names to include. \ud83e\udd13 Example: metric_args : list_of_cols : all drop_cols : [ 'id_column' ] \ud83d\udcd1 quality_checker \ud83d\udd0e Corresponds to data_analyzer.quality_checker This module assesses the data quality along different dimensions. Quality metrics are computed at both the row and column level. Further, the module includes appropriate treatment options to fix several common quality issues. duplicate_detection \ud83d\udd0e Corresponds to quality_checker.duplicate_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to consider when searching for duplicates. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to be excluded from duplicate detection. treatment : If False , duplicates are detected and reported. If True , duplicate rows are removed from the input dataset. \ud83e\udd13 Example: duplicate_detection : list_of_cols : all drop_cols : [ 'id_column' ] treatment : True nullRows_detection \ud83d\udd0e Corresponds to quality_checker.nullRows_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to consider during null rows detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from null rows detection. treatment : If False , null rows are detected and reported. If True , rows where more than treatment_threshold columns are null are removed from the input dataset. treatment_threshold : It takes a value between 0 and 1 (default 0.8 ) that specifies which fraction of columns has to be null for a row to be considered a null row. If the threshold is 0 , rows with any missing value will be flagged as null . If the threshold is 1 , only rows where all values are missing will be flagged as null . \ud83e\udd13 Example: nullRows_detection : list_of_cols : all drop_cols : [] treatment : True treatment_threshold : 0.75 invalidEntries_detection \ud83d\udd0e Corresponds to quality_checker.invalidEntries_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered during invalid entries' detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from invalid entries' detection. treatment : If False , invalid entries are detected and reported. If True , invalid entries are replaced with null . output_mode : Can be either \"replace\" or \"append\" . If set to \"replace\" , the original columns will be replaced with the treated columns. If set to \"append\" , the original columns will be kept and the treated columns will be appended to the dataset. The appended columns will be named as the original column with a suffix \"_cleaned\" (e.g., the column \"cost_of_living_cleaned\" corresponds to the original column \"cost_of_living\" ). \ud83e\udd13 Example: invalidEntries_detection : list_of_cols : all drop_cols : [ 'id_column' ] treatment : True output_mode : replace IDness_detection \ud83d\udd0e Corresponds to quality_checker.IDness_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for IDness detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from IDness detection. treatment : If False , columns with high IDness are detected and reported. If True , columns with an IDness above treatment_threshold are removed. treatment_threshold : A value between 0 and 1 (default 1.0 ). \ud83e\udd13 Example: IDness_detection : list_of_cols : all drop_cols : [ 'id_column' ] treatment : True treatment_threshold : 0.9 biasedness_detection \ud83d\udd0e Corresponds to quality_checker.biasedness_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for biasedness detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from biasedness detection. treatment : If False , columns with high IDness are detected and reported. If True , columns with a bias above treatment_threshold are removed. treatment_threshold : A value between 0 and 1 (default 1.0 ). \ud83e\udd13 Example: biasedness_detection : list_of_cols : all drop_cols : [ 'label_col' ] treatment : True treatment_threshold : 0.98 outlier_detection \ud83d\udd0e Corresponds to quality_checker.outlier_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for outlier detection. Alternatively, if set to \"all\" , all columns are included. \u26a0 Note that any column that contains just a single value or only null values is not subjected to outlier detection even if it is selected under this argument. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from outlier detection. detection_side : Whether outliers should be detected on the \"upper\" , the \"lower\" , or \"both\" sides. detection_configs : A map that defines the input parameters for different outlier detection methods. Possible keys are: pctile_lower (default 0.05 ) pctile_upper (default 0.95 ) stdev_lower (default 3.0 ) stdev_upper (default 3.0 ) IQR_lower (default 1.5 ) IQR_upper (default 1.5 ) min_validation (default 2 ) For details, see \ud83d\udcd6 the outlier_detection API documentation treatment : If False , outliers are detected and reported. If True , outliers are treated with the specified treatment_method . treatment_method : Specifies how outliers are treated. Possible options are \"null_replacement\" , \"row_removal\" , \"value_replacement\" . pre_existing_model : If True , the file specified under model_path with lower/upper bounds is loaded. If no such file exists, set to False (the default). model_path : The path to the file with lower/upper bounds. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). If pre_existing_model is True , the pre-saved will be loaded from this location. If pre_existing_model is False , a file with lower/upper bounds will be saved at this location. By default, it is set to NA , indicating that there is neither a pre-saved file nor should such a file be generated. output_mode : Can be either \"replace\" or \"append\" . If set to \"replace\" , the original columns will be replaced with the treated columns. If set to \"append\" , the original columns will be kept and the treated columns will be appended to the dataset. The appended columns will be named as the original column with a suffix \"_outliered\" (e.g., the column \"cost_of_living_outliered\" corresponds to the original column \"cost_of_living\" ). \ud83e\udd13 Example: outlier_detection : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] detection_side : upper detection_configs : pctile_lower : 0.05 pctile_upper : 0.90 stdev_lower : 3.0 stdev_upper : 3.0 IQR_lower : 1.5 IQR_upper : 1.5 min_validation : 2 treatment : True treatment_method : value_replacement pre_existing_model : False model_path : NA output_mode : replace nullColumns_detection \ud83d\udd0e Corresponds to quality_checker.nullColumns_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for null columns detection. Alternatively, if set to \"all\" , all columns are included. If set to \"missing\" (the default) only columns with missing values are included. One of the use cases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for future use. This can be useful, for example, if a column may not have missing values in the training dataset but missing values are acceptable in the test dataset. drop_cols : List of column names (list of strings or string of column names separated by | ) to be excluded from null columns detection. treatment : If False , null columns are detected and reported. If True , missing values are treated with the specified treatment_method . treatment_method : Specifies how null columns are treated. Possible values are \"MMM\" , \" row_removal\" , or \"column_removal\" . treatment_configs : Additional parameters for the treatment_method . If treatment_method is \"column_removal\" , the key treatment_threshold can be used to define the fraction of missing values above which a column is flagged as a null column and remove. If treatment_method is \"MMM\" , possible keys are the parameters of the imputation_MMM function. \ud83e\udd13 Example: nullColumns_detection : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] treatment : True treatment_method : MMM treatment_configs : method_type : median pre_existing_model : False model_path : NA output_mode : replace \ud83d\udcd1 association_evaluator \ud83d\udd0e Corresponds to data_analyzer.association_evaluator This block configures the association evaluator that focuses on understanding the interaction between different attributes or the relationship between an attribute and a binary target variable. correlation_matrix \ud83d\udd0e Corresponds to association_evaluator.correlation_matrix list_of_cols : List of column names (list of strings or string of column names separated by | ) to include in the correlation matrix. Alternatively, when set to all , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to be excluded from the correlation matrix. This is especially useful when almost all columns should be included in the correlation matrix: Set list_of_cols to all and drop the few excluded columns. \ud83e\udd13 Example: correlation_matrix : list_of_cols : all drop_cols : [ 'id_column' ] IV_calculation \ud83d\udd0e Corresponds to association_evaluator.IV_calculation list_of_cols : List of column names (list of strings or string of column names separated by | ) to include in the IV calculation. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from IV calculation. label_col : Name of label or target column in the input dataset. event_label : Value of event (label 1 / true ) in the label column. encoding_configs : Detailed configuration of the binning step. bin_method : The binning method. Defaults to equal_frequency . bin_size : The bin size. Defaults to 10 . monotonicity_check : If set to 1 , dynamically computes the bin_size such that monotonicity is ensured. Can be a computationally expensive calculation. Defaults to 0 . \ud83e\udd13 Example: IV_calculation : list_of_cols : all drop_cols : id_column label_col : label_col event_label : 'class1' encoding_configs : bin_method : equal_frequency bin_size : 10 monotonicity_check : 0 IG_calculation \ud83d\udd0e Corresponds to association_evaluator.IG_calculation list_of_cols : List of column names (list of strings or string of column names separated by | ) to consider for IG calculation. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from IG calculation. label_col : Name of label or target column in the input dataset. event_label : Value of event (label 1 / true ) in the label column. encoding_configs : Detailed configuration of the binning step. bin_method : The binning method. Defaults to equal_frequency . bin_size : The bin size. Defaults to 10 . monotonicity_check : If set to 1 , dynamically computes the bin_size such that monotonicity is ensured. Can be a computationally expensive calculation. Defaults to 0 . \ud83e\udd13 Example: IG_calculation : list_of_cols : all drop_cols : id_column label_col : label_col event_label : 'class1' encoding_configs : bin_method : equal_frequency bin_size : 10 monotonicity_check : 0 variable_clustering \ud83d\udd0e Corresponds to association_evaluator.variable_clustering list_of_cols : List of column names (list of strings or string of column names separated by | ) to include for variable clustering drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from variable clustering. \ud83e\udd13 Example: variable_clustering : list_of_cols : all drop_cols : id_column|label_col \ud83d\udcd1 drift_detector \ud83d\udd0e Corresponds to drift_stability.drift_detector This block configures the drift detector module that provides a range of methods to detect drift within and between datasets. drift_statistics \ud83d\udd0e Corresponds to drift_stability.drift_detector.statistics configs list_of_cols : List of columns to check drift (list or string of col names separated by | ) to include in the drift statistics. Can be set to all to include all non-array columns (except those given in drop_cols ). drop_cols : List of columns to be dropped (list or string of col names separated by | ) to exclude from the drift statistics. method_type : Method(s) to apply to detect drift (list or string of methods separated by | ). Possible values are PSI , JSD , HD , and KS . If set to all , all available metrics are calculated. threshold : Threshold above which attributes are flagged as exhibiting drift. bin_method : The binning method. Possible values are equal_frequency and equal_range . bin_size : The bin size. We recommend setting it to 10 to 20 for PSI and above 100 for all other metrics. pre_existing_source : Set to true if a pre-computed binning model as well as frequency counts and attributes are available. false otherwise. source_path : If pre_existing_source is true , this described from where the pre-computed data is loaded. drift_statistics_folder . drift_statistics folder must contain the output from attribute_binning and frequency_counts . If pre_existing_source is False, this can be used for saving the details. Default folder \"NA\" is used for saving the intermediate output. \ud83e\udd13 Example: configs : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] method_type : all threshold : 0.1 bin_method : equal_range bin_size : 10 pre_existing_source : False source_path : NA source_dataset The reference/baseline dataset. read_dataset file_path : The file (or directory) path to read the source dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the source data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers. delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data. select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing. rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on. recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See the \ud83d\udcd6 Spark documentation for a list of valid datatypes. Note that this field is case-insensitive. stability_index \ud83d\udd0e Corresponds to drift_detector.stability_index_computation configs metric_weightages : A dictionary where the keys are the metric names ( mean , stdev , kurtosis ) and the values are the weight of the metric (between 0 and 1 ). All weights must sum to 1 . existing_metric_path : Location of previously computed metrics of historical datasets ( idx , attribute , mean , stdev , kurtosis where idx is index number of the historical datasets in chronological order). appended_metric_path : The path where the input dataframe metrics are saved after they have been appended to the historical metrics. threshold : The threshold above which attributes are flagged as unstable. \ud83e\udd13 Example: configs : metric_weightages : mean : 0.5 stddev : 0.3 kurtosis : 0.2 existing_metric_path : '' appended_metric_path : 'si_metrics' threshold : 2 dataset1 read_dataset Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the other joining input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the other joining input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers. dataset2 , dataset3 , \u2026 Additional datasets are configured in the same manner as dataset1 . \ud83d\udcd1 report_preprocessing \ud83d\udd0e Corresponds to data_report.report_preprocessing This configuration block describes the data pre\u2013processing necessary for report generation. master_path The path where all outputs are saved. \ud83e\udd13 Example: master_path : 'report_stats' charts_to_objects \ud83d\udd0e Corresponds to report_preprocessing.charts_to_objects This is the core function of the report preprocessing stage. It saves the chart data in the form of objects that are used by the subsequent report generation scripts. See the intermediate report documentation for more details. list_of_cols : List of column names (list of strings or string of column names separated by | ) to include in preprocessing. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from preprocessing. label_col : Name of the label or target column in the input dataset. event_label : Value of the event (label 1 / true ) in the label column. bin_method : The binning method. Possible values are equal_frequency and equal_range . bin_size : The bin size. We recommend setting it to 10 to 20 for PSI and above 100 for all other metrics. drift_detector : Indicates whether data drift has already analyzed. Defaults to False . outlier_charts : Indicates whether outlier charts should be included. Defaults to False . source_path : The source data path for drift analysis. If it has not been computed or is not required, set it to the default value \"NA\" . \ud83e\udd13 Example: charts_to_objects : list_of_cols : all drop_cols : id_column label_col : label_col event_label : 'class1' bin_method : equal_frequency bin_size : 10 drift_detector : True outlier_charts : False source_path : \"NA\" \ud83d\udcd1 report_generation \ud83d\udd0e Corresponds to data_report.report_generation This configuration block controls the generation of the actual report, i.e., the data that is included and the layout. See the report generation documentation for more details. master_path : The path to the preprocessed data generated during the report_preprocessing step. id_col : The ID column present in the input dataset label_col : Name of label or target column in the input dataset. corr_threshold : The threshold above which attributes are considered to be correlated and thus, redundant. Its value is between 0 and 1 . iv_threshold : The threshold above which attributes are considered ot be significant. Its value is between 0 and 1 . Information Value Variable's Predictiveness <0.02 Not useful for prediction 0.02 to 0.1 Weak predictive power 0.1 to 0.3 Medium predictive power 0.3 to 0.5 Strong predictive power >0.5 Suspicious predictive power drift_threshold_model : The threshold above which an attribute is flagged as exhibiting drift. Its value is between 0 and 1 . dataDict_path : The path to the data dictionary containing the exact names and definitions of the attributes. This information is used in the report to aid comprehensibility. metricDict_path : Path to the metric dictionary. final_report_path : The path where final report will be saved. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). \ud83e\udd13 Example: report_generation : master_path : 'report_stats' id_col : 'id_column' label_col : 'label_col' corr_threshold : 0.4 iv_threshold : 0.02 drift_threshold_model : 0.1 dataDict_path : 'data/income_dataset/data_dictionary.csv' metricDict_path : 'data/metric_dictionary.csv' final_report_path : 'report_stats' \ud83d\udcd1 transformers \ud83d\udd0e Corresponds to data_transformer.transformers This block configures the data_transformer module that supports numerous pre-processing and transformation functions, such as binning, encoding, scaling, and imputation. numerical_mathops This group of functions is used to perform mathematical transformations of numerical attributes. feature_transformation \ud83d\udd0e Corresponds to transformers.feature_transformation list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all numerical columns. drop_cols : The numerical columns (list of strings or string of column names separated by | ) to exclude from feature transformation. method_type : The method to apply to use for transformation. The default method is sqrt ( \\sqrt{x} \\sqrt{x} ). Possible values are: ln log10 log2 exp powOf2 ( 2^x 2^x ) powOf10 ( 10^x 10^x ) powOfN ( N^x N^x )Z sqrt ( \\sqrt{x} \\sqrt{x} ) cbrt ( \\sqrt[3]{x} \\sqrt[3]{x} ) sq ( x^2 x^2 ) cb ( x^3 x^3 ) toPowerN ( x^N x^N ) sin cos tan asin acos atan radians remainderDivByN ( x % N x % N ) factorial ( x! x! ) mul_inv ( 1/x 1/x ) floor ceil roundN (round to N decimal places) N : None by default. If method_type is powOfN , toPowerN , remainderDivByN , or roundN , N will be used as the required constant. \ud83e\udd13 Example 1: feature_transformation : list_of_cols : all drop_cols : [] method_type : sqrt \ud83e\udd13 Example 2: feature_transformation : list_of_cols : [ 'capital-gain' , 'capital-loss' ] drop_cols : [] method_type : sqrt feature_transformation : list_of_cols : [ 'age' , 'education_num' ] drop_cols : [] method_type : sq boxcox_transformation \ud83d\udd0e Corresponds to transformers.boxcox_transformation list_of_cols : The columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from Box-Cox transformation. boxcox_lambda : The \\lambda \\lambda value for the Box-Cox transformation. It can be given as a list where each element represents the value of \\lambda \\lambda for a single attribute. The length of the list must be the same as the number of columns to transform. number that is used for all attributes. If no value is given (the default), a search for the best \\lambda \\lambda will be conducted among the following values: [1, -1, 0.5, -0.5, 2, -2, 0.25, -0.25, 3, -3, 4, -4, 5, -5] . The search is conducted independently for each column. \ud83e\udd13 Example 1: boxcox_transformation : list_of_cols : num_feature1|num_feature2 drop_cols : [] \ud83e\udd13 Example 2: boxcox_transformation : list_of_cols : num_feature3|num_feature4 drop_cols : [] boxcox_lambda : [ -2 , -1 ] numerical_binning This group of functions is used to transform numerical attributes into discrete (integer or categorical) attribute. attribute_binning \ud83d\udd0e Corresponds to transformers.attribute_binning list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from attribute binning. method_type : The binning method. Possible values are equal_frequency and equal_range . With equal_range , each bin is of equal size/width and with equal_frequency , each bin contains an equal number of rows. Defaults to equal_range . bin_size : The number of bins. Defaults to 10 . bin_dtype : The dtype of the transformed column. Possible values are numerical and categorical . With numerical , the values reflect the bin number ( 1 , 2 , \u2026). With categorical option, the values are a string that describes the minimal and maximal value of the bin. Defaults to numerical . \ud83e\udd13 Example: attribute_binning : list_of_cols : num_feature1|num_feature2 drop_cols : [] method_type : equal_frequency bin_size : 10 bin_dtype : numerical monotonic_binning \ud83d\udd0e Corresponds to transformers.monotonic_binning list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from monotonic binning. method_type : The binning method. Possible values are equal_frequency and equal_range . With equal_range , each bin is of equal size/width and with equal_frequency , each bin contains an equal number of rows. Defaults to equal_range . bin_size : The number of bins. Defaults to 10 . bin_dtype : The dtype of the transformed column. Possible values are numerical and categorical . With numerical , the values reflect the bin number ( 1 , 2 , \u2026). With categorical option, the values are a string that describes the minimal and maximal value of the bin. Defaults to numerical . \ud83e\udd13 Example: attribute_binning : list_of_cols : num_feature1|num_feature2 drop_cols : [] label_col : [ \"label_col\" ] event_label : [ \"class1\" ] method_type : equal_frequency bin_size : 10 bin_dtype : numerical numerical_expression expression_parser \ud83d\udd0e Corresponds to transformers.expression_parser This function can be used to evaluate a list of SQL expressions and output the result as new features. Columns used in the SQL expression must be available in the dataset. list_of_expr : List of expressions to evaluate as new features e.g., [\"expr1\", \"expr2\"]. Alternatively, expressions can be specified in a string format, where different expressions are separated by pipe delimiter \u201c|\u201d e.g., \"expr1|expr2\". postfix : postfix for new feature name.Naming convention \"f\" + expression_index + postfix e.g. with postfix of \"new\", new added features are named as f0new, f1new etc. (Default value = \"\"). \ud83e\udd13 Example 1: expression_parser : list_of_expr : 'log(age) + 1.5|sin(capital-gain)+cos(capital-loss)' \ud83e\udd13 Example 2: expression_parser : list_of_expr : [ 'log(age) + 1.5' , 'sin(capital-gain)+cos(capital-loss)' ] Both Example 1 and Example 2 generate 2 new features: log(age) + 1.5 and sin(capital-gain)+cos(capital-loss) . The newly generated features will be appended to the dataframe as new columns: f0 and f1. categorical_outliers This function assigns less frequently seen values in a categorical column to a new category others . outlier_categories \ud83d\udd0e Corresponds to transformers.outlier_categories list_of_cols : The categorical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all categorical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from outlier transformation. coverage : The minimum fraction of rows that remain in their original category, given as a value between 0 and 1 . For example, with a coverage of 0.8 , the categories that 80% of the rows belong to remain and the more seldom occurring categories are mapped to others . The default value is 1.0 , which means that no rows are changed to others . max_category : Even if coverage is less, only (max_category - 1) categories will be mapped to actual name and rest to others. Caveat is when multiple categories have same rank, then #categories can be more than max_category. Defaults to 50 . \ud83e\udd13 Example 1: outlier_categories : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] coverage : 0.9 max_category : 20 \ud83e\udd13 Example 2: outlier_categories : list_of_cols : [ \"cat_feature1\" , \"cat_feature2\" ] drop_cols : [] coverage : 0.8 max_category : 10 outlier_categories : list_of_cols : [ \"cat_feature3\" , \"cat_feature4\" ] drop_cols : [] coverage : 0.9 max_category : 15 categorical_encoding This group of transformers functions used to converting a categorical attribute into numerical attribute(s). cat_to_num_unsupervised \ud83d\udd0e Corresponds to transformers.cat_to_num_unsupervised list_of_cols : The categorical columns (list of strings or string of column names separated by | ) to encode. Can be set to \"all\" to include all categorical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from categorical encoding. method_type : The encoding method. Set to 1 for label encoding and to 0 for one-hot encoding. With label encoding, each categorical value is assigned a unique integer based on the ordering specified through index_order . With one-hot encoding, each categorical value will be represented by a binary column. Defaults to 1 (label encoding). index_order : The order assigned to the categorical values when method_type is set to 1 (label encoding). Possible values are: frequencyDesc (default): Order by descending frequency. frequencyAsc : Order by ascending frequency. alphabetDesc : Order alphabetically (descending). alphabetAsc : Order alphabetically (ascending). cardinality_threshold : Columns with a cardinality above this threshold are excluded from enconding. Defaults to 100 . \ud83e\udd13 Example 1: cat_to_num_unsupervised : list_of_cols : all drop_cols : [ 'id_column' ] method_type : 0 cardinality_threshold : 10 \ud83e\udd13 Example 2: cat_to_num_unsupervised : list_of_cols : [ \"cat_feature1\" , \"cat_feature2\" ] drop_cols : [] method_type : 0 cardinality_threshold : 10 cat_to_num_unsupervised : list_of_cols : [ \"cat_feature3\" , \"cat_feature4\" ] drop_cols : [] method_type : 1 cat_to_num_supervised \ud83d\udd0e Corresponds to transformers.cat_to_num_supervised list_of_cols : The categorical columns (list of strings or string of column names separated by | ) to encode. Can be set to \"all\" to include all categorical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from categorical encoding. label_col : The label/target column. Defaults to label . event_label : Value of the (positive) event (i.e, label 1 / true ). Defaults to 1 . \ud83e\udd13 Example: cat_to_num_supervised : list_of_cols : cat_feature1 | cat_feature2 drop_cols : [ 'id_column' ] label_col : income event_label : '>50K' numerical_rescaling Group of functions to rescale numerical attributes. normalization \ud83d\udd0e Corresponds to transformers.normalization list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to normalize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from normalization. \ud83e\udd13 Example: normalization : list_of_cols : [ \"num_feature1\" , \"num_feature2\" ] drop_cols : [] z_standardization \ud83d\udd0e Corresponds to transformers.z_standardization list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from standardization. \ud83e\udd13 Example: z_standardization : list_of_cols : [ \"num_feature1\" , \"num_feature2\" ] drop_cols : [] IQR_standardization \ud83d\udd0e Corresponds to transformers.IQR_standardization list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from standardization. \ud83e\udd13 Example: IQR_standardization : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] drop_cols : [] numerical_latentFeatures Group of functions to generate latent features to reduce the dimensionality of the input dataset. PCA_latentFeatures \ud83d\udd0e Corresponds to transformers.PCA_latentFeatures list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from latent features computation. explained_variance_cutoff : The required explained variance cutoff. Determines the number of encoded columns in the output. If N is the smallest integer such that the top N encoded columns explain more than the given variance threshold, these N columns be selected. Defaults to 0.95 . standardization : If True (the default), standardization is applied. False otherwise. standardization_configs : The arguments for the z_standardization function in dictionary format. Defaults to {\"pre_existing_model\": False} imputation : If True , imputation is applied. False (the default) otherwise. imputation_configs : Configuration for imputation in dictionary format. The name of the imputation function is specified with the key imputation_name (defaults to imputation_MMM ). Arguments for the imputation function can be passed using additional keys. \ud83e\udd13 Example 1: PCA_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] explained_variance_cutoff : 0.95 standardization : False imputation : True \ud83e\udd13 Example 2: PCA_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] explained_variance_cutoff : 0.8 standardization : False imputation : True PCA_latentFeatures : list_of_cols : [ \"num_feature4\" , \"num_feature5\" , \"num_feature6\" ] explained_variance_cutoff : 0.6 standardization : True imputation : True autoencoder_latentFeatures \ud83d\udd0e Corresponds to transformers.autoencoder_latentFeatures list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from latent features computation. reduction_params : Determines the number of resulting encoded features. If reduction_params is below 1 , reduction_params * \"number of columns\" columns will be generated. Else, reduction_params columns will be generated. Defaults to 0.5 , i.e., the number of columns in the result is half the of number of columns in the input. sample_size : Maximum number of rows used for training the autoencoder model. Defaults to 500000 ( 5e5 ). epochs : The number of epochs to train the autoencoder model. Defaults to 100 . batch_size : The batch size for autoencoder model training. Defaults to 256 . standardization : If True (the default), standardization is applied. False otherwise. standardization_configs : The arguments for the z_standardization function in dictionary format. Defaults to {\"pre_existing_model\": False} imputation : If True , imputation is applied. False (the default) otherwise. imputation_configs : Configuration for imputation in dictionary format. The name of the imputation function is specified with the key imputation_name (defaults to imputation_MMM ). Arguments for the imputation function can be passed using additional keys. \ud83e\udd13 Example 1: autoencoder_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] reduction_params : 0.5 sample_size : 10000 epochs : 20 batch_size : 256 \ud83e\udd13 Example 2: autoencoder_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" ] reduction_params : 0.5 sample_size : 10000 epochs : 20 batch_size : 256 autoencoder_latentFeatures : list_of_cols : [ \"num_feature3\" , \"num_feature4\" , \"num_feature5\" , \"num_feature6\" , \"num_feature7\" ] reduction_params : 0.8 sample_size : 10000 epochs : 100 batch_size : 256 \ud83d\udcd1 write_intermediate file_path : Path where intermediate datasets (after selecting, dropping, renaming, and recasting of columns) for quality checker operations, join dataset and concatenate dataset will be saved. file_type : (CSV, Parquet or Avro). file format of intermediate dataset file_configs (optional): Rest of the valid configuration can be passed through this options e.g., repartition, mode, compression, header, delimiter, inferSchema etc. This might look like: \ud83e\udd13 Example: file_configs : mode : overwrite header : True delimiter : \", \" inferSchema : True For more information on available configuration options, see the following external documentation: \ud83d\udcd6 Writing CSV files \ud83d\udcd6 Writing Parquet files \ud83d\udcd6 Writing Avro files \ud83d\udcd1 write_main file_path : Path where final cleaned input dataset will be saved. file_type : (CSV, Parquet or Avro). file format of final dataset file_configs (optional): Rest of the valid configuration can be passed through this options e.g., repartition, mode, compression, header, delimiter, inferSchema etc. This might look like: \ud83e\udd13 Example: file_configs : mode : overwrite header : True delimiter : \", \" inferSchema : True For more information on available configuration options, see the following external documentation: \ud83d\udcd6 Writing CSV files \ud83d\udcd6 Writing Parquet files \ud83d\udcd6 Writing Avro files \ud83d\udcd1 write_stats file_path : Path where all tables/stats of anovos modules (data drift & data analyzer) will be saved. file_type : (CSV, Parquet or Avro). file format of final dataset file_configs (optional): Rest of the valid configuration can be passed through this options e.g., repartition, mode, compression, header, delimiter, inferSchema etc. This might look like: file_configs : mode : overwrite header : True delimiter : \", \" inferSchema : True For more information on available configuration options, see the following external documentation: \ud83d\udcd6 Writing CSV files \ud83d\udcd6 Writing Parquet files \ud83d\udcd6 Writing Avro files \ud83d\udcd1 write_feast_features \ud83d\udd0e Corresponds to feature_store/feast_exporter.generate_feature_description \ud83d\udcd6 For details, see the Feature Store Integration documentation file_path : The path to the feast repository where the generated feature definitions will be stored. entity : The yml block to configure the definition of a feast entity. name : The name of the feast entity. description : A human readable description of the entity. id_col : Defines the identifying column of the Anovos dataframe which will be used as an id in feast. file_source : The yml block to configure the definition of a feast file source. description : A human readable description of the file source. owner : The email of the owner of this file source. timestamp_col : The name of the logical timestamp at which the feature was observed. create_timestamp_col : The name of the physical timestamp (wallclock time) of when the feature value was computed. feature_view : The yml block to configure the definition of a feast feature view. name : The name of the feature view. owner : The email of the owner of this feature view. ttl_in_seconds : The time to live in seconds for features in this view. Feast will use this value to look backwards when performing point in time joins. service_name (optional): The name of the feature service generated by the workflow. \ud83e\udd13 Example: write_feast_features : file_path : \"../anovos_repo/\" entity : name : \"income\" description : \"this entity is a ....\" id_col : 'ifa' file_source : description : 'data source description' owner : \"me@business.com\" timestamp_col : 'event_time' create_timestamp_col : 'create_time_col' feature_view : name : 'income_view' owner : 'view@owner.com' ttl_in_seconds : 36000000 service_name : 'income_feature_service'","title":"Configuration"},{"location":"using-anovos/config_file.html#configuring-workloads","text":"Anovos workloads can be described by a YAML configuration file. Such a configuration file defines: the input dataset(s) the analyses and transformations to be performed on the data the output files and dataset(s) the reports to be generated Defining workloads this way allows users to make full use of Anovos capabilities while maintaining an easy-to-grasp overview. Since each configuration file fully describes one workload, these files can be shared, versioned, and run across different compute environments. In the following, we'll describe in detail each of the sections in an Anovos configuration file. If you'd rather see a full example right away, have a look at this example . Note that each section of the configuration file maps to a module of Anovos . You'll find links to the respective sections of the API Documentation that provide much more detailed information on each modules' capabilities than we can squeeze into this guide.","title":"Configuring Workloads"},{"location":"using-anovos/config_file.html#input_dataset","text":"This configuration block describes how the input dataset is loaded and prepared using the data_ingest.data_ingest module. Each Anovos configuration file must contain exactly one input_dataset block. Note that the subsequent operations are performed in the order given here: First, columns are deleted, then selected, then renamed, and then recast.","title":"\ud83d\udcd1 input_dataset"},{"location":"using-anovos/config_file.html#read_dataset","text":"\ud83d\udd0e Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers. In the case of a CSV file, this might look like: file_configs : delimiter : \", \" header : True inferSchema : True For more information on available configuration options, see the following external documentation: Read CSV files Read Parquet files Read Avro files","title":"read_dataset"},{"location":"using-anovos/config_file.html#delete_column","text":"\ud83d\udd0e Corresponds to data_ingest.delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data. \ud83e\udd13 Example: delete_column : [ 'unnecessary' , 'obsolete' , 'outdated' ]","title":"delete_column"},{"location":"using-anovos/config_file.html#select_column","text":"\ud83d\udd0e Corresponds to data_ingest.select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing. \ud83e\udd13 Example: select_column : [ 'feature1' , 'feature2' , 'feature3' , 'label' ]","title":"select_column"},{"location":"using-anovos/config_file.html#rename_column","text":"\ud83d\udd0e Corresponds to data_ingest.rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on. \ud83e\udd13 Example: rename_column : list_of_cols : [ 'very_long_column_name' , 'price' ] list_of_newcols : [ 'short_name' , 'label' ] This will rename the column very_long_column_name to short_name and the column price to label .","title":"rename_column"},{"location":"using-anovos/config_file.html#recast_column","text":"\ud83d\udd0e Corresponds to data_ingest.recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See \ud83d\udcd6 the Spark documentation for a list of valid datatypes. Note that this field is case-insensitive. \ud83e\udd13 Example: recast_column : list_of_cols : [ 'price' , 'quantity' ] list_of_dtypes : [ 'double' , 'int' ]","title":"recast_column"},{"location":"using-anovos/config_file.html#concatenate_dataset","text":"\ud83d\udd0e Corresponds to data_ingest.concatenate_dataset This configuration block describes how to combine multiple loaded dataframes into a single one.","title":"\ud83d\udcd1 concatenate_dataset"},{"location":"using-anovos/config_file.html#method","text":"There are two different methods to concatenate dataframes: index : Concatenate by column index, i.e., the first column of the first dataframe is matched with the first column of the second dataframe and so forth. name : Concatenate by column name, i.e., columns of the same name are matched. Note that in both cases, the first dataframe will define both the names and the order of the columns in the final dataframe. If the subsequent dataframes have too few columns ( index ) or are missing named columns (`name\u00b4) for the concatenation to proceed, an error will be raised. \ud83e\udd13 Example: method : name","title":"method"},{"location":"using-anovos/config_file.html#dataset1","text":"","title":"dataset1"},{"location":"using-anovos/config_file.html#read_dataset_1","text":"\ud83d\udd0e Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the other concatenating input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the other concatenating input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers.","title":"read_dataset"},{"location":"using-anovos/config_file.html#delete_column_1","text":"\ud83d\udd0e Corresponds to data_ingest.delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data.","title":"delete_column"},{"location":"using-anovos/config_file.html#select_column_1","text":"\ud83d\udd0e Corresponds to data_ingest.select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing.","title":"select_column"},{"location":"using-anovos/config_file.html#rename_column_1","text":"\ud83d\udd0e Corresponds to data_ingest.rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on.","title":"rename_column"},{"location":"using-anovos/config_file.html#recast_column_1","text":"\ud83d\udd0e Corresponds to data_ingest.recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See \ud83d\udcd6 the Spark documentation for a list of valid datatypes. Note that this field is case-insensitive.","title":"recast_column"},{"location":"using-anovos/config_file.html#dataset2-dataset3","text":"Additional datasets are configured in the same manner as dataset1 .","title":"dataset2,  dataset3,  \u2026"},{"location":"using-anovos/config_file.html#join_dataset","text":"\ud83d\udd0e Corresponds to data_ingest.join_dataset This configuration block describes how multiple dataframes are joined into a single one.","title":"\ud83d\udcd1 join_dataset"},{"location":"using-anovos/config_file.html#join_cols","text":"The key of the column(s) to join on. In the case that the key consists of multiple columns, they can be passed as a list of strings or a single string where the column names are separated by | . \ud83e\udd13 Example: join_cols : id_column","title":"join_cols"},{"location":"using-anovos/config_file.html#join_type","text":"The type of join to perform: inner , full , left , right , left_semi , or left_anti . For a general introduction to joins, see \ud83d\udcd6 this tutorial . \ud83e\udd13 Example: join_type : inner","title":"join_type"},{"location":"using-anovos/config_file.html#dataset1_1","text":"","title":"dataset1"},{"location":"using-anovos/config_file.html#read_dataset_2","text":"\ud83d\udd0e Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the other joining input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the other joining input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers.","title":"read_dataset"},{"location":"using-anovos/config_file.html#delete_column_2","text":"\ud83d\udd0e Corresponds to data_ingest.delete_column List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data.","title":"delete_column"},{"location":"using-anovos/config_file.html#select_column_2","text":"\ud83d\udd0e Corresponds to data_ingest.select_column List of column names (list of strings or string of column names separated by | ) to be selected for further processing.","title":"select_column"},{"location":"using-anovos/config_file.html#rename_column_2","text":"\ud83d\udd0e Corresponds to data_ingest.rename_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on.","title":"rename_column"},{"location":"using-anovos/config_file.html#recast_column_2","text":"\ud83d\udd0e Corresponds to data_ingest.recast_column list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See \ud83d\udcd6 the Spark documentation for a list of valid datatypes. Note that this field is case-insensitive.","title":"recast_column"},{"location":"using-anovos/config_file.html#dataset2-dataset3_1","text":"Additional datasets are configured in the same manner as dataset1 .","title":"dataset2,  dataset3,  \u2026"},{"location":"using-anovos/config_file.html#timeseries_analyzer","text":"\ud83d\udd0e Corresponds to data_analyzer.ts_analyzer Configuration for the time series analyzer. auto_detection : Can be set to True or False . If True , it attempts to automatically infer the date/timestamp format in the input dataset. id_col : Name of the ID column in the input dataset. tz_offset : The timezone offset of the timestamps in the input dataset. Can be set to either local , gmt , or utc . The default setting is local . inspection : Can be set to True or False . If True , the time series elements undergo an inspection. analysis_level : Can be set to daily , weekly , or hourly . The default setting is daily . If set to daily , the daily view is populated. If set to hourly , the view is shown at a day part level. If set to weekly , the display it per individual weekdays (1-7) as captured. max_days : Maximum number of days up to which the data will be aggregated. If the dataset contains a timestamp/date field with very high number of unique dates (e.g., 20 years worth of daily data), this option can be used to reduce the timespan that is analyzed. \ud83e\udd13 Example: timeseries_analyzer : auto_detection : True id_col : 'id_column' tz_offset : 'local' inspection : True analysis_level : 'daily' max_days : 3600","title":"\ud83d\udcd1 timeseries_analyzer"},{"location":"using-anovos/config_file.html#anovos_basic_report","text":"\ud83d\udd0e Corresponds to data_report.basic_report_generation The basic report consists of a summary of the outputs of the stats_generator , quality_checker , and association evaluator See the \ud83d\udcd6 documentation for data reports for more details. The basic report can be customized using the following options:","title":"\ud83d\udcd1 anovos_basic_report"},{"location":"using-anovos/config_file.html#basic_report","text":"If True , a basic report is generated after completion of the data_analyzer modules. If False , no report is generated. Nevertheless, all the computed statistics and metrics will be available in the final report.","title":"basic_report"},{"location":"using-anovos/config_file.html#report_args","text":"id_col : The name of the ID column in the input dataset. label_col : The name of the label or target column in the input dataset. event_lable : The value of the event (label 1 / true ) in the label column. output_path : Path where the basic report is saved. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). \ud83e\udd13 Example: report_args : id_col : id_column label_col : label_col event_label : 'class1' output_path : report_stats","title":"report_args"},{"location":"using-anovos/config_file.html#stats_generator","text":"\ud83d\udd0e Corresponds to data_analyzer.stats_generator This module generates descriptive statistics of the ingested data. Descriptive statistics are split into different metric types. Each function corresponds to one metric type.","title":"\ud83d\udcd1 stats_generator"},{"location":"using-anovos/config_file.html#metric","text":"List of metrics to calculate for the input dataset. Available options are: \ud83d\udcd6 global_summary \ud83d\udcd6 measures_of_count \ud83d\udcd6 measures_of_centralTendency \ud83d\udcd6 measures_of_cardinality \ud83d\udcd6 measures_of_dispersion \ud83d\udcd6 measures_of_percentiles \ud83d\udcd6 measures_of_shape \ud83e\udd13 Example: metric : [ 'global_summary' , 'measures_of_counts' , 'measures_of_cardinality' , 'measures_of_dispersion' ]","title":"metric"},{"location":"using-anovos/config_file.html#metric_args","text":"list_of_cols : List of column names (list of strings or string of column names separated by | ) to compute the metrics for. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from metrics computation. This option is especially useful if list_of_cols is set to \"all\" , as it allows computing metrics for all except a few columns without having to specify a potentially very long list of column names to include. \ud83e\udd13 Example: metric_args : list_of_cols : all drop_cols : [ 'id_column' ]","title":"metric_args"},{"location":"using-anovos/config_file.html#quality_checker","text":"\ud83d\udd0e Corresponds to data_analyzer.quality_checker This module assesses the data quality along different dimensions. Quality metrics are computed at both the row and column level. Further, the module includes appropriate treatment options to fix several common quality issues.","title":"\ud83d\udcd1 quality_checker"},{"location":"using-anovos/config_file.html#duplicate_detection","text":"\ud83d\udd0e Corresponds to quality_checker.duplicate_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to consider when searching for duplicates. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to be excluded from duplicate detection. treatment : If False , duplicates are detected and reported. If True , duplicate rows are removed from the input dataset. \ud83e\udd13 Example: duplicate_detection : list_of_cols : all drop_cols : [ 'id_column' ] treatment : True","title":"duplicate_detection"},{"location":"using-anovos/config_file.html#nullrows_detection","text":"\ud83d\udd0e Corresponds to quality_checker.nullRows_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to consider during null rows detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from null rows detection. treatment : If False , null rows are detected and reported. If True , rows where more than treatment_threshold columns are null are removed from the input dataset. treatment_threshold : It takes a value between 0 and 1 (default 0.8 ) that specifies which fraction of columns has to be null for a row to be considered a null row. If the threshold is 0 , rows with any missing value will be flagged as null . If the threshold is 1 , only rows where all values are missing will be flagged as null . \ud83e\udd13 Example: nullRows_detection : list_of_cols : all drop_cols : [] treatment : True treatment_threshold : 0.75","title":"nullRows_detection"},{"location":"using-anovos/config_file.html#invalidentries_detection","text":"\ud83d\udd0e Corresponds to quality_checker.invalidEntries_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered during invalid entries' detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from invalid entries' detection. treatment : If False , invalid entries are detected and reported. If True , invalid entries are replaced with null . output_mode : Can be either \"replace\" or \"append\" . If set to \"replace\" , the original columns will be replaced with the treated columns. If set to \"append\" , the original columns will be kept and the treated columns will be appended to the dataset. The appended columns will be named as the original column with a suffix \"_cleaned\" (e.g., the column \"cost_of_living_cleaned\" corresponds to the original column \"cost_of_living\" ). \ud83e\udd13 Example: invalidEntries_detection : list_of_cols : all drop_cols : [ 'id_column' ] treatment : True output_mode : replace","title":"invalidEntries_detection"},{"location":"using-anovos/config_file.html#idness_detection","text":"\ud83d\udd0e Corresponds to quality_checker.IDness_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for IDness detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from IDness detection. treatment : If False , columns with high IDness are detected and reported. If True , columns with an IDness above treatment_threshold are removed. treatment_threshold : A value between 0 and 1 (default 1.0 ). \ud83e\udd13 Example: IDness_detection : list_of_cols : all drop_cols : [ 'id_column' ] treatment : True treatment_threshold : 0.9","title":"IDness_detection"},{"location":"using-anovos/config_file.html#biasedness_detection","text":"\ud83d\udd0e Corresponds to quality_checker.biasedness_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for biasedness detection. Alternatively, if set to \"all\" , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from biasedness detection. treatment : If False , columns with high IDness are detected and reported. If True , columns with a bias above treatment_threshold are removed. treatment_threshold : A value between 0 and 1 (default 1.0 ). \ud83e\udd13 Example: biasedness_detection : list_of_cols : all drop_cols : [ 'label_col' ] treatment : True treatment_threshold : 0.98","title":"biasedness_detection"},{"location":"using-anovos/config_file.html#outlier_detection","text":"\ud83d\udd0e Corresponds to quality_checker.outlier_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for outlier detection. Alternatively, if set to \"all\" , all columns are included. \u26a0 Note that any column that contains just a single value or only null values is not subjected to outlier detection even if it is selected under this argument. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from outlier detection. detection_side : Whether outliers should be detected on the \"upper\" , the \"lower\" , or \"both\" sides. detection_configs : A map that defines the input parameters for different outlier detection methods. Possible keys are: pctile_lower (default 0.05 ) pctile_upper (default 0.95 ) stdev_lower (default 3.0 ) stdev_upper (default 3.0 ) IQR_lower (default 1.5 ) IQR_upper (default 1.5 ) min_validation (default 2 ) For details, see \ud83d\udcd6 the outlier_detection API documentation treatment : If False , outliers are detected and reported. If True , outliers are treated with the specified treatment_method . treatment_method : Specifies how outliers are treated. Possible options are \"null_replacement\" , \"row_removal\" , \"value_replacement\" . pre_existing_model : If True , the file specified under model_path with lower/upper bounds is loaded. If no such file exists, set to False (the default). model_path : The path to the file with lower/upper bounds. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). If pre_existing_model is True , the pre-saved will be loaded from this location. If pre_existing_model is False , a file with lower/upper bounds will be saved at this location. By default, it is set to NA , indicating that there is neither a pre-saved file nor should such a file be generated. output_mode : Can be either \"replace\" or \"append\" . If set to \"replace\" , the original columns will be replaced with the treated columns. If set to \"append\" , the original columns will be kept and the treated columns will be appended to the dataset. The appended columns will be named as the original column with a suffix \"_outliered\" (e.g., the column \"cost_of_living_outliered\" corresponds to the original column \"cost_of_living\" ). \ud83e\udd13 Example: outlier_detection : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] detection_side : upper detection_configs : pctile_lower : 0.05 pctile_upper : 0.90 stdev_lower : 3.0 stdev_upper : 3.0 IQR_lower : 1.5 IQR_upper : 1.5 min_validation : 2 treatment : True treatment_method : value_replacement pre_existing_model : False model_path : NA output_mode : replace","title":"outlier_detection"},{"location":"using-anovos/config_file.html#nullcolumns_detection","text":"\ud83d\udd0e Corresponds to quality_checker.nullColumns_detection list_of_cols : List of column names (list of strings or string of column names separated by | ) to be considered for null columns detection. Alternatively, if set to \"all\" , all columns are included. If set to \"missing\" (the default) only columns with missing values are included. One of the use cases where \"all\" may be preferable over \"missing\" is when the user wants to save the imputation model for future use. This can be useful, for example, if a column may not have missing values in the training dataset but missing values are acceptable in the test dataset. drop_cols : List of column names (list of strings or string of column names separated by | ) to be excluded from null columns detection. treatment : If False , null columns are detected and reported. If True , missing values are treated with the specified treatment_method . treatment_method : Specifies how null columns are treated. Possible values are \"MMM\" , \" row_removal\" , or \"column_removal\" . treatment_configs : Additional parameters for the treatment_method . If treatment_method is \"column_removal\" , the key treatment_threshold can be used to define the fraction of missing values above which a column is flagged as a null column and remove. If treatment_method is \"MMM\" , possible keys are the parameters of the imputation_MMM function. \ud83e\udd13 Example: nullColumns_detection : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] treatment : True treatment_method : MMM treatment_configs : method_type : median pre_existing_model : False model_path : NA output_mode : replace","title":"nullColumns_detection"},{"location":"using-anovos/config_file.html#association_evaluator","text":"\ud83d\udd0e Corresponds to data_analyzer.association_evaluator This block configures the association evaluator that focuses on understanding the interaction between different attributes or the relationship between an attribute and a binary target variable.","title":"\ud83d\udcd1 association_evaluator"},{"location":"using-anovos/config_file.html#correlation_matrix","text":"\ud83d\udd0e Corresponds to association_evaluator.correlation_matrix list_of_cols : List of column names (list of strings or string of column names separated by | ) to include in the correlation matrix. Alternatively, when set to all , all columns are included. drop_cols : List of column names (list of strings or string of column names separated by | ) to be excluded from the correlation matrix. This is especially useful when almost all columns should be included in the correlation matrix: Set list_of_cols to all and drop the few excluded columns. \ud83e\udd13 Example: correlation_matrix : list_of_cols : all drop_cols : [ 'id_column' ]","title":"correlation_matrix"},{"location":"using-anovos/config_file.html#iv_calculation","text":"\ud83d\udd0e Corresponds to association_evaluator.IV_calculation list_of_cols : List of column names (list of strings or string of column names separated by | ) to include in the IV calculation. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from IV calculation. label_col : Name of label or target column in the input dataset. event_label : Value of event (label 1 / true ) in the label column. encoding_configs : Detailed configuration of the binning step. bin_method : The binning method. Defaults to equal_frequency . bin_size : The bin size. Defaults to 10 . monotonicity_check : If set to 1 , dynamically computes the bin_size such that monotonicity is ensured. Can be a computationally expensive calculation. Defaults to 0 . \ud83e\udd13 Example: IV_calculation : list_of_cols : all drop_cols : id_column label_col : label_col event_label : 'class1' encoding_configs : bin_method : equal_frequency bin_size : 10 monotonicity_check : 0","title":"IV_calculation"},{"location":"using-anovos/config_file.html#ig_calculation","text":"\ud83d\udd0e Corresponds to association_evaluator.IG_calculation list_of_cols : List of column names (list of strings or string of column names separated by | ) to consider for IG calculation. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from IG calculation. label_col : Name of label or target column in the input dataset. event_label : Value of event (label 1 / true ) in the label column. encoding_configs : Detailed configuration of the binning step. bin_method : The binning method. Defaults to equal_frequency . bin_size : The bin size. Defaults to 10 . monotonicity_check : If set to 1 , dynamically computes the bin_size such that monotonicity is ensured. Can be a computationally expensive calculation. Defaults to 0 . \ud83e\udd13 Example: IG_calculation : list_of_cols : all drop_cols : id_column label_col : label_col event_label : 'class1' encoding_configs : bin_method : equal_frequency bin_size : 10 monotonicity_check : 0","title":"IG_calculation"},{"location":"using-anovos/config_file.html#variable_clustering","text":"\ud83d\udd0e Corresponds to association_evaluator.variable_clustering list_of_cols : List of column names (list of strings or string of column names separated by | ) to include for variable clustering drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from variable clustering. \ud83e\udd13 Example: variable_clustering : list_of_cols : all drop_cols : id_column|label_col","title":"variable_clustering"},{"location":"using-anovos/config_file.html#drift_detector","text":"\ud83d\udd0e Corresponds to drift_stability.drift_detector This block configures the drift detector module that provides a range of methods to detect drift within and between datasets.","title":"\ud83d\udcd1 drift_detector"},{"location":"using-anovos/config_file.html#drift_statistics","text":"\ud83d\udd0e Corresponds to drift_stability.drift_detector.statistics","title":"drift_statistics"},{"location":"using-anovos/config_file.html#configs","text":"list_of_cols : List of columns to check drift (list or string of col names separated by | ) to include in the drift statistics. Can be set to all to include all non-array columns (except those given in drop_cols ). drop_cols : List of columns to be dropped (list or string of col names separated by | ) to exclude from the drift statistics. method_type : Method(s) to apply to detect drift (list or string of methods separated by | ). Possible values are PSI , JSD , HD , and KS . If set to all , all available metrics are calculated. threshold : Threshold above which attributes are flagged as exhibiting drift. bin_method : The binning method. Possible values are equal_frequency and equal_range . bin_size : The bin size. We recommend setting it to 10 to 20 for PSI and above 100 for all other metrics. pre_existing_source : Set to true if a pre-computed binning model as well as frequency counts and attributes are available. false otherwise. source_path : If pre_existing_source is true , this described from where the pre-computed data is loaded. drift_statistics_folder . drift_statistics folder must contain the output from attribute_binning and frequency_counts . If pre_existing_source is False, this can be used for saving the details. Default folder \"NA\" is used for saving the intermediate output. \ud83e\udd13 Example: configs : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] method_type : all threshold : 0.1 bin_method : equal_range bin_size : 10 pre_existing_source : False source_path : NA","title":"configs"},{"location":"using-anovos/config_file.html#source_dataset","text":"The reference/baseline dataset.","title":"source_dataset"},{"location":"using-anovos/config_file.html#read_dataset_3","text":"file_path : The file (or directory) path to read the source dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the source data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers.","title":"read_dataset"},{"location":"using-anovos/config_file.html#delete_column_3","text":"List of column names (list of strings or string of column names separated by | ) to be deleted from the loaded input data.","title":"delete_column"},{"location":"using-anovos/config_file.html#select_column_3","text":"List of column names (list of strings or string of column names separated by | ) to be selected for further processing.","title":"select_column"},{"location":"using-anovos/config_file.html#rename_column_3","text":"list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be renamed. list_of_newcols : The new column names. The first element in list_of_cols will be renamed to the first name in list_of_newcols and so on.","title":"rename_column"},{"location":"using-anovos/config_file.html#recast_column_3","text":"list_of_cols : List of the names of columns (list of strings or string of column names separated by | ) to be cast to a different type. list_of_dtypes : The new datatypes. The first element in list_of_cols will be recast to the first type in list_of_dtypes and so on. See the \ud83d\udcd6 Spark documentation for a list of valid datatypes. Note that this field is case-insensitive.","title":"recast_column"},{"location":"using-anovos/config_file.html#stability_index","text":"\ud83d\udd0e Corresponds to drift_detector.stability_index_computation","title":"stability_index"},{"location":"using-anovos/config_file.html#configs_1","text":"metric_weightages : A dictionary where the keys are the metric names ( mean , stdev , kurtosis ) and the values are the weight of the metric (between 0 and 1 ). All weights must sum to 1 . existing_metric_path : Location of previously computed metrics of historical datasets ( idx , attribute , mean , stdev , kurtosis where idx is index number of the historical datasets in chronological order). appended_metric_path : The path where the input dataframe metrics are saved after they have been appended to the historical metrics. threshold : The threshold above which attributes are flagged as unstable. \ud83e\udd13 Example: configs : metric_weightages : mean : 0.5 stddev : 0.3 kurtosis : 0.2 existing_metric_path : '' appended_metric_path : 'si_metrics' threshold : 2","title":"configs"},{"location":"using-anovos/config_file.html#dataset1_2","text":"","title":"dataset1"},{"location":"using-anovos/config_file.html#read_dataset_4","text":"Corresponds to data_ingest.read_dataset file_path : The file (or directory) path to read the other joining input dataset from. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). file_type : The file format of the other joining input data. Currently, Anovos supports CSV ( csv ), Parquet ( parquet ), and Avro ( avro ). (Please note that if you're using Avro data sources, you need to add the external package org.apache.spark:spark-avro when submitting the Spark job.) file_configs (optional): Options to pass to the respective Spark file reader, e.g., delimiters, schemas, headers.","title":"read_dataset"},{"location":"using-anovos/config_file.html#dataset2-dataset3_2","text":"Additional datasets are configured in the same manner as dataset1 .","title":"dataset2,  dataset3,  \u2026"},{"location":"using-anovos/config_file.html#report_preprocessing","text":"\ud83d\udd0e Corresponds to data_report.report_preprocessing This configuration block describes the data pre\u2013processing necessary for report generation.","title":"\ud83d\udcd1 report_preprocessing"},{"location":"using-anovos/config_file.html#master_path","text":"The path where all outputs are saved. \ud83e\udd13 Example: master_path : 'report_stats'","title":"master_path"},{"location":"using-anovos/config_file.html#charts_to_objects","text":"\ud83d\udd0e Corresponds to report_preprocessing.charts_to_objects This is the core function of the report preprocessing stage. It saves the chart data in the form of objects that are used by the subsequent report generation scripts. See the intermediate report documentation for more details. list_of_cols : List of column names (list of strings or string of column names separated by | ) to include in preprocessing. drop_cols : List of column names (list of strings or string of column names separated by | ) to exclude from preprocessing. label_col : Name of the label or target column in the input dataset. event_label : Value of the event (label 1 / true ) in the label column. bin_method : The binning method. Possible values are equal_frequency and equal_range . bin_size : The bin size. We recommend setting it to 10 to 20 for PSI and above 100 for all other metrics. drift_detector : Indicates whether data drift has already analyzed. Defaults to False . outlier_charts : Indicates whether outlier charts should be included. Defaults to False . source_path : The source data path for drift analysis. If it has not been computed or is not required, set it to the default value \"NA\" . \ud83e\udd13 Example: charts_to_objects : list_of_cols : all drop_cols : id_column label_col : label_col event_label : 'class1' bin_method : equal_frequency bin_size : 10 drift_detector : True outlier_charts : False source_path : \"NA\"","title":"charts_to_objects"},{"location":"using-anovos/config_file.html#report_generation","text":"\ud83d\udd0e Corresponds to data_report.report_generation This configuration block controls the generation of the actual report, i.e., the data that is included and the layout. See the report generation documentation for more details. master_path : The path to the preprocessed data generated during the report_preprocessing step. id_col : The ID column present in the input dataset label_col : Name of label or target column in the input dataset. corr_threshold : The threshold above which attributes are considered to be correlated and thus, redundant. Its value is between 0 and 1 . iv_threshold : The threshold above which attributes are considered ot be significant. Its value is between 0 and 1 . Information Value Variable's Predictiveness <0.02 Not useful for prediction 0.02 to 0.1 Weak predictive power 0.1 to 0.3 Medium predictive power 0.3 to 0.5 Strong predictive power >0.5 Suspicious predictive power drift_threshold_model : The threshold above which an attribute is flagged as exhibiting drift. Its value is between 0 and 1 . dataDict_path : The path to the data dictionary containing the exact names and definitions of the attributes. This information is used in the report to aid comprehensibility. metricDict_path : Path to the metric dictionary. final_report_path : The path where final report will be saved. It can be a local path, an \ud83d\udcd6 S3 path (when running on AWS), a path to a file resource on Google Colab (see \ud83d\udcd6 this tutorial for an overview), or a path on the \ud83d\udcd6 Databricks File System (when running on Azure). \ud83e\udd13 Example: report_generation : master_path : 'report_stats' id_col : 'id_column' label_col : 'label_col' corr_threshold : 0.4 iv_threshold : 0.02 drift_threshold_model : 0.1 dataDict_path : 'data/income_dataset/data_dictionary.csv' metricDict_path : 'data/metric_dictionary.csv' final_report_path : 'report_stats'","title":"\ud83d\udcd1 report_generation"},{"location":"using-anovos/config_file.html#transformers","text":"\ud83d\udd0e Corresponds to data_transformer.transformers This block configures the data_transformer module that supports numerous pre-processing and transformation functions, such as binning, encoding, scaling, and imputation.","title":"\ud83d\udcd1 transformers"},{"location":"using-anovos/config_file.html#numerical_mathops","text":"This group of functions is used to perform mathematical transformations of numerical attributes.","title":"numerical_mathops"},{"location":"using-anovos/config_file.html#feature_transformation","text":"\ud83d\udd0e Corresponds to transformers.feature_transformation list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all numerical columns. drop_cols : The numerical columns (list of strings or string of column names separated by | ) to exclude from feature transformation. method_type : The method to apply to use for transformation. The default method is sqrt ( \\sqrt{x} \\sqrt{x} ). Possible values are: ln log10 log2 exp powOf2 ( 2^x 2^x ) powOf10 ( 10^x 10^x ) powOfN ( N^x N^x )Z sqrt ( \\sqrt{x} \\sqrt{x} ) cbrt ( \\sqrt[3]{x} \\sqrt[3]{x} ) sq ( x^2 x^2 ) cb ( x^3 x^3 ) toPowerN ( x^N x^N ) sin cos tan asin acos atan radians remainderDivByN ( x % N x % N ) factorial ( x! x! ) mul_inv ( 1/x 1/x ) floor ceil roundN (round to N decimal places) N : None by default. If method_type is powOfN , toPowerN , remainderDivByN , or roundN , N will be used as the required constant. \ud83e\udd13 Example 1: feature_transformation : list_of_cols : all drop_cols : [] method_type : sqrt \ud83e\udd13 Example 2: feature_transformation : list_of_cols : [ 'capital-gain' , 'capital-loss' ] drop_cols : [] method_type : sqrt feature_transformation : list_of_cols : [ 'age' , 'education_num' ] drop_cols : [] method_type : sq","title":"feature_transformation"},{"location":"using-anovos/config_file.html#boxcox_transformation","text":"\ud83d\udd0e Corresponds to transformers.boxcox_transformation list_of_cols : The columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from Box-Cox transformation. boxcox_lambda : The \\lambda \\lambda value for the Box-Cox transformation. It can be given as a list where each element represents the value of \\lambda \\lambda for a single attribute. The length of the list must be the same as the number of columns to transform. number that is used for all attributes. If no value is given (the default), a search for the best \\lambda \\lambda will be conducted among the following values: [1, -1, 0.5, -0.5, 2, -2, 0.25, -0.25, 3, -3, 4, -4, 5, -5] . The search is conducted independently for each column. \ud83e\udd13 Example 1: boxcox_transformation : list_of_cols : num_feature1|num_feature2 drop_cols : [] \ud83e\udd13 Example 2: boxcox_transformation : list_of_cols : num_feature3|num_feature4 drop_cols : [] boxcox_lambda : [ -2 , -1 ]","title":"boxcox_transformation"},{"location":"using-anovos/config_file.html#numerical_binning","text":"This group of functions is used to transform numerical attributes into discrete (integer or categorical) attribute.","title":"numerical_binning"},{"location":"using-anovos/config_file.html#attribute_binning","text":"\ud83d\udd0e Corresponds to transformers.attribute_binning list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from attribute binning. method_type : The binning method. Possible values are equal_frequency and equal_range . With equal_range , each bin is of equal size/width and with equal_frequency , each bin contains an equal number of rows. Defaults to equal_range . bin_size : The number of bins. Defaults to 10 . bin_dtype : The dtype of the transformed column. Possible values are numerical and categorical . With numerical , the values reflect the bin number ( 1 , 2 , \u2026). With categorical option, the values are a string that describes the minimal and maximal value of the bin. Defaults to numerical . \ud83e\udd13 Example: attribute_binning : list_of_cols : num_feature1|num_feature2 drop_cols : [] method_type : equal_frequency bin_size : 10 bin_dtype : numerical","title":"attribute_binning"},{"location":"using-anovos/config_file.html#monotonic_binning","text":"\ud83d\udd0e Corresponds to transformers.monotonic_binning list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from monotonic binning. method_type : The binning method. Possible values are equal_frequency and equal_range . With equal_range , each bin is of equal size/width and with equal_frequency , each bin contains an equal number of rows. Defaults to equal_range . bin_size : The number of bins. Defaults to 10 . bin_dtype : The dtype of the transformed column. Possible values are numerical and categorical . With numerical , the values reflect the bin number ( 1 , 2 , \u2026). With categorical option, the values are a string that describes the minimal and maximal value of the bin. Defaults to numerical . \ud83e\udd13 Example: attribute_binning : list_of_cols : num_feature1|num_feature2 drop_cols : [] label_col : [ \"label_col\" ] event_label : [ \"class1\" ] method_type : equal_frequency bin_size : 10 bin_dtype : numerical","title":"monotonic_binning"},{"location":"using-anovos/config_file.html#numerical_expression","text":"","title":"numerical_expression"},{"location":"using-anovos/config_file.html#expression_parser","text":"\ud83d\udd0e Corresponds to transformers.expression_parser This function can be used to evaluate a list of SQL expressions and output the result as new features. Columns used in the SQL expression must be available in the dataset. list_of_expr : List of expressions to evaluate as new features e.g., [\"expr1\", \"expr2\"]. Alternatively, expressions can be specified in a string format, where different expressions are separated by pipe delimiter \u201c|\u201d e.g., \"expr1|expr2\". postfix : postfix for new feature name.Naming convention \"f\" + expression_index + postfix e.g. with postfix of \"new\", new added features are named as f0new, f1new etc. (Default value = \"\"). \ud83e\udd13 Example 1: expression_parser : list_of_expr : 'log(age) + 1.5|sin(capital-gain)+cos(capital-loss)' \ud83e\udd13 Example 2: expression_parser : list_of_expr : [ 'log(age) + 1.5' , 'sin(capital-gain)+cos(capital-loss)' ] Both Example 1 and Example 2 generate 2 new features: log(age) + 1.5 and sin(capital-gain)+cos(capital-loss) . The newly generated features will be appended to the dataframe as new columns: f0 and f1.","title":"expression_parser"},{"location":"using-anovos/config_file.html#categorical_outliers","text":"This function assigns less frequently seen values in a categorical column to a new category others .","title":"categorical_outliers"},{"location":"using-anovos/config_file.html#outlier_categories","text":"\ud83d\udd0e Corresponds to transformers.outlier_categories list_of_cols : The categorical columns (list of strings or string of column names separated by | ) to transform. Can be set to \"all\" to include all categorical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from outlier transformation. coverage : The minimum fraction of rows that remain in their original category, given as a value between 0 and 1 . For example, with a coverage of 0.8 , the categories that 80% of the rows belong to remain and the more seldom occurring categories are mapped to others . The default value is 1.0 , which means that no rows are changed to others . max_category : Even if coverage is less, only (max_category - 1) categories will be mapped to actual name and rest to others. Caveat is when multiple categories have same rank, then #categories can be more than max_category. Defaults to 50 . \ud83e\udd13 Example 1: outlier_categories : list_of_cols : all drop_cols : [ 'id_column' , 'label_col' ] coverage : 0.9 max_category : 20 \ud83e\udd13 Example 2: outlier_categories : list_of_cols : [ \"cat_feature1\" , \"cat_feature2\" ] drop_cols : [] coverage : 0.8 max_category : 10 outlier_categories : list_of_cols : [ \"cat_feature3\" , \"cat_feature4\" ] drop_cols : [] coverage : 0.9 max_category : 15","title":"outlier_categories"},{"location":"using-anovos/config_file.html#categorical_encoding","text":"This group of transformers functions used to converting a categorical attribute into numerical attribute(s).","title":"categorical_encoding"},{"location":"using-anovos/config_file.html#cat_to_num_unsupervised","text":"\ud83d\udd0e Corresponds to transformers.cat_to_num_unsupervised list_of_cols : The categorical columns (list of strings or string of column names separated by | ) to encode. Can be set to \"all\" to include all categorical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from categorical encoding. method_type : The encoding method. Set to 1 for label encoding and to 0 for one-hot encoding. With label encoding, each categorical value is assigned a unique integer based on the ordering specified through index_order . With one-hot encoding, each categorical value will be represented by a binary column. Defaults to 1 (label encoding). index_order : The order assigned to the categorical values when method_type is set to 1 (label encoding). Possible values are: frequencyDesc (default): Order by descending frequency. frequencyAsc : Order by ascending frequency. alphabetDesc : Order alphabetically (descending). alphabetAsc : Order alphabetically (ascending). cardinality_threshold : Columns with a cardinality above this threshold are excluded from enconding. Defaults to 100 . \ud83e\udd13 Example 1: cat_to_num_unsupervised : list_of_cols : all drop_cols : [ 'id_column' ] method_type : 0 cardinality_threshold : 10 \ud83e\udd13 Example 2: cat_to_num_unsupervised : list_of_cols : [ \"cat_feature1\" , \"cat_feature2\" ] drop_cols : [] method_type : 0 cardinality_threshold : 10 cat_to_num_unsupervised : list_of_cols : [ \"cat_feature3\" , \"cat_feature4\" ] drop_cols : [] method_type : 1","title":"cat_to_num_unsupervised"},{"location":"using-anovos/config_file.html#cat_to_num_supervised","text":"\ud83d\udd0e Corresponds to transformers.cat_to_num_supervised list_of_cols : The categorical columns (list of strings or string of column names separated by | ) to encode. Can be set to \"all\" to include all categorical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from categorical encoding. label_col : The label/target column. Defaults to label . event_label : Value of the (positive) event (i.e, label 1 / true ). Defaults to 1 . \ud83e\udd13 Example: cat_to_num_supervised : list_of_cols : cat_feature1 | cat_feature2 drop_cols : [ 'id_column' ] label_col : income event_label : '>50K'","title":"cat_to_num_supervised"},{"location":"using-anovos/config_file.html#numerical_rescaling","text":"Group of functions to rescale numerical attributes.","title":"numerical_rescaling"},{"location":"using-anovos/config_file.html#normalization","text":"\ud83d\udd0e Corresponds to transformers.normalization list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to normalize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from normalization. \ud83e\udd13 Example: normalization : list_of_cols : [ \"num_feature1\" , \"num_feature2\" ] drop_cols : []","title":"normalization"},{"location":"using-anovos/config_file.html#z_standardization","text":"\ud83d\udd0e Corresponds to transformers.z_standardization list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from standardization. \ud83e\udd13 Example: z_standardization : list_of_cols : [ \"num_feature1\" , \"num_feature2\" ] drop_cols : []","title":"z_standardization"},{"location":"using-anovos/config_file.html#iqr_standardization","text":"\ud83d\udd0e Corresponds to transformers.IQR_standardization list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from standardization. \ud83e\udd13 Example: IQR_standardization : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] drop_cols : []","title":"IQR_standardization"},{"location":"using-anovos/config_file.html#numerical_latentfeatures","text":"Group of functions to generate latent features to reduce the dimensionality of the input dataset.","title":"numerical_latentFeatures"},{"location":"using-anovos/config_file.html#pca_latentfeatures","text":"\ud83d\udd0e Corresponds to transformers.PCA_latentFeatures list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from latent features computation. explained_variance_cutoff : The required explained variance cutoff. Determines the number of encoded columns in the output. If N is the smallest integer such that the top N encoded columns explain more than the given variance threshold, these N columns be selected. Defaults to 0.95 . standardization : If True (the default), standardization is applied. False otherwise. standardization_configs : The arguments for the z_standardization function in dictionary format. Defaults to {\"pre_existing_model\": False} imputation : If True , imputation is applied. False (the default) otherwise. imputation_configs : Configuration for imputation in dictionary format. The name of the imputation function is specified with the key imputation_name (defaults to imputation_MMM ). Arguments for the imputation function can be passed using additional keys. \ud83e\udd13 Example 1: PCA_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] explained_variance_cutoff : 0.95 standardization : False imputation : True \ud83e\udd13 Example 2: PCA_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] explained_variance_cutoff : 0.8 standardization : False imputation : True PCA_latentFeatures : list_of_cols : [ \"num_feature4\" , \"num_feature5\" , \"num_feature6\" ] explained_variance_cutoff : 0.6 standardization : True imputation : True","title":"PCA_latentFeatures"},{"location":"using-anovos/config_file.html#autoencoder_latentfeatures","text":"\ud83d\udd0e Corresponds to transformers.autoencoder_latentFeatures list_of_cols : The numerical columns (list of strings or string of column names separated by | ) to standardize. Can be set to \"all\" to include all numerical columns. drop_cols : The columns (list of strings or string of column names separated by | ) to exclude from latent features computation. reduction_params : Determines the number of resulting encoded features. If reduction_params is below 1 , reduction_params * \"number of columns\" columns will be generated. Else, reduction_params columns will be generated. Defaults to 0.5 , i.e., the number of columns in the result is half the of number of columns in the input. sample_size : Maximum number of rows used for training the autoencoder model. Defaults to 500000 ( 5e5 ). epochs : The number of epochs to train the autoencoder model. Defaults to 100 . batch_size : The batch size for autoencoder model training. Defaults to 256 . standardization : If True (the default), standardization is applied. False otherwise. standardization_configs : The arguments for the z_standardization function in dictionary format. Defaults to {\"pre_existing_model\": False} imputation : If True , imputation is applied. False (the default) otherwise. imputation_configs : Configuration for imputation in dictionary format. The name of the imputation function is specified with the key imputation_name (defaults to imputation_MMM ). Arguments for the imputation function can be passed using additional keys. \ud83e\udd13 Example 1: autoencoder_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" , \"num_feature3\" ] reduction_params : 0.5 sample_size : 10000 epochs : 20 batch_size : 256 \ud83e\udd13 Example 2: autoencoder_latentFeatures : list_of_cols : [ \"num_feature1\" , \"num_feature2\" ] reduction_params : 0.5 sample_size : 10000 epochs : 20 batch_size : 256 autoencoder_latentFeatures : list_of_cols : [ \"num_feature3\" , \"num_feature4\" , \"num_feature5\" , \"num_feature6\" , \"num_feature7\" ] reduction_params : 0.8 sample_size : 10000 epochs : 100 batch_size : 256","title":"autoencoder_latentFeatures"},{"location":"using-anovos/config_file.html#write_intermediate","text":"file_path : Path where intermediate datasets (after selecting, dropping, renaming, and recasting of columns) for quality checker operations, join dataset and concatenate dataset will be saved. file_type : (CSV, Parquet or Avro). file format of intermediate dataset file_configs (optional): Rest of the valid configuration can be passed through this options e.g., repartition, mode, compression, header, delimiter, inferSchema etc. This might look like: \ud83e\udd13 Example: file_configs : mode : overwrite header : True delimiter : \", \" inferSchema : True For more information on available configuration options, see the following external documentation: \ud83d\udcd6 Writing CSV files \ud83d\udcd6 Writing Parquet files \ud83d\udcd6 Writing Avro files","title":"\ud83d\udcd1 write_intermediate"},{"location":"using-anovos/config_file.html#write_main","text":"file_path : Path where final cleaned input dataset will be saved. file_type : (CSV, Parquet or Avro). file format of final dataset file_configs (optional): Rest of the valid configuration can be passed through this options e.g., repartition, mode, compression, header, delimiter, inferSchema etc. This might look like: \ud83e\udd13 Example: file_configs : mode : overwrite header : True delimiter : \", \" inferSchema : True For more information on available configuration options, see the following external documentation: \ud83d\udcd6 Writing CSV files \ud83d\udcd6 Writing Parquet files \ud83d\udcd6 Writing Avro files","title":"\ud83d\udcd1 write_main"},{"location":"using-anovos/config_file.html#write_stats","text":"file_path : Path where all tables/stats of anovos modules (data drift & data analyzer) will be saved. file_type : (CSV, Parquet or Avro). file format of final dataset file_configs (optional): Rest of the valid configuration can be passed through this options e.g., repartition, mode, compression, header, delimiter, inferSchema etc. This might look like: file_configs : mode : overwrite header : True delimiter : \", \" inferSchema : True For more information on available configuration options, see the following external documentation: \ud83d\udcd6 Writing CSV files \ud83d\udcd6 Writing Parquet files \ud83d\udcd6 Writing Avro files","title":"\ud83d\udcd1 write_stats"},{"location":"using-anovos/config_file.html#write_feast_features","text":"\ud83d\udd0e Corresponds to feature_store/feast_exporter.generate_feature_description \ud83d\udcd6 For details, see the Feature Store Integration documentation file_path : The path to the feast repository where the generated feature definitions will be stored. entity : The yml block to configure the definition of a feast entity. name : The name of the feast entity. description : A human readable description of the entity. id_col : Defines the identifying column of the Anovos dataframe which will be used as an id in feast. file_source : The yml block to configure the definition of a feast file source. description : A human readable description of the file source. owner : The email of the owner of this file source. timestamp_col : The name of the logical timestamp at which the feature was observed. create_timestamp_col : The name of the physical timestamp (wallclock time) of when the feature value was computed. feature_view : The yml block to configure the definition of a feast feature view. name : The name of the feature view. owner : The email of the owner of this feature view. ttl_in_seconds : The time to live in seconds for features in this view. Feast will use this value to look backwards when performing point in time joins. service_name (optional): The name of the feature service generated by the workflow. \ud83e\udd13 Example: write_feast_features : file_path : \"../anovos_repo/\" entity : name : \"income\" description : \"this entity is a ....\" id_col : 'ifa' file_source : description : 'data source description' owner : \"me@business.com\" timestamp_col : 'event_time' create_timestamp_col : 'create_time_col' feature_view : name : 'income_view' owner : 'view@owner.com' ttl_in_seconds : 36000000 service_name : 'income_feature_service'","title":"\ud83d\udcd1 write_feast_features"},{"location":"using-anovos/feature_mapper.html","text":"Feature Mapper Feature engineering has always played a crucial role in solving problems related to machine learning (ML). Features and predictors determine whether machine learning projects are successful or not. However, coming up with intuitive features is not an easy task. Identifying the right features to build is challenging and requires both domain knowledge and expertise in the technical aspects of machine learning. In fact, 80% of data scientists' time is spent on data wrangling and feature engineering, while only 20% is spent on fine-tuning and testing the models. Building features from scratch and figuring out what features they should use to create models is a daunting cold-start problem for any data scientist. There are many tools that help data scientists to narrow down the features, but they are always either not scalable, or complicated to understand and operate. Within Anovos , we've launched an open-source tool, the Feature Explorer and Mapper (FER) module, to help the machine learning community tackle the cold-start problem. With Feature Explorer and Mapper, we mainly address two problems: Create a platform for data scientists to explore available and already used features based on their in industries/domains and use cases of interest The Feature Mapper helps data scientists in identifying better features based on the data they have on hand The Feature Explorer and Mapper both use semantic-similarity-based language modeling based on recent Natural Language Processing (NLP) techniques. Semantic matching aims to determine the similarity between words, lines, and sentences through multiple metrics. In this module, we use all-mpnet-base-v2 as our semantic model. This model is based on Microsoft\u2019s Mpnet base model that offers masked and permuted pre-training for language understanding. Its performance beats BERT, XLNet, and RoBERTa in language modeling and text recognition. The important features of this model are: Trained on more than a billion training pairs, including approximately 300 million research paper citation pairs Fine-tuned using cosine similarity from sentence pairs, then apply cross entropy loss by comparing true pairs Our solution consists of three main steps: Convert textual data into tensors using the pretrained model Compute similarity scores of each input attribute name and description across both corpora ( Anovos Feature Corpus & the user-defined data dictionary corpus) Sort the results and get the matches for each input feature based on its scores The following diagram illustrates this workflow of the Feature Explorer and Mapper:","title":"Feature Mapper"},{"location":"using-anovos/feature_mapper.html#feature-mapper","text":"Feature engineering has always played a crucial role in solving problems related to machine learning (ML). Features and predictors determine whether machine learning projects are successful or not. However, coming up with intuitive features is not an easy task. Identifying the right features to build is challenging and requires both domain knowledge and expertise in the technical aspects of machine learning. In fact, 80% of data scientists' time is spent on data wrangling and feature engineering, while only 20% is spent on fine-tuning and testing the models. Building features from scratch and figuring out what features they should use to create models is a daunting cold-start problem for any data scientist. There are many tools that help data scientists to narrow down the features, but they are always either not scalable, or complicated to understand and operate. Within Anovos , we've launched an open-source tool, the Feature Explorer and Mapper (FER) module, to help the machine learning community tackle the cold-start problem. With Feature Explorer and Mapper, we mainly address two problems: Create a platform for data scientists to explore available and already used features based on their in industries/domains and use cases of interest The Feature Mapper helps data scientists in identifying better features based on the data they have on hand The Feature Explorer and Mapper both use semantic-similarity-based language modeling based on recent Natural Language Processing (NLP) techniques. Semantic matching aims to determine the similarity between words, lines, and sentences through multiple metrics. In this module, we use all-mpnet-base-v2 as our semantic model. This model is based on Microsoft\u2019s Mpnet base model that offers masked and permuted pre-training for language understanding. Its performance beats BERT, XLNet, and RoBERTa in language modeling and text recognition. The important features of this model are: Trained on more than a billion training pairs, including approximately 300 million research paper citation pairs Fine-tuned using cosine similarity from sentence pairs, then apply cross entropy loss by comparing true pairs Our solution consists of three main steps: Convert textual data into tensors using the pretrained model Compute similarity scores of each input attribute name and description across both corpora ( Anovos Feature Corpus & the user-defined data dictionary corpus) Sort the results and get the matches for each input feature based on its scores The following diagram illustrates this workflow of the Feature Explorer and Mapper:","title":"Feature Mapper"},{"location":"using-anovos/feature_store.html","text":"Feature Store Integration Feature stores are an essential building block of a modern MLOps setup. For an introduction into the concept and an overview of available options and vendors, see the Feature Store Comparison & Evaluation on the MLOps Community website . Anovos provides integration with Feast , a widely used open source feature store, out of the box. Using the same abstractions, it is straightforward to integrate Anovos with other feature stores. If there is a particular feature store integration you'd like to see supported by Anovos , let us know! Using Anovos with Feast The following guide describes how to use Anovos to push data to Feast. We assume that you are familiar with the fundamentals of both Anovos workflows and Feast. For an introduction to Feast, see \ud83d\udcd6 the Feast Quickstart guide . Prerequisites In order to use Anovos with Feast, you need to install it: pip install feast Next, we'll instantiate a new Feast repository: feast init anovos_repo \ud83e\udd13 Note: You can also use an existing repository. In this case, Anovos will simply add a new file anovos.py containing the feature definitions as well as the output file to the existing repository. Adding the Feast export to your Anovos workflow To export data to Feat at the end of a workflow run, you need to add the write_feast_features block to the configuration file. (To learn more about the configuration file in general and available options, see \ud83d\udcd6 the configuration file documentation .) You can use the following template as a starting point: write_feast_features : file_path : \"../anovos_repo/\" # the location of your Feast repository entity : name : \"income\" # the Feast entity description : \"this entity is a ....\" # the entity description used by Feast id_col : 'ifa' # the primary key column to identify this entity by file_source : description : 'data source description' # the data source description used by Feast owner : \"me@business.com\" # the data source owner registered in Feast timestamp_col : 'event_time' # the name of the logical timestamp at which the feature was observed create_timestamp_col : 'create_time_col' # the name of the physical timestamp (wallclock time) # of when the feature value was computed feature_view : name : 'income_view' # the name of the generated feature view owner : 'view@owner.com' # the view owner registered in Feast ttl_in_seconds : 36000000 # the time to live in seconds for features in this view. # Feast will use this value to look backwards when performing # point-in-time joins service_name : 'income_feature_service' # the name of the feature service generated by the workflow Let's break this down! The following block generates an entity definition in Feast . This block and all its child elements are mandatory. the name elements specifies the entity name. The description element provides a human-readable description to be displayed in the Feast UI. The element id_col specifies the primary key of the entity. entity : name : \"income\" description : \"this entity is a ....\" id_col : 'ifa' The subsequent block generates a file source definition in Feast . This block and all its children are mandatory. The owner element describes the owner of the file data source in the shape of an email address. The two elements timestamp_col and create_timestamp_col refer to timestamped columns used when retrieving data . file_source : description : 'data source description' owner : \"me@business.com\" timestamp_col : 'event_time' create_timestamp_col : 'create_time_col' The next block generates a feature view definition in Feast . This block and all its children are mandatory. feature_view : name : 'income_view' owner : 'view@owner.com' ttl_in_seconds : 3600 The following element generates a feature service definition in Feast . This element is optional. service_name : 'income_feature_service' Setting repartition value to 1 in write_main The current version of the feast integration only supports adding single output files to feast repositories. Thus, it is required to set the value of the repartition attribute to 1 in the write_main config. write_main : #... set your output path config values etc here file_configs : repartition : 1 Exporting data to Feast First, run your Anovos workflow with the configuration above. Once the workflow has finished, switch into the folder of anovos_feature repository, apply the changed feature definitions, and materialize the features: cd anovos_repo feast apply feast materialize ` date \"+%Y-%m-%dT\" ` ` date \"+%Y-%m-%dT%H:%M:%S\" ` To verify that the features have been loaded correctly, you can check them using Feast's UI. Run feast ui and access the Feast UI at http://127.0.0.1:8888 . The UI gives a realtime overview about data sources, entities, feature views etc. of the entire feature repository (i.e., across multiple .py files that contain feature definitions). Retrieve feature data from Feast The following script shows how to access historical data, e.g., for the purpose of training an ML model. For more information, see the feast documentation on feature retrieval . Documentation on how to specify event_time and its use in point in time joins can be found here . import datetime import feast import pandas as pd repo_path = \"./anovos_repo\" store = feast . FeatureStore ( repo_path = repo_path ) # ACCESS HISTORICAL FEATURES # Either read directly from parquet file generated by the Anovos workflow or generated manually income_entities = pd . DataFrame . from_dict ( { \"ifa\" : [ \"27a\" , \"30a\" , \"475a\" , \"965a\" , \"1678a\" , \"1698a\" , \"1807a\" , \"1951a\" , \"2041a\" , \"2215a\" , ], \"event_time\" : [ datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), ], } ) fs = feast . FeatureStore ( repo_path = repo_path ) # Alternative 1: retrieve features via explicit specification income_features_df = fs . get_historical_features ( entity_df = income_entities , features = [ \"income_view:income\" , \"income_view:latent_0\" , \"income_view:latent_1\" , \"income_view:latent_2\" , \"income_view:latent_3\" , ], ) . to_df () print ( income_features_df . head ()) # Alternative 2: retrieve features using the feature service feature_service = fs . get_feature_service ( \"income_feature_service\" ) income_features_by_service_df = fs . get_historical_features ( features = feature_service , entity_df = income_entities ) . to_df () print ( income_features_by_service_df . head ()) # Now, you can use the features to train your model ... Integrating Anovos with other feature stores We're exploring further support for Feast and the integration of Anovos with other feature stores. Let us know which capabilities you'd like to see in future versions of Anovos !","title":"Feature Store Integration"},{"location":"using-anovos/feature_store.html#feature-store-integration","text":"Feature stores are an essential building block of a modern MLOps setup. For an introduction into the concept and an overview of available options and vendors, see the Feature Store Comparison & Evaluation on the MLOps Community website . Anovos provides integration with Feast , a widely used open source feature store, out of the box. Using the same abstractions, it is straightforward to integrate Anovos with other feature stores. If there is a particular feature store integration you'd like to see supported by Anovos , let us know!","title":"Feature Store Integration"},{"location":"using-anovos/feature_store.html#using-anovos-with-feast","text":"The following guide describes how to use Anovos to push data to Feast. We assume that you are familiar with the fundamentals of both Anovos workflows and Feast. For an introduction to Feast, see \ud83d\udcd6 the Feast Quickstart guide .","title":"Using Anovos with Feast"},{"location":"using-anovos/feature_store.html#prerequisites","text":"In order to use Anovos with Feast, you need to install it: pip install feast Next, we'll instantiate a new Feast repository: feast init anovos_repo \ud83e\udd13 Note: You can also use an existing repository. In this case, Anovos will simply add a new file anovos.py containing the feature definitions as well as the output file to the existing repository.","title":"Prerequisites"},{"location":"using-anovos/feature_store.html#adding-the-feast-export-to-your-anovos-workflow","text":"To export data to Feat at the end of a workflow run, you need to add the write_feast_features block to the configuration file. (To learn more about the configuration file in general and available options, see \ud83d\udcd6 the configuration file documentation .) You can use the following template as a starting point: write_feast_features : file_path : \"../anovos_repo/\" # the location of your Feast repository entity : name : \"income\" # the Feast entity description : \"this entity is a ....\" # the entity description used by Feast id_col : 'ifa' # the primary key column to identify this entity by file_source : description : 'data source description' # the data source description used by Feast owner : \"me@business.com\" # the data source owner registered in Feast timestamp_col : 'event_time' # the name of the logical timestamp at which the feature was observed create_timestamp_col : 'create_time_col' # the name of the physical timestamp (wallclock time) # of when the feature value was computed feature_view : name : 'income_view' # the name of the generated feature view owner : 'view@owner.com' # the view owner registered in Feast ttl_in_seconds : 36000000 # the time to live in seconds for features in this view. # Feast will use this value to look backwards when performing # point-in-time joins service_name : 'income_feature_service' # the name of the feature service generated by the workflow Let's break this down! The following block generates an entity definition in Feast . This block and all its child elements are mandatory. the name elements specifies the entity name. The description element provides a human-readable description to be displayed in the Feast UI. The element id_col specifies the primary key of the entity. entity : name : \"income\" description : \"this entity is a ....\" id_col : 'ifa' The subsequent block generates a file source definition in Feast . This block and all its children are mandatory. The owner element describes the owner of the file data source in the shape of an email address. The two elements timestamp_col and create_timestamp_col refer to timestamped columns used when retrieving data . file_source : description : 'data source description' owner : \"me@business.com\" timestamp_col : 'event_time' create_timestamp_col : 'create_time_col' The next block generates a feature view definition in Feast . This block and all its children are mandatory. feature_view : name : 'income_view' owner : 'view@owner.com' ttl_in_seconds : 3600 The following element generates a feature service definition in Feast . This element is optional. service_name : 'income_feature_service'","title":"Adding the Feast export to your Anovos workflow"},{"location":"using-anovos/feature_store.html#setting-repartition-value-to-1-in-write_main","text":"The current version of the feast integration only supports adding single output files to feast repositories. Thus, it is required to set the value of the repartition attribute to 1 in the write_main config. write_main : #... set your output path config values etc here file_configs : repartition : 1","title":"Setting repartition value to 1 in write_main"},{"location":"using-anovos/feature_store.html#exporting-data-to-feast","text":"First, run your Anovos workflow with the configuration above. Once the workflow has finished, switch into the folder of anovos_feature repository, apply the changed feature definitions, and materialize the features: cd anovos_repo feast apply feast materialize ` date \"+%Y-%m-%dT\" ` ` date \"+%Y-%m-%dT%H:%M:%S\" ` To verify that the features have been loaded correctly, you can check them using Feast's UI. Run feast ui and access the Feast UI at http://127.0.0.1:8888 . The UI gives a realtime overview about data sources, entities, feature views etc. of the entire feature repository (i.e., across multiple .py files that contain feature definitions).","title":"Exporting data to Feast"},{"location":"using-anovos/feature_store.html#retrieve-feature-data-from-feast","text":"The following script shows how to access historical data, e.g., for the purpose of training an ML model. For more information, see the feast documentation on feature retrieval . Documentation on how to specify event_time and its use in point in time joins can be found here . import datetime import feast import pandas as pd repo_path = \"./anovos_repo\" store = feast . FeatureStore ( repo_path = repo_path ) # ACCESS HISTORICAL FEATURES # Either read directly from parquet file generated by the Anovos workflow or generated manually income_entities = pd . DataFrame . from_dict ( { \"ifa\" : [ \"27a\" , \"30a\" , \"475a\" , \"965a\" , \"1678a\" , \"1698a\" , \"1807a\" , \"1951a\" , \"2041a\" , \"2215a\" , ], \"event_time\" : [ datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), datetime . now (), ], } ) fs = feast . FeatureStore ( repo_path = repo_path ) # Alternative 1: retrieve features via explicit specification income_features_df = fs . get_historical_features ( entity_df = income_entities , features = [ \"income_view:income\" , \"income_view:latent_0\" , \"income_view:latent_1\" , \"income_view:latent_2\" , \"income_view:latent_3\" , ], ) . to_df () print ( income_features_df . head ()) # Alternative 2: retrieve features using the feature service feature_service = fs . get_feature_service ( \"income_feature_service\" ) income_features_by_service_df = fs . get_historical_features ( features = feature_service , entity_df = income_entities ) . to_df () print ( income_features_by_service_df . head ()) # Now, you can use the features to train your model ...","title":"Retrieve feature data from Feast"},{"location":"using-anovos/feature_store.html#integrating-anovos-with-other-feature-stores","text":"We're exploring further support for Feast and the integration of Anovos with other feature stores. Let us know which capabilities you'd like to see in future versions of Anovos !","title":"Integrating Anovos with other feature stores"},{"location":"using-anovos/limitations.html","text":"Current Limitations of Anovos The current 1.0 release of Anovos still has some limitations, which we will address in the upcoming releases. To learn more about what's on the horizon, check out our roadmap . \ud83d\udd23 Data Anovos currently supports numerical, categorical, geospatial, and datetime/timestamp columns (at the cross-sectional and transactional level). We plan to add support for additional data types such as (struct) arrays in the future. Anovos currently relies on Apache Spark's automatic schema detection . In case some numerical columns were deliberately saved as string, they will show up as categorical columns when loaded into a DataFrame (except for CSV files). \ud83c\udfce Performance Computing the mode and/or distinct value counts are the most expensive operations in Anovos . We aim to further optimize them in the upcoming releases. Correlation matrix only supports numerical data. Support for categorical data has been removed due to performance concerns and will return in a later release. The invalid entries detection may yield false positives. Hence, be cautious when using the inbuilt treatment option. The categorical encoding functions cat_to_num_supervised and cat_to_num_unsupervised may exhibit poor performance and scaling behavior with very high-cardinality columns. Therefore, it is recommended to reduce cardinality before subjecting them to encoding or specifying an appropriate threshold to drop them from the analysis while encoding. The sample size for constructing the imputation models in imputation_sklearn or creating latent features through autoencoder_latentFeatures should be selected with caution, taking into the consideration the dataset size and the number of columns. This sample dataset is converted into a Pandas DataFrame and subsequent operations are run on a single node (driver). If the sample dataset is too large to fit into the driver's memory, this will result in a memory overflow error. \ud83d\udd29 Other The stability index can currently only be calculated for numerical columns. Due to incompatibilities in TensorFlow and Docker, Anovos may not run well on Apple's M1 chips. You can find out more here: Docker documentation on running on Apple hardware Pythonspeed article on Docker build problems on Macs Installing TensorFlow on M1-chip-based Macs The exception and error handling within Anovos is at times inconsistent. Please don't hesitate to file an issue on GitHub if you encounter any problems.","title":"Limitations"},{"location":"using-anovos/limitations.html#current-limitations-of-anovos","text":"The current 1.0 release of Anovos still has some limitations, which we will address in the upcoming releases. To learn more about what's on the horizon, check out our roadmap .","title":"Current Limitations of Anovos"},{"location":"using-anovos/limitations.html#data","text":"Anovos currently supports numerical, categorical, geospatial, and datetime/timestamp columns (at the cross-sectional and transactional level). We plan to add support for additional data types such as (struct) arrays in the future. Anovos currently relies on Apache Spark's automatic schema detection . In case some numerical columns were deliberately saved as string, they will show up as categorical columns when loaded into a DataFrame (except for CSV files).","title":"\ud83d\udd23 Data"},{"location":"using-anovos/limitations.html#performance","text":"Computing the mode and/or distinct value counts are the most expensive operations in Anovos . We aim to further optimize them in the upcoming releases. Correlation matrix only supports numerical data. Support for categorical data has been removed due to performance concerns and will return in a later release. The invalid entries detection may yield false positives. Hence, be cautious when using the inbuilt treatment option. The categorical encoding functions cat_to_num_supervised and cat_to_num_unsupervised may exhibit poor performance and scaling behavior with very high-cardinality columns. Therefore, it is recommended to reduce cardinality before subjecting them to encoding or specifying an appropriate threshold to drop them from the analysis while encoding. The sample size for constructing the imputation models in imputation_sklearn or creating latent features through autoencoder_latentFeatures should be selected with caution, taking into the consideration the dataset size and the number of columns. This sample dataset is converted into a Pandas DataFrame and subsequent operations are run on a single node (driver). If the sample dataset is too large to fit into the driver's memory, this will result in a memory overflow error.","title":"\ud83c\udfce Performance"},{"location":"using-anovos/limitations.html#other","text":"The stability index can currently only be calculated for numerical columns. Due to incompatibilities in TensorFlow and Docker, Anovos may not run well on Apple's M1 chips. You can find out more here: Docker documentation on running on Apple hardware Pythonspeed article on Docker build problems on Macs Installing TensorFlow on M1-chip-based Macs The exception and error handling within Anovos is at times inconsistent. Please don't hesitate to file an issue on GitHub if you encounter any problems.","title":"\ud83d\udd29 Other"},{"location":"using-anovos/mlflow.html","text":"MLflow Integration MLflow is a popular open source solution for managing all aspects of the machine learning lifecycle. The platform encompasses four components: MLflow Tracking to record code, data, configuration, and results of ML experiments MLflow Projects to package data science code in a format that allows it to run reproducibly in different environments MLflow Models to deploy ML models in different environments MLflow Model Registry to store and manage ML models in a central repository To learn more about MLflow and its capabilities, see the MLflow documentation . Reporting Anovos data to MLflow Tracking Anovos integrates with MLflow by reporting workflow metadata and results to MLflow Tracking . To track your workflows with MLflow , add an mlflow block to your workflow configuration file : mlflow : experiment : \"Anovos\" # The name of the MLflow experiment associated with your workflow tracking_uri : \"http://127.0.0.1:8889\" # The URL of the MLflow Tracking server track_output : True # Store the workflow output (i.e., resulting dataset(s)) track_reports : True # Store the generated reports track_intermediates : False # Store any intermediate data generated by your workflow Current Limitations It is currently not possible to select which intermediate outputs are stored. If track_intermediate is set to True , all intermediate outputs will be stored. Using MLflow on Azure Databricks If you are running Anovos workloads on Azure Databricks , you can use the integrated Managed MLflow to track your Anovos runs and artifacts. To learn more about moving your Anovos workloads to Azure Databricks, see the \ud83d\udcd6 Setting up Anovos on Azure Databricks guide. To track an Anovos workflow with Managed MLflow, you first need to create a new MLflow experiment. This is possible either through the Databricks Machine Learning UI or the MLflow API. Please refer to the Azure Databricks documentation for detailed and up-to-date instructions. Once you have created an experiment for your workflow, you can then use its \"Location\" as the experiment_name in the Anovos workflow configuration's mlflow config block. The tracking_uri needs to be set to databricks . \ud83e\udd13 Example: mlflow : experiment : \"/Users/your_user_name@your_domain.tld/your_experiment_name\" tracking_uri : \"databricks\" track_output : True # Store the workflow output (i.e., resulting dataset(s)) track_reports : True # Store the generated reports track_intermediates : False # Store any intermediate data generated by your workflow Roadmap We're exploring integration of Anovos with MLflow Projects and MLFlow Pipelines . Let us know which capabilities you'd like to see in future versions of Anovos !","title":"MLflow Integration"},{"location":"using-anovos/mlflow.html#mlflow-integration","text":"MLflow is a popular open source solution for managing all aspects of the machine learning lifecycle. The platform encompasses four components: MLflow Tracking to record code, data, configuration, and results of ML experiments MLflow Projects to package data science code in a format that allows it to run reproducibly in different environments MLflow Models to deploy ML models in different environments MLflow Model Registry to store and manage ML models in a central repository To learn more about MLflow and its capabilities, see the MLflow documentation .","title":"MLflow Integration"},{"location":"using-anovos/mlflow.html#reporting-anovos-data-to-mlflow-tracking","text":"Anovos integrates with MLflow by reporting workflow metadata and results to MLflow Tracking . To track your workflows with MLflow , add an mlflow block to your workflow configuration file : mlflow : experiment : \"Anovos\" # The name of the MLflow experiment associated with your workflow tracking_uri : \"http://127.0.0.1:8889\" # The URL of the MLflow Tracking server track_output : True # Store the workflow output (i.e., resulting dataset(s)) track_reports : True # Store the generated reports track_intermediates : False # Store any intermediate data generated by your workflow","title":"Reporting Anovos data to MLflow Tracking"},{"location":"using-anovos/mlflow.html#current-limitations","text":"It is currently not possible to select which intermediate outputs are stored. If track_intermediate is set to True , all intermediate outputs will be stored.","title":"Current Limitations"},{"location":"using-anovos/mlflow.html#using-mlflow-on-azure-databricks","text":"If you are running Anovos workloads on Azure Databricks , you can use the integrated Managed MLflow to track your Anovos runs and artifacts. To learn more about moving your Anovos workloads to Azure Databricks, see the \ud83d\udcd6 Setting up Anovos on Azure Databricks guide. To track an Anovos workflow with Managed MLflow, you first need to create a new MLflow experiment. This is possible either through the Databricks Machine Learning UI or the MLflow API. Please refer to the Azure Databricks documentation for detailed and up-to-date instructions. Once you have created an experiment for your workflow, you can then use its \"Location\" as the experiment_name in the Anovos workflow configuration's mlflow config block. The tracking_uri needs to be set to databricks . \ud83e\udd13 Example: mlflow : experiment : \"/Users/your_user_name@your_domain.tld/your_experiment_name\" tracking_uri : \"databricks\" track_output : True # Store the workflow output (i.e., resulting dataset(s)) track_reports : True # Store the generated reports track_intermediates : False # Store any intermediate data generated by your workflow","title":"Using MLflow on Azure Databricks"},{"location":"using-anovos/mlflow.html#roadmap","text":"We're exploring integration of Anovos with MLflow Projects and MLFlow Pipelines . Let us know which capabilities you'd like to see in future versions of Anovos !","title":"Roadmap"},{"location":"using-anovos/roadmap.html","text":"Anovos Product Roadmap Anovos is built and released as an open source project based on our experience in handling massive data sets to produce predictive features. At Mobilewalla , we process terabytes of mobile engagement signals daily to mine consumer behavior and use features from that data to build distributed machine learning models to solve a wide range of business problems. On this journey, we faced lots of challenges due to the lack of a comprehensive and scalable library. After realizing the unavailability of such libraries, we designed and implemented Anovos as an open source library for every data scientists\u2019 use. \ud83d\udee3 The Roadmap Following the 1.0 release (see the History section below ) our main focus for the upcoming incremental releases is on making Anovos even easier to use and more performant. Our goal is to develop Anovos into a tool that can be used on a data scientist's machines as well as state-of-the-art distributed data processing infrastructure. As we're identifying specific next steps, we'll continuously update this page. If you have any suggestions or feedback, let us know! . The History We developed the first fully functional version of Anovos over the course of three major releases: versions 0.1, 0.2, and 1.0. Version 0.1 (November 2021) The 0.1 release of Anovos had all the essential data ingestion and comprehensive data analysis functionalities, as well as the data pre-processing and cleaning mechanisms. It also included some key differentiating functionalities, like data drift and stability computations, which are crucial in deciding the need for model refresh/tweak options. Another benefit of Anovos is a dynamic visualization component configured based on data ingestion pipelines. Every data metric computed from the Anovos ingestion process can be visualized and utilized for CXO level decision-making. Details Data Ingest AWS S3 Storage integration Read and write to/from local files Column selection and renaming Support for Parquet and CSV files Support for numerical and categorical data types Data Analyzer and Diagnostics Frequency analysis Attribute/feature vs. target Attribute/feature interaction/association Data Preprocessing and Cleaning Outlier detection (IQR/Standardization) Treatment of invalid values Missing attributes analysis Data Health and Monitoring Data drift identification (Hellinger Distance, KS, JSD, and PSI) Attribute stability analysis Overall data quality analysis Runtime Environment support Local Docker-based AWS EMR Report Visualization Comprehensive 360 degree view report of the ingested data (Numerical & Categorical) Executive summary Wiki Descriptive statistics Quality Checker Attribute association Data drift & stability Version 0.2 Release (March 2022) In this release of Anovos , the library supported ingesting data from cloud service providers like Microsoft Azure. The release also added mechanisms to read and write different file formats such as Avro and nested JSON. The key differentiating functionality of this release is the Feature Explorer & Feature Mapper for data scientists and end-users to resolve their cold-start problems, which will immensely reduce their literature review time. The V0.2 release also added a capability called Feature Stability estimator based on the composition of a given feature using set of attributes. This will greatly benefit data scientists to understand the potential feature instabilities that could harm the resiliency of an ML model. With the 0.2 release Anovos was ready to be used in the day-to-day practices of any data scientists or analyst. Details Data Ingest Microsoft Azure Blob Storage integration Support for Avro and nested JSON files Support for additional data types: Time stamps columns Support for Timeseries data ingestion Data Cleaning and Transformation Parsing Merging Converting/Coding Derivations Calculations Imputations Auto encoders Dimension reduction Date/Time related transformations Feature Explorer / Feature recommender (Semantic search enabled) To recommend potential features based on the industry, use case, and the ingested data dictionary Industry specific use cases and respective features Telco BFSI Retail Healthcare Transportation Supply chain Recommendations are enabled by Semantic search capability Supported by pre-compiled feature corpus Feature Stability This will be an extension of the attribute stability capabilities of the V0.1 release Extended Spark & Python support Compatibility with different Spark & Python versions Apache Spark 2.4.x on Java 8 with Python 3.7.x Apache Spark 3.1.x on Java 11 with Python 3.9.x Apache Spark 3.2.x on Java 11 with Python 3.9.x Runtime Environment support Microsoft Azure Databricks Version 1.0 (September 2022) We released version 1.0 of Anovos in September 2022 with all the functionalities needed to support an end-to-end machine learning workflow. Anovos 1.0 is able to store the generated features in an open source feature store, like Feast (see the docs for more information). It further provided functions to work with geospatial data, running Anovos on the Azure Kubernetes Service, and numerous performance enhancements.","title":"Roadmap"},{"location":"using-anovos/roadmap.html#anovos-product-roadmap","text":"Anovos is built and released as an open source project based on our experience in handling massive data sets to produce predictive features. At Mobilewalla , we process terabytes of mobile engagement signals daily to mine consumer behavior and use features from that data to build distributed machine learning models to solve a wide range of business problems. On this journey, we faced lots of challenges due to the lack of a comprehensive and scalable library. After realizing the unavailability of such libraries, we designed and implemented Anovos as an open source library for every data scientists\u2019 use.","title":"Anovos Product Roadmap"},{"location":"using-anovos/roadmap.html#the-roadmap","text":"Following the 1.0 release (see the History section below ) our main focus for the upcoming incremental releases is on making Anovos even easier to use and more performant. Our goal is to develop Anovos into a tool that can be used on a data scientist's machines as well as state-of-the-art distributed data processing infrastructure. As we're identifying specific next steps, we'll continuously update this page. If you have any suggestions or feedback, let us know! .","title":"\ud83d\udee3 The Roadmap"},{"location":"using-anovos/roadmap.html#the-history","text":"We developed the first fully functional version of Anovos over the course of three major releases: versions 0.1, 0.2, and 1.0.","title":"The History"},{"location":"using-anovos/roadmap.html#version-01-november-2021","text":"The 0.1 release of Anovos had all the essential data ingestion and comprehensive data analysis functionalities, as well as the data pre-processing and cleaning mechanisms. It also included some key differentiating functionalities, like data drift and stability computations, which are crucial in deciding the need for model refresh/tweak options. Another benefit of Anovos is a dynamic visualization component configured based on data ingestion pipelines. Every data metric computed from the Anovos ingestion process can be visualized and utilized for CXO level decision-making.","title":"Version 0.1 (November 2021)"},{"location":"using-anovos/roadmap.html#details","text":"","title":"Details"},{"location":"using-anovos/roadmap.html#data-ingest","text":"AWS S3 Storage integration Read and write to/from local files Column selection and renaming Support for Parquet and CSV files Support for numerical and categorical data types","title":"Data Ingest"},{"location":"using-anovos/roadmap.html#data-analyzer-and-diagnostics","text":"Frequency analysis Attribute/feature vs. target Attribute/feature interaction/association","title":"Data Analyzer and Diagnostics"},{"location":"using-anovos/roadmap.html#data-preprocessing-and-cleaning","text":"Outlier detection (IQR/Standardization) Treatment of invalid values Missing attributes analysis","title":"Data Preprocessing and Cleaning"},{"location":"using-anovos/roadmap.html#data-health-and-monitoring","text":"Data drift identification (Hellinger Distance, KS, JSD, and PSI) Attribute stability analysis Overall data quality analysis","title":"Data Health and Monitoring"},{"location":"using-anovos/roadmap.html#runtime-environment-support","text":"Local Docker-based AWS EMR","title":"Runtime Environment support"},{"location":"using-anovos/roadmap.html#report-visualization","text":"Comprehensive 360 degree view report of the ingested data (Numerical & Categorical) Executive summary Wiki Descriptive statistics Quality Checker Attribute association Data drift & stability","title":"Report Visualization"},{"location":"using-anovos/roadmap.html#version-02-release-march-2022","text":"In this release of Anovos , the library supported ingesting data from cloud service providers like Microsoft Azure. The release also added mechanisms to read and write different file formats such as Avro and nested JSON. The key differentiating functionality of this release is the Feature Explorer & Feature Mapper for data scientists and end-users to resolve their cold-start problems, which will immensely reduce their literature review time. The V0.2 release also added a capability called Feature Stability estimator based on the composition of a given feature using set of attributes. This will greatly benefit data scientists to understand the potential feature instabilities that could harm the resiliency of an ML model. With the 0.2 release Anovos was ready to be used in the day-to-day practices of any data scientists or analyst.","title":"Version 0.2 Release (March 2022)"},{"location":"using-anovos/roadmap.html#details_1","text":"","title":"Details"},{"location":"using-anovos/roadmap.html#data-ingest_1","text":"Microsoft Azure Blob Storage integration Support for Avro and nested JSON files Support for additional data types: Time stamps columns Support for Timeseries data ingestion","title":"Data Ingest"},{"location":"using-anovos/roadmap.html#data-cleaning-and-transformation","text":"Parsing Merging Converting/Coding Derivations Calculations Imputations Auto encoders Dimension reduction Date/Time related transformations","title":"Data Cleaning and Transformation"},{"location":"using-anovos/roadmap.html#feature-explorer-feature-recommender-semantic-search-enabled","text":"To recommend potential features based on the industry, use case, and the ingested data dictionary Industry specific use cases and respective features Telco BFSI Retail Healthcare Transportation Supply chain Recommendations are enabled by Semantic search capability Supported by pre-compiled feature corpus","title":"Feature Explorer / Feature recommender (Semantic search enabled)"},{"location":"using-anovos/roadmap.html#feature-stability","text":"This will be an extension of the attribute stability capabilities of the V0.1 release","title":"Feature Stability"},{"location":"using-anovos/roadmap.html#extended-spark-python-support","text":"Compatibility with different Spark & Python versions Apache Spark 2.4.x on Java 8 with Python 3.7.x Apache Spark 3.1.x on Java 11 with Python 3.9.x Apache Spark 3.2.x on Java 11 with Python 3.9.x","title":"Extended Spark &amp; Python support"},{"location":"using-anovos/roadmap.html#runtime-environment-support_1","text":"Microsoft Azure Databricks","title":"Runtime Environment support"},{"location":"using-anovos/roadmap.html#version-10-september-2022","text":"We released version 1.0 of Anovos in September 2022 with all the functionalities needed to support an end-to-end machine learning workflow. Anovos 1.0 is able to store the generated features in an open source feature store, like Feast (see the docs for more information). It further provided functions to work with geospatial data, running Anovos on the Azure Kubernetes Service, and numerous performance enhancements.","title":"Version 1.0 (September 2022)"},{"location":"using-anovos/scaling.html","text":"Using Anovos at Scale Anovos is built for feature engineering and data processing at scale. The library was built for and tested on Mobilewalla's mobile engagement data with the following attributes: Property Value Size 50 GB No. of Rows 384,694,946 No. of Columns 35 No. of Numerical Columns 4 No. of Categorical Columns 31 \u23f1 Benchmark To benchmark Anovos ' performance, we ran a pipeline on this dataset. The entire pipeline was optimized such that the computed statistics could be reused by other functions as much as possible. For example, the modes (the most frequently seen values) computed by the measures_of_centralTendency function were also used for imputation while treating null values in a column with nullColumns_detection or detecting a columns' biasedness using biasedness_detection . Hence, the time recorded for a function in the benchmark might (Pipeline Mode) differ significantly from the time taken by the same function when running in isolation (Standalone Mode). Further, Apache Spark does its own set of optimizations of transformations under the hood while running multiple functions together, which further adds to the time difference. Function Pipeline (mins) Standalone (minutes) global_summary 1 1 measures_of_counts 5 5 measures_of_centralTendency 30 30 measures_of_cardinality 49 32 measures_of_percentiles 1 1 measures_of_dispersion 1 1 measures_of_shape 3 3 duplicate_detection 5 5 nullRows_detection 4 5 invalidEntries_detection 15 41 IDness_detection 2 8 biasedness_detection 2 28 outlier_detection 4 8 nullColumns_detection 2 63 variable_clustering 2 3 IV_calculation * 8 9 IG_calculation * 6 8 * A binary categorical column was selected as a target variable to test this function. To see if the library works with large number of attributes, we horizontally scale tested on different dataset with the following attributes: Property Value Size 15 GB No. of Rows 40,507,005 No. of Columns 284 No. of Numerical Columns 252 No. of Categorical Columns 23 Function Time (mins) global_summary 0.2 measures_of_counts 3 measures_of_centralTendency 9 measures_of_cardinality 12 measures_of_percentiles 7 measures_of_dispersion 9 measures_of_shape 5 duplicate_detection 2 nullRows_detection 4 invalidEntries_detection 9 IDness_detection 2 biasedness_detection 2 outlier_detection 85 nullColumns_detection 3 cat_to_num_unsupervised 4 cat_to_num_supervised 2 z_standardization 6 IQR_standardization 3 normalization 6 PCA_latentFeatures 20 Limitations For current performance limitations, see the dedicated overview of Anovos ' limitations .","title":"Scaling Up"},{"location":"using-anovos/scaling.html#using-anovos-at-scale","text":"Anovos is built for feature engineering and data processing at scale. The library was built for and tested on Mobilewalla's mobile engagement data with the following attributes: Property Value Size 50 GB No. of Rows 384,694,946 No. of Columns 35 No. of Numerical Columns 4 No. of Categorical Columns 31","title":"Using Anovos at Scale"},{"location":"using-anovos/scaling.html#benchmark","text":"To benchmark Anovos ' performance, we ran a pipeline on this dataset. The entire pipeline was optimized such that the computed statistics could be reused by other functions as much as possible. For example, the modes (the most frequently seen values) computed by the measures_of_centralTendency function were also used for imputation while treating null values in a column with nullColumns_detection or detecting a columns' biasedness using biasedness_detection . Hence, the time recorded for a function in the benchmark might (Pipeline Mode) differ significantly from the time taken by the same function when running in isolation (Standalone Mode). Further, Apache Spark does its own set of optimizations of transformations under the hood while running multiple functions together, which further adds to the time difference. Function Pipeline (mins) Standalone (minutes) global_summary 1 1 measures_of_counts 5 5 measures_of_centralTendency 30 30 measures_of_cardinality 49 32 measures_of_percentiles 1 1 measures_of_dispersion 1 1 measures_of_shape 3 3 duplicate_detection 5 5 nullRows_detection 4 5 invalidEntries_detection 15 41 IDness_detection 2 8 biasedness_detection 2 28 outlier_detection 4 8 nullColumns_detection 2 63 variable_clustering 2 3 IV_calculation * 8 9 IG_calculation * 6 8 * A binary categorical column was selected as a target variable to test this function. To see if the library works with large number of attributes, we horizontally scale tested on different dataset with the following attributes: Property Value Size 15 GB No. of Rows 40,507,005 No. of Columns 284 No. of Numerical Columns 252 No. of Categorical Columns 23 Function Time (mins) global_summary 0.2 measures_of_counts 3 measures_of_centralTendency 9 measures_of_cardinality 12 measures_of_percentiles 7 measures_of_dispersion 9 measures_of_shape 5 duplicate_detection 2 nullRows_detection 4 invalidEntries_detection 9 IDness_detection 2 biasedness_detection 2 outlier_detection 85 nullColumns_detection 3 cat_to_num_unsupervised 4 cat_to_num_supervised 2 z_standardization 6 IQR_standardization 3 normalization 6 PCA_latentFeatures 20","title":"\u23f1 Benchmark"},{"location":"using-anovos/scaling.html#limitations","text":"For current performance limitations, see the dedicated overview of Anovos ' limitations .","title":"Limitations"},{"location":"using-anovos/workflow.html","text":"The Anovos Workflow We designed Anovos with an end-to-end machine learning workflow mind. Teams can use Anovos either as the foundation of their entire workflow or use functions from the library as part of an existing pipeline. For example, an organization might have an end-to-end workflow that lacks components offered by Anovos . The organization can incorporate additional components by a simple API call for a function from Anovos . The following workflow diagram shows the potential ways to use Anovos in an end-to-end workflow settings: Anovos Feature Engineering Video Series We have created a series of videos that walk through a typical Anovos feature engineering workflow. Video 1: Introduction to Feature Engineering with Anovos \\ Video 2: Exploratory Data Analysis Using Reports \\ Video 3: Exploratory Data Analysis with the Data Analyzer \\ Video 4: Feature Composition in Anovos \\ Video 5: Producing Model Ready Features \\ Anovos Workshop You can find the material for a comprehensive introductory workshop as part of the Anovos GitHub repository: https://github.com/anovos/anovos/tree/main/tutorial . The accompanying documentation walks you through installing Anovos and using it as part of an end-to-end feature engineering workflow. If you would like to receive hands-on training in Anovos for your team, don't hesitate to reach out to us .","title":"Workflow"},{"location":"using-anovos/workflow.html#the-anovos-workflow","text":"We designed Anovos with an end-to-end machine learning workflow mind. Teams can use Anovos either as the foundation of their entire workflow or use functions from the library as part of an existing pipeline. For example, an organization might have an end-to-end workflow that lacks components offered by Anovos . The organization can incorporate additional components by a simple API call for a function from Anovos . The following workflow diagram shows the potential ways to use Anovos in an end-to-end workflow settings:","title":"The Anovos Workflow"},{"location":"using-anovos/workflow.html#anovos-feature-engineering-video-series","text":"We have created a series of videos that walk through a typical Anovos feature engineering workflow. Video 1: Introduction to Feature Engineering with Anovos \\ Video 2: Exploratory Data Analysis Using Reports \\ Video 3: Exploratory Data Analysis with the Data Analyzer \\ Video 4: Feature Composition in Anovos \\ Video 5: Producing Model Ready Features \\","title":"Anovos Feature Engineering Video Series"},{"location":"using-anovos/workflow.html#anovos-workshop","text":"You can find the material for a comprehensive introductory workshop as part of the Anovos GitHub repository: https://github.com/anovos/anovos/tree/main/tutorial . The accompanying documentation walks you through installing Anovos and using it as part of an end-to-end feature engineering workflow. If you would like to receive hands-on training in Anovos for your team, don't hesitate to reach out to us .","title":"Anovos Workshop"},{"location":"using-anovos/data-reports/final_report.html","text":"Generating Finalized Reports with Anovos This section covers the final execution part where primarily the output generated by the previous step is being fetched upon and structured in the desirable UI layout. The primary function dealt here is anovos_report which caters to the: reading of available data produced from data analyzer module and chart objects as produced by the report preprocessing module computation of additional charts based on available data populating the reporting layer leveraging an open-sourced python package called datapane. capability of producing stand alone reports for individual sections (Descriptive Statistics, Quality Check, Attribute Associations, Data Drift & Stability & Time Series Analyzer) The following parameters are specified in the function anovos_report : master_path : The path which contains the data of intermediate output in terms of json chart objects, csv file (pandas df) id_col : The ID column is accepted to ensure & restrict unnecessary analysis to be performed on the same label_col : Name of label or target column in the input dataset. By default, the label_col is set as blank. corr_threshold : The threshold chosen beyond which the attributes are found to be redundant iv_threshold : The threshold beyond which the attributes are found to be significant in terms of model. drift_threshold_model: The threshold beyond which the attribute can be flagged as 1 or drifted as measured across different drift metrices specified by the user dataDict_path : The path containing the exact name, definition mapping of the attributes. This is eventually used to populate at the report for easy referencing metricDict_path : The path containing the metric dictionary run_type : Option to specify whether the execution happen locally or in EMR way final_report_path : Path where the final report needs to be saved output_type : Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\"","title":"Final Report"},{"location":"using-anovos/data-reports/final_report.html#generating-finalized-reports-with-anovos","text":"This section covers the final execution part where primarily the output generated by the previous step is being fetched upon and structured in the desirable UI layout. The primary function dealt here is anovos_report which caters to the: reading of available data produced from data analyzer module and chart objects as produced by the report preprocessing module computation of additional charts based on available data populating the reporting layer leveraging an open-sourced python package called datapane. capability of producing stand alone reports for individual sections (Descriptive Statistics, Quality Check, Attribute Associations, Data Drift & Stability & Time Series Analyzer) The following parameters are specified in the function anovos_report : master_path : The path which contains the data of intermediate output in terms of json chart objects, csv file (pandas df) id_col : The ID column is accepted to ensure & restrict unnecessary analysis to be performed on the same label_col : Name of label or target column in the input dataset. By default, the label_col is set as blank. corr_threshold : The threshold chosen beyond which the attributes are found to be redundant iv_threshold : The threshold beyond which the attributes are found to be significant in terms of model. drift_threshold_model: The threshold beyond which the attribute can be flagged as 1 or drifted as measured across different drift metrices specified by the user dataDict_path : The path containing the exact name, definition mapping of the attributes. This is eventually used to populate at the report for easy referencing metricDict_path : The path containing the metric dictionary run_type : Option to specify whether the execution happen locally or in EMR way final_report_path : Path where the final report needs to be saved output_type : Time category of analysis which can be between \"Daily\", \"Hourly\", \"Weekly\"","title":"Generating Finalized Reports with Anovos"},{"location":"using-anovos/data-reports/html_report.html","text":"Generating HTML Reports with Anovos The final output is generated in form of HTML report. This has 7 sections viz. Executive Summary , Wiki , Descriptive Statistics , Quality Check , Attribute Associations , Data Drift & Data Stability and Time Series Analyzer at most which can be seen basis user input. We\u2019ve tried to detail each section based on the analysis performed referring to a publicly available Income Dataset . Executive Summary The Executive Summary gives an overall summary of the key statistics obtained from the analyzed data such as : - dimensions of the analysis data - nature of use case whether target variable is involved or not along with the distribution - overall data diagnosis view as seen across some of the key metrices. Wiki The Wiki tab has two different sections consisting of: Data Dictionary : This section contains the details of the attributes present in the data frame. The user if specifies the attribute wise definition at a specific path, then the details of the same will be populated along with the data type else only the attribute wise datatype will be seen. Metric Dictionary : Mostly containing the details about the different sections under the report. This could be a quick reference for the user. Descriptive Statistics The Descriptive Statistics section summarizes the dataset with key statistical metrics and distribution plots through the following modules: Global Summary : Details about the data dimensions and the attribute wise information. Statistics By Metric Type includes the following modules: Measures of Counts : Details about the attribute wise count, fill rate, etc. Measures of Central Tendency : Details about the measurement of central tendency in terms of mean, median, mode. Measures of Cardinality : Details about the uniqueness in categories for each attribute. Measures of Percentiles : Indicates the different attribute value associated against the range of percentile cut offs. This helps to understand the spread of attributes. Measures of Dispersion : Explains how much the data is dispersed through metrics like Standard Deviation, Variance, Covariance, IQR and range for each attribute Measures of Shape : Describe the tail ness of distribution (or pattern) for different attributes through skewness and kurtosis. Attribute Visuations includes the following modules: Numeric : Visualize the distributions of Numerical attributes using Histograms Categorical : Visualize the distributions of Categorical attributes using Barplot Quality Check The Quality Check section consists of a qualitative inspection of the data at a row & columnar level. It consists of the following modules: Column Level Null Columns Detections \u2013 Detect the sparsity of the datasets, e.g., count and percentage of missing value of attributes Outlier Detection \u2013 Used to detect and visualize the outlier present in numerical attributes of the datasets Violin Plot - Displays the spread of numerical attributes IDness Detection - Measures the cardinality associated across attributes Biasedness Detection - Useful to identifying columns to see if they are biased or skewed towards one specific value Invalid Entries Detection - Checks for certain suspicious patterns in attributes\u2019 values Row Level Duplicate Detection \u2013 Measure number of rows in the datasets that have same value for each attribute NullRows Detection - Measure the count/percentage of rows which have missing/null attributes Attribute Associations Attribute Associations section details of the interaction between different attributes and/or the relationship between an attribute & the binary target variable Association Matrix & Plot is a Correlation Measure of the strength of relationship among each attribute by finding correlation coefficient having range -1.0 to 1.0. Visualization is shown through heat map to describe the strength of relationship among attributes. Information Value Computation is used to rank variables on the basis of their importance. Greater the value of IV higher rthe attribute importance. IV less than 0.02 is not useful for prediction. Bar plot is used to show the significance in descending order. Information Gain Computation measures the reduction in entropy by splitting a dataset according to given value of a attribute. Bar plot is used to show the significance in descending order. Variable Clustering divides the numerical attributes into disjoint or hierarchical clusters based on linear relationship of attributes. Attribute to Target Association determines the event rate trend across different attribute categories : Numeric Categorical Data Drift & Data Stability Data Drift Analysis This module mainly focus on covariate shift based drift detection. The table below describes about the statistical metrics measuring the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). An attribute is flagged as drifted if any drift metric is found to be above the threshold set by the user or 0.1 (default) Overall Data Health The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 9 datasets collected in 9 consecutive time periods (D1, D2, \u2026, D9), data stability index of an attribute measures how stable the attribute is from D1 to D9. Data Stability Analysis The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. Attribute wise stability charts have been plotted and the results are being highlighted. Time Series Analyzer This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications. The Basic Landscaping The initial analysis details we records where we understand whether a particular field qualifies for Time Series check or not. Time Stamp Data Diagnosis The landscaping & diagnosis work done on the fields which have been auto-detected as time series. Different statistics are taken out pertaining to the association of devices for id_date & date_id pair combination as specified. Additionally, vital stats are also produced. Visualization across the Shortlisted Timestamp Attributes The visualization below shows the typical time series plots generated based on the analysis attributes and the granularity of data preferred for analysis ( daily , weekly , hourly ). The decomposed view largely describes about some of the typical components of time series forecasting like Trend, Seasonal & Residual on top of the Observed series. Inspecting on the decomposed view of Time Series is supposedly one of the key steps from analysis point irrespective of the model used later. The stationarity & transformation view help us in determinining how much the data can be quantified (through KPSS & ADSS test) in terms of transformation needed to attain stationarity. Additionally, we're showing on how a post transformation view basis Box-Cox-Transformation can be further used in the downstream applications.","title":"HTML Report"},{"location":"using-anovos/data-reports/html_report.html#generating-html-reports-with-anovos","text":"The final output is generated in form of HTML report. This has 7 sections viz. Executive Summary , Wiki , Descriptive Statistics , Quality Check , Attribute Associations , Data Drift & Data Stability and Time Series Analyzer at most which can be seen basis user input. We\u2019ve tried to detail each section based on the analysis performed referring to a publicly available Income Dataset .","title":"Generating HTML Reports with Anovos"},{"location":"using-anovos/data-reports/html_report.html#executive-summary","text":"The Executive Summary gives an overall summary of the key statistics obtained from the analyzed data such as : - dimensions of the analysis data - nature of use case whether target variable is involved or not along with the distribution - overall data diagnosis view as seen across some of the key metrices.","title":"Executive Summary"},{"location":"using-anovos/data-reports/html_report.html#wiki","text":"The Wiki tab has two different sections consisting of: Data Dictionary : This section contains the details of the attributes present in the data frame. The user if specifies the attribute wise definition at a specific path, then the details of the same will be populated along with the data type else only the attribute wise datatype will be seen. Metric Dictionary : Mostly containing the details about the different sections under the report. This could be a quick reference for the user.","title":"Wiki"},{"location":"using-anovos/data-reports/html_report.html#descriptive-statistics","text":"The Descriptive Statistics section summarizes the dataset with key statistical metrics and distribution plots through the following modules: Global Summary : Details about the data dimensions and the attribute wise information. Statistics By Metric Type includes the following modules: Measures of Counts : Details about the attribute wise count, fill rate, etc. Measures of Central Tendency : Details about the measurement of central tendency in terms of mean, median, mode. Measures of Cardinality : Details about the uniqueness in categories for each attribute. Measures of Percentiles : Indicates the different attribute value associated against the range of percentile cut offs. This helps to understand the spread of attributes. Measures of Dispersion : Explains how much the data is dispersed through metrics like Standard Deviation, Variance, Covariance, IQR and range for each attribute Measures of Shape : Describe the tail ness of distribution (or pattern) for different attributes through skewness and kurtosis. Attribute Visuations includes the following modules: Numeric : Visualize the distributions of Numerical attributes using Histograms Categorical : Visualize the distributions of Categorical attributes using Barplot","title":"Descriptive Statistics"},{"location":"using-anovos/data-reports/html_report.html#quality-check","text":"The Quality Check section consists of a qualitative inspection of the data at a row & columnar level. It consists of the following modules: Column Level Null Columns Detections \u2013 Detect the sparsity of the datasets, e.g., count and percentage of missing value of attributes Outlier Detection \u2013 Used to detect and visualize the outlier present in numerical attributes of the datasets Violin Plot - Displays the spread of numerical attributes IDness Detection - Measures the cardinality associated across attributes Biasedness Detection - Useful to identifying columns to see if they are biased or skewed towards one specific value Invalid Entries Detection - Checks for certain suspicious patterns in attributes\u2019 values Row Level Duplicate Detection \u2013 Measure number of rows in the datasets that have same value for each attribute NullRows Detection - Measure the count/percentage of rows which have missing/null attributes","title":"Quality Check"},{"location":"using-anovos/data-reports/html_report.html#attribute-associations","text":"Attribute Associations section details of the interaction between different attributes and/or the relationship between an attribute & the binary target variable Association Matrix & Plot is a Correlation Measure of the strength of relationship among each attribute by finding correlation coefficient having range -1.0 to 1.0. Visualization is shown through heat map to describe the strength of relationship among attributes. Information Value Computation is used to rank variables on the basis of their importance. Greater the value of IV higher rthe attribute importance. IV less than 0.02 is not useful for prediction. Bar plot is used to show the significance in descending order. Information Gain Computation measures the reduction in entropy by splitting a dataset according to given value of a attribute. Bar plot is used to show the significance in descending order. Variable Clustering divides the numerical attributes into disjoint or hierarchical clusters based on linear relationship of attributes. Attribute to Target Association determines the event rate trend across different attribute categories : Numeric Categorical","title":"Attribute Associations"},{"location":"using-anovos/data-reports/html_report.html#data-drift-data-stability","text":"Data Drift Analysis This module mainly focus on covariate shift based drift detection. The table below describes about the statistical metrics measuring the data drift of an attribute from source to target distribution: Population Stability Index (PSI), Jensen-Shannon Divergence (JSD), Hellinger Distance (HD) and Kolmogorov-Smirnov Distance (KS). An attribute is flagged as drifted if any drift metric is found to be above the threshold set by the user or 0.1 (default) Overall Data Health The data stability is represented by a single metric to summarise the stability of an attribute over multiple time periods. For example, given 9 datasets collected in 9 consecutive time periods (D1, D2, \u2026, D9), data stability index of an attribute measures how stable the attribute is from D1 to D9. Data Stability Analysis The major difference between data drift and data stability is that data drift analysis is only based on 2 datasets: source and target. However data stability analysis can be performed on multiple datasets. In addition, the source dataset is not required indicating that the stability index can be directly computed among multiple target datasets by comparing the statistical properties among them. Attribute wise stability charts have been plotted and the results are being highlighted.","title":"Data Drift &amp; Data Stability"},{"location":"using-anovos/data-reports/html_report.html#time-series-analyzer","text":"This section summarizes the information about timestamp features and how they are interactive with other attributes. An exhaustive diagnosis is done by looking at different time series components, how they could be useful in deriving insights for further downstream applications. The Basic Landscaping The initial analysis details we records where we understand whether a particular field qualifies for Time Series check or not. Time Stamp Data Diagnosis The landscaping & diagnosis work done on the fields which have been auto-detected as time series. Different statistics are taken out pertaining to the association of devices for id_date & date_id pair combination as specified. Additionally, vital stats are also produced. Visualization across the Shortlisted Timestamp Attributes The visualization below shows the typical time series plots generated based on the analysis attributes and the granularity of data preferred for analysis ( daily , weekly , hourly ). The decomposed view largely describes about some of the typical components of time series forecasting like Trend, Seasonal & Residual on top of the Observed series. Inspecting on the decomposed view of Time Series is supposedly one of the key steps from analysis point irrespective of the model used later. The stationarity & transformation view help us in determinining how much the data can be quantified (through KPSS & ADSS test) in terms of transformation needed to attain stationarity. Additionally, we're showing on how a post transformation view basis Box-Cox-Transformation can be further used in the downstream applications.","title":"Time Series Analyzer"},{"location":"using-anovos/data-reports/intermediate_report.html","text":"Generating Intermediate Data for Reports This section largely covers the data pre\u2013processing. The primary function which is used to address all the subsequent modules is charts_to_objects . It precisely helps in saving the chart data in form of objects, which is eventually read by the final report generation script. The objects saved are specifically used at the modules shown at the Report based on the user input. Wide variations of chart are used for showcasing the data trends through Bar Plot, Histogram, Violin Plot, Heat Map, Gauge Chart, Line Chart, etc. Following arguments are specified in the primary function charts_to_objects : spark : Spark session idf : Input Dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols : This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. label_col : Name of label or target column in the input dataset. By default, the label_col is set as None to accommodate unsupervised use case. event_label : Value of event (label 1) in the label column. By default, the event_label is kept as 1 unless otherwise specified explicitly. bin_method : equal_frequency or equal_range. The bin method is set as \u201cequal_range\u201d and is being further referred to the attribute binning function where the necessary aggregation / binning is done. bin_size : The maximum number of categories which the user wants to retain is to be set here. By default the size is kept as 10 beyond which remaining records would be grouped under \u201cothers\u201d. coverage : Minimum % of rows mapped to actual category name and rest will be mapped to others. The default value kept is 1.0 which is the maximum at 100%. drift_detector : This argument takes Boolean type input \u2013 True or False. It indicates whether the drift component is already analyzed or not. By default it is kept as False. outlier_charts : This argument takes Boolean type input - True or False. It indicates whether the Outlier Chart needs to be displayed or not. By default it is kept as False. source_path : The source data path which is needed for drift analysis. If it\u2019s not computed / out of scope, the default value of \"NA\" is considered. master_path : The path which will contain the data of intermediate output in terms of json chart objects, csv file (pandas df) stats_unique : Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type : local or EMR. Option to specify whether the execution happen locally or in EMR way as it requires reading & writing to s3. The two form of output generated from this are chart objects and data frame . There are some secondary functions used alongside as a part of charts_to_objects processing.","title":"Intermediate Report"},{"location":"using-anovos/data-reports/intermediate_report.html#generating-intermediate-data-for-reports","text":"This section largely covers the data pre\u2013processing. The primary function which is used to address all the subsequent modules is charts_to_objects . It precisely helps in saving the chart data in form of objects, which is eventually read by the final report generation script. The objects saved are specifically used at the modules shown at the Report based on the user input. Wide variations of chart are used for showcasing the data trends through Bar Plot, Histogram, Violin Plot, Heat Map, Gauge Chart, Line Chart, etc. Following arguments are specified in the primary function charts_to_objects : spark : Spark session idf : Input Dataframe list_of_cols : This argument, in a list format, is used to specify the columns which are subjected to the analysis in the input dataframe. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. The user can also use \u201call\u201d as an input to this argument to consider all columns. This is super useful instead of specifying all column names manually. drop_cols : This argument, in a list format, is used to specify the columns which needs to be dropped from list_of_cols. Alternatively, instead of list, columns can be specified in a single text format where different column names are separated by pipe delimiter \u201c|\u201d. It is most useful when used coupled with \u201call\u201d value of list_of_cols, when we need to consider all columns except few handful of them. label_col : Name of label or target column in the input dataset. By default, the label_col is set as None to accommodate unsupervised use case. event_label : Value of event (label 1) in the label column. By default, the event_label is kept as 1 unless otherwise specified explicitly. bin_method : equal_frequency or equal_range. The bin method is set as \u201cequal_range\u201d and is being further referred to the attribute binning function where the necessary aggregation / binning is done. bin_size : The maximum number of categories which the user wants to retain is to be set here. By default the size is kept as 10 beyond which remaining records would be grouped under \u201cothers\u201d. coverage : Minimum % of rows mapped to actual category name and rest will be mapped to others. The default value kept is 1.0 which is the maximum at 100%. drift_detector : This argument takes Boolean type input \u2013 True or False. It indicates whether the drift component is already analyzed or not. By default it is kept as False. outlier_charts : This argument takes Boolean type input - True or False. It indicates whether the Outlier Chart needs to be displayed or not. By default it is kept as False. source_path : The source data path which is needed for drift analysis. If it\u2019s not computed / out of scope, the default value of \"NA\" is considered. master_path : The path which will contain the data of intermediate output in terms of json chart objects, csv file (pandas df) stats_unique : Takes arguments for read_dataset (data_ingest module) function in a dictionary format to read pre-saved statistics on unique value count i.e. if measures_of_cardinality or uniqueCount_computation (data_analyzer.stats_generator module) has been computed & saved before. (Default value = {}) run_type : local or EMR. Option to specify whether the execution happen locally or in EMR way as it requires reading & writing to s3. The two form of output generated from this are chart objects and data frame . There are some secondary functions used alongside as a part of charts_to_objects processing.","title":"Generating Intermediate Data for Reports"},{"location":"using-anovos/data-reports/overview.html","text":"Creating Data Reports with Anovos The data report module is composed of two sections details of which is described further. The primary utility of keeping the two modules is to decouple the two steps in a way that one happens at a distributed way involving the intermediate report data generation while the other is restricted to only the pre-processing and generation of the Anovos report .","title":"Overview"},{"location":"using-anovos/data-reports/overview.html#creating-data-reports-with-anovos","text":"The data report module is composed of two sections details of which is described further. The primary utility of keeping the two modules is to decouple the two steps in a way that one happens at a distributed way involving the intermediate report data generation while the other is restricted to only the pre-processing and generation of the Anovos report .","title":"Creating Data Reports with Anovos"},{"location":"using-anovos/setting-up/locally.html","text":"Setting up Anovos locally There are several ways to setting up and running Anovos locally: \ud83d\udc0b Running Anovos workloads through Docker You can run Anovos workloads defined in a configuration file using the anovos-worker Docker image, without having to set up Spark or anything else. This is the recommended option for developing and testing Anovos workloads without access to a Spark cluster. If you're just starting with Anovos and don't have any special requirements, pick this option. \ud83d\udc0d Using Anovos as a Python library There are two ways to install Anovos to use it as a Python library in your own code: Installation through pip . If you need more fine-grained control than the configuration file offers, or you have a way to execute Spark jobs, this is likely the best option for you. Cloning the GitHub repository. This is recommended if you would like to get access to the latest development version. It is also the way to go if you would like to build custom wheel files. \ud83d\udc0b Running Anovos workloads through Docker \ud83d\udcbf Software Prerequisites Running Anovos workloads through Docker requires Python 3.x, a Bash-compatible shell, and Docker. (See the Docker documentation for instructions how to install Docker on your machine.) At the moment, you need to download two scripts from the Anovos GitHub repository. You can either download the scripts individually: mkdir local && cd local wget https://raw.githubusercontent.com/anovos/anovos/main/local/rewrite_configuration.py wget https://raw.githubusercontent.com/anovos/anovos/main/local/run_workload.sh chmod +x run_workload.sh Or you can clone the entire Anovos GitHub repository, which will also give you access to example configurations: git clone https://github.com/anovos/anovos.git cd anovos/local chmod +x run_workload.sh In both cases, you will have a folder named local that contains the run_workload.sh shell script and the rewrite_configuration.py Python script. Launching an Anovos run To run an Anovos workload defined in a configuration file , you need to execute run_workload.sh and pass the name of the configuration file as the first parameter. All paths to input data in the configuration file have to be given relative to the directory you are calling run_workload.sh from. For example, the configuration for the basic demo workload (available here ) defines input_dataset : read_dataset : file_path : \"data/income_dataset/csv\" Hence, we need to ensure that the data directory is a subdirectory of the directory we are launching the workload from. Otherwise, the Anovos process inside the anovos-worker Docker container won't be able to access it. The following command will run the basic demo workflow included in the Anovos repository on Spark 3.2.2: # enter the root folder of the repository cd anovos # place the input data that is processed by the basic demo at the location # expected by the configuration mkdir data & cp ./examples/data/income_dataset ./data # launch the anovos-worker container ./local/run_workload.sh config/configs_basic.yaml 3 .2.2 Once processing has finished, you will find the output in a folder output within the directory you called run_workload.sh from. \ud83d\udca1 Note that the anovos-worker images provided through Docker Hub do not support the Feast or MLflow integrations out of the box, as they require interaction with third-party components, access to network communication, and/or interaction with the file system beyond the pre-configured paths. You can find the list of currently unsupported configuration blocks at the top of rewrite_configuration.py . If you try to run an _Anovos workload that uses unsupported features, _you will receive an error message and no anovos-worker container will be launched. If you would like, you can build and configure a custom pre-processing and launch script by adapting the files in anovos/local to your specific needs. For example, for Feast you will likely want to configure an additional volume or bind mount in run_workload.sh , whereas MLflow requires some network configuration. Specifying the Spark and Anovos versions You can optionally define the Anovos version by adding it as a third parameter: ./local/run_workload.sh config.yaml 3 .2.2 1 .0.1 This will use Anovos 1.0.1 on Spark 3.2.2. If no version for Anovos is given, the latest release available for the specified Spark version will be used. Please note that the corresponding anovos-worker image has to be available on Docker Hub for this to work out of the box. If you need a specific configuration not available as a pre-built image, you can follow the instructions here to build your own anovos-worker image. In that case, you can then launch run_workload.sh without specifying the Spark or Anovos version: ./local/run_workload.sh config.yaml \ud83d\udc0d Using Anovos as a Python library \ud83d\udcbf Software Prerequisites Anovos requires Spark, Python, and Java to be set up. We test for and officially support the following combinations: Spark 2.4.8 , Python 3.7 , and Java 8 Spark 3.1.3 , Python 3.9 , and Java 11 Spark 3.2.2 , Python 3.10 , and Java 11 Spark 3.3.0 , Python 3.10 , and Java 11 The following tutorials can be helpful in setting up Apache Spark: Installing Apache Spark on Mac OSX Installing Apache Spark and using PySpark on Windows \ud83d\udca1 For the foreseeable future, _Anovos will support Spark 3.1.x, 3.3.x, and 3.3.x. _We will phase out 2.4.x over the course of the next releases. To see which precise combinations we're currently testing, see this workflow configuration . Installation through pip To install Anovos , simply run pip install anovos You can select a specific version of Anovos by specifying the version: pip install anovos == 1 .0.1 For more information on specifying package versions, see the pip documentation . Then, you can import Anovos as a module into your Python applications using import anovos To trigger Spark workloads from Python, you have to ensure that the necessary external packages are included in the SparkSession . For this, you can either use the pre-configured SparkSession provided by Anovos : from anovos.shared.spark import spark If you need to use your own custom SparkSession , make sure to include the following dependencies: io.github.histogrammar:histogrammar_2.11:1.0.20 io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20 org.apache.spark:spark-avro_2.11:2.4.0 Cloning the GitHub repository Clone the Anovos repository to your local environment using the command: git clone https://github.com/anovos/anovos.git For production use, you'll always want to clone a specific version, e.g., git clone -b v1.0.1 --depth 1 https://github.com/anovos/anovos to just get the code for version 1.0.1 . Afterwards, go to the newly created anovos directory and execute the following command to clean and build the latest modules: make clean build Next, install Anovos ' dependencies by running pip install -r requirements.txt and go to the dist/ folder. There, you should Update the input and output paths in configs.yaml and configure the data set. You might also want to adapt the threshold settings to your needs. Adapt the main.py sample script. It demonstrates how different functions from Anovos can be stitched together to create a workflow. If necessary, update spark-submit.sh . This is the shell script used to run the Spark application via spark-submit . Once everything is configured, you can start your workflow run using the aforementioned script: nohup ./spark-submit.sh > run.txt & While the job is running, you can check the logs written to stdout using tail -f run.txt Once the run completes, the script will attempt to automatically open the final report ( report_stats/ml_anovos_report.html ) in your web browser.","title":"Locally"},{"location":"using-anovos/setting-up/locally.html#setting-up-anovos-locally","text":"There are several ways to setting up and running Anovos locally: \ud83d\udc0b Running Anovos workloads through Docker You can run Anovos workloads defined in a configuration file using the anovos-worker Docker image, without having to set up Spark or anything else. This is the recommended option for developing and testing Anovos workloads without access to a Spark cluster. If you're just starting with Anovos and don't have any special requirements, pick this option. \ud83d\udc0d Using Anovos as a Python library There are two ways to install Anovos to use it as a Python library in your own code: Installation through pip . If you need more fine-grained control than the configuration file offers, or you have a way to execute Spark jobs, this is likely the best option for you. Cloning the GitHub repository. This is recommended if you would like to get access to the latest development version. It is also the way to go if you would like to build custom wheel files.","title":"Setting up Anovos locally"},{"location":"using-anovos/setting-up/locally.html#running-anovos-workloads-through-docker","text":"","title":"\ud83d\udc0b Running Anovos workloads through Docker"},{"location":"using-anovos/setting-up/locally.html#software-prerequisites","text":"Running Anovos workloads through Docker requires Python 3.x, a Bash-compatible shell, and Docker. (See the Docker documentation for instructions how to install Docker on your machine.) At the moment, you need to download two scripts from the Anovos GitHub repository. You can either download the scripts individually: mkdir local && cd local wget https://raw.githubusercontent.com/anovos/anovos/main/local/rewrite_configuration.py wget https://raw.githubusercontent.com/anovos/anovos/main/local/run_workload.sh chmod +x run_workload.sh Or you can clone the entire Anovos GitHub repository, which will also give you access to example configurations: git clone https://github.com/anovos/anovos.git cd anovos/local chmod +x run_workload.sh In both cases, you will have a folder named local that contains the run_workload.sh shell script and the rewrite_configuration.py Python script.","title":"\ud83d\udcbf Software Prerequisites"},{"location":"using-anovos/setting-up/locally.html#launching-an-anovos-run","text":"To run an Anovos workload defined in a configuration file , you need to execute run_workload.sh and pass the name of the configuration file as the first parameter. All paths to input data in the configuration file have to be given relative to the directory you are calling run_workload.sh from. For example, the configuration for the basic demo workload (available here ) defines input_dataset : read_dataset : file_path : \"data/income_dataset/csv\" Hence, we need to ensure that the data directory is a subdirectory of the directory we are launching the workload from. Otherwise, the Anovos process inside the anovos-worker Docker container won't be able to access it. The following command will run the basic demo workflow included in the Anovos repository on Spark 3.2.2: # enter the root folder of the repository cd anovos # place the input data that is processed by the basic demo at the location # expected by the configuration mkdir data & cp ./examples/data/income_dataset ./data # launch the anovos-worker container ./local/run_workload.sh config/configs_basic.yaml 3 .2.2 Once processing has finished, you will find the output in a folder output within the directory you called run_workload.sh from. \ud83d\udca1 Note that the anovos-worker images provided through Docker Hub do not support the Feast or MLflow integrations out of the box, as they require interaction with third-party components, access to network communication, and/or interaction with the file system beyond the pre-configured paths. You can find the list of currently unsupported configuration blocks at the top of rewrite_configuration.py . If you try to run an _Anovos workload that uses unsupported features, _you will receive an error message and no anovos-worker container will be launched. If you would like, you can build and configure a custom pre-processing and launch script by adapting the files in anovos/local to your specific needs. For example, for Feast you will likely want to configure an additional volume or bind mount in run_workload.sh , whereas MLflow requires some network configuration.","title":"Launching an Anovos run"},{"location":"using-anovos/setting-up/locally.html#specifying-the-spark-and-anovos-versions","text":"You can optionally define the Anovos version by adding it as a third parameter: ./local/run_workload.sh config.yaml 3 .2.2 1 .0.1 This will use Anovos 1.0.1 on Spark 3.2.2. If no version for Anovos is given, the latest release available for the specified Spark version will be used. Please note that the corresponding anovos-worker image has to be available on Docker Hub for this to work out of the box. If you need a specific configuration not available as a pre-built image, you can follow the instructions here to build your own anovos-worker image. In that case, you can then launch run_workload.sh without specifying the Spark or Anovos version: ./local/run_workload.sh config.yaml","title":"Specifying the Spark and Anovos versions"},{"location":"using-anovos/setting-up/locally.html#using-anovos-as-a-python-library","text":"","title":"\ud83d\udc0d Using Anovos as a Python library"},{"location":"using-anovos/setting-up/locally.html#software-prerequisites_1","text":"Anovos requires Spark, Python, and Java to be set up. We test for and officially support the following combinations: Spark 2.4.8 , Python 3.7 , and Java 8 Spark 3.1.3 , Python 3.9 , and Java 11 Spark 3.2.2 , Python 3.10 , and Java 11 Spark 3.3.0 , Python 3.10 , and Java 11 The following tutorials can be helpful in setting up Apache Spark: Installing Apache Spark on Mac OSX Installing Apache Spark and using PySpark on Windows \ud83d\udca1 For the foreseeable future, _Anovos will support Spark 3.1.x, 3.3.x, and 3.3.x. _We will phase out 2.4.x over the course of the next releases. To see which precise combinations we're currently testing, see this workflow configuration .","title":"\ud83d\udcbf Software Prerequisites"},{"location":"using-anovos/setting-up/locally.html#installation-through-pip","text":"To install Anovos , simply run pip install anovos You can select a specific version of Anovos by specifying the version: pip install anovos == 1 .0.1 For more information on specifying package versions, see the pip documentation . Then, you can import Anovos as a module into your Python applications using import anovos To trigger Spark workloads from Python, you have to ensure that the necessary external packages are included in the SparkSession . For this, you can either use the pre-configured SparkSession provided by Anovos : from anovos.shared.spark import spark If you need to use your own custom SparkSession , make sure to include the following dependencies: io.github.histogrammar:histogrammar_2.11:1.0.20 io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20 org.apache.spark:spark-avro_2.11:2.4.0","title":"Installation through pip"},{"location":"using-anovos/setting-up/locally.html#cloning-the-github-repository","text":"Clone the Anovos repository to your local environment using the command: git clone https://github.com/anovos/anovos.git For production use, you'll always want to clone a specific version, e.g., git clone -b v1.0.1 --depth 1 https://github.com/anovos/anovos to just get the code for version 1.0.1 . Afterwards, go to the newly created anovos directory and execute the following command to clean and build the latest modules: make clean build Next, install Anovos ' dependencies by running pip install -r requirements.txt and go to the dist/ folder. There, you should Update the input and output paths in configs.yaml and configure the data set. You might also want to adapt the threshold settings to your needs. Adapt the main.py sample script. It demonstrates how different functions from Anovos can be stitched together to create a workflow. If necessary, update spark-submit.sh . This is the shell script used to run the Spark application via spark-submit . Once everything is configured, you can start your workflow run using the aforementioned script: nohup ./spark-submit.sh > run.txt & While the job is running, you can check the logs written to stdout using tail -f run.txt Once the run completes, the script will attempt to automatically open the final report ( report_stats/ml_anovos_report.html ) in your web browser.","title":"Cloning the GitHub repository"},{"location":"using-anovos/setting-up/on_aks.html","text":"Setting up Anovos on Azure Kubernetes (AKS) In this section we detailed how to run on Anovos on Azure Kubernetes Environment (AKS). Azure Cloud Infrastructure Firstly we detail why the cloud architecture plays a cruclai role in AKS setup. Although cloud architectures can vary depending on custom configurations, the following diagram represents the most common structure and flow of data for the Anovos on Azure K8s. anovos-client is a Azure VM. It serves as the platform from which the Spark jobs are submitted to the K8s cluster. Kubernetes-services can be any K8s setup, in this case it represents Azure AKS. container-registries to store the anovos-spark docker images for K8s. Azure Storage containers to store the data-sets to be used in Anovos workflow and the resulting data-sets and reports are also stored there. Requirements Resource Group requirements Dedicated resource group to contain all the resources to be created for the anovos project. VNET requirements Azure VM and AKS to be located within the same VNET. Subnet to allow public access (not via NAT gateway). Different subnet for AKS and VM is recommended. VM subnet size to be at least /27 CIDR range. AKS subnet size to be at least /24 CIDR range or bigger. VM requirements Ubuntu 20 recommended Single VM with at least 8 core CPU, 16 GB RAM and 128 GB disk. Install necessary pks and software(incl but not limited to anovos, spark, jdk) Managed Identity of the VM to have permissions to AKS, Container Registry and Blob Storage. Ex: Contributor Role with Resource Group scope, for the VM\u2019s managed identify. AKS requirements Dedicated cluster will be preferred but optional. Dedicated namespace, service principals for Anovos, recommended. Sufficient permissions to allow AKS to pull images from ACR. Azure Storage Container Dedicated container for Anovos to store all input dataset, results and reports. At this point, the code accepts only single container as it is capable of accepting only one SAS token. An SAS token with sufficient validity and full permissions on the anovos containers. This will be used within the python code and spark configuration. Azure Container Registry To store anovos-spark docker image, for the executor pods. Step 1: Setup Anovos Client on VM Software Reqired on VM Python 3.9 Spark 3.x JDK 8 Maven Jupyter zure CLI and AZCOPY Kubectl Azure CLI and AZCOPY Docker Anovos Part 1 Switch to root user # Switch to root user $ sudo su - Install Azure CLI and Azcopy #Install AZ CLI $ curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash #Download AzCopy $ wget https://aka.ms/downloadazcopy-v10-linux #Expand Archive $ tar -xvf downloadazcopy-v10-linux #(Optional) Remove existing AzCopy version $ rm /usr/bin/azcopy #Move AzCopy to the destination you want to store it $ cp ./azcopy_linux_amd64_*/azcopy /usr/bin/ #Ensure azcopy has execute permissions for user, group and others. $ chmod +x /usr/bin/azcopy Install Kubectl $ apt-get update $ apt-get install -y apt-transport-https ca-certificates curl $ curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg $ echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list $ apt-get update $ apt-get install -y kubectl Install JDK 8 $ apt-get update $ apt-get install openjdk-8-jdk Install Maven $ apt install maven Install Docker Client # Note: Python should be set to 3.8 (default for Ubuntu 20) # Verify python version $ python3 -V # Install docker client $ apt-get install apt-transport-https ca-certificates curl software-properties-common $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - $ add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" $ apt update $ apt-get install docker-ce Install Python 3.9 # Install Python3.9 $ apt-get update $ apt install software-properties-common $ add-apt-repository ppa:deadsnakes/ppa $ apt-get update $ apt install python3.9 # Set python3.9 as default $ update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2 $ update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1 $ update-alternatives --config python3 # [Optional] The following is required to fix the error with apt-pkg $ cd /usr/lib/python3/dist-packages $ cp -p apt_pkg.cpython-34m-i386-linux-gnu.so apt_pkg.so or ln -s apt_pkg.cpython-38-x86_64-linux-gnu.so apt_pkg.so $ apt install python3.9-distutils $ apt install python3-pip $ python3.9 -m pip install --upgrade pip $ apt install python3.9-venv $ pip3 install pyspark==3.2.1 $ pip3 install py4j Install Jupyter Notebook $ pip3 install jupyter Install Zip # This command is required if you wnat to run `make clean build` to build Anovos $ apt install zip Part 2 Entire Part 2 setup to be done as azureuser user. Install Spark $ wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz $ tar -xvf spark-3.2.1-bin-hadoop3.2.tgz Configure Jupyter $ jupyter notebook --generate-config # add the following line in .jupyter/jupyter_notebook_config.py c.NotebookApp.ip = '*' $ jupyter notebook password Clone Anovos Repo $ git clone https://github.com/anovos/anovos.git Install Anovos (This step is required everytime when we want to use the updated Anovos code) # install build, required only the first time $ pip3 install build # go to the root folder of Anovos repo and build python wheel file $ python3 -m build --wheel --outdir dist/ . # uninstall the old version of Anovos $ pip3 uninstall anovos # install the latest Anovos from .whl $ pip3 install dist/anovos-0.3.0-py3-none-any.whl # The following command is required if you encounter ERROR: Cannot uninstall 'PyYAML' $ pip3 install --ignore-installed PyYAML Configure env variables for azureuser # Set SPARK_HOME $ echo -e \"export SPARK_HOME=/home/azureuser/spark-3.2.1-bin-hadoop3.2\" >> ~/.profile # Set JAVA_HOME $ echo -e \"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\" >> ~/.profile # Set PYSPARK $ echo -e \"export PYSPARK_PYTHON=/usr/bin/python3\" >> ~/.profile $ echo -e \"export PYSPARK_DRIVER_PYTHON=/usr/bin/python3\" >> ~/.profile # Set PATH $ echo -e 'export PATH=\"$SPARK_HOME/bin:$PATH\"' >> ~/.profile # Source the new Env Variables $ source ~/.profile Configure kubectl (NOTE: Substitute correct values for variables within <> ) $ az login --identity $ az aks get-credentials --resource-group <resource-grp> --name <k8s-name> --subscription <subscription> Create Namespace in K8s cluster $ kubectl get namespace $ kubectl create namespace anovos Create AKS Service Accoun $ kubectl get serviceaccount --namespace=anovos $ kubectl create serviceaccount spark --namespace=anovos $ kubectl get clusterrolebinding --namespace=anovos $ kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=anovos:spark --namespace=anovos Step 2: Build Anovos Spark Container We need to custom build a docker image to be used by K8s to run the spark executor pods. Substitute correct values for variables within <> Authenticate using Azure Managed Identity $ sudo az login --identity Login to the Azure Container Registry $ sudo az acr login --name <registryname> Build and Push Docker for Executor Pods # build Python wheel file, can copy .whl file under /home/azureuser/spark-3.2.1-bin-hadoop3.2/python $ python3 -m build --wheel --outdir dist/ . $ cp dist/anovos-0.3.0-py3-none-any.whl /home/azureuser/spark-3.2.1-bin-hadoop3.2/python/anovos-0.3.0-py3-none-any.whl $ cd /home/azureuser/spark-3.2.1-bin-hadoop3.2 # build Dockerfile as image. Replace the Dockerfile path with the correct one if required $ sudo ./bin/docker-image-tool.sh -r <registryname>.azurecr.io -t <tag> -p ../Dockerfile build # push Docker image to registry $ sudo docker push <registryname>.azurecr.io/spark-py:<tag> NOTE: Dockerfile is provided at the end of this document. Step 3: Run Anovos Build Anovos # Create log4j.properties under /home/azureuser/spark-3.2.1-bin-hadoop3.2/conf, you can also customise your log4j configuration inside this file $ mv /home/azureuser/spark-3.2.1-bin-hadoop3.2/conf/log4j.properties.template /home/azureuser/spark-3.2.1-bin-hadoop3.2/conf/log4j.properties # Go to Anovos repo $ cd /home/azureuser/anovos # Build Anovos. After building Anovos, you can see a new folder `dist/`. $ make clean build Using Spark-Submit To run Anovos, we can write our own workflow file to call the different modules and submodules of Anovos, as per our need and execute in any spark application by just importing Anovos and using it. Alternatively, there is also a ready-made workflow file in the repo that can be modified to call the different modules in a pre-set order based on the config file and this can be used to executed via spark-submit.\u2028 Get the latest jars and main.py and config file from the repo # Clone the repo $ sudo git clone https://github.com/anovos/anovos.git Here is an example of spark-submit script (NOTE: Substitute correct values for variables within < > ) ./bin/spark-submit \\ --master k8s://https://<K8s-master-dns>:443 \\ --deploy-mode client \\ --name anovos-job-01 \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.executor.instances=10 \\ --conf spark.executor.cores=4 \\ --conf spark.executor.memory=16g \\ --conf spark.kubernetes.namespace=anovos \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --conf spark.kubernetes.container.image=<registryname>.azurecr.io/spark-py:<tag> \\ --conf spark.hadoop.fs.azure.sas.<container>.<storageaccount>.blob.core.windows.net=\"<sas_token>\" \\ --packages org.apache.hadoop:hadoop-azure:3.2.0,com.microsoft.azure:azure-storage:8.6.3,org.apache.spark:spark-avro_2.12:3.2.1 \\ --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\ --conf spark.executor.extraJavaOptions=\"-Dlog4j.configuration=file://$SPARK_HOME/conf/log4j.properties\" \\ --conf spark.driver.extraJavaOptions=\"-Dlog4j.configuration=file://$SPARK_HOME/conf/log4j.properties\" \\ --conf spark.kubernetes.file.upload.path=wasbs://<container>@<storageaccount>.blob.core.windows.net \\ --jars /home/azureuser/anovos/jars/${histogrammar_jar},/home/azureuser/anovos/jars/${histogrammar_sql_jar} \\ \u2776 /home/azureuser/anovos/dist/main.py \\ \u2777 /home/azureuser/anovos/config/configs.yaml \\ \u2778 ak8s \\ \u2779 '{\"fs.azure.sas.<container>.<storageaccount>.blob.core.windows.net\":\"<sas_token>\"}' \u277a \u2776 Anovos uses few external third party packages for some of its sub modules and we specify its jars using the --jars tag. This is found from anovos/jars/*.jar in the repo.\u2028 \u2777 you can use the main.py file from either anovos/src/main/main.py or anovos/dist/main.py . \u2778 you can find sample configs.yaml file from anovos/config/configs.yaml - Edit configs.yaml to enter correct values for Azure storage @ wasbs://\u2026 \u2779 There is a keyword passed as argument to the main.py along with config file specifying the run-type for Anovos. Current options are - local - for local run - emr \u2013 for AWS EMR based run - databricks \u2013 for runs using a DBFS file system - ak8s \u2013 for runs on Azure K8s setup reading and writing data to Azure Blob Storage. \u277a This is an argument specific to Azure K8s run. Specify the container, storage account name and the sas token with permission to read input/write output in the mentioned paths in the configs.yaml file. For other run_types can set this value as \"NA\". The final report should be generated at two places local path where spark-submit is being executed from, as \"ml_anovos_report.html\" which can be opened in Google Chrome browser in the final_report path specified in the configs.yaml file. More in depth documentation in each of the modules and api docs can be found at https://docs.anovos.ai Using Jupyter Notebook Running Anovos on Jupyter notebook is straightforward as using any python package. \u2028 Kindly refer to the https://github.com/anovos/anovos/tree/main/examples/notebook in repo for sample notebooks for each module or the startup-guide for anovos to do a quick run on Jupyter. Open Jupyter Notebook $ jupyter notebook We can now access the Jupyter Notebook Web UI by http:// :8888 Here is an example of how to set SparkConf in Notebook # set run type variable run_type = \"ak8s\" # \"local\", \"emr\", \"databricks\", \"ak8s\" #For run_type Azure Kubernetes, run the following block import os from pyspark import SparkContext, SparkConf from pyspark.sql import SparkSession if run_type == \"ak8s\": fs_path = \"spark.hadoop.fs.azure.sas.<container>.<storageaccount>.blob.core.windows.net\" auth_key=\"<sas_token>\" master_url=\"k8s://https://<K8s-master-dns>:443\" docker_image=\"<registryname>.azurecr.io/spark-py:<tag>\" kubernetes_namespace =\"anovos\" # Create Spark config for our Kubernetes based cluster manager sparkConf = SparkConf() sparkConf.setMaster(master_url) sparkConf.setAppName(\"Anovos_pipeline\") sparkConf.set(\"spark.submit.deployMode\",\"client\") sparkConf.set(\"spark.kubernetes.container.image\", docker_image) sparkConf.set(\"spark.kubernetes.namespace\", kubernetes_namespace) sparkConf.set(\"spark.executor.instances\", \"4\") sparkConf.set(\"spark.executor.cores\", \"4\") sparkConf.set(\"spark.executor.memory\", \"16g\") sparkConf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\") sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\") sparkConf.set(fs_path,auth_key) sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark\") sparkConf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.2.0,com.microsoft.azure:azure-storage:8.6.3,io.github.histogrammar:histogrammar_2.12:1.0.20,io.github.histogrammar:histogrammar-sparksql_2.12:1.0.20,org.apache.spark:spark-avro_2.12:3.2.1\") # Initialize our Spark cluster, this will actually # generate the worker nodes. spark = SparkSession.builder.config(conf=sparkConf).getOrCreate() sc = spark.sparkContext #For other run types import from anovos.shared. else: from anovos.shared.spark import * auth_key = \"NA\" Please note that you can only use the files in Azure Blob Storage ( wasbs://<container>@<storageaccount>.blob.core.windows.net/ ). If you use local file, it will throw error. Verification This is applicable for both by spark-submit or using Jupyter notebooks There are a few steps that can be executed to confirm the run is happening in distirbuted manner using Kubernetes services. kubectl get pods -n anovos or kubectl get all The above commands can be used to view the current running pods for the namespace. Dockerfile ARG base_img FROM $base_img WORKDIR / # Reset to root to run installation tasks USER 0 RUN mkdir ${SPARK_HOME}/python COPY python/anovos-0.3.0-py3-none-any.whl ${SPARK_HOME}/python/anovos-0.3.0-py3-none-any.whl RUN apt-get update && \\ apt install -y wget python3 python3-pip && \\ pip3 install --upgrade pip setuptools && \\ pip3 install ${SPARK_HOME}/python/anovos-0.3.0-py3-none-any.whl && \\ rm -r /root/.cache && rm -rf /var/cache/apt/* COPY python/pyspark ${SPARK_HOME}/python/pyspark COPY python/lib ${SPARK_HOME}/python/lib WORKDIR /opt/spark/work-dir # Download hadoop-azure, azure-storage, and dependencies (See above) RUN wget --quiet https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.2.0/hadoop-azure-3.2.0.jar -O /opt/spark/jars/hadoop-azure-3.2.0.jar RUN wget --quiet https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.3/azure-storage-8.6.3.jar -O /opt/spark/jars/azure-storage-8.6.3.jar RUN wget --quiet https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.2/httpclient-4.5.2.jar -O /opt/spark/jars/httpclient-4.5.2.jar RUN wget --quiet https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util-ajax/9.3.24.v20180605/jetty-util-ajax-9.3.24.v20180605.jar -O /opt/spark/jars/jetty-util-ajax-9.3.24.v20180605.jar RUN wget --quiet https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar -O /opt/spark/jars/jackson-mapper-asl-1.9.13.jar RUN wget --quiet https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar -O /opt/spark/jars/jackson-core-asl-1.9.13.jar RUN wget --quiet https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.4.Final/wildfly-openssl-1.0.4.Final.jar -O /opt/spark/jars/wildfly-openssl-1.0.4.Final.jar RUN wget --quiet https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar -O /opt/spark/jars/httpcore-4.4.4.jar RUN wget --quiet https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar -O /opt/spark/jars/commons-logging-1.1.3.jar RUN wget --quiet https://repo1.maven.org/maven2/commons-codec/commons-codec/1.11/commons-codec-1.11.jar -O /opt/spark/jars/commons-codec-1.11.jar RUN wget --quiet https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar -O /opt/spark/jars/jetty-util-9.3.24.v20180605.jar RUN wget --quiet https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.9.4/jackson-core-2.9.4.jar -O /opt/spark/jars/jackson-core-2.9.4.jar RUN wget --quiet https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.12/slf4j-api-1.7.12.jar -O /opt/spark/jars/slf4j-api-1.7.12.jar RUN wget --quiet https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar -O /opt/spark/jars/commons-lang3-3.4.jar RUN wget --quiet https://repo1.maven.org/maven2/com/microsoft/azure/azure-keyvault-core/1.0.0/azure-keyvault-core-1.0.0.jar -O /opt/spark/jars/azure-keyvault-core-1.0.0.jar RUN wget --quiet https://repo1.maven.org/maven2/com/google/guava/guava/20.0/guava-20.0.jar -O /opt/spark/jars/guava-20.0.jar ENTRYPOINT [ \"/opt/entrypoint.sh\" ]","title":"On Azure Kubernetes"},{"location":"using-anovos/setting-up/on_aks.html#setting-up-anovos-on-azure-kubernetes-aks","text":"In this section we detailed how to run on Anovos on Azure Kubernetes Environment (AKS).","title":"Setting up Anovos on Azure Kubernetes (AKS)"},{"location":"using-anovos/setting-up/on_aks.html#azure-cloud-infrastructure","text":"Firstly we detail why the cloud architecture plays a cruclai role in AKS setup. Although cloud architectures can vary depending on custom configurations, the following diagram represents the most common structure and flow of data for the Anovos on Azure K8s. anovos-client is a Azure VM. It serves as the platform from which the Spark jobs are submitted to the K8s cluster. Kubernetes-services can be any K8s setup, in this case it represents Azure AKS. container-registries to store the anovos-spark docker images for K8s. Azure Storage containers to store the data-sets to be used in Anovos workflow and the resulting data-sets and reports are also stored there.","title":"Azure Cloud Infrastructure"},{"location":"using-anovos/setting-up/on_aks.html#requirements","text":"Resource Group requirements Dedicated resource group to contain all the resources to be created for the anovos project. VNET requirements Azure VM and AKS to be located within the same VNET. Subnet to allow public access (not via NAT gateway). Different subnet for AKS and VM is recommended. VM subnet size to be at least /27 CIDR range. AKS subnet size to be at least /24 CIDR range or bigger. VM requirements Ubuntu 20 recommended Single VM with at least 8 core CPU, 16 GB RAM and 128 GB disk. Install necessary pks and software(incl but not limited to anovos, spark, jdk) Managed Identity of the VM to have permissions to AKS, Container Registry and Blob Storage. Ex: Contributor Role with Resource Group scope, for the VM\u2019s managed identify. AKS requirements Dedicated cluster will be preferred but optional. Dedicated namespace, service principals for Anovos, recommended. Sufficient permissions to allow AKS to pull images from ACR. Azure Storage Container Dedicated container for Anovos to store all input dataset, results and reports. At this point, the code accepts only single container as it is capable of accepting only one SAS token. An SAS token with sufficient validity and full permissions on the anovos containers. This will be used within the python code and spark configuration. Azure Container Registry To store anovos-spark docker image, for the executor pods.","title":"Requirements"},{"location":"using-anovos/setting-up/on_aks.html#step-1-setup-anovos-client-on-vm","text":"Software Reqired on VM Python 3.9 Spark 3.x JDK 8 Maven Jupyter zure CLI and AZCOPY Kubectl Azure CLI and AZCOPY Docker Anovos","title":"Step 1: Setup Anovos Client on VM"},{"location":"using-anovos/setting-up/on_aks.html#part-1","text":"Switch to root user # Switch to root user $ sudo su - Install Azure CLI and Azcopy #Install AZ CLI $ curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash #Download AzCopy $ wget https://aka.ms/downloadazcopy-v10-linux #Expand Archive $ tar -xvf downloadazcopy-v10-linux #(Optional) Remove existing AzCopy version $ rm /usr/bin/azcopy #Move AzCopy to the destination you want to store it $ cp ./azcopy_linux_amd64_*/azcopy /usr/bin/ #Ensure azcopy has execute permissions for user, group and others. $ chmod +x /usr/bin/azcopy Install Kubectl $ apt-get update $ apt-get install -y apt-transport-https ca-certificates curl $ curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg $ echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list $ apt-get update $ apt-get install -y kubectl Install JDK 8 $ apt-get update $ apt-get install openjdk-8-jdk Install Maven $ apt install maven Install Docker Client # Note: Python should be set to 3.8 (default for Ubuntu 20) # Verify python version $ python3 -V # Install docker client $ apt-get install apt-transport-https ca-certificates curl software-properties-common $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - $ add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" $ apt update $ apt-get install docker-ce Install Python 3.9 # Install Python3.9 $ apt-get update $ apt install software-properties-common $ add-apt-repository ppa:deadsnakes/ppa $ apt-get update $ apt install python3.9 # Set python3.9 as default $ update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2 $ update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1 $ update-alternatives --config python3 # [Optional] The following is required to fix the error with apt-pkg $ cd /usr/lib/python3/dist-packages $ cp -p apt_pkg.cpython-34m-i386-linux-gnu.so apt_pkg.so or ln -s apt_pkg.cpython-38-x86_64-linux-gnu.so apt_pkg.so $ apt install python3.9-distutils $ apt install python3-pip $ python3.9 -m pip install --upgrade pip $ apt install python3.9-venv $ pip3 install pyspark==3.2.1 $ pip3 install py4j Install Jupyter Notebook $ pip3 install jupyter Install Zip # This command is required if you wnat to run `make clean build` to build Anovos $ apt install zip","title":"Part 1"},{"location":"using-anovos/setting-up/on_aks.html#part-2","text":"Entire Part 2 setup to be done as azureuser user. Install Spark $ wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz $ tar -xvf spark-3.2.1-bin-hadoop3.2.tgz Configure Jupyter $ jupyter notebook --generate-config # add the following line in .jupyter/jupyter_notebook_config.py c.NotebookApp.ip = '*' $ jupyter notebook password Clone Anovos Repo $ git clone https://github.com/anovos/anovos.git Install Anovos (This step is required everytime when we want to use the updated Anovos code) # install build, required only the first time $ pip3 install build # go to the root folder of Anovos repo and build python wheel file $ python3 -m build --wheel --outdir dist/ . # uninstall the old version of Anovos $ pip3 uninstall anovos # install the latest Anovos from .whl $ pip3 install dist/anovos-0.3.0-py3-none-any.whl # The following command is required if you encounter ERROR: Cannot uninstall 'PyYAML' $ pip3 install --ignore-installed PyYAML Configure env variables for azureuser # Set SPARK_HOME $ echo -e \"export SPARK_HOME=/home/azureuser/spark-3.2.1-bin-hadoop3.2\" >> ~/.profile # Set JAVA_HOME $ echo -e \"export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\" >> ~/.profile # Set PYSPARK $ echo -e \"export PYSPARK_PYTHON=/usr/bin/python3\" >> ~/.profile $ echo -e \"export PYSPARK_DRIVER_PYTHON=/usr/bin/python3\" >> ~/.profile # Set PATH $ echo -e 'export PATH=\"$SPARK_HOME/bin:$PATH\"' >> ~/.profile # Source the new Env Variables $ source ~/.profile Configure kubectl (NOTE: Substitute correct values for variables within <> ) $ az login --identity $ az aks get-credentials --resource-group <resource-grp> --name <k8s-name> --subscription <subscription> Create Namespace in K8s cluster $ kubectl get namespace $ kubectl create namespace anovos Create AKS Service Accoun $ kubectl get serviceaccount --namespace=anovos $ kubectl create serviceaccount spark --namespace=anovos $ kubectl get clusterrolebinding --namespace=anovos $ kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=anovos:spark --namespace=anovos","title":"Part 2"},{"location":"using-anovos/setting-up/on_aks.html#step-2-build-anovos-spark-container","text":"We need to custom build a docker image to be used by K8s to run the spark executor pods. Substitute correct values for variables within <> Authenticate using Azure Managed Identity $ sudo az login --identity Login to the Azure Container Registry $ sudo az acr login --name <registryname> Build and Push Docker for Executor Pods # build Python wheel file, can copy .whl file under /home/azureuser/spark-3.2.1-bin-hadoop3.2/python $ python3 -m build --wheel --outdir dist/ . $ cp dist/anovos-0.3.0-py3-none-any.whl /home/azureuser/spark-3.2.1-bin-hadoop3.2/python/anovos-0.3.0-py3-none-any.whl $ cd /home/azureuser/spark-3.2.1-bin-hadoop3.2 # build Dockerfile as image. Replace the Dockerfile path with the correct one if required $ sudo ./bin/docker-image-tool.sh -r <registryname>.azurecr.io -t <tag> -p ../Dockerfile build # push Docker image to registry $ sudo docker push <registryname>.azurecr.io/spark-py:<tag> NOTE: Dockerfile is provided at the end of this document.","title":"Step 2: Build Anovos Spark Container"},{"location":"using-anovos/setting-up/on_aks.html#step-3-run-anovos","text":"Build Anovos # Create log4j.properties under /home/azureuser/spark-3.2.1-bin-hadoop3.2/conf, you can also customise your log4j configuration inside this file $ mv /home/azureuser/spark-3.2.1-bin-hadoop3.2/conf/log4j.properties.template /home/azureuser/spark-3.2.1-bin-hadoop3.2/conf/log4j.properties # Go to Anovos repo $ cd /home/azureuser/anovos # Build Anovos. After building Anovos, you can see a new folder `dist/`. $ make clean build","title":"Step 3: Run Anovos"},{"location":"using-anovos/setting-up/on_aks.html#using-spark-submit","text":"To run Anovos, we can write our own workflow file to call the different modules and submodules of Anovos, as per our need and execute in any spark application by just importing Anovos and using it. Alternatively, there is also a ready-made workflow file in the repo that can be modified to call the different modules in a pre-set order based on the config file and this can be used to executed via spark-submit.\u2028 Get the latest jars and main.py and config file from the repo # Clone the repo $ sudo git clone https://github.com/anovos/anovos.git Here is an example of spark-submit script (NOTE: Substitute correct values for variables within < > ) ./bin/spark-submit \\ --master k8s://https://<K8s-master-dns>:443 \\ --deploy-mode client \\ --name anovos-job-01 \\ --class org.apache.spark.examples.SparkPi \\ --conf spark.executor.instances=10 \\ --conf spark.executor.cores=4 \\ --conf spark.executor.memory=16g \\ --conf spark.kubernetes.namespace=anovos \\ --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\ --conf spark.kubernetes.container.image=<registryname>.azurecr.io/spark-py:<tag> \\ --conf spark.hadoop.fs.azure.sas.<container>.<storageaccount>.blob.core.windows.net=\"<sas_token>\" \\ --packages org.apache.hadoop:hadoop-azure:3.2.0,com.microsoft.azure:azure-storage:8.6.3,org.apache.spark:spark-avro_2.12:3.2.1 \\ --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\ --conf spark.executor.extraJavaOptions=\"-Dlog4j.configuration=file://$SPARK_HOME/conf/log4j.properties\" \\ --conf spark.driver.extraJavaOptions=\"-Dlog4j.configuration=file://$SPARK_HOME/conf/log4j.properties\" \\ --conf spark.kubernetes.file.upload.path=wasbs://<container>@<storageaccount>.blob.core.windows.net \\ --jars /home/azureuser/anovos/jars/${histogrammar_jar},/home/azureuser/anovos/jars/${histogrammar_sql_jar} \\ \u2776 /home/azureuser/anovos/dist/main.py \\ \u2777 /home/azureuser/anovos/config/configs.yaml \\ \u2778 ak8s \\ \u2779 '{\"fs.azure.sas.<container>.<storageaccount>.blob.core.windows.net\":\"<sas_token>\"}' \u277a \u2776 Anovos uses few external third party packages for some of its sub modules and we specify its jars using the --jars tag. This is found from anovos/jars/*.jar in the repo.\u2028 \u2777 you can use the main.py file from either anovos/src/main/main.py or anovos/dist/main.py . \u2778 you can find sample configs.yaml file from anovos/config/configs.yaml - Edit configs.yaml to enter correct values for Azure storage @ wasbs://\u2026 \u2779 There is a keyword passed as argument to the main.py along with config file specifying the run-type for Anovos. Current options are - local - for local run - emr \u2013 for AWS EMR based run - databricks \u2013 for runs using a DBFS file system - ak8s \u2013 for runs on Azure K8s setup reading and writing data to Azure Blob Storage. \u277a This is an argument specific to Azure K8s run. Specify the container, storage account name and the sas token with permission to read input/write output in the mentioned paths in the configs.yaml file. For other run_types can set this value as \"NA\". The final report should be generated at two places local path where spark-submit is being executed from, as \"ml_anovos_report.html\" which can be opened in Google Chrome browser in the final_report path specified in the configs.yaml file. More in depth documentation in each of the modules and api docs can be found at https://docs.anovos.ai","title":"Using Spark-Submit"},{"location":"using-anovos/setting-up/on_aks.html#using-jupyter-notebook","text":"Running Anovos on Jupyter notebook is straightforward as using any python package. \u2028 Kindly refer to the https://github.com/anovos/anovos/tree/main/examples/notebook in repo for sample notebooks for each module or the startup-guide for anovos to do a quick run on Jupyter. Open Jupyter Notebook $ jupyter notebook We can now access the Jupyter Notebook Web UI by http:// :8888 Here is an example of how to set SparkConf in Notebook # set run type variable run_type = \"ak8s\" # \"local\", \"emr\", \"databricks\", \"ak8s\" #For run_type Azure Kubernetes, run the following block import os from pyspark import SparkContext, SparkConf from pyspark.sql import SparkSession if run_type == \"ak8s\": fs_path = \"spark.hadoop.fs.azure.sas.<container>.<storageaccount>.blob.core.windows.net\" auth_key=\"<sas_token>\" master_url=\"k8s://https://<K8s-master-dns>:443\" docker_image=\"<registryname>.azurecr.io/spark-py:<tag>\" kubernetes_namespace =\"anovos\" # Create Spark config for our Kubernetes based cluster manager sparkConf = SparkConf() sparkConf.setMaster(master_url) sparkConf.setAppName(\"Anovos_pipeline\") sparkConf.set(\"spark.submit.deployMode\",\"client\") sparkConf.set(\"spark.kubernetes.container.image\", docker_image) sparkConf.set(\"spark.kubernetes.namespace\", kubernetes_namespace) sparkConf.set(\"spark.executor.instances\", \"4\") sparkConf.set(\"spark.executor.cores\", \"4\") sparkConf.set(\"spark.executor.memory\", \"16g\") sparkConf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\") sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\") sparkConf.set(fs_path,auth_key) sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark\") sparkConf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.2.0,com.microsoft.azure:azure-storage:8.6.3,io.github.histogrammar:histogrammar_2.12:1.0.20,io.github.histogrammar:histogrammar-sparksql_2.12:1.0.20,org.apache.spark:spark-avro_2.12:3.2.1\") # Initialize our Spark cluster, this will actually # generate the worker nodes. spark = SparkSession.builder.config(conf=sparkConf).getOrCreate() sc = spark.sparkContext #For other run types import from anovos.shared. else: from anovos.shared.spark import * auth_key = \"NA\" Please note that you can only use the files in Azure Blob Storage ( wasbs://<container>@<storageaccount>.blob.core.windows.net/ ). If you use local file, it will throw error.","title":"Using Jupyter Notebook"},{"location":"using-anovos/setting-up/on_aks.html#verification","text":"This is applicable for both by spark-submit or using Jupyter notebooks There are a few steps that can be executed to confirm the run is happening in distirbuted manner using Kubernetes services. kubectl get pods -n anovos or kubectl get all The above commands can be used to view the current running pods for the namespace.","title":"Verification"},{"location":"using-anovos/setting-up/on_aks.html#dockerfile","text":"ARG base_img FROM $base_img WORKDIR / # Reset to root to run installation tasks USER 0 RUN mkdir ${SPARK_HOME}/python COPY python/anovos-0.3.0-py3-none-any.whl ${SPARK_HOME}/python/anovos-0.3.0-py3-none-any.whl RUN apt-get update && \\ apt install -y wget python3 python3-pip && \\ pip3 install --upgrade pip setuptools && \\ pip3 install ${SPARK_HOME}/python/anovos-0.3.0-py3-none-any.whl && \\ rm -r /root/.cache && rm -rf /var/cache/apt/* COPY python/pyspark ${SPARK_HOME}/python/pyspark COPY python/lib ${SPARK_HOME}/python/lib WORKDIR /opt/spark/work-dir # Download hadoop-azure, azure-storage, and dependencies (See above) RUN wget --quiet https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.2.0/hadoop-azure-3.2.0.jar -O /opt/spark/jars/hadoop-azure-3.2.0.jar RUN wget --quiet https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/8.6.3/azure-storage-8.6.3.jar -O /opt/spark/jars/azure-storage-8.6.3.jar RUN wget --quiet https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.2/httpclient-4.5.2.jar -O /opt/spark/jars/httpclient-4.5.2.jar RUN wget --quiet https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util-ajax/9.3.24.v20180605/jetty-util-ajax-9.3.24.v20180605.jar -O /opt/spark/jars/jetty-util-ajax-9.3.24.v20180605.jar RUN wget --quiet https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar -O /opt/spark/jars/jackson-mapper-asl-1.9.13.jar RUN wget --quiet https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar -O /opt/spark/jars/jackson-core-asl-1.9.13.jar RUN wget --quiet https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.4.Final/wildfly-openssl-1.0.4.Final.jar -O /opt/spark/jars/wildfly-openssl-1.0.4.Final.jar RUN wget --quiet https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar -O /opt/spark/jars/httpcore-4.4.4.jar RUN wget --quiet https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar -O /opt/spark/jars/commons-logging-1.1.3.jar RUN wget --quiet https://repo1.maven.org/maven2/commons-codec/commons-codec/1.11/commons-codec-1.11.jar -O /opt/spark/jars/commons-codec-1.11.jar RUN wget --quiet https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar -O /opt/spark/jars/jetty-util-9.3.24.v20180605.jar RUN wget --quiet https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.9.4/jackson-core-2.9.4.jar -O /opt/spark/jars/jackson-core-2.9.4.jar RUN wget --quiet https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.12/slf4j-api-1.7.12.jar -O /opt/spark/jars/slf4j-api-1.7.12.jar RUN wget --quiet https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.4/commons-lang3-3.4.jar -O /opt/spark/jars/commons-lang3-3.4.jar RUN wget --quiet https://repo1.maven.org/maven2/com/microsoft/azure/azure-keyvault-core/1.0.0/azure-keyvault-core-1.0.0.jar -O /opt/spark/jars/azure-keyvault-core-1.0.0.jar RUN wget --quiet https://repo1.maven.org/maven2/com/google/guava/guava/20.0/guava-20.0.jar -O /opt/spark/jars/guava-20.0.jar ENTRYPOINT [ \"/opt/entrypoint.sh\" ]","title":"Dockerfile"},{"location":"using-anovos/setting-up/on_aws.html","text":"Setting up Anovos on AWS EMR For large workloads, you can set up Anovos on AWS EMR . Installing/ Downloading Anovos Clone the Anovos repository on you local environment using command: git clone https://github.com/anovos/anovos.git After cloning, go to the anovos directory and execute the following command to clean and build the latest modules in dist folder: make clean build Copy all required files into an S3 bucket Copy the following files to AWS S3 : /anovos/dist/anovos.zip This file contains all Anovos modules Zipped version is mandatory for running importing the modules as \u2013py-files /anovos/dist/data/income_dataset (optional) This folder contains our demo dataset - income dataset. /anovos/dist/main.py This is sample script to show how different functions from Anovos module can be stitched together to create a workflow. The users can create their own workflow script by importing the necessary functions. This script takes input from a yaml configuration file /anovos/dist/configs.yaml This is the sample yaml configuration file that sets the argument for all functions. Update configs.yaml for all input & output s3 paths - Typically variables that have _path in its name such as final_report_path, file_path, appended_metric_path, output_path, etc.. All other changes depend on the dataset used and modules to be run. /anovos/bin/aws_bootstrap_files/setup_on_aws_emr_5_30_and_above.sh This shell script is used to install all required packages to run Anovos on EMR (bootstrapping). Also, Kindly copy the requirements.txt in the repo to an accessible s3 bucket. Edit the fist line in setup_on_aws_emr_5_30_and_above.sh before and then proceed to using it as a bootstrap file for AWS EMR. /anovos/data/metric_dictionary.csv This contains a static dictionary file for the different metrics generated by Anovos in the report. This helps generate the wiki tab about the different modules/submodules metrics in the final Anovos report. /anovos/jars/histogrammar*.jar \\ These jars are to make use external dependent libraries for Anovos. Specify and use the correct version 2.12 or 2.11 based on the Scala version in your environment. AWS CLI Installation Instructions can be found here AWS copy command: aws s3 cp --recursive <local file path> <s3 path> --profile <profile name> Create a cluster Software Configuration Emr-5.33.0 Hadoop-2.10.1 Spark-2.4.7 Hive-2.3.7 TensorFlow 2.4.1 Spark Submit Details Deploy mode : client Spark-submit options : --num-executors 1000 --executor-cores 4 --executor-memory 20g --driver-memory 20G --driver-cores 4 --conf spark.driver.maxResultSize=15g --conf spark.yarn.am.memoryOverhead=1000m --conf spark.executor.memoryOverhead=2000m --conf spark.kryo.referenceTracking=false --conf spark.network.timeout=18000s --conf spark.executor.heartbeatInterval=12000s --conf spark.dynamicAllocation.executorIdleTimeout=12000s --conf spark.rpc.message.maxSize=1024 --conf spark.yarn.maxAppAttempts=1 --conf spark.speculation=false --conf spark.kryoserializer.buffer.max=1024 --conf spark.executor.extraJavaOptions=-XX:+UseG1GC --conf spark.driver.extraJavaOptions=-XX:+UseG1GC --packages org.apache.spark:spark-avro_2.11:2.4.0 --jars /anovos/jars/histogrammar-sparksql_2.11-1.0.20.jar,/anovos/jars/histogrammar_2.11-1.0.20.jar --py-files {s3_bucket}/anovos.zip When launching cluster need to make driver/worker node memory size is higher than total of configured one. For example, for the above config, there should be atleast than 20 + 2 = 22 GB per worker or driver. So user must select machine types with RAM around 26GB or more. Example machines - m3.2xlarge - 8 core, 30 GB. The above histogram jars version and avro package version should follow scala version. Application location - s3 path of main.py file Arguments for main.py file - s3:// /configs.yaml emr Final spark submit command example : spark-submit --deploy-mode client --num-executors 1000 --executor-cores 4 --executor-memory 20g --driver-memory 20G --driver-cores 4 --conf spark.driver.maxResultSize=15g --conf spark.yarn.am.memoryOverhead=1000m --conf spark.executor.memoryOverhead=2000m --conf spark.kryo.referenceTracking=false --conf spark.network.timeout=18000s --conf spark.executor.heartbeatInterval=12000s --conf spark.dynamicAllocation.executorIdleTimeout=12000s --conf spark.rpc.message.maxSize=1024 --conf spark.yarn.maxAppAttempts=1 --conf spark.speculation=false --conf spark.kryoserializer.buffer.max=1024 --conf spark.executor.extraJavaOptions=-XX:+UseG1GC --conf spark.driver.extraJavaOptions=-XX:+UseG1GC --packages org.apache.spark:spark-avro_2.11:2.4.0 --jars s3://<s3-bucket>/jars/histogrammar-sparksql_2.11-1.0.20.jar,s3://<s3-bucket>/jars/histogrammar_2.11-1.0.20.jar --py-files s3://<s3-bucket>/anovos.zip s3://<s3-bucket>/main.py s3://<s3-bucket>/configs.yaml emr Bootstrap Actions script location : specify the bootstrap_shell_script_s3_path/setup_on_aws_emr_5_30_and_above.sh","title":"On AWS EMR"},{"location":"using-anovos/setting-up/on_aws.html#setting-up-anovos-on-aws-emr","text":"For large workloads, you can set up Anovos on AWS EMR .","title":"Setting up Anovos on AWS EMR"},{"location":"using-anovos/setting-up/on_aws.html#installing-downloading-anovos","text":"Clone the Anovos repository on you local environment using command: git clone https://github.com/anovos/anovos.git After cloning, go to the anovos directory and execute the following command to clean and build the latest modules in dist folder: make clean build","title":"Installing/ Downloading Anovos"},{"location":"using-anovos/setting-up/on_aws.html#copy-all-required-files-into-an-s3-bucket","text":"Copy the following files to AWS S3 : /anovos/dist/anovos.zip This file contains all Anovos modules Zipped version is mandatory for running importing the modules as \u2013py-files /anovos/dist/data/income_dataset (optional) This folder contains our demo dataset - income dataset. /anovos/dist/main.py This is sample script to show how different functions from Anovos module can be stitched together to create a workflow. The users can create their own workflow script by importing the necessary functions. This script takes input from a yaml configuration file /anovos/dist/configs.yaml This is the sample yaml configuration file that sets the argument for all functions. Update configs.yaml for all input & output s3 paths - Typically variables that have _path in its name such as final_report_path, file_path, appended_metric_path, output_path, etc.. All other changes depend on the dataset used and modules to be run. /anovos/bin/aws_bootstrap_files/setup_on_aws_emr_5_30_and_above.sh This shell script is used to install all required packages to run Anovos on EMR (bootstrapping). Also, Kindly copy the requirements.txt in the repo to an accessible s3 bucket. Edit the fist line in setup_on_aws_emr_5_30_and_above.sh before and then proceed to using it as a bootstrap file for AWS EMR. /anovos/data/metric_dictionary.csv This contains a static dictionary file for the different metrics generated by Anovos in the report. This helps generate the wiki tab about the different modules/submodules metrics in the final Anovos report. /anovos/jars/histogrammar*.jar \\ These jars are to make use external dependent libraries for Anovos. Specify and use the correct version 2.12 or 2.11 based on the Scala version in your environment. AWS CLI Installation Instructions can be found here AWS copy command: aws s3 cp --recursive <local file path> <s3 path> --profile <profile name>","title":"Copy all required files into an S3 bucket"},{"location":"using-anovos/setting-up/on_aws.html#create-a-cluster","text":"Software Configuration Emr-5.33.0 Hadoop-2.10.1 Spark-2.4.7 Hive-2.3.7 TensorFlow 2.4.1 Spark Submit Details Deploy mode : client Spark-submit options : --num-executors 1000 --executor-cores 4 --executor-memory 20g --driver-memory 20G --driver-cores 4 --conf spark.driver.maxResultSize=15g --conf spark.yarn.am.memoryOverhead=1000m --conf spark.executor.memoryOverhead=2000m --conf spark.kryo.referenceTracking=false --conf spark.network.timeout=18000s --conf spark.executor.heartbeatInterval=12000s --conf spark.dynamicAllocation.executorIdleTimeout=12000s --conf spark.rpc.message.maxSize=1024 --conf spark.yarn.maxAppAttempts=1 --conf spark.speculation=false --conf spark.kryoserializer.buffer.max=1024 --conf spark.executor.extraJavaOptions=-XX:+UseG1GC --conf spark.driver.extraJavaOptions=-XX:+UseG1GC --packages org.apache.spark:spark-avro_2.11:2.4.0 --jars /anovos/jars/histogrammar-sparksql_2.11-1.0.20.jar,/anovos/jars/histogrammar_2.11-1.0.20.jar --py-files {s3_bucket}/anovos.zip When launching cluster need to make driver/worker node memory size is higher than total of configured one. For example, for the above config, there should be atleast than 20 + 2 = 22 GB per worker or driver. So user must select machine types with RAM around 26GB or more. Example machines - m3.2xlarge - 8 core, 30 GB. The above histogram jars version and avro package version should follow scala version. Application location - s3 path of main.py file Arguments for main.py file - s3:// /configs.yaml emr Final spark submit command example : spark-submit --deploy-mode client --num-executors 1000 --executor-cores 4 --executor-memory 20g --driver-memory 20G --driver-cores 4 --conf spark.driver.maxResultSize=15g --conf spark.yarn.am.memoryOverhead=1000m --conf spark.executor.memoryOverhead=2000m --conf spark.kryo.referenceTracking=false --conf spark.network.timeout=18000s --conf spark.executor.heartbeatInterval=12000s --conf spark.dynamicAllocation.executorIdleTimeout=12000s --conf spark.rpc.message.maxSize=1024 --conf spark.yarn.maxAppAttempts=1 --conf spark.speculation=false --conf spark.kryoserializer.buffer.max=1024 --conf spark.executor.extraJavaOptions=-XX:+UseG1GC --conf spark.driver.extraJavaOptions=-XX:+UseG1GC --packages org.apache.spark:spark-avro_2.11:2.4.0 --jars s3://<s3-bucket>/jars/histogrammar-sparksql_2.11-1.0.20.jar,s3://<s3-bucket>/jars/histogrammar_2.11-1.0.20.jar --py-files s3://<s3-bucket>/anovos.zip s3://<s3-bucket>/main.py s3://<s3-bucket>/configs.yaml emr Bootstrap Actions script location : specify the bootstrap_shell_script_s3_path/setup_on_aws_emr_5_30_and_above.sh","title":"Create a cluster"},{"location":"using-anovos/setting-up/on_azure_databricks.html","text":"Setting up Anovos on Azure Databricks Azure Databricks is a hosted version of Apache Spark on Microsoft Azure . It is a convenient way to handle big data workloads of Spark without having to set up and maintain your own cluster. To learn more about Azure Databricks, have a look at the official documentation or the following introductory tutorials: A beginner\u2019s guide to Azure Databricks Azure Databricks Hands-on Currently, Anovos supports two ways of running workflows on Azure Databricks: Processing datasets stored directly on DBFS Processing datasets stored on Azure Blob Storage Generally, we recommend the first option, as it requires slightly less configuration. However, if you're already storing your datasets on Azure Blob Storage, mounting the respective containers to DBFS allows you to directly process them with Anovos . 1. Anovos on Azure Databricks using DBFS The following steps are required for running Anovos workloads on Azure Databricks that process datasets stored on DBFS. Step 1.1: Installing Anovos on Azure Databricks To make Anovos available on Azure Databricks, you need to provide access to the Anovos Python package. The easiest way is to point Azure Databricks to the current release of Anovos on the Python Package Index (PyPI) . (This is where pip install anovos goes to fetch Anovos when installing from the terminal.) This has the advantage that you will get a well-tested and stable version of Anovos . We recommend this option. If you choose this option then you can directly go to Step 2. But if you need to make custom modifications to Anovos or need access to new features or bugfixes that have not been released yet, you can choose any of the options below. (We will configure Azure Databricks to retrieve the correct Anovos Python package as part of Step 4.) Alternative: Manually uploading a wheel file Instead of pointing Azure Databricks to the Python Package Index (PyPI), you can make Anovos available by downloading the respective wheel file from PyPI yourself and manually uploading it to Azure Databricks. You'll find the link to the latest wheel file on the \"Download files\" tab , it's the file with the extension .whl (for example: anovos-1.0.1-py3-none-any.whl ). If you'd like to use an older version, you can navigate to the respective version in the release history and access the \"Download files\" tab from there. Download the Anovos wheel file to your local machine and move on to Step 2. Alternative: Use a development version of Anovos If you would like to try the latest version of Anovos on Azure Databricks (or would like to make custom modifications to the library), you can also create a wheel file yourself. First, clone the Anovos GitHub repository to your local machine: git clone --depth 1 <https://github.com/anovos/anovos.git> \ud83d\udca1 Using the --branch flag allows you to select a specific release of Anovos. For example, adding --branch v1.0.1 will give you the state of the 1.0.1 release. If you omit the flag, you will get the latest development version of Anovos, which might not be fully functional or exhibit unexpected behavior. After cloning, go to the anovos directory that was automatically created in the process and execute the following command to clean and prepare the environment: make clean It is a good practice to always run this command prior to generating a wheel file or another kind of build artifact. \ud83d\udca1 To be able to create a wheel file, wheel , build , and setuptools need to be installed in the current Python environment. You can do so by running pip install build wheel setuptools . Then, to create the wheel file, run the following command directly inside the anovos folder: python -m build --wheel --outdir dist/ . Once the process is finished, the folder dist will contain the wheel file. It will have the file extension *.whl and might carry the latest version in its name. \ud83d\udca1 The version in the file name will be that of the latest version of Anovos, even if you cloned the repository yourself and used the latest state of the code. This is due to the fact that the version is only updated right before new release is published. To avoid confusion, it's a good practice to rename the wheel file to a custom name. Step 1.2: Prepare and copy the workflow configuration and data to DBFS To run an Anovos workflow, both the data to be processed and the workflow configuration need to be stored on DBFS. You can either use the UI or the CLI to copy files from your local machine to DBFS. For detailed instructions, see the respective subsections below. In this tutorial, we will use the \"income dataset\" and an accompanying pre-defined workflow. You can obtain these files by cloning the Anovos GitHub repository: \ud83d\udca1 Note that you need to use the dataset version and workflow configuration files from the same _Anovos version_ that you have set up in Step 2.1. Sometimes the version on PyPI that you obtain when running pip install anovos version is older than the latest development version on GitHub. git clone https://github.com/anovos/anovos.git You'll find the dataset under examples/data/income_dataset and the configuration file under config/configs_income_azure.yaml . You'll also need the metric_dictionary.csv file found under data/ . The configs_income_azure.yaml file contains the definition of the Anovos workflow. (To learn more about this file, see \ud83d\udcd6 Configuring Workloads .) First, you should have a look at the configured input paths to make sure that Anovos can find the data to be processed. It is also important to check that the output paths are set to a location on DBFS that suits your needs. For example, in the input_dataset block, you can see that by default the file_path is set to dbfs:/FileStore/tables/income_dataset/csv/ . If you would like to store your data at a different location, you need to adapt this path accordingly. Output paths are defined in several blocks. The output path for the report data is specified as master_path in the blocks report_preprocessing and report_generation . The path for the report is specified as final_report_path in the report_generation block. In this tutorial, by default, all these paths are set to dbfs:/FileStore/tables/report_stats . The location where the processed data is stored is given by file_path in the blocks write_main , write_intermediate , and write_stats . In this tutorial, by default, these are set to sub-folders of dbfs:/FileStore/tables/result . Finally, you need to ensure that the path to the metric_dictionary.csv file as well as the data_dictionary.csv file, which is part of the \"income dataset\", are correctly specified in the report_generation block. You can also make other changes to the workflow. For example, you can define which columns from the input dataset are used in the workflow. To try it yourself, find the delete_column configuration in the input_dataset block and add the column workclass to the list of columns to be deleted: delete_column : [ 'logfnl' , 'workclass' ] To learn more about defining workflows through config files, see \ud83d\udcd6 Configuring Workloads . Once the configs_income_azure.yaml file is complete, you can copy this file and the dataset to DBFS. For this, you can choose to either upload the files through the UI or use the Azure Databricks CLI. We describe both options in the following sections. In any case, make sure that you place the data files in the location defined in the configuration file. You should also remember the location of the configs_income_azure.yaml , as you will need this information in the subsequent steps. (For this tutorial, we have decided to place all files under dbfs:FileStore/tables/ .) Copying files to DBFS using the UI Launch the Azure Databricks workspace. Enter the data menu. Upload files by dragging files onto the marked area or click on it to upload using the file browser. For more detailed instructions, see the Databricks documentation . Copying files to DBFS using the CLI Install databricks-cli into a local Python environment by running pip install databricks-cli . Generate a personal access token for your Databricks workspace by going to Settings > User Settings > Generate new token . For details, see the Databricks documentation . Configure the CLI to access your workspace by running databricks configure --token . Enter the URL of the databricks host (the domain of your workspace, usually of the pattern https://<UNIQUE ID OF YOUR WORKSPACE>.azuredatabricks.net/ ) and the token when prompted for it. To verify the configuration, run databricks fs ls and check whether you are able to see the files stored on DBFS. Then copy the files using the dbfs cp command: For example: dbfs cp anovos/config/configs_income_azure.yaml dbfs:/FileStore/tables/configs_income_azure.yaml dbfs cp anovos/data/metric_dictionary.csv dbfs:/FileStore/tables/metric_dictionary.csv dbfs cp -r anovos/examples/data/income_dataset dbfs:/FileStore/tables/income_dataset For more information on the Databricks CLI, see the Databricks documentation . Step 1.3: Create a workflow script To launch the workflow on Azure Databricks, we need a single Python script as the entry point. Hence, we'll create a main.py script that invokes the Anovos' workflow runner: import sys from anovos import workflow workflow . run ( config_path = sys . argv [ 1 ], run_type = \"databricks\" ) Upload this script to DBFS as well using either of the methods described above. Again, you can place this file at a location of your choosing. In this tutorial, we have placed it at dbfs:/FileStore/tables/scripts/main.py . Step 1.4: Configure and launch an Anovos workflow as a Databricks job There are several types of jobs available on the Azure Databricks platform. For Anovos , the following job types are suitable choices: \"Python:\" The job runs from a single Python script. Anovos and the required Scala dependencies are installed through the respective package repositories. \"Spark Submit:\" The job is invoked through a bare spark-submit call. The installation of Anovos is handled by a cluster initialization script and the required Scala dependencies have to be provided as JAR files through DBFS. Note that there are several limitations for \"Spark Submit\" tasks: You can only run them on new clusters and autoscaling is not available. For more information, see the Databricks documentation on jobs . Unless you require the fine-grained control that this option offers with regard to cluster initialization and spark-submit options, we recommend to select \"Python\" as the job type. 1.4.1: Using the \"Python\" job type Once all files have been copied to DBFS, we can create an Azure Databricks job that starts a cluster and launches the Anovos workflow. Here's an example of a job configuration: You can see that we set the \"Type\" to \"Python\" and have provided the path of the main.py script. In the parameters section, we pass the DBFS path of the config file. The cluster configuration comprises settings for the Databricks Runtime, the number of workers, worker and driver types, as well as the cluster's scaling behavior. Here's an example of a cluster configuration for this tutorials: For more detailed information, refer to the Databricks documentation . To give the Databricks platform access to Anovos , click on \"Advanced options\" and select \"Add dependent libraries\" in the Job Configuration Window . If you chose the default way of obtaining Anovos directly from the Python Package Index, select \"PyPI\" as the \"Library Source\" and enter anovos as the \"Package\": \ud83d\udca1 In case you encounter the error \"ImportError: cannot import name 'soft_unicode' from 'markupsafe'\" while running the job, then you need to add markupsafe==2.0.1 as another dependency from PyPI. If you chose to provide your own wheel file, select \"Upload\" as the library source and follow the instructions. In addition to the Anovos wheel file, we need to provide the histogrammar package to Azure Databricks. Anovos uses this library internally to compute correlation matrices. Following the same procedure as for Anovos , you can add histogrammar as a dependent library. This time, we use \"Maven\" as the \"Library Source\". Then, select io.github.histogrammar:histogrammar-sparksql_2.12:1.0.20 and io.github.histogrammar:histogrammar_2.12:1.0.20 as the \"Coordinates\" (copy the coordinate one by one, paste in Coordinates box and then add it by clicking on Install button): (In case you're running Anovos on Spark 2.4.x, you need to add io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20 and io.github.histogrammar:histogrammar_2.11:1.0.20 ) Once the job is configured, click \"Create\" to instantiate it. Then, you'll see the full task configuration: On the subsequent screen, click on \"Run now\" to launch the job: For more information on creating and maintaining jobs, see the Databricks documentation . 1.4.2: Using the \"Spark Submit\" job type Anovos internally uses the histogrammar library to compute correlation matrices. Hence, we need to provide the package to Azure Databricks. As the \"Spark Submit\" job type requires any dependency to be available through DBFS, you first need to upload the histogrammar JAR files to DBFS. If you're using Spark 3.x, download the following files and upload them to DBFS: io.github.histogrammar.histogrammar_2.12-1.0.20.jar io.github.histogrammar.histogrammar-sparksql_2.12-1.0.20.jar If you're using Spark 2.x, download the following files and upload them to DBFS: io.github.histogrammar.histogrammar_2.11-1.0.20.jar io.github.histogrammar.histogrammar-sparksql_2.11-1.0.20.jar Once these files have been uploaded to DBFS, we can create an Azure Databricks job that starts a cluster and launches the Anovos workflow. Here's an example of a job configuration: You can see that we set the \"Type\" to \"Spark Submit\". In the parameters section, we pass the DBFS paths of the histogrammar JAR files, the sample class, the main.py script, and configuration file. For example: [ \"--jars\" , \"dbfs:/FileStore/tables/histogramm_jar/histogrammar_sparksql_2_12_1_0_20.jar,dbfs:/FileStore/tables/histogramm_jar/histogrammar_2_12_1_0_20.jar\" , \"--class\" , \"org.apache.spark.examples.SparkPi\" , \"/dbfs/FileStore/tables/scripts/main.py\" , \"/dbfs/FileStore/tables/configs_income_azure.yaml\" ] The cluster configuration comprises settings for the Databricks Runtime, the number of workers, worker and driver types. (Note that autoscaling is not available for \"Spark Submit\" jobs on Azure Databricks.) Here's an example of a cluster configuration for this tutorial: For more detailed information, refer to the Databricks documentation . To give the Databricks platform access to Anovos , you need to create a shell script that is executed upon cluster initialization and fetches the package from PyPI. The anovos_packages.sh script contains just one line: sudo pip3 install anovos Note that you should specify the version of Anovos if you're running production workloads to ensure reproducibility: sudo pip3 install anovos == 1 .0.1 Place this script on DBFS as well. During cluster configuration, click on \"Advanced options\" and specify the path to the script in the \"Init Script\" section: To enable logging, configure a DBFS path in the \"Log\" section: Once the job is configured, click \"Create\" to instantiate it. On the subsequent screen, click on \"Run now\" to launch the job: For more information on creating and maintaining jobs, see the Azure Databricks documentation . Step 1.5: Retrieve the output Once the job finishes successfully, it will show up under \"Completed runs\". The intermediate data and the report data are saved at the master_path and the final_report_path specified in the configs_income_azure.yaml file. In this tutorial, we have set these paths to dbfs:FileStore/tables/report_stats/ . To retrieve the HTML report and the report data, you can either go to this path in the UI and copy the files, or use the CLI to copy everything to your local machine: dbfs cp -r dbfs:/FileStore/tables/report_stats/ ./ For more details regarding accessing files on DBFS, see the instructions on uploading files to DBFS in Step 2. 2. Anovos on Azure Databricks Using an Azure Blob Storage Container Mounted to DBFS Step 2.1: Installing/Downloading Anovos This step is identical to Step 1.1: Installing Anovos on Azure Databricks . Step 2.2: Copy the dataset to an Azure Blob Storage container To run an Anovos workflow, the dataset needs to be stored on an Azure Blob Storage container. You can either use the UI by clicking the upload button or the CLI to copy files from your local machine to an Azure Blob Storage container. For detailed instructions, see the respective subsections below. In this tutorial, we will use the \"income dataset\" provided with Anovos and an accompanying pre-defined workflow. You can obtain these files by cloning the Anovos GitHub repository: git clone https://github.com/anovos/anovos.git Afterwards, you'll find the dataset under examples/data/income_dataset \ud83d\udca1 Note that you need to use the dataset version and workflow configuration files from the same _Anovos version_ that you have set up in Step 2.1. Sometimes the version on PyPI that you obtain when running pip install anovos version is older than the latest development version on GitHub. The syntax to upload a file using the command line are as follows: azcopy copy source_file \"<storage-account-name>.blob.core.windows.net/<container-name>?<sas-token>\" To learn more about transferring data to Azure Blob Storage containers, please refer to the Azure documentation . Step 2.3: Mount an Azure Blob Storage Container as a DBFS path in Azure Databricks To access files in an Azure Blob Storage container for running Anovos in Azure Databricks platform, you need to mount that container in the DBFS path. To mount the Azure Blob Storage container, execute the following commands in an Azure Databricks notebook: dbutils . fs . mount ( source = \"wasbs://<container-name>@<storage-account-name>.blob.core.windows.net\" , mount_point = \"/mnt/<mount-name>\" , extra_configs = { \"fs.azure.sas.<container-name>.<storage-account-name>.blob.core.windows.net\" : \"<sas-token>\" }) Here, - <storage-account-name> is the name of your Azure Blob Storage account - <container-name> is the name of a container in your Azure Blob Storage account - <mount-name> is the DBFS path where the Blob Storage container or a folder inside the container will be mounted to - <sas_token> is the SAS token for that storage account To learn more about mounting Azure Blob Storage containers in DBFS, please refer to the Azure Blob Storage documentation . \ud83d\udca1 Note that you only need to mount the container once. The container will remain mounted at the given mount point. To unmount a container, you can run dbutils.fs.unmount(\"/mnt/<mount-name>\") in an Azure Databricks notebook. Step 2.4: Update the workflow configuration for all input and output paths according to the DBFS mount point Once mounting is completed, the data is present in DBFS at the path specified as the mount point. All operations performed by Anovos when running a workflow will result in changes in the data stored in the Azure Blob Storage container. The example configuration file we use in this tutorial can be found at config/configs_income_azure_blob_mount.yaml in the Anovos repository. It will need to be updated to reflect the path of the mount point set above. In order for Anovos to be able to find the input data and write the output to the correct location, update all paths to contain the path of the mount point: file_path : \"dbfs:/mnt/<mount-name>/...\" \ud83e\udd13 Example: read_dataset : file_path : \"dbfs:/mnt/anovos1/income_dataset/csv/\" file_type : csv Here, the mount points is dbfs:/mnt/anovos1 and the input dataset is stored in a folder called income_dataset/csv within the Azure Blob Storage container. To learn more about the Anovos workflow configuration file and specifying paths for input and output data, have a look at the Configuring Workloads page. Step 2.5: Copy the updated configuration file from the local machine to the Azure Blob Storage container Once you have updated the configuration file, copy it to Azure Databricks using the same command that was used in Step 2.2 . Remaining Steps The remaining steps are the same as above, so please continue with Step 1.4","title":"On Azure Databricks"},{"location":"using-anovos/setting-up/on_azure_databricks.html#setting-up-anovos-on-azure-databricks","text":"Azure Databricks is a hosted version of Apache Spark on Microsoft Azure . It is a convenient way to handle big data workloads of Spark without having to set up and maintain your own cluster. To learn more about Azure Databricks, have a look at the official documentation or the following introductory tutorials: A beginner\u2019s guide to Azure Databricks Azure Databricks Hands-on Currently, Anovos supports two ways of running workflows on Azure Databricks: Processing datasets stored directly on DBFS Processing datasets stored on Azure Blob Storage Generally, we recommend the first option, as it requires slightly less configuration. However, if you're already storing your datasets on Azure Blob Storage, mounting the respective containers to DBFS allows you to directly process them with Anovos .","title":"Setting up Anovos on Azure Databricks"},{"location":"using-anovos/setting-up/on_azure_databricks.html#1-anovos-on-azure-databricks-using-dbfs","text":"The following steps are required for running Anovos workloads on Azure Databricks that process datasets stored on DBFS.","title":"1. Anovos on Azure Databricks using DBFS"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-11-installing-anovos-on-azure-databricks","text":"To make Anovos available on Azure Databricks, you need to provide access to the Anovos Python package. The easiest way is to point Azure Databricks to the current release of Anovos on the Python Package Index (PyPI) . (This is where pip install anovos goes to fetch Anovos when installing from the terminal.) This has the advantage that you will get a well-tested and stable version of Anovos . We recommend this option. If you choose this option then you can directly go to Step 2. But if you need to make custom modifications to Anovos or need access to new features or bugfixes that have not been released yet, you can choose any of the options below. (We will configure Azure Databricks to retrieve the correct Anovos Python package as part of Step 4.)","title":"Step 1.1: Installing Anovos on Azure Databricks"},{"location":"using-anovos/setting-up/on_azure_databricks.html#alternative-manually-uploading-a-wheel-file","text":"Instead of pointing Azure Databricks to the Python Package Index (PyPI), you can make Anovos available by downloading the respective wheel file from PyPI yourself and manually uploading it to Azure Databricks. You'll find the link to the latest wheel file on the \"Download files\" tab , it's the file with the extension .whl (for example: anovos-1.0.1-py3-none-any.whl ). If you'd like to use an older version, you can navigate to the respective version in the release history and access the \"Download files\" tab from there. Download the Anovos wheel file to your local machine and move on to Step 2.","title":"Alternative: Manually uploading a wheel file"},{"location":"using-anovos/setting-up/on_azure_databricks.html#alternative-use-a-development-version-of-anovos","text":"If you would like to try the latest version of Anovos on Azure Databricks (or would like to make custom modifications to the library), you can also create a wheel file yourself. First, clone the Anovos GitHub repository to your local machine: git clone --depth 1 <https://github.com/anovos/anovos.git> \ud83d\udca1 Using the --branch flag allows you to select a specific release of Anovos. For example, adding --branch v1.0.1 will give you the state of the 1.0.1 release. If you omit the flag, you will get the latest development version of Anovos, which might not be fully functional or exhibit unexpected behavior. After cloning, go to the anovos directory that was automatically created in the process and execute the following command to clean and prepare the environment: make clean It is a good practice to always run this command prior to generating a wheel file or another kind of build artifact. \ud83d\udca1 To be able to create a wheel file, wheel , build , and setuptools need to be installed in the current Python environment. You can do so by running pip install build wheel setuptools . Then, to create the wheel file, run the following command directly inside the anovos folder: python -m build --wheel --outdir dist/ . Once the process is finished, the folder dist will contain the wheel file. It will have the file extension *.whl and might carry the latest version in its name. \ud83d\udca1 The version in the file name will be that of the latest version of Anovos, even if you cloned the repository yourself and used the latest state of the code. This is due to the fact that the version is only updated right before new release is published. To avoid confusion, it's a good practice to rename the wheel file to a custom name.","title":"Alternative: Use a development version of Anovos"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-12-prepare-and-copy-the-workflow-configuration-and-data-to-dbfs","text":"To run an Anovos workflow, both the data to be processed and the workflow configuration need to be stored on DBFS. You can either use the UI or the CLI to copy files from your local machine to DBFS. For detailed instructions, see the respective subsections below. In this tutorial, we will use the \"income dataset\" and an accompanying pre-defined workflow. You can obtain these files by cloning the Anovos GitHub repository: \ud83d\udca1 Note that you need to use the dataset version and workflow configuration files from the same _Anovos version_ that you have set up in Step 2.1. Sometimes the version on PyPI that you obtain when running pip install anovos version is older than the latest development version on GitHub. git clone https://github.com/anovos/anovos.git You'll find the dataset under examples/data/income_dataset and the configuration file under config/configs_income_azure.yaml . You'll also need the metric_dictionary.csv file found under data/ . The configs_income_azure.yaml file contains the definition of the Anovos workflow. (To learn more about this file, see \ud83d\udcd6 Configuring Workloads .) First, you should have a look at the configured input paths to make sure that Anovos can find the data to be processed. It is also important to check that the output paths are set to a location on DBFS that suits your needs. For example, in the input_dataset block, you can see that by default the file_path is set to dbfs:/FileStore/tables/income_dataset/csv/ . If you would like to store your data at a different location, you need to adapt this path accordingly. Output paths are defined in several blocks. The output path for the report data is specified as master_path in the blocks report_preprocessing and report_generation . The path for the report is specified as final_report_path in the report_generation block. In this tutorial, by default, all these paths are set to dbfs:/FileStore/tables/report_stats . The location where the processed data is stored is given by file_path in the blocks write_main , write_intermediate , and write_stats . In this tutorial, by default, these are set to sub-folders of dbfs:/FileStore/tables/result . Finally, you need to ensure that the path to the metric_dictionary.csv file as well as the data_dictionary.csv file, which is part of the \"income dataset\", are correctly specified in the report_generation block. You can also make other changes to the workflow. For example, you can define which columns from the input dataset are used in the workflow. To try it yourself, find the delete_column configuration in the input_dataset block and add the column workclass to the list of columns to be deleted: delete_column : [ 'logfnl' , 'workclass' ] To learn more about defining workflows through config files, see \ud83d\udcd6 Configuring Workloads . Once the configs_income_azure.yaml file is complete, you can copy this file and the dataset to DBFS. For this, you can choose to either upload the files through the UI or use the Azure Databricks CLI. We describe both options in the following sections. In any case, make sure that you place the data files in the location defined in the configuration file. You should also remember the location of the configs_income_azure.yaml , as you will need this information in the subsequent steps. (For this tutorial, we have decided to place all files under dbfs:FileStore/tables/ .)","title":"Step 1.2: Prepare and copy the workflow configuration and data to DBFS"},{"location":"using-anovos/setting-up/on_azure_databricks.html#copying-files-to-dbfs-using-the-ui","text":"Launch the Azure Databricks workspace. Enter the data menu. Upload files by dragging files onto the marked area or click on it to upload using the file browser. For more detailed instructions, see the Databricks documentation .","title":"Copying files to DBFS using the UI"},{"location":"using-anovos/setting-up/on_azure_databricks.html#copying-files-to-dbfs-using-the-cli","text":"Install databricks-cli into a local Python environment by running pip install databricks-cli . Generate a personal access token for your Databricks workspace by going to Settings > User Settings > Generate new token . For details, see the Databricks documentation . Configure the CLI to access your workspace by running databricks configure --token . Enter the URL of the databricks host (the domain of your workspace, usually of the pattern https://<UNIQUE ID OF YOUR WORKSPACE>.azuredatabricks.net/ ) and the token when prompted for it. To verify the configuration, run databricks fs ls and check whether you are able to see the files stored on DBFS. Then copy the files using the dbfs cp command: For example: dbfs cp anovos/config/configs_income_azure.yaml dbfs:/FileStore/tables/configs_income_azure.yaml dbfs cp anovos/data/metric_dictionary.csv dbfs:/FileStore/tables/metric_dictionary.csv dbfs cp -r anovos/examples/data/income_dataset dbfs:/FileStore/tables/income_dataset For more information on the Databricks CLI, see the Databricks documentation .","title":"Copying files to DBFS using the CLI"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-13-create-a-workflow-script","text":"To launch the workflow on Azure Databricks, we need a single Python script as the entry point. Hence, we'll create a main.py script that invokes the Anovos' workflow runner: import sys from anovos import workflow workflow . run ( config_path = sys . argv [ 1 ], run_type = \"databricks\" ) Upload this script to DBFS as well using either of the methods described above. Again, you can place this file at a location of your choosing. In this tutorial, we have placed it at dbfs:/FileStore/tables/scripts/main.py .","title":"Step 1.3: Create a workflow script"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-14-configure-and-launch-an-anovos-workflow-as-a-databricks-job","text":"There are several types of jobs available on the Azure Databricks platform. For Anovos , the following job types are suitable choices: \"Python:\" The job runs from a single Python script. Anovos and the required Scala dependencies are installed through the respective package repositories. \"Spark Submit:\" The job is invoked through a bare spark-submit call. The installation of Anovos is handled by a cluster initialization script and the required Scala dependencies have to be provided as JAR files through DBFS. Note that there are several limitations for \"Spark Submit\" tasks: You can only run them on new clusters and autoscaling is not available. For more information, see the Databricks documentation on jobs . Unless you require the fine-grained control that this option offers with regard to cluster initialization and spark-submit options, we recommend to select \"Python\" as the job type.","title":"Step 1.4: Configure and launch an Anovos workflow as a Databricks job"},{"location":"using-anovos/setting-up/on_azure_databricks.html#141-using-the-python-job-type","text":"Once all files have been copied to DBFS, we can create an Azure Databricks job that starts a cluster and launches the Anovos workflow. Here's an example of a job configuration: You can see that we set the \"Type\" to \"Python\" and have provided the path of the main.py script. In the parameters section, we pass the DBFS path of the config file. The cluster configuration comprises settings for the Databricks Runtime, the number of workers, worker and driver types, as well as the cluster's scaling behavior. Here's an example of a cluster configuration for this tutorials: For more detailed information, refer to the Databricks documentation . To give the Databricks platform access to Anovos , click on \"Advanced options\" and select \"Add dependent libraries\" in the Job Configuration Window . If you chose the default way of obtaining Anovos directly from the Python Package Index, select \"PyPI\" as the \"Library Source\" and enter anovos as the \"Package\": \ud83d\udca1 In case you encounter the error \"ImportError: cannot import name 'soft_unicode' from 'markupsafe'\" while running the job, then you need to add markupsafe==2.0.1 as another dependency from PyPI. If you chose to provide your own wheel file, select \"Upload\" as the library source and follow the instructions. In addition to the Anovos wheel file, we need to provide the histogrammar package to Azure Databricks. Anovos uses this library internally to compute correlation matrices. Following the same procedure as for Anovos , you can add histogrammar as a dependent library. This time, we use \"Maven\" as the \"Library Source\". Then, select io.github.histogrammar:histogrammar-sparksql_2.12:1.0.20 and io.github.histogrammar:histogrammar_2.12:1.0.20 as the \"Coordinates\" (copy the coordinate one by one, paste in Coordinates box and then add it by clicking on Install button): (In case you're running Anovos on Spark 2.4.x, you need to add io.github.histogrammar:histogrammar-sparksql_2.11:1.0.20 and io.github.histogrammar:histogrammar_2.11:1.0.20 ) Once the job is configured, click \"Create\" to instantiate it. Then, you'll see the full task configuration: On the subsequent screen, click on \"Run now\" to launch the job: For more information on creating and maintaining jobs, see the Databricks documentation .","title":"1.4.1: Using the \"Python\" job type"},{"location":"using-anovos/setting-up/on_azure_databricks.html#142-using-the-spark-submit-job-type","text":"Anovos internally uses the histogrammar library to compute correlation matrices. Hence, we need to provide the package to Azure Databricks. As the \"Spark Submit\" job type requires any dependency to be available through DBFS, you first need to upload the histogrammar JAR files to DBFS. If you're using Spark 3.x, download the following files and upload them to DBFS: io.github.histogrammar.histogrammar_2.12-1.0.20.jar io.github.histogrammar.histogrammar-sparksql_2.12-1.0.20.jar If you're using Spark 2.x, download the following files and upload them to DBFS: io.github.histogrammar.histogrammar_2.11-1.0.20.jar io.github.histogrammar.histogrammar-sparksql_2.11-1.0.20.jar Once these files have been uploaded to DBFS, we can create an Azure Databricks job that starts a cluster and launches the Anovos workflow. Here's an example of a job configuration: You can see that we set the \"Type\" to \"Spark Submit\". In the parameters section, we pass the DBFS paths of the histogrammar JAR files, the sample class, the main.py script, and configuration file. For example: [ \"--jars\" , \"dbfs:/FileStore/tables/histogramm_jar/histogrammar_sparksql_2_12_1_0_20.jar,dbfs:/FileStore/tables/histogramm_jar/histogrammar_2_12_1_0_20.jar\" , \"--class\" , \"org.apache.spark.examples.SparkPi\" , \"/dbfs/FileStore/tables/scripts/main.py\" , \"/dbfs/FileStore/tables/configs_income_azure.yaml\" ] The cluster configuration comprises settings for the Databricks Runtime, the number of workers, worker and driver types. (Note that autoscaling is not available for \"Spark Submit\" jobs on Azure Databricks.) Here's an example of a cluster configuration for this tutorial: For more detailed information, refer to the Databricks documentation . To give the Databricks platform access to Anovos , you need to create a shell script that is executed upon cluster initialization and fetches the package from PyPI. The anovos_packages.sh script contains just one line: sudo pip3 install anovos Note that you should specify the version of Anovos if you're running production workloads to ensure reproducibility: sudo pip3 install anovos == 1 .0.1 Place this script on DBFS as well. During cluster configuration, click on \"Advanced options\" and specify the path to the script in the \"Init Script\" section: To enable logging, configure a DBFS path in the \"Log\" section: Once the job is configured, click \"Create\" to instantiate it. On the subsequent screen, click on \"Run now\" to launch the job: For more information on creating and maintaining jobs, see the Azure Databricks documentation .","title":"1.4.2: Using the \"Spark Submit\" job type"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-15-retrieve-the-output","text":"Once the job finishes successfully, it will show up under \"Completed runs\". The intermediate data and the report data are saved at the master_path and the final_report_path specified in the configs_income_azure.yaml file. In this tutorial, we have set these paths to dbfs:FileStore/tables/report_stats/ . To retrieve the HTML report and the report data, you can either go to this path in the UI and copy the files, or use the CLI to copy everything to your local machine: dbfs cp -r dbfs:/FileStore/tables/report_stats/ ./ For more details regarding accessing files on DBFS, see the instructions on uploading files to DBFS in Step 2.","title":"Step 1.5: Retrieve the output"},{"location":"using-anovos/setting-up/on_azure_databricks.html#2-anovos-on-azure-databricks-using-an-azure-blob-storage-container-mounted-to-dbfs","text":"","title":"2. Anovos on Azure Databricks Using an Azure Blob Storage Container Mounted to DBFS"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-21-installingdownloading-anovos","text":"This step is identical to Step 1.1: Installing Anovos on Azure Databricks .","title":"Step 2.1: Installing/Downloading Anovos"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-22-copy-the-dataset-to-an-azure-blob-storage-container","text":"To run an Anovos workflow, the dataset needs to be stored on an Azure Blob Storage container. You can either use the UI by clicking the upload button or the CLI to copy files from your local machine to an Azure Blob Storage container. For detailed instructions, see the respective subsections below. In this tutorial, we will use the \"income dataset\" provided with Anovos and an accompanying pre-defined workflow. You can obtain these files by cloning the Anovos GitHub repository: git clone https://github.com/anovos/anovos.git Afterwards, you'll find the dataset under examples/data/income_dataset \ud83d\udca1 Note that you need to use the dataset version and workflow configuration files from the same _Anovos version_ that you have set up in Step 2.1. Sometimes the version on PyPI that you obtain when running pip install anovos version is older than the latest development version on GitHub. The syntax to upload a file using the command line are as follows: azcopy copy source_file \"<storage-account-name>.blob.core.windows.net/<container-name>?<sas-token>\" To learn more about transferring data to Azure Blob Storage containers, please refer to the Azure documentation .","title":"Step 2.2: Copy the dataset to an Azure Blob Storage container"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-23-mount-an-azure-blob-storage-container-as-a-dbfs-path-in-azure-databricks","text":"To access files in an Azure Blob Storage container for running Anovos in Azure Databricks platform, you need to mount that container in the DBFS path. To mount the Azure Blob Storage container, execute the following commands in an Azure Databricks notebook: dbutils . fs . mount ( source = \"wasbs://<container-name>@<storage-account-name>.blob.core.windows.net\" , mount_point = \"/mnt/<mount-name>\" , extra_configs = { \"fs.azure.sas.<container-name>.<storage-account-name>.blob.core.windows.net\" : \"<sas-token>\" }) Here, - <storage-account-name> is the name of your Azure Blob Storage account - <container-name> is the name of a container in your Azure Blob Storage account - <mount-name> is the DBFS path where the Blob Storage container or a folder inside the container will be mounted to - <sas_token> is the SAS token for that storage account To learn more about mounting Azure Blob Storage containers in DBFS, please refer to the Azure Blob Storage documentation . \ud83d\udca1 Note that you only need to mount the container once. The container will remain mounted at the given mount point. To unmount a container, you can run dbutils.fs.unmount(\"/mnt/<mount-name>\") in an Azure Databricks notebook.","title":"Step 2.3: Mount an Azure Blob Storage Container as a DBFS path in Azure Databricks"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-24-update-the-workflow-configuration-for-all-input-and-output-paths-according-to-the-dbfs-mount-point","text":"Once mounting is completed, the data is present in DBFS at the path specified as the mount point. All operations performed by Anovos when running a workflow will result in changes in the data stored in the Azure Blob Storage container. The example configuration file we use in this tutorial can be found at config/configs_income_azure_blob_mount.yaml in the Anovos repository. It will need to be updated to reflect the path of the mount point set above. In order for Anovos to be able to find the input data and write the output to the correct location, update all paths to contain the path of the mount point: file_path : \"dbfs:/mnt/<mount-name>/...\" \ud83e\udd13 Example: read_dataset : file_path : \"dbfs:/mnt/anovos1/income_dataset/csv/\" file_type : csv Here, the mount points is dbfs:/mnt/anovos1 and the input dataset is stored in a folder called income_dataset/csv within the Azure Blob Storage container. To learn more about the Anovos workflow configuration file and specifying paths for input and output data, have a look at the Configuring Workloads page.","title":"Step 2.4: Update the workflow configuration for all input and output paths according to the DBFS mount point"},{"location":"using-anovos/setting-up/on_azure_databricks.html#step-25-copy-the-updated-configuration-file-from-the-local-machine-to-the-azure-blob-storage-container","text":"Once you have updated the configuration file, copy it to Azure Databricks using the same command that was used in Step 2.2 .","title":"Step 2.5: Copy the updated configuration file from the local machine to the Azure Blob Storage container"},{"location":"using-anovos/setting-up/on_azure_databricks.html#remaining-steps","text":"The remaining steps are the same as above, so please continue with Step 1.4","title":"Remaining Steps"},{"location":"using-anovos/setting-up/on_google_colab.html","text":"Setting Up Anovos on Google Colab Colab is an offer by Google Research that provides access to cloud-hosted Jupyter notebooks for collaborating on and sharing data science work. Colab offers substantial compute resources even in its free tier and is integrated with Google Drive , making it an excellent place to explore libraries like Anovos without setting up anything on your local machine. If you're not yet familiar with Google Colab, the following selection of introductory tutorials are an excellent starting point to familiarize yourself with this platform: LeanIn Women In Tech India: Google Colab \u2014 The Beginner\u2019s Guide GeeksForGeeks: How to use Google Colab DataCamp: Google Colab Tutorial for Data Scientists Step-by-step Instructions for Using Anovos on Google Colab The following four steps will guide you through the entire setup of Anovos on Google Colab. The instructions assume that you're starting out with a fresh, empty notebook environment. Step 1: Installing Spark dependencies Anovos builds on Apache Spark , which is not available by default in Google Colab. Hence, before we can start working Anovos , we need to install Spark and set up a Spark environment. Since Spark is a Java application, we start out by installing the Java Development Kit: !apt-get install openjdk-8-jdk-headless -qq > /dev/null Then, we can download Spark: !wget https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz \ud83d\udca1 In this tutorial, we use Java 8 and Spark 2.4.8. You can use more recent versions as well. See the list of currently supported versions to learn about available options. Next, unzip the downloaded Spark archive to the current folder: !tar xf spark-2.4.8-bin-hadoop2.7.tgz Now we'll let the Colab notebook know where Java and Spark can be found by setting the corresponding environment variables: import os os . environ [ \"JAVA_HOME\" ] = \"/usr/lib/jvm/java-8-openjdk-amd64\" os . environ [ \"SPARK_HOME\" ] = \"/content/spark-2.4.8-bin-hadoop2.7\" To access Spark through Python, we need the pyspark library as well as the findspark utility: !pip install findspark pyspark == 2 .4.8 \ud83d\udca1 Make sure that the version of pyspark matches the Spark versions you downloaded. Step 2: Installing Anovos and its dependencies Clone the Anovos GitHub repository to Google Colab: !git clone --branch v1.0.1 https://github.com/anovos/anovos.git \ud83d\udca1 Using the --branch flag allows you to select the desired release of Anovos. If you omit the flag, you will get the latest development version of Anovos, which might not be fully functional or exhibit unexpected behavior. After cloning, let's enter the newly created Anovos directory: cd anovos As indicated by the output shown, Anovos was placed in the folder /content/anovos , which you can also access through the sidebar: The next step is to build Anovos : !make clean build As the final step before we can start working with Anovos , we need to install the required Python dependencies: !pip install -r requirements.txt Step 3: Configuring an Anovos workflow Anovos workflows are configured through a YAML configuration file. To learn more, have a look at the exhaustive Configuring Workflows documentation. But don't worry: We'll guide you through the necessary steps! First, open the file viewer in the sidebar and download the configs.yaml file from the dist folder by right-clicking on the file and selecting Download : After downloading the configs.yaml file, you can now adapt the workflow it describes to your needs. For example, you can define which columns from the input dataset are used in the workflow. To try it yourself, find the delete_column configuration in the input_dataset block and add the column workclass to the list of columns to be deleted: input_dataset : ... delete_column : [ 'logfnl' , 'workclass' ] ... You can learn more about this and all other configuration options in the Configuring Workflows documentation. Each configuration block is associated with one of the various Anovos modules and functions . Once you adapted the configs.yaml file, you can upload it again by right-clicking on the dist folder and selecting Upload : Step 4: Trigger a workflow run Once the workflow configuration has been uploaded, you can run your workflow. Anovos workflows are triggered by executing the spark-submit.sh file that you'll find in the dist folder. This script contains the configuration for the Spark executor. To change the number of executors, the executor's memory, driver memory, and other parameters, you can edit this file. For example, in case of a very large dataset of several GB in size, you might want to allocate more memory to the Anovos workflow. Let's go ahead and change the executor memory from the pre-defined 20g to 32g : spark-submit \\ ... --executor-memory 32g \\ ... To make this or any other change, you need to download and upload the spark-submit.sh file similarly to the configs.yaml file as described in the previous section. Once the adapted spark-submit.sh has been uploaded, we can trigger the Anovos workflow run by entering the dist directory and running spark-submit.sh : cd dist !nohup ./spark-submit.sh > run.txt & The nohup command together with the & at the end of line ensures that the workflow is executed in the background, allowing us to continue working in the Colab notebook. To see what your workflow is doing, have a look at run.txt , where all logs are collected: !tail -f run.txt Once the run completes, the reports generated by Anovos and all intermediate outputs are stored at the specified path. The intermediate data and the report data are saved at the master_path and the final_report_path as specified by the user inside the configs.yaml file. By default, these are set to report_stats and you should find all output files in this folder: !cd report_stats !ls -l To view the HTML report, you'll have to download the basic_report.html file to your local machine, using the same steps you took to download the configs.yaml and spark-submit.sh files. What's next? In this tutorial, you've learned the basics of running Anovos workflows on Google Colab. To learn all about the different modules and functions of Anovos , have a look at the API documentation . The Configuring Workflows documentation contains a complete list of all possible configuration options.","title":"On Google Colab"},{"location":"using-anovos/setting-up/on_google_colab.html#setting-up-anovos-on-google-colab","text":"Colab is an offer by Google Research that provides access to cloud-hosted Jupyter notebooks for collaborating on and sharing data science work. Colab offers substantial compute resources even in its free tier and is integrated with Google Drive , making it an excellent place to explore libraries like Anovos without setting up anything on your local machine. If you're not yet familiar with Google Colab, the following selection of introductory tutorials are an excellent starting point to familiarize yourself with this platform: LeanIn Women In Tech India: Google Colab \u2014 The Beginner\u2019s Guide GeeksForGeeks: How to use Google Colab DataCamp: Google Colab Tutorial for Data Scientists","title":"Setting Up Anovos on Google Colab"},{"location":"using-anovos/setting-up/on_google_colab.html#step-by-step-instructions-for-using-anovos-on-google-colab","text":"The following four steps will guide you through the entire setup of Anovos on Google Colab. The instructions assume that you're starting out with a fresh, empty notebook environment.","title":"Step-by-step Instructions for Using Anovos on Google Colab"},{"location":"using-anovos/setting-up/on_google_colab.html#step-1-installing-spark-dependencies","text":"Anovos builds on Apache Spark , which is not available by default in Google Colab. Hence, before we can start working Anovos , we need to install Spark and set up a Spark environment. Since Spark is a Java application, we start out by installing the Java Development Kit: !apt-get install openjdk-8-jdk-headless -qq > /dev/null Then, we can download Spark: !wget https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz \ud83d\udca1 In this tutorial, we use Java 8 and Spark 2.4.8. You can use more recent versions as well. See the list of currently supported versions to learn about available options. Next, unzip the downloaded Spark archive to the current folder: !tar xf spark-2.4.8-bin-hadoop2.7.tgz Now we'll let the Colab notebook know where Java and Spark can be found by setting the corresponding environment variables: import os os . environ [ \"JAVA_HOME\" ] = \"/usr/lib/jvm/java-8-openjdk-amd64\" os . environ [ \"SPARK_HOME\" ] = \"/content/spark-2.4.8-bin-hadoop2.7\" To access Spark through Python, we need the pyspark library as well as the findspark utility: !pip install findspark pyspark == 2 .4.8 \ud83d\udca1 Make sure that the version of pyspark matches the Spark versions you downloaded.","title":"Step 1: Installing Spark dependencies"},{"location":"using-anovos/setting-up/on_google_colab.html#step-2-installing-anovos-and-its-dependencies","text":"Clone the Anovos GitHub repository to Google Colab: !git clone --branch v1.0.1 https://github.com/anovos/anovos.git \ud83d\udca1 Using the --branch flag allows you to select the desired release of Anovos. If you omit the flag, you will get the latest development version of Anovos, which might not be fully functional or exhibit unexpected behavior. After cloning, let's enter the newly created Anovos directory: cd anovos As indicated by the output shown, Anovos was placed in the folder /content/anovos , which you can also access through the sidebar: The next step is to build Anovos : !make clean build As the final step before we can start working with Anovos , we need to install the required Python dependencies: !pip install -r requirements.txt","title":"Step 2: Installing Anovos and its dependencies"},{"location":"using-anovos/setting-up/on_google_colab.html#step-3-configuring-an-anovos-workflow","text":"Anovos workflows are configured through a YAML configuration file. To learn more, have a look at the exhaustive Configuring Workflows documentation. But don't worry: We'll guide you through the necessary steps! First, open the file viewer in the sidebar and download the configs.yaml file from the dist folder by right-clicking on the file and selecting Download : After downloading the configs.yaml file, you can now adapt the workflow it describes to your needs. For example, you can define which columns from the input dataset are used in the workflow. To try it yourself, find the delete_column configuration in the input_dataset block and add the column workclass to the list of columns to be deleted: input_dataset : ... delete_column : [ 'logfnl' , 'workclass' ] ... You can learn more about this and all other configuration options in the Configuring Workflows documentation. Each configuration block is associated with one of the various Anovos modules and functions . Once you adapted the configs.yaml file, you can upload it again by right-clicking on the dist folder and selecting Upload :","title":"Step 3: Configuring an Anovos workflow"},{"location":"using-anovos/setting-up/on_google_colab.html#step-4-trigger-a-workflow-run","text":"Once the workflow configuration has been uploaded, you can run your workflow. Anovos workflows are triggered by executing the spark-submit.sh file that you'll find in the dist folder. This script contains the configuration for the Spark executor. To change the number of executors, the executor's memory, driver memory, and other parameters, you can edit this file. For example, in case of a very large dataset of several GB in size, you might want to allocate more memory to the Anovos workflow. Let's go ahead and change the executor memory from the pre-defined 20g to 32g : spark-submit \\ ... --executor-memory 32g \\ ... To make this or any other change, you need to download and upload the spark-submit.sh file similarly to the configs.yaml file as described in the previous section. Once the adapted spark-submit.sh has been uploaded, we can trigger the Anovos workflow run by entering the dist directory and running spark-submit.sh : cd dist !nohup ./spark-submit.sh > run.txt & The nohup command together with the & at the end of line ensures that the workflow is executed in the background, allowing us to continue working in the Colab notebook. To see what your workflow is doing, have a look at run.txt , where all logs are collected: !tail -f run.txt Once the run completes, the reports generated by Anovos and all intermediate outputs are stored at the specified path. The intermediate data and the report data are saved at the master_path and the final_report_path as specified by the user inside the configs.yaml file. By default, these are set to report_stats and you should find all output files in this folder: !cd report_stats !ls -l To view the HTML report, you'll have to download the basic_report.html file to your local machine, using the same steps you took to download the configs.yaml and spark-submit.sh files.","title":"Step 4: Trigger a workflow run"},{"location":"using-anovos/setting-up/on_google_colab.html#whats-next","text":"In this tutorial, you've learned the basics of running Anovos workflows on Google Colab. To learn all about the different modules and functions of Anovos , have a look at the API documentation . The Configuring Workflows documentation contains a complete list of all possible configuration options.","title":"What's next?"},{"location":"using-anovos/setting-up/spark_cluster.html","text":"Setting up to run Anovos workloads on your Spark cluster COMING SOON Let us know if you're interested in learning more!","title":"On a Spark cluster"},{"location":"using-anovos/setting-up/spark_cluster.html#setting-up-to-run-anovos-workloads-on-your-spark-cluster","text":"COMING SOON Let us know if you're interested in learning more!","title":"Setting up to run Anovos workloads on your Spark cluster"}]}